<!DOCTYPE html>
<html lang="ru-RU">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Statistical Learning | ILyaKlishin</title>
    <meta name="description" content="Сайт Клишина Ильи Николаевича для открытой информации и лушего Интернета html 5.0">
    <link rel="stylesheet" href="/assets/style.abb680c9.css">
    <link rel="modulepreload" href="/assets/Home.13b0f7a8.js">
    <link rel="modulepreload" href="/assets/app.e2261b09.js">
    <link rel="modulepreload" href="/assets/blog_The-Field-Guide-to-Data-Science.md.e31f9edd.lean.js">
    <link rel="modulepreload" href="/assets/app.e2261b09.js">
    <meta name="twitter:title" content="Statistical Learning | ILyaKlishin">
    <meta property="og:title" content="Statistical Learning | ILyaKlishin">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme"><header class="nav-bar" data-v-7161a24b><div class="sidebar-button" data-v-7161a24b><svg class="icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z" class></path></svg></div><a class="nav-bar-title" href="/" aria-label="ILyaKlishin, back to home" data-v-7161a24b data-v-4a583abe><!----> ILyaKlishin</a><div class="flex-grow" data-v-7161a24b></div><div class="nav" data-v-7161a24b><nav class="nav-links" data-v-7161a24b data-v-15acbf05><!--[--><div class="item" data-v-15acbf05><div class="nav-link" data-v-15acbf05 data-v-641633f9><a class="item" href="/" data-v-641633f9>THINK EXAMINING <!----></a></div></div><div class="item" data-v-15acbf05><div class="nav-link" data-v-15acbf05 data-v-641633f9><a class="item" href="/pip/" data-v-641633f9>Продвижение Интеллектуальной собственности <!----></a></div></div><!--]--><!----><div class="item" data-v-15acbf05><div class="nav-link" data-v-15acbf05 data-v-641633f9><a class="item isExternal" href="https://github.com/ink-kin/vitepress-starter" target="_blank" rel="noopener noreferrer" data-v-641633f9>GitHub <svg class="icon outbound" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15" data-v-641633f9><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div></div></nav></div><!--[--><!--]--></header><aside class="sidebar" data-v-6b49cdcd><nav class="nav-links nav" data-v-6b49cdcd data-v-15acbf05><!--[--><div class="item" data-v-15acbf05><div class="nav-link" data-v-15acbf05 data-v-641633f9><a class="item" href="/" data-v-641633f9>THINK EXAMINING <!----></a></div></div><div class="item" data-v-15acbf05><div class="nav-link" data-v-15acbf05 data-v-641633f9><a class="item" href="/pip/" data-v-641633f9>Продвижение Интеллектуальной собственности <!----></a></div></div><!--]--><!----><div class="item" data-v-15acbf05><div class="nav-link" data-v-15acbf05 data-v-641633f9><a class="item isExternal" href="https://github.com/ink-kin/vitepress-starter" target="_blank" rel="noopener noreferrer" data-v-641633f9>GitHub <svg class="icon outbound" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15" data-v-641633f9><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div></div></nav><!--[--><!--]--><ul class="sidebar-links" data-v-6b49cdcd><!--[--><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/2020-11-11-introdiction">Введение</a><ul class="sidebar-links"><li class="sidebar-link"><a class="sidebar-link-item" href="/">Зачем я публикую всё это здесь?</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/about">Что Я ищу?</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/about_my_edi">Что Я пишу?</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/configuration">Что предлагаю</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/todone">Вместо резюме</a><!----></li></ul></li><li class="sidebar-link"><p class="sidebar-link-item">Продвижение Интеллектуальной собственности</p><ul class="sidebar-links"><li class="sidebar-link"><a class="sidebar-link-item" href="/pip/marketing">Маркетинг/Реклама</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/sales/">Продажи</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/people/">Кадры/Таланты</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/buh/">Бух. и Финансы</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/law/">Закон</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/innovate/">Инноваци</a><!----></li></ul></li><li class="sidebar-link"><p class="sidebar-link-item">Блог</p><ul class="sidebar-links"><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/2020-11-11-introdiction">Вместо вступления</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/analist">Что такое правильно оставленная задача</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/paketnoe-izmenenie-izobrazhenij">images resize</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/xmlstar">XMLSTARLET USER`S GUIDE</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/convert_--help">ImageMagick convert --help</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/clamav">ClamAV</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/chrome_answer">Chrome быстрее, ещё и ещё быстрее</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/why-vuejs-need">Зачем нужен Vue.js</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/what-is-kubernetes">Kubernetes</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/web-servers">Список веб-серверов</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/vybirayem-generator-staticheskikh-saytov">Обзор генераторов статических сайтов</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/vidy-elektronnoy-podpisi">Виды электронной подписи</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/top-issue-support-and-bug-tracking-tools">Жизнь полна ошибок</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/top-9-mongodb-tools">MongoDB GUI Tool</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/time">Cмысл фильма Время</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item active" href="/blog/The-Field-Guide-to-Data-Science">THE FIELD GUIDE to DATA SCIENCE</a><ul class="sidebar-links"><li class="sidebar-link"><a class="sidebar-link-item" href="#foreword">FOREWORD</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#the-story-of-the">THE STORY of THE</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#field">FIELD</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#guide">GUIDE</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#we-are-all">WE ARE ALL</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#authors-of-this">AUTHORS of THIS</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#story">STORY</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#the-outline">THE OUTLINE</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#of-our-story">of OUR STORY</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#meet-your-guides">MEET your GUIDES</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#the-short">The SHORT</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#version">VERSION</a><ul class="sidebar-links"><li class="sidebar-link"><a class="sidebar-link-item" href="#what-do-we-mean-by">What do We Mean by</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#data-science">Data Science?</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#how-does-data-science">How does Data Science</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#actually-work">Actually Work?</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#what-does-it-take-to-create">What does it Take to Create</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#a-data-science-capability">a Data Science Capability?</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#guiding-principles">Guiding Principles</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#e-importance-of-reason">#e Importance of Reason</a><!----></li></ul></li><li class="sidebar-link"><a class="sidebar-link-item" href="#the-dangers-of-rejection">The Dangers of Rejection</a><ul class="sidebar-links"><li class="sidebar-link"><a class="sidebar-link-item" href="#component-parts-of">Component Parts of</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#data-science-1">Data Science</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#fractal-analytic-model">Fractal Analytic Model</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#e-analytic">#e Analytic</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#selection-process">Selection Process</a><!----></li></ul></li><li class="sidebar-link"><a class="sidebar-link-item" href="#identifying-spoofed-domains">Identifying Spoofed Domains</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#_4">4</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#_3">3</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#_2">2</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#_1">1</a><ul class="sidebar-links"><li class="sidebar-link"><a class="sidebar-link-item" href="#guide-to-analytic-selection">Guide to Analytic Selection</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#detailed-table-of-analytics">Detailed Table of Analytics</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#feature-engineering">Feature Engineering</a><!----></li></ul></li><li class="sidebar-link"><a class="sidebar-link-item" href="#chemoinformatic-search">Chemoinformatic Search</a><ul class="sidebar-links"><li class="sidebar-link"><a class="sidebar-link-item" href="#feature-selection">Feature Selection</a><!----></li></ul></li><li class="sidebar-link"><a class="sidebar-link-item" href="#cancer-cell-classification">Cancer Cell Classification</a><ul class="sidebar-links"><li class="sidebar-link"><a class="sidebar-link-item" href="#data-veracity">Data Veracity</a><!----></li></ul></li><li class="sidebar-link"><a class="sidebar-link-item" href="#time-series-modeling">Time Series Modeling</a><ul class="sidebar-links"><li class="sidebar-link"><a class="sidebar-link-item" href="#application-of">Application of</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#domain-knowledge">Domain Knowledge</a><!----></li></ul></li><li class="sidebar-link"><a class="sidebar-link-item" href="#motor-vehicle-theft">Motor Vehicle Theft</a><ul class="sidebar-links"><li class="sidebar-link"><a class="sidebar-link-item" href="#e-curse-of">!e Curse of</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#dimensionality">Dimensionality</a><!----></li></ul></li><li class="sidebar-link"><a class="sidebar-link-item" href="#baking-the-cake">Baking the Cake</a><ul class="sidebar-links"><li class="sidebar-link"><a class="sidebar-link-item" href="#model-validation">Model Validation</a><!----></li></ul></li><li class="sidebar-link"><a class="sidebar-link-item" href="#consumer-behavior">Consumer Behavior</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#analysis-from-a">Analysis from a</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#multi-terabyte">Multi-Terabyte</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#dataset">Dataset</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#strategic-insights">Strategic Insights</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#within-terabytes-of">within Terabytes of</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#passenger-data">Passenger Data</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#savings-through">Savings Through</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#better-manufacturing">Better Manufacturing</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#realizing-higher">Realizing Higher</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#returns-through">Returns Through</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#predictive-analytics">Predictive Analytics</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#parting">PARTING</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#thoughts">THOUGHTS</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#references">REFERENCES</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#about">About</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#booz-allen">BOOZ ALLEN</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="#hamilton">HAMILTON</a><!----></li></ul></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/strugackiy_arkadiy-za_milliard_let_do_konca_sveta">За миллиард лет до конца света</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/StatLearnSparsity">Statistical Learning</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/sqlite">SQLite</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/shtat">Штатное расписание</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/shaping_the_fourth_industrial_revolution">Технологии Четвертой промышленной революции</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/shalyapin_maska-i-dusha">МАСКА И ДУША</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/service">Простое обяснение</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/scenic_speech">Сценическая речь: Методическое пособие</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/pyton-tools">Python коротко пометки</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/philosophy">Время. Отношения. Выбор.</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/osnovy_matematicheskogo_modelirovaniya">Основы математического моделирования</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/o-nakonets-nastal-tot-chas">О, наконец настал тот час</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/obama_budget_proposal_2012">Коротко об Обама Бюджет</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/NY2020">От ажиотажа к зрелости</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/NanoBlogger">NanoBlogger</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/mvp">Минимально жизнеспособный продукт</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/mongodb_7_competitive_advantages">MongoDB Atlas</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/methods-of-optimal-solutions">МЕТОДЫ ОПТИМАЛЬНЫХ РЕШЕНИЙ</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/man-joe">Joe`s Own Editor</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/luchshih-distributivov-linux-dlia-usilennoi-konfidencialnosti-i-bezopasnosti">Linux для приватности</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/linux-rmlint">Линус Торвальдс. Человек</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/linux-command-basics-7-commands-process-management">Управление процессами</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/linux">Linux</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/lessons">Авторский курс на реальном опыте</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/jupyter">Начало работы с Jupyter Notebooks</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/jamstack">Jamstack</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/intellectual-values">Ценности сообщества</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/How-Do-Engineering-Scientists-Think">Моделирование</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/gost-34-comments">Документирование по ГОСТ 34</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/gost-34">ГОСТ 34</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/git-backuper">GIT для резервного копирования</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/do-five-things">Люди часто 5 поступков</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/data-analist">Аналитика данных</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/chetvertaya_promyshlennaya_revolyuciya_2016">Великая перезагрузка</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/chetvertaya_promyshlennaya_revolyuciya">Четвертая промышленная революция</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/business-analysis">Словарь BI-СИСТЕМ</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/brd-mrd-prd-fsd-psd-srs">BRD, MRD и PRD</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/bpm_soft">BPM Софт</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/bpm_book_cbok">Управление бизнес-процессами</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/bomb">Ты и атомная бомба</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/BlueSpice_XWiki">BlueSpice, XWiki и DokuWiki</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/best-practices">Методы и правила разработки ПО</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/babok">Business Analysis Body of Knowledge</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/avoid-burnout-live-happy">Как избежать выгорания?</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/asciinema">Проект asciinema</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/arhivatsiya-v-linux-tar">Архивирование</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/archi_modelling_tool">Archi</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/archimate_courseware">ArchiMate</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/analist-todo">Услуги аналитика</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/Anaconda-SODS-Report-2020-Final">Anaconda SODS Report 2020</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/america-smm">Cила Америки в противостоянии с Россией подорвана</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/agile">Agile</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/12-best-charting-libraries-for-web-developers">Маркетинг/Реклама</a><!----></li></ul></li><li class="sidebar-link"><p class="sidebar-link-item">Книги</p><ul class="sidebar-links"><li class="sidebar-link"><a class="sidebar-link-item" href="/books/marketing">Книги</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/books/uml">Проектирование на UML</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/kotler_f_marketing_ot_a_do_ya_80_k">Маркетинг от А до Я: 80 концепций</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/22857009">Психодиагностика</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/7459819">Удержать клиентов</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/6158845">Фиолетовая корова</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/5814905">Кремлевская школа переговоров</a><!----></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/2063485">Маркетинг для топ–менеджеров</a><!----></li></ul></li><li class="sidebar-link"><a class="sidebar-link-item" href="/blog/privacy-policy">Политика конфиденциальности</a><!----></li><!--]--></ul><!--[--><!--]--></aside><!-- TODO: make this button accessible --><div class="sidebar-mask"></div><main class="page" data-v-7eddb2c4><div class="container" data-v-7eddb2c4><!--[--><!--]--><div style="position:relative;" class="content" data-v-7eddb2c4><div><div class="language-"><pre><code>Every aspect of our lives, from life-saving disease
treatments, to national security, to economic stability
and even the convenience of selecting a restaurant,
can be improved by creating better data analytics
through Data Science.
</code></pre></div><p>THE FIELD GUIDE to DATA SCIENCE</p><div class="language-"><pre><code>© COPYRIGHT 2013 BOOZ ALLEN HAMILTON INC. ALL RIGHTS RESERVED.
</code></pre></div><h2 id="foreword"><a class="header-anchor" href="#foreword" aria-hidden="true">#</a> FOREWORD</h2><div class="language-"><pre><code>Every aspect of our lives, from life-saving disease
treatments, to national security, to economic stability
and even the convenience of selecting a restaurant,
can be improved by creating better data analytics
through Data Science.
</code></pre></div><div class="language-"><pre><code>We live in a world of incredible
beauty and complexity. A world
increasingly measured, mapped,
and recorded into digital bits for
eternity. Our human existence
is pouring into the digital realm
faster than ever. From global
business operations to simple
expressions of love – an essential
part of humanity now exists in
the digital world.
</code></pre></div><div class="language-"><pre><code>Data is the byproduct of our
new digital existence. Recorded
bits of data from mundane
tra!c cameras to telescopes
peering into the depths of
space are propelling us into
the greatest age of discovery
our species has ever known.
</code></pre></div><div class="language-"><pre><code>As we move from isolation into
our ever-connected and recorded
future, data is becoming the
new currency and a vital natural
resource. &quot;e power, importance,
</code></pre></div><div class="language-"><pre><code>and responsibility such incredible
data stewardship will demand of
us in the coming decades is hard
to imagine – but we often fail to
fully appreciate the insights data
can provide us today. Businesses
that do not rise to the occasion
and garner insights from this new
resource are destined for failure.
</code></pre></div><div class="language-"><pre><code>An essential part of human
nature is our insatiable curiosity
and the need to #nd answers to
our hardest problems. Today, the
emerging #eld of Data Science is
an auspicious and profound new
way of applying our curiosity
and technical tradecraft to create
value from data that solves our
hardest problems. Leaps in
human imagination, vast amounts
of data on hundreds of topics,
and humble algorithms can be
combined to create a radical new
way of thinking about data. Our
future is inextricably tied to data.
</code></pre></div><div class="language-"><pre><code>We want to share our passion for Data Science and start a
conversation with you. &quot;is is a journey worth taking.
</code></pre></div><h1 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> !!</h1><h4 id="everyone-you"><a class="header-anchor" href="#everyone-you" aria-hidden="true">#</a> Everyone you</h4><h4 id="will-ever-meet"><a class="header-anchor" href="#will-ever-meet" aria-hidden="true">#</a> will ever meet</h4><h4 id="knows-something"><a class="header-anchor" href="#knows-something" aria-hidden="true">#</a> knows something</h4><h4 id="you-don’t"><a class="header-anchor" href="#you-don’t" aria-hidden="true">#</a> you don’t.</h4><div class="language-"><pre><code>[1]
</code></pre></div><h2 id="the-story-of-the"><a class="header-anchor" href="#the-story-of-the" aria-hidden="true">#</a> THE STORY of THE</h2><h2 id="field"><a class="header-anchor" href="#field" aria-hidden="true">#</a> FIELD</h2><h2 id="guide"><a class="header-anchor" href="#guide" aria-hidden="true">#</a> GUIDE</h2><p>While there are countless industry and academic publications describing <em>what</em> Data Science is and <em>why</em> we should care, little information is available to explain how to make use of data as a resource. At Booz Allen, we built an industry-leading team of Data Scientists. Over the course of hundreds of analytic challenges for dozens of clients, we’ve unraveled the DNA of Data Science. We mapped the Data Science DNA to unravel the <em>what</em> , the <em>why</em> , the <em>who</em> and the <em>how</em>.</p><p>Many people have put forth their thoughts on single aspects of Data Science. We believe we can o!er a broad perspective on the conceptual models, tradecraft, processes and culture of Data Science. Companies with strong Data Science teams often focus on a single class of problems – graph algorithms for social network analysis and recommender models for online shopping are two notable examples. Booz Allen is di!erent. In our role as consultants, we support a diverse set of clients across a variety of domains. &quot;is allows us to uniquely understand the DNA of Data Science. Our goal in creating <em>!e Field Guide to Data Science</em> is to capture what we have learned and to share it broadly. We want this e!ort to help drive forward the science and art of Data Science.</p><p>&quot;is #eld guide came from the passion our team feels for its work. It is not a textbook nor is it a super#cial treatment. Senior leaders will walk away with a deeper understanding of the concepts at the heart of Data Science. Practitioners will add to their toolbox. We hope everyone will enjoy the journey.</p><div class="language-"><pre><code>» Why Data Science DNA?
</code></pre></div><div class="language-"><pre><code>We view Data Science as having
DNA-like characteristics. Much like
DNA, Data Science is composed
of basic building blocks that are
woven into a thing of great beauty
and complexity. &quot;e building blocks
create the blueprint, but successful
replication also requires carefully
balanced processes and the right
set of environmental conditions.
In the end, every instance may look
super#cially di!erent, but the raw
materials remain the same.
</code></pre></div><h1 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> !!</h1><h2 id="we-are-all"><a class="header-anchor" href="#we-are-all" aria-hidden="true">#</a> WE ARE ALL</h2><h2 id="authors-of-this"><a class="header-anchor" href="#authors-of-this" aria-hidden="true">#</a> AUTHORS of THIS</h2><h2 id="story"><a class="header-anchor" href="#story" aria-hidden="true">#</a> STORY</h2><div class="language-"><pre><code>We recognize that Data Science is a team sport. !e Field Guide
to Data Science provides Booz Allen’s perspective on the complex
and sometimes mysterious !eld of Data Science. We cannot
capture all that is Data Science. Nor can we keep up - the pace at
which this !eld progresses outdates work as fast as it is produced.
As a result, we have opened this !eld guide to the world as a
living document to bend and grow with technology, expertise, and
evolving techniques. If you !nd the guide to be useful, neat, or even
lacking, then we encourage you to add your expertise, including:
</code></pre></div><div class="language-"><pre><code>› Case studies from which you have learned
› Citations for journal articles or papers that inspire you
› Algorithms and techniques that you love
› Your thoughts and comments on other people’s additions
</code></pre></div><div class="language-"><pre><code>Email us your ideas and perspectives at data_science@bah.com
or submit them via a pull request on the Github repository.
</code></pre></div><div class="language-"><pre><code>Join our conversation and take the journey with us. Tell us and
the world what you know. Become an author of this story.
</code></pre></div><h1 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> !!</h1><h2 id="the-outline"><a class="header-anchor" href="#the-outline" aria-hidden="true">#</a> THE OUTLINE</h2><h2 id="of-our-story"><a class="header-anchor" href="#of-our-story" aria-hidden="true">#</a> of OUR STORY</h2><p>(^10) <strong><em>!!</em></strong> <strong>Meet Your Guides</strong> (^13) <strong><em>!!</em></strong> <strong>!e Short Version –</strong> !e Core Concepts of Data Science (^14) <strong><em>!!</em></strong> <strong>Start Here for the Basics –</strong>^ An Introduction to Data Science What Do We Mean by Data Science? How Does Data Science Actually Work? What Does It Take to Create a Data Science Capability? (^40) <strong><em>!!</em></strong> <strong>Ta k e o &quot; t h e Tr a i n i n g W h e e l s –</strong>^ !e Practitioner’s Guide to Data Science Guiding Principles !e Importance of Reason Component Parts of Data Science Fractal Analytic Model !e Analytic Selection Process Guide to Analytic Selection Detailed Table of Analytics (^76) <strong><em>!!</em></strong> <strong>Life in the Trenches –</strong> Navigating Neck Deep in Data Feature Engineering Feature Selection Data Veracity Application of Domain Knowledge !e Curse of Dimensionality Model Validation (^90) <strong><em>!!</em></strong> <strong>Putting it all Together –</strong>^ Our Case Studies Consumer Behavior Analysis from a Multi-Terabyte Data Set Strategic Insights with Terabytes of Passenger Data Savings !rough Better Manufacturing Realizing Higher Returns !rough Predictive Analytics (^100) <strong><em>!!</em></strong> <strong>Closing Time</strong>^ Parting !oughts References About Booz Allen Hamilton</p><h1 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> !!</h1><h2 id="meet-your-guides"><a class="header-anchor" href="#meet-your-guides" aria-hidden="true">#</a> MEET your GUIDES</h2><div class="language-"><pre><code>Mark Herman
(@cloudEBITDA)
</code></pre></div><p>End every analysis with ... ‘and therefore’</p><div class="language-"><pre><code>Josh Sullivan
(@joshdsullivan)
</code></pre></div><p>Leading our Data Science team shows me every day the incredible power of discovery and human curiosity. Don’t be afraid to blend art and science to advance your own view of data analytics – it can be a powerful mixture.</p><div class="language-"><pre><code>Stephanie Rivera
(@boozallen)
</code></pre></div><div class="language-"><pre><code>I treat Data Science like I do rock
climbing: awesome dedication
leads to incremental improvement.
Persistence leads to the top.
</code></pre></div><div class="language-"><pre><code>Peter Guerra
(@petrguerra)
</code></pre></div><div class="language-"><pre><code>Data Science is the most fascinating
blend of art and math and code
and sweat and tears. It can take
you to the highest heights and the
lowest depths in an instant, but it
is the only way we will be able to
understand and describe the why.
</code></pre></div><div class="language-"><pre><code>Steven Mills
(@stevndmills)
</code></pre></div><div class="language-"><pre><code>Data Science, like life, is not linear.
It’s complex, intertwined, and can be
beautiful. Success requires the support
of your friends and colleagues.
</code></pre></div><div class="language-"><pre><code>Alex Cosmas
(@boozallen)
</code></pre></div><div class="language-"><pre><code>Data miners produce bottle cap facts
(e.g., animals that lay eggs don’t have
belly buttons). Data Scientists produce
insights - they require the intellectual
curiosity to ask “why” or “so what”?
</code></pre></div><h1 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> !!</h1><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Drew Farris
(@drewfarris)
</code></pre></div><p>Don’t forget to play. Play with tools, play with data, and play with algorithms. You just might discover something that will help you solve that next nagging problem.</p><div class="language-"><pre><code>Brian Keller
(@boozallen)
</code></pre></div><p>Grit will get you farther than talent.</p><div class="language-"><pre><code>Ed Kohlwey
(@ekohlwey)
</code></pre></div><div class="language-"><pre><code>Data Science is about formally
analyzing everything around you
and becoming data driven.
</code></pre></div><div class="language-"><pre><code>Armen Kherlopian
(@akherlopian)
</code></pre></div><div class="language-"><pre><code>A Data Scientist must
continuously seek truth in spite
of ambiguity; therein rests the
basis of rigor and insight.
</code></pre></div><div class="language-"><pre><code>Paul Yacci
(@paulyacci)
</code></pre></div><div class="language-"><pre><code>In the jungle of data, don’t
miss the forest for the trees,
or the trees for the forest.
</code></pre></div><div class="language-"><pre><code>Michael Kim
(@boozallen)
</code></pre></div><div class="language-"><pre><code>Data science is both an art
and science.
</code></pre></div><div class="language-"><pre><code>We would like to thank the following people for their
contributions and edits:
Tim Andrews, Mike Delurey, Greg Dupier, Jason Escaravage,
Christine Fantaskey, Juergen Klenk, and Mark Rockley.
</code></pre></div><div class="language-"><pre><code>Meet Your Guides 11
</code></pre></div><h2 id="the-short"><a class="header-anchor" href="#the-short" aria-hidden="true">#</a> The SHORT</h2><h2 id="version"><a class="header-anchor" href="#version" aria-hidden="true">#</a> VERSION</h2><div class="language-"><pre><code>› Data Science is the art of turning data into actions.
It’s all about the tradecraft. Tradecraft is the process, tools and
technologies for humans and computers to work together to
transform data into insights.
</code></pre></div><div class="language-"><pre><code>› Data Science tradecraft creates data products.
Data products provide actionable information without exposing
decision makers to the underlying data or analytics (e.g., buy/sell
strategies for !nancial instruments, a set of actions to improve
product yield, or steps to improve product marketing).
</code></pre></div><div class="language-"><pre><code>› Data Science supports and encourages shifting between
deductive (hypothesis-based) and inductive (pattern-
based) reasoning.
&quot;is is a fundamental change from traditional analysis approaches.
Inductive reasoning and exploratory data analysis provide a means
to form or re!ne hypotheses and discover new analytic paths.
Models of reality no longer need to be static. &quot;ey are constantly
tested, updated and improved until better models are found.
</code></pre></div><div class="language-"><pre><code>› Data Science is necessary for companies to stay with the
pack and compete in the future.
Organizations are constantly making decisions based on gut
instinct, loudest voice and best argument – sometimes they are
even informed by real information. &quot;e winners and the losers in
the emerging data economy are going to be determined by their
Data Science teams.
</code></pre></div><div class="language-"><pre><code>› Data Science capabilities can be built over time.
Organizations mature through a series of stages – Collect,
Describe, Discover, Predict, Advise – as they move from data
deluge to full Data Science maturity. At each stage, they can
tackle increasingly complex analytic goals with a wider breadth
of analytic capabilities. However, organizations need not reach
maximum Data Science maturity to achieve success. Signi!cant
gains can be found in every stage.
</code></pre></div><div class="language-"><pre><code>› Data Science is a di!erent kind of team sport.
Data Science teams need a broad view of the organization. Leaders
must be key advocates who meet with stakeholders to ferret out
the hardest challenges, locate the data, connect disparate parts of
the business, and gain widespread buy-in.
</code></pre></div><h1 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> &quot;&quot;</h1><div class="language-"><pre><code>The Short Version 1313
</code></pre></div><div class="language-"><pre><code>AN INTRODUCTION TO DATA SCIENCE
If you haven’t heard of Data Science, you’re behind the
times. Just renaming your Business Intelligence group
the Data Science group is not the solution.
</code></pre></div><h5 id="start-here-for-the-basics"><a class="header-anchor" href="#start-here-for-the-basics" aria-hidden="true">#</a> START HERE for THE BASICS</h5><h3 id="what-do-we-mean-by"><a class="header-anchor" href="#what-do-we-mean-by" aria-hidden="true">#</a> What do We Mean by</h3><h3 id="data-science"><a class="header-anchor" href="#data-science" aria-hidden="true">#</a> Data Science?</h3><div class="language-"><pre><code>Describing Data Science is like trying to describe a sunset – it
should be easy, but somehow capturing the words is impossible.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 1717
</code></pre></div><h6 id="data-science-de-ned"><a class="header-anchor" href="#data-science-de-ned" aria-hidden="true">#</a> Data Science De!ned</h6><p>Data Science is the art of turning data into actions. #is is accomplished through the creation of data products, which provide actionable information without exposing decision makers to the underlying data or analytics (e.g., buy/sell strategies for !nancial instruments, a set of actions to improve product yield, or steps to improve product marketing).</p><p>Performing Data Science requires the extraction of timely, actionable information from diverse data sources to drive data products. Examples of data products include answers to questions such as: “Which of my products should I advertise more heavily to increase pro!t? How can I improve my compliance program, while reducing costs? What manufacturing process change will allow me to build a better product?” #e key to answering these questions is: understand the data you have and what the data inductively tells you.</p><div class="language-"><pre><code>» Data Product
</code></pre></div><div class="language-"><pre><code>A data product provides actionable
information without exposing
decision makers to the underlying
data or analytics. Examples include:
</code></pre></div><ul><li>Movie Recommendations</li><li>We a t h e r Fo recasts</li><li>Stock Market Predictions</li><li>Production Process Improvements</li><li>Health Diagnosis</li><li>Flu Trend Predictions</li><li>Ta r g e t e d Advertising</li></ul><p><strong><em>Read this for additional background:</em></strong></p><p>#e term Data Science appeared in the computer science literature throughout the 1960s-1980s. It was not until the late 1990s however, that the !eld as we describe it here, began to emerge from the statistics and data mining communities (e.g., [2] and [3]). Data Science was !rst introduced as an independent discipline in 2001.[4]^ Since that time, there have been countless articles advancing the discipline, culminating with Data Scientist being declared the sexiest job of the 21st century.[5]</p><div class="language-"><pre><code>We e s t a b l i s h e d o u r !r s t D a t a
Science team at Booz Allen
in 2010. It began as a natural
extension of our Business
Intelligence and cloud
</code></pre></div><div class="language-"><pre><code>infrastructure development
work. We saw the need for a
new approach to distill value
from our clients’ data. We
approached the problem
with a multidisciplinary
team of computer scientists,
mathematicians and domain
experts. #ey immediately
produced new insights and
analysis paths, solidifying the
validity of the approach. Since
that time, our Data Science
team has grown to 250 sta$
supporting dozens of clients
across a variety of domains.
#is breadth of experience
provides a unique perspective
on the conceptual models,
tradecraft, processes and culture
of Data Science.
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><h6 id="what-makes-data-science-di-erent"><a class="header-anchor" href="#what-makes-data-science-di-erent" aria-hidden="true">#</a> What makes Data Science Di&quot;erent?</h6><div class="language-"><pre><code>Data Science supports and encourages shifting between deductive
(hypothesis-based) and inductive (pattern-based) reasoning. #is is
a fundamental change from traditional analytic approaches. Inductive
reasoning and exploratory data analysis provide a means to form or
re!ne hypotheses and discover new analytic paths. In fact, to do the
discovery of signi!cant insights that are the hallmark of Data Science,
you must have the tradecraft and the interplay between inductive
and deductive reasoning. By actively combining the ability to reason
deductively and inductively, Data Science creates an environment
where models of reality no longer need to be static and empirically
based. Instead, they are constantly tested, updated and improved until
better models are found. #ese concepts are summarized in the !gure,
!e Types of Reason and !eir Role in Data Science Tradecraft.
</code></pre></div><div class="language-"><pre><code>THE TYPES OF REASON...
</code></pre></div><div class="language-"><pre><code>DEDUCTIVE REASONING:
</code></pre></div><div class="language-"><pre><code>› Commonly associated
with “formal logic.”
› Involves reasoning from known
premises, or premises presumed
to be true, to a certain conclusion.
› The conclusions reached are
certain, inevitable, inescapable.
</code></pre></div><div class="language-"><pre><code>INDUCTIVE REASONING
</code></pre></div><div class="language-"><pre><code>› Commonly known as “informal
logic,” or “everyday argument.”
› Involves drawing uncertain
inferences, based on
probabilistic reasoning.
› The conclusions reached
are probable, reasonable,
plausible, believable.
</code></pre></div><div class="language-"><pre><code>...AND THEIR ROLE IN DATA SCIENCE TRADECRAFT.
</code></pre></div><div class="language-"><pre><code>DEDUCTIVE REASONING:
</code></pre></div><div class="language-"><pre><code>› Formulate hypotheses about
relationships and underlying models.
› Carry out experiments with the data
to test hypotheses and models.
</code></pre></div><div class="language-"><pre><code>INDUCTIVE REASONING
</code></pre></div><div class="language-"><pre><code>› Exploratory data analysis to
discover or refine hypotheses.
› Discover new relationships, insights
and analytic paths from the data.
</code></pre></div><div class="language-"><pre><code>The Types of Reason and Their Role in Data Science Tradecraft
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 1919
</code></pre></div><p>#e di$erences between Data Science and traditional analytic approaches do not end at seamless shifting between deductive and inductive reasoning. Data Science o$ers a distinctly di$erent perspective than capabilities such as Business Intelligence. Data Science should not replace Business Intelligence functions within an organization, however. #e two capabilities are additive and complementary, each o$ering a necessary view of business operations and the operating environment. #e !gure, <em>Business Intelligence and Data Science – A Comparison,</em> highlights the di$erences between the two capabilities. Key contrasts include:</p><div class="language-"><pre><code>› Discovery vs. Pre-canned Questions: Data Science actually
works on discovering the question to ask as opposed to just
asking it.
› Power of Many vs. Ability of One: An entire team provides
a common forum for pulling together computer science,
mathematics and domain expertise.
› Prospective vs. Retrospective: Data Science is focused on
obtaining actionable information from data as opposed to
reporting historical facts.
</code></pre></div><div class="language-"><pre><code>LOOKING BACKWARD AND FORWARD
</code></pre></div><div class="language-"><pre><code>FIRST THERE WAS
BUSINESS INTELLIGENCE
</code></pre></div><div class="language-"><pre><code>Deductive Reasoning
Backward Looking
Slice and Dice Data
Warehoused and Siloed Data
Analyze the Past, Guess the Future
Creates Reports
Analytic Output
</code></pre></div><div class="language-"><pre><code>NOW WE&#39;VE ADDED
DATA SCIENCE
</code></pre></div><div class="language-"><pre><code>Inductive and Deductive Reasoning
Forward Looking
Interact with Data
Distributed, Real Time Data
Predict and Advise
Creates Data Products
Answer Questions and Create New Ones
Actionable Answer
</code></pre></div><div class="language-"><pre><code>Business Intelligence and Data Science - A Comparison (adapted in part from [6])
</code></pre></div><h6 id="what-is-the-impact-of-data-science"><a class="header-anchor" href="#what-is-the-impact-of-data-science" aria-hidden="true">#</a> What is the Impact of Data Science?</h6><div class="language-"><pre><code>As we move into the data economy, Data Science is the competitive
advantage for organizations interested in winning – in whatever way
winning is de!ned. #e manner in which the advantage is de!ned
is through improved decision-making. A former colleague liked to
describe data-informed decision making like this: If you have perfect
information or zero information then your task is easy – it is in between
those two extremes that the trouble begins. What he was highlighting is
the stark reality that whether or not information is available, decisions
must be made.
</code></pre></div><div class="language-"><pre><code>#e way organizations make decisions has been evolving for half a
century. Before the introduction of Business Intelligence, the only
options were gut instinct, loudest voice, and best argument. Sadly, this
method still exists today, and in some pockets it is the predominant
means by which the organization acts. Take our advice and never, ever
work for such a company!
</code></pre></div><div class="language-"><pre><code>Fortunately for our economy, most organizations began to inform
their decisions with real information through the application of
simple statistics. #ose that did it well were rewarded; those that did
not failed. We are outgrowing the ability of simple stats to keep pace
with market demands, however. #e rapid expansion of available data,
and the tools to access and make use of the data at scale, are enabling
fundamental changes to the way organizations make decisions.
</code></pre></div><div class="language-"><pre><code>Data Science is required to maintain competitiveness in the
increasingly data-rich environment. Much like the application of
simple statistics, organizations that embrace Data Science will be
rewarded while those that do not will be challenged to keep pace.
As more complex, disparate data sets become available, the chasm
between these groups will only continue to widen. #e !gure,
!e Business Impacts of Data Science, highlights the value awaiting
organizations that embrace Data Science.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 2121
</code></pre></div><div class="language-"><pre><code>DATA SCIENCE IS NECESSARY...
</code></pre></div><p>17-49%^ increase in productivity when organizations increase data usability by 10%</p><p>11-42% return on assets (ROA) when organizations increase data access by 10%</p><p>241%^ increase in ROI when organizations use big data to^ improve competitiveness</p><p>1000%</p><div class="language-"><pre><code>increase in ROI when deploying analytics across most of
the organization, aligning daily operations with senior
management&#39;s goals, and incorporating big data
</code></pre></div><p>5-6%^ performance improvement for organizations making^ data-driven decisions.</p><div class="language-"><pre><code>...TO COMPETE IN THE FUTURE
</code></pre></div><div class="language-"><pre><code>The Business Impacts of Data Science (adapted from [7], [8] and [9])
</code></pre></div><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 2323
</code></pre></div><h6 id="what-is-di-erent-now"><a class="header-anchor" href="#what-is-di-erent-now" aria-hidden="true">#</a> What is Di&quot;erent Now?</h6><p>For 20 years IT systems were built the same way. We separated the people who ran the business from the people who managed the infrastructure (and therefore saw data as simply another thing they had to manage). With the advent of new technologies and analytic techniques, this arti!cial – and highly ine$ective – separation of critical skills is no longer necessary. For the !rst time, organizations can directly connect business decision makers to the data. #is simple step transforms data from being ‘something to be managed’ into</p><p>‘something to be valued.’</p><p>In the wake of the transformation, organizations face a stark choice: you can continue to build data silos and piece together disparate information or you can consolidate your data and distill answers.</p><p>From the Data Science perspective, this is a false choice: #e siloed approach is untenable when you consider the (a) the opportunity cost of not making maximum use of all available data to help an organization succeed, and (b) the resource and time costs of continuing down the same path with outdated processes. #e tangible bene!ts of data products include:</p><p>› <strong><em>Opportunity Costs:</em></strong> Because Data Science is an emerging !eld, opportunity costs arise when a competitor implements and generates value from data before you. Failure to learn and account for changing customer demands will inevitably drive customers away from your current o$erings. When competitors are able to successfully leverage Data Science to gain insights, they can drive di$erentiated customer value propositions and lead their industries as a result.</p><p>› <strong><em>Enhanced Processes:</em></strong> As a result of the increasingly interconnected world, huge amounts of data are being generated and stored every instant. Data Science can be used to transform data into insights that help improve existing processes. Operating costs can be driven down dramatically by e$ectively incorporating the complex interrelationships in data like never before. #is results in better quality assurance, higher product yield and more e$ective operations.</p><h3 id="how-does-data-science"><a class="header-anchor" href="#how-does-data-science" aria-hidden="true">#</a> How does Data Science</h3><h3 id="actually-work"><a class="header-anchor" href="#actually-work" aria-hidden="true">#</a> Actually Work?</h3><div class="language-"><pre><code>It’s not rocket science... it’s something better - Data Science
</code></pre></div><div class="language-"><pre><code>Let’s not kid ourselves - Data Science is a complex !eld. It is di%cult,
intellectually taxing work, which requires the sophisticated integration
of talent, tools and techniques. But as a !eld guide, we need to cut
through the complexity and provide a clear, yet e$ective way to
understand this new world.
</code></pre></div><div class="language-"><pre><code>To do this, we will transform the !eld of Data Science into a set of
simpli!ed activities as shown in the !gure, !e Four Key Activities of a
Data Science Endeavor. Data Science purists will likely disagree with
this approach, but then again, they probably don’t need a !eld guide,
sitting as they do in their ivory towers! In the real world, we need
clear and simple operating models to help drive us forward.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><p>1 2 3 4</p><p><strong>Acquire Prepare Analyze Act</strong></p><div class="language-"><pre><code>Low
</code></pre></div><div class="language-"><pre><code>High
</code></pre></div><p>Degree of Effort</p><div class="language-"><pre><code>Data Science Activities
</code></pre></div><div class="language-"><pre><code>Try
</code></pre></div><div class="language-"><pre><code>Evaluate
</code></pre></div><div class="language-"><pre><code>Setup Do
</code></pre></div><div class="language-"><pre><code>Evaluate
</code></pre></div><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 2525
</code></pre></div><div class="language-"><pre><code>Activity 1: Acquire
This activity focuses
on obtaining the
data you need.
Given the nature of
data, the details of
this activity depend
heavily on who you
are and what you
do. As a result, we
will not spend a
lot of time on this
activity other than
to emphasize its
importance and
to encourage an
expansive view on
which data can and
should be used.
</code></pre></div><div class="language-"><pre><code>Activity 2: Prepare
Great outcomes
don’t just happen
by themselves.
A lot depends on
preparation, and
in Data Science,
that means
manipulating the
data to fit your
analytic needs.
This stage can
consume a great
deal of time, but
it is an excellent
investment. The
benefits are
immediate and
long term.
</code></pre></div><div class="language-"><pre><code>Activity 3: Analyze
This is the activity
that consumes the
lion’s share of the
team’s attention.
It is also the most
challenging and
exciting (you will
see a lot of ‘aha
moments’ occur in
this space). As the
most challenging
and vexing of the
four activities,
this field guide
focuses on helping
you do this better
and faster.
</code></pre></div><div class="language-"><pre><code>Activity 4: Act
Every effective
Data Science team
analyzes its data
with a purpose
</code></pre></div><ul><li>that is, to turn data into actions. Actionable and impactful insights are the holy grail of Data Science. Converting insights into action can be a politically charged activity, however. This activity depends heavily on the culture and character of your organization, so we will leave you to figure out those details for yourself.</li></ul><div class="language-"><pre><code>The Four Key Activities of a Data Science Endeavor
</code></pre></div><h6 id="acquire"><a class="header-anchor" href="#acquire" aria-hidden="true">#</a> Acquire</h6><div class="language-"><pre><code>All analysis starts with access to data, and for the Data Scientist
this axiom holds true. But there are some signi!cant di$erences –
particularly with respect to the question of who stores, maintains and
owns the data in an organization.
</code></pre></div><div class="language-"><pre><code>But before we go there, lets look at what is changing. Traditionally,
rigid data silos arti!cially de!ne the data to be acquired. Stated
another way, the silos create a !lter that lets in a very small amount of
data and ignores the rest. #ese !ltered processes give us an arti!cial
view of the world based on the ‘surviving data,’ rather than one that
shows full reality and meaning. Without a broad and expansive data
set, we can never immerse ourselves in the diversity of the data. We
instead make decisions based on limited and constrained information.
</code></pre></div><div class="language-"><pre><code>Eliminating the need for silos gives us access to all the data at once –
including data from multiple outside sources. It embraces the reality
that diversity is good and complexity is okay. #is mindset creates a
completely di$erent way of thinking about data in an organization by
giving it a new and di$erentiated role. Data represents a signi!cant
new pro!t and mission-enhancement opportunity for organizations.
</code></pre></div><div class="language-"><pre><code>But as mentioned earlier, this !rst activity is heavily dependent upon
the situation and circumstances. We can’t leave you with anything
more than general guidance to help ensure maximum value:
</code></pre></div><div class="language-"><pre><code>› Look inside &quot;rst: What data do you have current access
to that you are not using? #is is in large part the data
being left behind by the !ltering process, and may be
incredibly valuable.
› Remove the format constraints: Stop limiting your data
acquisition mindset to the realm of structured databases.
Instead, think about unstructured and semi-structured data
as viable sources.
› Figure out what’s missing: Ask yourself what data would
make a big di$erence to your processes if you had access to it.
#en go !nd it!
› Embrace diversity: Try to engage and connect to publicly
available sources of data that may have relevance to your
domain area.
</code></pre></div><div class="language-"><pre><code>» Not All Data Is Created Equal
</code></pre></div><div class="language-"><pre><code>As you begin to aggregate data,
remember that not all data is
created equally. Organizations have
a tendency to collect any data that
is available. Data that is nearby
(readily accessible and easily
obtained) may be cheap to collect,
but there is no guarantee it is the
right data to collect. Focus on the
data with the highest ROI for your
organization. Your Data Science
team can help identify that data.
Also remember that you need to
strike a balance between the data
that you need and the data that you
have. Collecting huge volumes of
data is useless and costly if it is not
the data that you need.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 2727
</code></pre></div><h6 id="prepare"><a class="header-anchor" href="#prepare" aria-hidden="true">#</a> Prepare</h6><p>Once you have the data, you need to prepare it for analysis.</p><p>Organizations often make decisions based on inexact data. Data stovepipes mean that organizations may have blind spots. #ey are not able to see the whole picture and fail to look at their data and challenges holistically. #e end result is that valuable information is withheld from decision makers. Research has shown almost 33% of decisions are made without good data or information. [10]</p><p>When Data Scientists are able to explore and analyze all the data, new opportunities arise for analysis and data-driven decision making. #e insights gained from these new opportunities will signi!cantly change the course of action and decisions within an organization. Gaining access to an organization’s complete repository of data, however, requires preparation.</p><p>Our experience shows time and time again that the best tool for Data Scientists to prepare for analysis is a lake – speci!cally, the Data Lake.[11] #is is a new approach to collecting, storing and integrating data that helps organizations maximize the utility of their data. Instead of storing information in discrete data structures, the Data Lake consolidates an organization’s complete repository of data in a single, large view. It eliminates the expensive and cumbersome data-preparation process, known as Extract/Transform/Load (ETL), necessary with data silos. #e entire body of information in the Data Lake is available for every inquiry – and all at once.</p><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><h6 id="analyze"><a class="header-anchor" href="#analyze" aria-hidden="true">#</a> Analyze</h6><div class="language-"><pre><code>We have acquired the data... we have prepared it... now it is time to
analyze it.
</code></pre></div><div class="language-"><pre><code>#e Analyze activity requires the greatest e$ort of all the activities
in a Data Science endeavor. #e Data Scientist actually builds the
analytics that create value from data. Analytics in this context is
an iterative application of specialized and scalable computational
resources and tools to provide relevant insights from exponentially
growing data. #is type of analysis enables real-time understanding
of risks and opportunities by evaluating situational, operational and
behavioral data.
</code></pre></div><div class="language-"><pre><code>With the totality of data fully accessible in the Data Lake,
organizations can use analytics to !nd the kinds of connections and
patterns that point to promising opportunities. #is high-speed
analytic connection is done within the Data Lake, as opposed to
older style sampling methods that could only make use of a narrow
slice of the data. In order to understand what was in the lake, you had
to bring the data out and study it. Now you can dive into the lake,
bringing your analytics to the data. #e !gure, Analytic Connection in
the Data Lake, highlights the concept of diving into the Data Lake to
discover new connections and patterns.
</code></pre></div><div class="language-"><pre><code>Analytic Connection in the Data Lake
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 2929
</code></pre></div><p>Data Scientists work across the spectrum of analytic goals – Describe, Discover, Predict and Advise. #e maturity of an analytic capability determines the analytic goals encompassed. Many variables play key roles in determining the di%culty and suitability of each goal for an organization. Some of these variables are the size and budget of an organization and the type of data products needed by the decision makers. A detailed discussion on analytic maturity can be found in <em>Data Science Maturity within an Organization</em>.</p><p>In addition to consuming the greatest e$ort, the Analyze activity is by far the most complex. #e tradecraft of Data Science is an art. While we cannot teach you how to be an artist, we can share foundational tools and techniques that can help you be successful. #e entirety of <em>Take O&quot; the Training Wheels</em> is dedicated to sharing insights we have learned over time while serving countless clients. #is includes descriptions of a Data Science product lifecycle and the <em>Fractal Analytic Model</em> (FAM). #e <em>Analytic Selection Process</em> and accompanying <em>Guide to Analytic Selection</em> provide key insights into one of the most challenging tasks in all of Data Science – selecting the right technique for the job.</p><h6 id="act"><a class="header-anchor" href="#act" aria-hidden="true">#</a> Act</h6><p>Now that we have analyzed the data, it’s time to take action.</p><p>#e ability to make use of the analysis is critical. It is also very situational. Like the Acquire activity, the best we can hope for is to provide some guiding principles to help you frame the output for maximum impact. Here are some key points to keep in mind when presenting your results:</p><ol><li>#e !nding must make sense with relatively little up-front training or preparation on the part of the decision maker.</li><li>#e !nding must make the most meaningful patterns, trends and exceptions easy to see and interpret.</li><li>Every e$ort must be made to encode quantitative data accurately so the decision maker can accurately interpret and compare the data.</li><li>#e logic used to arrive at the !nding must be clear and compelling as well as traceable back through the data.</li><li>#e !ndings must answer real business questions.</li></ol><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Proportion
of
Effort
</code></pre></div><div class="language-"><pre><code>Maturity
</code></pre></div><div class="language-"><pre><code>Stages of Maturity
</code></pre></div><div class="language-"><pre><code>Collect
</code></pre></div><div class="language-"><pre><code>Describe
</code></pre></div><div class="language-"><pre><code>Discover
</code></pre></div><div class="language-"><pre><code>Predict
</code></pre></div><div class="language-"><pre><code>Advise
</code></pre></div><div class="language-"><pre><code>Data Silos
</code></pre></div><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 3131
</code></pre></div><h6 id="data-science-maturity-within"><a class="header-anchor" href="#data-science-maturity-within" aria-hidden="true">#</a> Data Science Maturity within</h6><h6 id="an-organization"><a class="header-anchor" href="#an-organization" aria-hidden="true">#</a> an Organization</h6><p>#e four activities discussed thus far provide a simpli!ed view of Data Science. Organizations will repeat these activities with each new Data Science endeavor. Over time, however, the level of e$ort necessary for each activity will change. As more data is Acquired and Prepared in the Data Lake, for example, signi!cantly less e$ort will need to be expended on these activities. #is is indicative of a maturing Data Science capability.</p><p>Assessing the maturity of your Data Science capability calls for a slightly di$erent view. We use <em>!e Data Science Maturity Model</em> as a common framework for describing the maturity progression and components that make up a Data Science capability. #is framework can be applied to an organization’s Data Science capability or even to the maturity of a speci!c solution, namely a data product. At each stage of maturity, powerful insight can be gained.</p><div class="language-"><pre><code>The Data Science Maturity Model
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>When organizations start out, they have Data Silos. At this stage,
they have not carried out any broad Aggregate activities. #ey may
not have a sense of all the data they have or the data they need. #e
decision to create a Data Science capability signals the transition into
the Collect stage.
</code></pre></div><div class="language-"><pre><code>All of your initial e$ort will be focused on identifying and aggregating
data. Over time, you will have the data you need and a smaller
proportion of your e$ort can focus on Collect. You can now begin to
Describe your data. Note, however, that while the proportion of time
spent on Collect goes down dramatically, it never goes away entirely.
#is is indicative of the four activities outlined earlier – you will
continue to Aggregate and Prepare data as new analytic questions
arise, additional data is needed and new data sources become available.
</code></pre></div><div class="language-"><pre><code>Organizations continue to advance in maturity as they move through
the stages from Describe to Advise. At each stage they can tackle
increasingly complex analytic goals with a wider breadth of analytic
capabilities. As described for Collect , each stage never goes away
entirely. Instead, the proportion of time spent focused on it goes
down and new, more mature activities begin. A brief description
of each stage of maturity is shown in the table !e Stages of Data
Science Maturity.
</code></pre></div><div class="language-"><pre><code>The Stages of Data Science Maturity
</code></pre></div><div class="language-"><pre><code>Stage Description Example
</code></pre></div><div class="language-"><pre><code>Collect Focuses on collecting internal or external datasets. Gathering sales records and corresponding weather data.
</code></pre></div><div class="language-"><pre><code>Describe
</code></pre></div><div class="language-"><pre><code>Seeks to enhance or
refine raw data as well
as leverage basic analytic
functions such as counts.
</code></pre></div><div class="language-"><pre><code>How are my customers
distributed with respect to
location, namely zip code?
</code></pre></div><div class="language-"><pre><code>Discover Identifies hidden relationships or patterns.
</code></pre></div><div class="language-"><pre><code>Are there groups within
my regular customers that
purchase similarly?
</code></pre></div><div class="language-"><pre><code>Predict
</code></pre></div><div class="language-"><pre><code>Utilizes past observations to
predict future observations.
</code></pre></div><div class="language-"><pre><code>Can we predict which products
that certain customer groups
are more likely to purchase?
</code></pre></div><div class="language-"><pre><code>Advise
</code></pre></div><div class="language-"><pre><code>Defines your possible decisions,
optimizes over those decisions,
and advises to use the decision
that gives the best outcome.
</code></pre></div><div class="language-"><pre><code>Your advice is to target advertise
to specific groups for certain
products to maximize revenue.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 3333
</code></pre></div><p>#e maturity model provides a powerful tool for understanding and appreciating the maturity of a Data Science capability. Organizations need not reach maximum maturity to achieve success. Signi!cant gains can be found in every stage. We believe strongly that one does not engage in a Data Science e$ort, however, unless it is intended to produce an output – that is, you have the intent to <em>Advise</em>. #is means simply that each step forward in maturity drives you to the right in the model diagram. Moving to the right requires the correct processes, people, culture and operating model – a robust Data Science capability. <em>What Does it Take to Create a Data Science Capability?</em> addresses this topic.</p><p>We have observed very few organizations actually operating at the highest levels of maturity, the <em>Predict</em> and <em>Advise</em> stages. #e tradecraft of <em>Discover</em> is only now maturing to the point that organizations can focus on advanced <em>Predict</em> and <em>Advise</em> activities. #is is the new frontier of Data Science. #is is the space in which we will begin to understand how to close the cognitive gap between humans and computers. Organizations that reach <em>Advise</em> will be met with true insights and real competitive advantage.</p><div class="language-"><pre><code>» Where does your organization
fall in analytic maturity?
</code></pre></div><div class="language-"><pre><code>Take the quiz!
</code></pre></div><p><strong>1. How many data sources do</strong><strong>you collect?</strong> a. Why do we need a bunch of data? <em>- 0 points, end here.</em> b. I don’t know the exact number. <em>- 5 points</em> c. We identified the required data and collect it. <em>– 10 points</em><strong>2. Do you know what questions</strong><strong>your Data Science team is trying</strong><strong>to answer?</strong> a. Why do we need questions? <em>- 0 points</em> b. No, they figure it out for themselves. <em>- 5 points</em> c. Yes, we evaluated the questions that will have the largest impact to the business. <em>– 10 points</em><strong>3. Do you know the important factors</strong><strong>driving your business?</strong> a. I have no idea. <em>– 0 points</em> b. Our quants help me figure it out. <em>- 5 points</em> c. We have a data product for that. <em>- 10 points</em><strong>4. Do you have an understanding of</strong><strong>future conditions?</strong> a. I look at the current conditions and read the tea leaves. <em>– 0 points</em> b. We have a data product for that. <em>- 5 points</em><strong>5. Do you know the best course</strong><strong>of action to take for your key</strong><strong>decisions?</strong> a. I look at the projections and plan a course. <em>– 0 points</em> b. We have a data product for that. <em>- 5 points</em></p><div class="language-"><pre><code>Check your score:
</code></pre></div><div class="language-"><pre><code>0 – Data Silos, 5-10 – Collect,
10-20 – Describe, 20-30 – Discover,
30-35 – Predict, 35-40 - Advise
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>COMPUTER SCIENCE
Provides the environment
in which data products
are created.
</code></pre></div><div class="language-"><pre><code>DOMAIN EXPERTISE
Provides understanding
of the reality in which a
problem space exists.
</code></pre></div><div class="language-"><pre><code>MATHEMATICS
Provides the theoretical
structure in which Data
Science problems
are examined.
</code></pre></div><h3 id="what-does-it-take-to-create"><a class="header-anchor" href="#what-does-it-take-to-create" aria-hidden="true">#</a> What does it Take to Create</h3><h3 id="a-data-science-capability"><a class="header-anchor" href="#a-data-science-capability" aria-hidden="true">#</a> a Data Science Capability?</h3><div class="language-"><pre><code>Data Science is all about building teams and culture.
</code></pre></div><div class="language-"><pre><code>As with any team sport, Data Science depends on a diverse set of skills
to achieve its objective – winning at the game of improved insights.
You need the three skill sets shown in !e Data Science Venn Diagram
to create a winning team in the world of Data Science.
</code></pre></div><div class="language-"><pre><code>Building Data Science teams is di%cult. It requires an understanding
of the types of personalities that make Data Science possible, as well
as a willingness to establish a culture of innovation and curiosity in
your organization. You must also consider how to deploy the team and
gain widespread buy-in from across your organization.
</code></pre></div><div class="language-"><pre><code>The Data Science Venn Diagram (inspired by [12])
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 3535
</code></pre></div><h6 id="understanding-what-makes"><a class="header-anchor" href="#understanding-what-makes" aria-hidden="true">#</a> Understanding What Makes</h6><h6 id="a-data-scientist"><a class="header-anchor" href="#a-data-scientist" aria-hidden="true">#</a> a Data Scientist</h6><p>Data Science often requires a signi!cant investment of time across a variety of tasks. Hypotheses must be generated and data must be acquired, prepared, analyzed, and acted upon. Multiple techniques are often applied before one yields interesting results. If that seems daunting, it is because it is. Data Science is di%cult, intellectually taxing work, which requires lots of talent: both tangible technical skills as well as the intangible ‘x-factors.’</p><p>#e most important qualities of Data Scientists tend to be the intangible aspects of their personalities. Data Scientists are by nature curious, creative, focused, and detail-oriented.</p><p>› <strong><em>Curiosity</em></strong> is necessary to peel apart a problem and examine the interrelationships between data that may appear super!cially unrelated.</p><p>› <strong><em>Creativity</em></strong> is required to invent and try new approaches to solving a problem, which often times have never been applied in such a context before.</p><p>› <strong><em>Focus</em></strong> is required to design and test a technique over days and weeks, !nd it doesn’t work, learn from the failure, and try again.</p><p>› <strong><em>Attention to Detail</em></strong> is needed to maintain rigor, and to detect and avoid over-reliance on intuition when examining data.</p><p>Success of a Data Science team requires pro!ciency in three foundational technical skills: computer science, mathematics and domain expertise, as re&amp;ected in <em>!e Data Science Venn Diagram</em>. Computers provide the environment in which data-driven hypotheses are tested, and as such computer science is necessary for data manipulation and processing. Mathematics provides the theoretical structure in which Data Science problems are examined. A rich background in statistics, geometry, linear algebra, and calculus are all important to understand the basis for many algorithms and tools. Finally, domain expertise contributes to an understanding of what problems actually need to be solved, what kind of data exists in the domain and how the problem space may be instrumented and measured.</p><div class="language-"><pre><code>» !e Triple !reat Unicorn
</code></pre></div><div class="language-"><pre><code>Individuals who are great at all three
of the Data Science foundational
technical skills are like unicorns –
very rare and if you’re ever lucky
enough to !nd one they should be
treated carefully. When you manage
these people:
› Encourage them to lead
your team, but not manage
it. Don’t bog them down
with responsibilities of
management that could be
done by other sta$.
› Put extra e$ort into managing
their careers and interests
within your organization.
Build opportunities for
promotion into your
organization that allow them
to focus on mentoring other
Data Scientists and progressing
the state of the art while also
advancing their careers.
› Make sure that they have the
opportunity to present and
spread their ideas in many
di$erent forums, but also be
sensitive to their time.
</code></pre></div><h6 id="finding-the-athletes-for-your-team"><a class="header-anchor" href="#finding-the-athletes-for-your-team" aria-hidden="true">#</a> Finding the Athletes for Your Team</h6><div class="language-"><pre><code>Building a Data Science team is complex. Organizations must
simultaneously engage existing internal sta$ to create an “anchor” that
can be used to recruit and grow the team, while at the same time
undergo organizational change and transformation to meaningfully
incorporate this new class of employee.
</code></pre></div><div class="language-"><pre><code>Building a team starts with identifying existing sta$ within an
organization who have a high aptitude for Data Science. Good
candidates will have a formal background in any of the three
foundational technical skills we mentioned, and will most importantly
have the personality traits necessary for Data Science. #ey may often
have advanced (masters or higher) degrees, but not always. #e very
!rst sta$ you identify should also have good leadership traits and a
sense of purpose for the organization, as they will lead subsequent
sta%ng and recruiting e$orts. Don’t discount anyone – you will !nd
Data Scientists in the strangest places with the oddest combinations
of backgrounds.
</code></pre></div><h6 id="shaping-the-culture"><a class="header-anchor" href="#shaping-the-culture" aria-hidden="true">#</a> Shaping the Culture</h6><div class="language-"><pre><code>Good Data Science requires a highly academic culture of peer review,
where no member of the organization is immune from constructive
criticism. As you build your Data Science practice, you should be
prepared to subject all aspects of your corporate operations to the
curious nature of your Data Science teams. Failure to do so creates
a negative image of a culture that fails to “eat its own dog food,”
and will invite negative re&amp;ection on the brand, both internally and
externally. You should be conscious of any cultural legacies existing in
an organization that are antithetical to Data Science.
</code></pre></div><div class="language-"><pre><code>Data Scientists are fundamentally curious and imaginative. We have
a saying on our team, “We’re not nosy, we’re Data Scientists.” #ese
qualities are fundamental to the success of the project and to gaining
new dimensions on challenges and questions. Often Data Science
projects are hampered by the lack of the ability to imagine something
new and di$erent. Fundamentally, organizations must foster trust and
transparent communication across all levels, instead of deference to
authority, in order to establish a strong Data Science team. Managers
should be prepared to invite participation more frequently, and o$er
explanation or apology less frequently.
</code></pre></div><div class="language-"><pre><code>» Don’t judge a book by its
cover, or a Data Scientist
by his or her degree in
this case. Amazing Data
Scientists can be found
anywhere. Just look at the
diverse and surprising
sampling of degrees
held by Our Experts:
</code></pre></div><div class="language-"><pre><code>› Bioinformatics
› Biomedical Engineering
› Biophysics
› Business
› Computer Graphics
› Computer Science
› English
› Forest Management
› History
› Industrial Engineering
› Information Technology
› Mathematics
› National Security Studies
› Operations Research
› Physics
› Wildlife &amp; Fisheries
Management
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 3737
</code></pre></div><h6 id="selecting-your-operating-model"><a class="header-anchor" href="#selecting-your-operating-model" aria-hidden="true">#</a> Selecting Your Operating Model</h6><p>Depending on the size, complexity, and the business drivers, organizations should consider one of three Data Science operating models: Centralized, Deployed, or Di$used. #ese three models are shown in the !gure, <em>Data Science Operating Models.</em></p><p><strong><em>Centralized Data Science teams</em></strong> serve the organization across all business units. #e team is centralized under a Chief Data Scientist. #ey serve all the analytical needs of an organization and they all co-locate together. #e domain experts come to this organization for brief rotational stints to solve challenges around the business.</p><p><strong><em>Deployed Data Science teams</em></strong> go to the business unit or group and reside there for short- or long-term assignments. #ey are their own entity and they work with the domain experts within the group to solve hard problems. #ey may be working independently on particular challenges, but they should always collaborate with the other teams to exchange tools, techniques and war stories.</p><p><strong><em>#e Di$used Data Science team</em></strong> is one that is fully embedded with each group and becomes part of the long-term organization. #ese teams work best when the nature of the domain or business unit is already one focused on analytics. However, building a cross-cut view into the team that can collaborate with other Data Science teams is critical to the success.</p><div class="language-"><pre><code>CENTRALIZED
Business units bring their
problems to a centralized
Data Science team.
</code></pre></div><div class="language-"><pre><code>DIFFUSED
Data Scientists are fully
embedded within the
business units.
</code></pre></div><div class="language-"><pre><code>DEPLOYED
Small Data Science teams
are forward deployed to
business units.
</code></pre></div><div class="language-"><pre><code>Data Science Operating Models
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>BALANCING THE DATA
SCIENCE TEAM EQUATION
</code></pre></div><div class="language-"><pre><code>Balancing the composition of a Data Science team
is much like balancing the reactants and products in
a chemical reaction. Each side of the equation must
represent the same quantity of any particular element.
In the case of Data Science, these elements are the
foundational technical skills computer science (CS),
mathematics (M) and domain expertise (DE). The
reactants, your Data Scientists, each have their own
unique skills composition. You must balance the staff
mix to meet the skill requirements of the Data Science
team, the product in the reaction. If you don’t correctly
balance the equation, your Data Science team will not
have the desired impact on the organization.
</code></pre></div><h6 id="_2-cs-m-2-2-cs-m-de-→-cs-4-m-5-de"><a class="header-anchor" href="#_2-cs-m-2-2-cs-m-de-→-cs-4-m-5-de" aria-hidden="true">#</a> 2 CS M 2 + 2 CS + M DE → CS 4 M 5 DE</h6><div class="language-"><pre><code>I n the example above, your project requires four parts
computer science, five parts mathematics and one
part domain expertise. Given the skills mix of the
staff, five people are needed to balance the equation.
Throughout your Data Science project, the skills
requirements of the team will change. You will need
to re-balance the equation to ensure the reactants
balance with the products.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 3939
</code></pre></div><h6 id="success-starts-at-the-top"><a class="header-anchor" href="#success-starts-at-the-top" aria-hidden="true">#</a> Success Starts at the Top</h6><p>Data Science teams, no matter how they are deployed, must have sponsorship. #ese can start as grass roots e$orts by a few folks to start tackling hard problems, or as e$orts directed by the CEO. Depending on the complexity of the organization, direction from top- down for large organizations is the best for assuaging fears and doubts of these new groups.</p><p>Data Science teams often face harder political headwinds when solving problems than any technical hurdles. To prove a Data Science team’s value, the team needs to initially focus on the hardest problems within an organization that have the highest return for key stakeholders and will change how the organization approaches challenges in the future. #is has the e$ect of keeping the team motivated and encouraged in the face of di%cult challenges. Leaders must be key advocates who meet with stakeholders to ferret out the hardest problems, locate the data, connect disparate parts of the business and gain widespread buy-in.</p><h5 id="ta-k-e-o-f-f-the-training-wheels"><a class="header-anchor" href="#ta-k-e-o-f-f-the-training-wheels" aria-hidden="true">#</a> TA K E O F F the TRAINING WHEELS</h5><div class="language-"><pre><code>THE PRACTITIONER’S GUIDE
TO DATA SCIENCE
Read this section to get beyond the hype and
learn the secrets of being a Data Scientist.
</code></pre></div><h3 id="guiding-principles"><a class="header-anchor" href="#guiding-principles" aria-hidden="true">#</a> Guiding Principles</h3><div class="language-"><pre><code>Failing is good; failing quickly is even better.
</code></pre></div><div class="language-"><pre><code>#e set of guiding principles that govern how we conduct the
tradecraft of Data Science are based loosely on the central tenets
of innovation, as the two areas are highly connected. #ese principles
are not hard and fast rules to strictly follow, but rather key tenets
that have emerged in our collective consciousness. You should use
these to guide your decisions, from problem decomposition
through implementation.
</code></pre></div><div class="language-"><pre><code>› Be willing to fail. At the core of Data Science is the idea of
experimentation. Truly innovative solutions only emerge when
you experiment with new ideas and applications. Failure is an
acceptable byproduct of experimentation. Failures locate regions
that no longer need to be considered as you search for a solution.
› Fail often and learn quickly. In addition to a willingness to fail, be
ready to fail repeatedly. #ere are times when a dozen approaches
must be explored in order to !nd the one that works. While you
shouldn’t be concerned with failing, you should strive to learn from
the attempt quickly. #e only way you can explore a large number
of solutions is to do so quickly.
› Keep the goal in mind. You can often get lost in the details and
challenges of an implementation. When this happens, you lose
sight of your goal and begin to drift o$ the path from data to
analytic action. Periodically step back, contemplate your goal, and
evaluate whether your current approach can really lead you where
you want to go.
› Dedication and focus lead to success. Yo u m u s t o f t e n e x p l o r e
many approaches before !nding the one that works. It’s easy to
become discouraged. You must remain dedicated to your analytic
goal. Focus on the details and the insights revealed by the data.
Sometimes seemingly small observations lead to big successes.
› Complicated does not equal better. As technical practitioners, we
have a tendency to explore highly complex, advanced approaches.
While there are times where this is necessary, a simpler approach
can often provide the same insight. Simpler means easier and
faster to prototype, implement and verify.
</code></pre></div><div class="language-"><pre><code>» Tips From the Pros
</code></pre></div><div class="language-"><pre><code>It can be easier to rule out a solution
than confirm its correctness. As a
result, focus on exploring obvious
shortcomings that can quickly
disqualify an approach. This will allow
you to focus your time on exploring
truly viable approaches as opposed to
dead ends.
</code></pre></div><div class="language-"><pre><code>» Tips From the Pros
</code></pre></div><div class="language-"><pre><code>If the first thing you try to do is to
create the ultimate solution, you will
fail, but only after banging your head
against a wall for several weeks.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h3 id="e-importance-of-reason"><a class="header-anchor" href="#e-importance-of-reason" aria-hidden="true">#</a> #e Importance of Reason</h3><p>Beware: in the world of Data Science, if it walks like a duck and quacks like a duck, it might just be a moose.</p><p>Data Science supports and encourages shifting between deductive (hypothesis-based) and inductive (pattern-based) reasoning. Inductive reasoning and exploratory data analysis provide a means to form or re!ne hypotheses and discover new analytic paths. Models of reality no longer need to be static. #ey are constantly tested, updated and improved until better models are found.</p><p>#e analysis of big data has brought inductive reasoning to the forefront. Massive amounts of data are analyzed to identify correlations. However, a common pitfall to this approach is confusing correlation with causation. Correlation implies but does not prove causation. Conclusions cannot be drawn from correlations until the underlying mechanisms that relate the data elements are understood. Without a suitable model relating the data, a correlation may simply be a coincidence.</p><div class="language-"><pre><code>» Correlation without
Causation
</code></pre></div><div class="language-"><pre><code>A common example of this
phenomenon is the high correlation
between ice cream consumption and
the murder rate during the summer
months. Does this mean ice cream
consumption causes murder or,
conversely, murder causes ice cream
consumption? Most likely not, but
you can see the danger in mistaking
correlation for causation. Our job as
Data Scientists is making sure we
understand the di$erence.
</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 43
</code></pre></div><h1 id="-6"><a class="header-anchor" href="#-6" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>Paul Yacci
</code></pre></div><h2 id="the-dangers-of-rejection"><a class="header-anchor" href="#the-dangers-of-rejection" aria-hidden="true">#</a> The Dangers of Rejection</h2><div class="language-"><pre><code>In the era of big
data, one piece
of analysis that
is frequently
overlooked is
the problem of
finding patterns
when there
are actually no
apparent patterns. In statistics
this is referred to as Type I error.
As scientists, we are always
on the lookout for a new or
interesting breakthrough that
could explain a phenomenon.
We hope to see a pattern in our
data that explains something
or that can give us an answer.
The primary goal of hypothesis
testing is to limit Type I error.
This is accomplished by using
small Į values. For example,
a Į value of 0.05 states that
there is a 1 in 20 chance that
the test will show that there
is something significant when
in actuality there isn’t. This
problem compounds when
testing multiple hypotheses.
When running multiple
hypothesis tests, we are likely
to encounter Type I error. As
more data becomes available
for analysis, Type I error
needs to be controlled.
</code></pre></div><div class="language-"><pre><code>One of my projects required
testing the difference between
the means of two microarray
data samples. Microarray
data contains thousands of
measurements but is limited
in the number of observations.
A common analysis approach
is to measure the same genes
under different conditions. If
there is a significant enough
difference in the amount of
gene expression between the
two samples, we can say that
the gene is correlated with a
particular phenotype. One way
to do this is to take the mean of
each phenotype for a particular
</code></pre></div><div class="language-"><pre><code>gene and formulate a hypothesis
to test whether there is a
significant difference between
the means. Given that we were
running thousands of these tests
at Į = 0.05, we found several
differences that were significant.
The problem was that some
of these could be caused by
random chance.
</code></pre></div><div class="language-"><pre><code>Many corrections exist to
control for false indications of
significance. The Bonferroni
correction is one of the most
conservative. This calculation
lowers the level below which you
will reject the null hypothesis
(your p value). The formula is
alpha/n , where n equals the
number of hypothesis tests
that you are running. Thus, if
you were to run 1,000 tests of
significance at Į= 0.05, your
p value should be less than
0.00005 (0.05/1,000) to reject the
null hypothesis. This is obviously
a much more stringent value.
A large number of the previously
significant values were no longer
significant, revealing the true
relationships within the data.
</code></pre></div><div class="language-"><pre><code>The corrected significance gave
us confidence that the observed
expression levels were due to
differences in the cellular gene
expression rather than noise. We
were able to use this information
to begin investigating what
proteins and pathways were
active in the genes expressing
the phenotype of interest. By
solidifying our understanding
of the causal relationships, we
focused our research on the
areas that could lead to new
discoveries about gene function
and, ultimately to improved
medical treatments.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><p>Reason and common sense are foundational to Data Science. Without it, data is simply a collection of bits. Context, inferences and models are created by humans and carry with them biases and assumptions. Blindly trusting your analyses is a dangerous thing that can lead to erroneous conclusions. When you approach an analytic challenge, you should always pause to ask yourself the following questions:</p><p>› <strong><em>What problem are we trying to solve?</em></strong> Articulate the answer as a sentence, especially when communicating with the end- user. Make sure that it sounds like an answer. For example, “Given a !xed amount of human capital, deploying people with these priorities will generate the best return on their time.”</p><p>› <strong><em>Does the approach make sense?</em></strong> Write out your analytic plan. Embrace the discipline of writing, as it brings structure to your thinking. Back of the envelope calculations are an existence proof of your approach. Without this kind of preparation, computers are power tools that can produce lots of bad answers really fast.</p><p>› <strong><em>Does the answer make sense?</em></strong> Can you explain the answer? Computers, unlike children, do what they are told. Make sure you spoke to it clearly by validating that the instructions you provided are the ones you intended. Document your assumptions and make sure they have not introduced bias in your work.</p><p>› <strong><em>Is it a &quot;nding or a mistake?</em></strong> Be skeptical of surprise !ndings. Experience says that it if seems wrong, it probably is wrong. Before you accept that conclusion, however, make sure you understand and can clearly explain why it is wrong.</p><div class="language-"><pre><code>› Does the analysis address the
original intent? Make sure
that you are not aligning the
answer with the expectations
of the client. Always speak
the truth, but remember that
answers of “your baby is ugly”
require more, not less, analysis.
</code></pre></div><div class="language-"><pre><code>› Is the story complete? #e goal
of your analysis is to tell an
actionable story. You cannot
rely on the audience to stitch
the pieces together. Identify
potential holes in your
story and !ll them to avoid
surprises. Grammar, spelling
and graphics matter; your
audience will lose con!dence
in your analysis if your results
look sloppy.
</code></pre></div><div class="language-"><pre><code>› Where would we head next?
No analysis is every !nished,
you just run out of resources.
Understand and explain what
additional measures could
be taken if more resources
are found.
</code></pre></div><div class="language-"><pre><code>» Tips From the Pros
</code></pre></div><div class="language-"><pre><code>Better a short pencil than a
long memory. End every day by
documenting where you are; you
may learn something along the way.
Document what you learned and why
you changed your plan.
</code></pre></div><div class="language-"><pre><code>» Tips From the Pros
</code></pre></div><div class="language-"><pre><code>Test your answers with a friendly
audience to make sure your findings
hold water. Red teams save careers.
</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 45
</code></pre></div><h3 id="component-parts-of"><a class="header-anchor" href="#component-parts-of" aria-hidden="true">#</a> Component Parts of</h3><h3 id="data-science-1"><a class="header-anchor" href="#data-science-1" aria-hidden="true">#</a> Data Science</h3><div class="language-"><pre><code>#ere is a web of components that interact to create your
solution space. Understanding how they are connected
is critical to your ability to engineer solutions to Data
Science problems.
</code></pre></div><div class="language-"><pre><code>#e components involved in any Data Science project fall into a
number of di$erent categories including the data types analyzed, the
analytic classes used, the learning models employed and the execution
models used to run the analytics. #e interconnection across these
components, shown in the !gure, Interconnection Among the Component
Parts of Data Science , speaks to the complexity of engineering Data
Science solutions. A choice made for one component exerts in&amp;uence
over choices made for others categories. For example, data types
lead the choices in analytic class and learning models, while latency,
timeliness and algorithmic parallelization strategy inform the
execution model. As we dive deeper into the technical aspects of
Data Science, we will begin with an exploration of these components
and touch on examples of each.
</code></pre></div><p><strong><em>Read this to get the quick and dirty:</em></strong></p><div class="language-"><pre><code>When engineering a Data
Science solution, work from an
understanding of the components
that de!ne the solution space.
Regardless of your analytic goal,
you must consider the data types
with which you will be working,
the classes of analytics you will use
to generate your data product,
</code></pre></div><div class="language-"><pre><code>how the learning models embodied
will operate and evolve, and the
execution models that will govern
how the analytic will be run.
You will be able to articulate a
complete Data Science solution
only after considering each of
these aspects.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>streamingdata
</code></pre></div><div class="language-"><pre><code>data
</code></pre></div><div class="language-"><pre><code>batch
</code></pre></div><div class="language-"><pre><code>data
</code></pre></div><div class="language-"><pre><code>structured
</code></pre></div><div class="language-"><pre><code>data
</code></pre></div><div class="language-"><pre><code>unstructured
</code></pre></div><div class="language-"><pre><code>analytics
</code></pre></div><div class="language-"><pre><code>transforming
</code></pre></div><div class="language-"><pre><code>analytics
</code></pre></div><div class="language-"><pre><code>learning
</code></pre></div><div class="language-"><pre><code>analyticspredictive
learningsupervised
</code></pre></div><div class="language-"><pre><code>learning
unsupervised
</code></pre></div><div class="language-"><pre><code>learning
</code></pre></div><div class="language-"><pre><code>online
</code></pre></div><div class="language-"><pre><code>learning
</code></pre></div><div class="language-"><pre><code>offline
</code></pre></div><p>execution</p><div class="language-"><pre><code>batch
</code></pre></div><div class="language-"><pre><code>execution
</code></pre></div><div class="language-"><pre><code>streaming
</code></pre></div><div class="language-"><pre><code>executionparallel
</code></pre></div><div class="language-"><pre><code>execution
serial
</code></pre></div><div class="language-"><pre><code>EXECUTION
MODELS
</code></pre></div><p>DATA TYPES</p><div class="language-"><pre><code>ANALYTIC
CLASSES
</code></pre></div><div class="language-"><pre><code>LEARNING
MODELS
</code></pre></div><div class="language-"><pre><code>Interconnection Among the Component Parts of Data Science
</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 47
</code></pre></div><h6 id="data-types"><a class="header-anchor" href="#data-types" aria-hidden="true">#</a> Data Types</h6><p>Data types and analytic goals go hand-in-hand much like the chicken and the egg; it is not always clear which comes !rst. Analytic goals are derived from business objectives, but the data type also in&amp;uences the goals. For example, the business objective of understanding consumer product perception drives the analytic goal of sentiment analysis. Similarly, the goal of sentiment analysis drives the selection of a text-like data type such as social media content. Data type also drives many other choices when engineering your solutions.</p><p>#ere are a number of ways to classify data. It is common to characterize data as <em>structured</em> or <em>unstructured</em>. Structured data exists when information is clearly broken out into !elds that have an explicit meaning and are highly categorical, ordinal or numeric. A related category, semi-structured, is sometimes used to describe structured data that does not conform to the formal structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers. Unstructured data, such as natural language text, has less clearly delineated meaning. Still images, video and audio often fall under the category of unstructured data. Data in this form requires preprocessing to identify and extract relevant ‘features.’ #e features are structured information that are used for indexing and retrieval, or training classi!cation, or clustering models.</p><p>Data may also be classi!ed by the rate at which it is generated, collected or processed. #e distinction is drawn between streaming data that arrives constantly like a torrent of water from a !re hose, and batch data, which arrives in buckets. While there is rarely a connection between data type and data rate, data rate has signi!cant in&amp;uence over the execution model chosen for analytic implementation and may also inform a decision of analytic class or learning model.</p><div class="language-"><pre><code>Take off the Training Wheels 49
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><h6 id="classes-of-analytic-techniques"><a class="header-anchor" href="#classes-of-analytic-techniques" aria-hidden="true">#</a> Classes of Analytic Techniques</h6><div class="language-"><pre><code>As a means for helping conceptualize the universe of possible analytic
techniques, we grouped them into nine basic classes. Note that
techniques from a given class may be applied in multiple ways to
achieve various analytic goals. Membership in a class simply indicates
a similar analytic function. #e nine analytic classes are shown in the
!gure, Classes of Analytic Techniques.
</code></pre></div><div class="language-"><pre><code>»  Transforming Analytics
› Aggregation : Techniques to summarize the data. #ese
include basic statistics (e.g., mean, standard deviation),
distribution !tting, and graphical plotting.
› Enrichment : Techniques for adding additional information
to the data, such as source information or other labels.
› Processing : Techniques that address data cleaning,
preparation, and separation. #is group also includes
common algorithm pre-processing activities such as
transformations and feature extraction.
</code></pre></div><div class="language-"><pre><code>»  Learning Analytics
› Regression : Techniques for estimating relationships among
variables, including understanding which variables are
important in predicting future values.
› Clustering : Techniques to segment the data into naturally
similar groups.
› Classi&quot;cation : Techniques to identify data element
group membership.
› Recommendation : Techniques to predict the rating or
preference for a new entity, based on historic preference
or behavior.
</code></pre></div><div class="language-"><pre><code>»  Predictive Analytics
› Simulation : Techniques to imitate the operation of a real-
world process or system. #ese are useful for predicting
behavior under new conditions.
› Optimization : Operations Research techniques focused on
selecting the best element from a set of available alternatives
to maximize a utility function.
</code></pre></div><div class="language-"><pre><code>Aggregation Enrichment Processing Regression Clustering Classification Recommend Simulation Optimization
</code></pre></div><div class="language-"><pre><code>TRANSFORMING LEARNING PREDICTIVE
</code></pre></div><div class="language-"><pre><code>Classes of Analytic Techniques
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><h6 id="learning-models"><a class="header-anchor" href="#learning-models" aria-hidden="true">#</a> Learning Models</h6><p>Analytic classes that perform predictions such as regression, clustering, classi!cation, and recommendation employ learning models. #ese models characterize how the analytic is trained to perform judgments on new data based on historic observation. Aspects of learning models describe both the types of judgments performed and how the models evolve over time, as shown in the !gure, <em>Analytic Learning Models</em>.</p><p>#e learning models are typically described as employing unsupervised or supervised learning. Supervised learning takes place when a model is trained using a labeled data set that has a known class or category associated with each data element. #e model relates the features found in training instances with the labels so that predictions can be made for unlabeled instances. Unsupervised learning models have no a-priori knowledge about the classes into which data can be placed. #ey use the features in the dataset to form groupings based on feature similarity.</p><p>A useful distinction of learning models is between those that are trained in a single pass, which are known as o&#39;ine models, and those that are trained incrementally over time, known as online models. Many learning approaches have online or o&#39;ine variants. #e decision to use one or another is based on the analytic goals and execution models chosen.</p><p>Generating an o&#39;ine model requires taking a pass over the entire training data set. Improving the model requires making separate passes over the data. #ese models are static in that once trained, their predictions will not change until a new model is created through a subsequent training stage. O&#39;ine model performance is easier to evaluate due to this deterministic behavior. Deployment of the model into a production environment involves swapping out the old model for the new.</p><p>Online models have both advantages and disadvantages. #ey dynamically evolve over time, meaning they only require a single deployment into a production setting. #e fact that these models do not have the entire dataset available when being trained, however, is a challenge. #ey must make assumptions about the data based</p><div class="language-"><pre><code>TRAINING STYLE
</code></pre></div><div class="language-"><pre><code>Unsupervised Supervised Offline Online
</code></pre></div><div class="language-"><pre><code>LEARNING STYLE
</code></pre></div><div class="language-"><pre><code>Analytic Learning Models
</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 51
</code></pre></div><div class="language-"><pre><code>on the examples observed; these assumptions may be sub-optimal.
#is can be o$set somewhat in cases where feedback on the model’s
predictions is available since online models can rapidly incorporate
feedback to improve performance.
</code></pre></div><h6 id="execution-models"><a class="header-anchor" href="#execution-models" aria-hidden="true">#</a> Execution Models</h6><div class="language-"><pre><code>Execution models describe how data is manipulated to perform
an analytic function. #ey may be categorized across a number
of dimensions. Execution Models are embodied by an execution
framework, which orchestrates the sequencing of analytic
computation. In this sense, a framework might be as simple as a
programming language runtime, such as the Python interpreter, or
a distributed computing framework that provides a speci!c API for
one or more programming languages such as Hadoop, MapReduce
or Spark. Grouping execution models based on how they handle data
is common, classifying them as either batch or streaming execution
models. #e categories of execution model are shown in the !gure,
Analytic Execution Models.
</code></pre></div><div class="language-"><pre><code>Analytic Execution Models
</code></pre></div><div class="language-"><pre><code>A batch execution model implies that data is analyzed in large
segments, that the analytic has a state where it is running and a state
where it is not running and that little state is maintained in memory
between executions. Batch execution may also imply that the analytic
produces results with a frequency on the order of several minutes or
more. Batch workloads tend to be fairly easy to conceptualize because
they represent discrete units of work. As such, it is easy to identify
a speci!c series of execution steps as well as the proper execution
frequency and time bounds based on the rate at which data arrives.
Depending on the algorithm choice, batch execution models are
easily scalable through parallelism. #ere are a number of frameworks
that support parallel batch analytic execution. Most famously,
Hadoop provides a distributed batch execution model in its
MapReduce framework.
</code></pre></div><div class="language-"><pre><code>Conversely, a streaming model analyzes data as it arrives. Streaming
execution models imply that under normal operation, the analytic
is always executing. #e analytic can hold state in memory and
</code></pre></div><div class="language-"><pre><code>SEQUENCING
</code></pre></div><div class="language-"><pre><code>Batch Streaming Serial Parallel
</code></pre></div><div class="language-"><pre><code>SCHEDULING
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><p>constantly deliver results as new data arrives, on the order of seconds or less. Many of the concepts in streaming are inherent in the Unix- pipeline design philosophy; processes are chained together by linking the output of one process to the input of the next. As a result, many developers are already familiar with the basic concepts of streaming. A number of frameworks are available that support the parallel execution of streaming analytics such as Storm, S4 and Samza.</p><p>#e choice between batch and streaming execution models often hinges on analytic latency and timeliness requirements. Latency refers to the amount of time required to analyze a piece of data once it arrives at the system, while timeliness refers to the average age of an answer or result generated by the analytic system. For many analytic goals, a latency of hours and timeliness of days is acceptable and thus lend themselves to the implementation enabled by the batch approach. Some analytic goals have up-to-the-second requirements where a result that is minutes old has little worth. #e streaming execution model better supports such goals.</p><p>Batch and streaming execution models are not the only dimensions within which to categorize analytic execution methods. Another distinction is drawn when thinking about scalability. In many cases, scale can be achieved by spreading computation over a number of computers. In this context, certain algorithms require a large shared memory state, while others are easily parallelizable in a context where no shared state exists between machines. #is distinction has signi!cant impacts on both software and hardware selection when building out a parallel analytic execution environment.</p><div class="language-"><pre><code>» Tips From the Pros
</code></pre></div><div class="language-"><pre><code>In order to understand system capacity
in the context of streaming analytic
execution, collect metrics including:
the amount of data consumed, data
emitted, and latency. This will help
you understand when scale limits
are reached.
</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 53
</code></pre></div><h3 id="fractal-analytic-model"><a class="header-anchor" href="#fractal-analytic-model" aria-hidden="true">#</a> Fractal Analytic Model</h3><div class="language-"><pre><code>Data Science analytics are a lot like broccoli.
</code></pre></div><div class="language-"><pre><code>Fractals are mathematical sets that display self-similar patterns. As
you zoom in on a fractal, the same patterns reappear. Imagine a stalk
of broccoli. Rip o$ a piece of broccoli and the piece looks much like
the original stalk. Progressively smaller pieces of broccoli still look like
the original stalk.
</code></pre></div><div class="language-"><pre><code>Data Science analytics are a lot like broccoli – fractal in nature in
both time and construction. Early versions of an analytic follow the
same development process as later versions. At any given iteration, the
analytic itself is a collection of smaller analytics that often decompose
into yet smaller analytics.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Setup Try
</code></pre></div><div class="language-"><pre><code>Evaluate
</code></pre></div><div class="language-"><pre><code>Do
</code></pre></div><div class="language-"><pre><code>Evaluate
</code></pre></div><h6 id="iterative-by-nature"><a class="header-anchor" href="#iterative-by-nature" aria-hidden="true">#</a> Iterative by Nature</h6><p>Good Data Science is fractal in time — an iterative process. Getting an imperfect solution out the door quickly will gain more interest from stakeholders than a perfect solution that is never completed. #e !gure, <em>!e Data Science Product Lifecycle,</em> summarizes the lifecycle of the Data Science product.</p><p><em>Set up</em> the infrastructure, aggregate and prepare the data, and incorporate domain expert knowledge. <em>Try</em> di$erent analytic techniques and models on subsets of the data. <em>Evaluate</em> the models, re!ne, evaluate again, and select a model. <em>Do</em> something with your models and results – deploy the models to inform, inspire action, and act. <em>Evaluate</em> the business results to improve the overall product.</p><div class="language-"><pre><code>The Data Science Product Lifecycle
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 55
</code></pre></div><h6 id="smaller-pieces-of-broccoli-a-data"><a class="header-anchor" href="#smaller-pieces-of-broccoli-a-data" aria-hidden="true">#</a> Smaller Pieces of Broccoli: A Data</h6><h6 id="science-product"><a class="header-anchor" href="#science-product" aria-hidden="true">#</a> Science Product</h6><div class="language-"><pre><code>Components inside and outside of the Data Science product will
change with each iteration. Let’s take a look under the hood of a
Data Science product and examine the components during one
such iteration.
</code></pre></div><div class="language-"><pre><code>In order to achieve a greater analytic goal, you need to !rst decompose
the problem into sub-components to divide and conquer. #e !gure,
!e Fractal Analytic Model , shows a decomposition of the Data Science
product into four component pieces.
</code></pre></div><div class="language-"><pre><code>GOAL
! Describe
! Discover
! Predict
! Advise
</code></pre></div><div class="language-"><pre><code>ACTION
! Productization
! Data Monetization
! Insights &amp; Relationships
</code></pre></div><div class="language-"><pre><code>DATA
! Text
! Imagery
! Waveform
! Geo
! Time Series
</code></pre></div><div class="language-"><pre><code>COMPUTATION
</code></pre></div><div class="language-"><pre><code>Aggregation
</code></pre></div><div class="language-"><pre><code>Enrichment
</code></pre></div><div class="language-"><pre><code>Clustering
</code></pre></div><div class="language-"><pre><code>Classification
</code></pre></div><div class="language-"><pre><code>CLASSES OF ANALYTICS
</code></pre></div><div class="language-"><pre><code>The Fractal Analytic Model
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><p><strong>Goal</strong></p><p>You must !rst have some idea of your analytic goal and the end state of the analysis. Is it to Discover, Describe, Predict, or Advise? It is probably a combination of several of those. Be sure that before you start, you de!ne the business value of the data and how you plan to use the insights to drive decisions, or risk ending up with interesting but non-actionable trivia.</p><p><strong>Data</strong></p><p>Data dictates the potential insights that analytics can provide. Data Science is about !nding patterns in variable data and comparing those patterns. If the data is not representative of the universe of events you wish to analyze, you will want to collect that data through carefully planned variations in events or processes through A/B testing or design of experiments. Data sets are never perfect so don’t wait for perfect data to get started. A good Data Scientist is adept at handling messy data with missing or erroneous values. Just make sure to spend the time upfront to clean the data or risk generating garbage results.</p><p><strong>Computation</strong></p><p>Computation aligns the data to goals through the process of creating insights. #rough divide and conquer, computation decomposes into several smaller analytic capabilities with their own goals, data, computation and resulting actions, just like a smaller piece of broccoli maintains the structure of the original stalk. In this way, computation itself is fractal. Capability building blocks may utilize di$erent types of execution models such as batch computation or streaming, that individually accomplish small tasks. When properly combined together, the small tasks produce complex, actionable results.</p><p><strong>Action</strong></p><p>How should engineers change the manufacturing process to generate higher product yield? How should an insurance company choose which policies to o$er to whom and at what price? #e output of computation should enable actions that align to the goals of the data product. Results that do not support or inspire action are nothing but interesting trivia.</p><p>Given the fractal nature of Data Science analytics in time and construction, there are many opportunities to choose fantastic or shoddy analytic building blocks. <em>!e Analytic Selection Process</em> o$ers some guidance.</p><div class="language-"><pre><code>Take off the Training Wheels 57
</code></pre></div><h3 id="e-analytic"><a class="header-anchor" href="#e-analytic" aria-hidden="true">#</a> #e Analytic</h3><h3 id="selection-process"><a class="header-anchor" href="#selection-process" aria-hidden="true">#</a> Selection Process</h3><div class="language-"><pre><code>If you focus only on the science aspect of Data Science you will
never become a data artist.
</code></pre></div><div class="language-"><pre><code>A critical step in Data Science is to identify an analytic technique that
will produce the desired action. Sometimes it is clear; a characteristic
of the problem (e.g., data type) points to the technique you should
implement. Other times, however, it can be di%cult to know where
to begin. #e universe of possible analytic techniques is large. Finding
your way through this universe is an art that must be practiced. We
are going to guide you on the next portion of your journey - becoming
a data artist.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h6 id="decomposing-the-problem"><a class="header-anchor" href="#decomposing-the-problem" aria-hidden="true">#</a> Decomposing the Problem</h6><p>Decomposing the problem into manageable pieces is the !rst step in the analytic selection process. Achieving a desired analytic action often requires combining multiple analytic techniques into a holistic, end-to-end solution. Engineering the complete solution requires that the problem be decomposed into progressively smaller sub-problems.</p><p>#e <em>Fractal Analytic Model</em> embodies this approach. At any given stage, the analytic itself is a collection of smaller computations that decompose into yet smaller computations. When the problem is decomposed far enough, only a single analytic technique is needed to achieve the analytic goal. Problem decomposition creates multiple sub-problems, each with their own goals, data, computations, and actions. #e concept behind problem decomposition is shown in the !gure, <em>Problem Decomposition Using the Fractal Analytic Model</em>.</p><div class="language-"><pre><code>GOAL
! Describe
! Discover
! Predict
! Advise
</code></pre></div><div class="language-"><pre><code>ACTION
! Productization
! Data Monetization
! Insights &amp; Relationships
</code></pre></div><div class="language-"><pre><code>DATA
! Text
! Imagery
! Waveform
! Geo
! Time Series
</code></pre></div><div class="language-"><pre><code>DATA
</code></pre></div><div class="language-"><pre><code>GOAL
</code></pre></div><div class="language-"><pre><code>ACTION
</code></pre></div><div class="language-"><pre><code>DATA
</code></pre></div><div class="language-"><pre><code>GOAL
</code></pre></div><div class="language-"><pre><code>ACTION
</code></pre></div><div class="language-"><pre><code>Aggregation
</code></pre></div><div class="language-"><pre><code>Enrichment
</code></pre></div><div class="language-"><pre><code>Clustering
</code></pre></div><div class="language-"><pre><code>Classification
</code></pre></div><div class="language-"><pre><code>CLASSES OF ANALYTICS
</code></pre></div><div class="language-"><pre><code>Problem Decomposition Using the Fractal Analytic Model
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 59
</code></pre></div><div class="language-"><pre><code>On the surface, problem decomposition appears to be a mechanical,
repeatable process. While this may be true conceptually, it is
really the performance of an art as opposed to the solving of an
engineering problem. #ere may be many valid ways to decompose
the problem, each leading to a di$erent solution. #ere may be
hidden dependencies or constraints that only emerge after you begin
developing a solution. #is is where art meets science. Although the
art behind problem decomposition cannot be taught, we have distilled
some helpful hints to help guide you. When you begin to think about
decomposing your problem, look for:
</code></pre></div><div class="language-"><pre><code>› Compound analytic goals that create natural segmentation.
For example, many problems focused on predicting future
conditions include both Discover and Predict goals.
› Natural orderings of analytic goals. For example, when extracting
features you must !rst identify candidate features and then
select the features set with the highest information value. #ese
two activities form distinct analytic goals.
› Data types that dictate processing activities. For example, text or
imagery both require feature extraction.
› Requirements for human-in-the-loop feedback. For example,
when developing alerting thresholds, you might need to solicit
analyst feedback and update the threshold based on their
assessment.
› #e need to combine multiple data sources. For example, you may
need to correlate two data sets to achieve your broader goal.
Often this indicates the presence of a Discover goal.
</code></pre></div><div class="language-"><pre><code>In addition to problem decomposition providing a tractable approach
to analytic selection, it has the added bene!t of simplifying a highly
complex problem. Rather than being faced with understanding the
entire end-to-end solution, the computations are discrete segments
that can be explored. Note, however, that while this technique helps
the Data Scientist approach the problem, it is the complete end-to-
end solution that must be evaluated.
</code></pre></div><div class="language-"><pre><code>» Tips From the Pros
</code></pre></div><div class="language-"><pre><code>One of your first steps should be to
explore available data sources that
have not been previously combined.
Emerging relationships between data
sources often allow you to pick low
hanging fruit.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Compare
List of recently Datasets
registered
company
domains
</code></pre></div><div class="language-"><pre><code>List of
candidate
spoofed
domains
List of
recently
registered
company
domains
</code></pre></div><div class="language-"><pre><code>Discover spoofed
domains
</code></pre></div><div class="language-"><pre><code>Test &amp;
Evaluation
</code></pre></div><div class="language-"><pre><code>Test &amp;
Evaluation
</code></pre></div><div class="language-"><pre><code>Test &amp;
Evaluation
</code></pre></div><div class="language-"><pre><code>Calculate
Metric
</code></pre></div><div class="language-"><pre><code>Set
Threshold
</code></pre></div><div class="language-"><pre><code>Data
Collection
</code></pre></div><div class="language-"><pre><code>Simulate
Spoofed
Data
</code></pre></div><div class="language-"><pre><code>Store
Generated
Domains
Generate
Candidate
Domains
</code></pre></div><div class="language-"><pre><code>Alert on spoofed domains to
provide opportuinity to
minimize brand image and
consumer confidence damage
</code></pre></div><div class="language-"><pre><code>Discover likely
candidates for
spoofed domains
</code></pre></div><div class="language-"><pre><code>List of candidate
spoofed domains
</code></pre></div><div class="language-"><pre><code>List of recently
registered
company domains
</code></pre></div><div class="language-"><pre><code>Describe closeness of
spoof to valid domains
</code></pre></div><div class="language-"><pre><code>Quantitative
measure of feature
information value
</code></pre></div><div class="language-"><pre><code>Threshold that balances
false positive and false
negative rate
</code></pre></div><div class="language-"><pre><code>Quantitative threshold for
automated result ranking
</code></pre></div><div class="language-"><pre><code>!
</code></pre></div><div class="language-"><pre><code>!
</code></pre></div><div class="language-"><pre><code>List of candidate spoofed
domains
List of recently registered
company domains
Quantitative measure of
feature information value
</code></pre></div><div class="language-"><pre><code>!
!
!
</code></pre></div><div class="language-"><pre><code>Stephanie
Rivera
</code></pre></div><h2 id="identifying-spoofed-domains"><a class="header-anchor" href="#identifying-spoofed-domains" aria-hidden="true">#</a> Identifying Spoofed Domains</h2><div class="language-"><pre><code>Identifying spoofed domains is important for an organization
to preserve their brand image and to avoid eroded customer
confidence. Spoofed domains occur when a malicious actor
creates a website, URL or email address that users believe is
associated with a valid organization. When users click the link,
visit the website or receive emails, they are subjected to some
type of nefarious activity.
</code></pre></div><p>Our team was faced with the problem of identifying spoofed domains for a commercial company. On the surface, the problem sounded easy; take a recently registered domain, check to see if it is similar to the company’s domain and alert when the similarity is sufficiently high. Upon decomposing the problem, however, the main computation quickly became complicated.</p><p>We needed a computation that determined similarity between two domains. As we decomposed the similarity computation, complexity and speed became a concern. As with many security-related problems, fast</p><div class="language-"><pre><code>alert speeds are vital. Result speed created
an implementation constraint that forced us to
re-evaluate how we decomposed the problem.
</code></pre></div><div class="language-"><pre><code>Revisiting the decomposition process led us
to a completely new approach. In the end,
we derived a list of domains similar to those
registered by the company. We then compared
that list against a list of recently registered
domains. The figure, Spoofed Domain Problem
Decomposition, illustrates our approach. Upon
testing and initial deployment, our analytic
discovered a spoofed domain within 48 hours.
</code></pre></div><h1 id="-7"><a class="header-anchor" href="#-7" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>Spoofed Domain Problem Decomposition
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 61
</code></pre></div><div class="language-"><pre><code>SPEED: The speed at which an
analytic outcome must be produced
(e.g., near real-time, hourly, daily) or the time
it takes to develop and implement the
analytic solution
</code></pre></div><div class="language-"><pre><code>ANALYTIC COMPLEXITY:
Algorithmic complexity (e.g., complexity class
and execution resources)
</code></pre></div><div class="language-"><pre><code>ACCURACY &amp; PRECISION: The ability
to produce exact versus approximate
solutions as well as the ability to provide a
PHDVXUHRIFRQȌGHQFH
</code></pre></div><div class="language-"><pre><code>DATA SIZE: The size of the data set
(e.g., number of rows)
</code></pre></div><div class="language-"><pre><code>DATA COMPLEXITY: The data
type, formal complexity measures
including measures of overlap and
linear separability, number of
dimensions /columns, and linkages
between data sets
</code></pre></div><div class="language-"><pre><code>SPEED
</code></pre></div><div class="language-"><pre><code>ANALYTIC
COMPLEXITY
</code></pre></div><div class="language-"><pre><code>DATA
COMPLEXITY
</code></pre></div><div class="language-"><pre><code>ACCURACY
&amp;
PRECISION
</code></pre></div><div class="language-"><pre><code>DATA SIZE
</code></pre></div><h6 id="implementation-constraints"><a class="header-anchor" href="#implementation-constraints" aria-hidden="true">#</a> Implementation Constraints</h6><p>In the spoofed domains case study, the emergence of an implementation constraint caused the team to revisit its approach. #is demonstrates that analytic selection does not simply mean choosing an analytic technique to achieve a desired outcome. It also means ensuring that the solution is feasible to implement.</p><p>#e Data Scientist may encounter a wide variety of implementation constraints. #ey can be conceptualized, however, in the context of !ve dimensions that compete for your attention: analytic complexity, speed, accuracy &amp; precision, data size, and data complexity. Balancing these dimensions is a zero sum game - an analytic solution cannot simultaneously exhibit all !ve dimensions, but instead must make trades between them. #e !gure, <em>Balancing the Five Analytic Dimensions,</em> illustrates this relationship.</p><p>Implementation constraints occur when an aspect of the problem dictates the value for one or more of these dimensions. As soon as one dimension is !xed, the Data Scientist is forced to make trades among the others. For example, if the analytic problem requires actions to be produced in near real-time, the speed dimension is !xed and trades must be made among the other four dimensions. Understanding which trades will achieve the right balance among the !ve dimensions is an art that must be learned over time.</p><div class="language-"><pre><code>As we compiled this section, we
talked extensively about ways to
group and classify implementation
constraints. After much discussion
we settled on these !ve dimensions.
We present this model in hopes
that others weigh in and o$er
their own perspectives.
</code></pre></div><div class="language-"><pre><code>Balancing the Five Analytic Dimensions
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 63
</code></pre></div><div class="language-"><pre><code>Some common examples of implementation
constraints include:
</code></pre></div><p><strong><em>- Computation frequency.</em></strong> #e solution may need to run on a regular basis (e.g., hourly), requiring that computations be completed within a speci!ed window of time. #e best analytic is useless if it cannot solve the problem within the required time. <strong><em>- Solution timeliness.</em></strong> Some applications require near real-time results, pointing toward streaming approaches. While some algorithms can be implemented within streaming frameworks, many others cannot. <strong><em>- Implementation speed.</em></strong> A project may require that you rapidly develop and implement a solution to quickly produce analytic insights. In these cases, you may need to focus on less complex techniques that can be quickly implemented and veri!ed. <strong><em>- Computational resource limitations.</em></strong> Although you may be able to store and analyze your data, data size may be su%ciently large that algorithms requiring multiple computations across the full data set are too resource intensive. #is may point toward needing approaches that only require a single pass on the data (e.g., canopy cluster as opposed to k-means clustering). <strong><em>- Data storage limitations</em></strong>. #ere are times when big data becomes so big it cannot be stored or only a short time horizon can be stored. Analytic approaches that require long time horizons may not be possible.</p><div class="language-"><pre><code>Organizational policies and regulatory requirements are a major
source of implicit constraints that merit a brief discussion. Policies
are often established around speci!c classes of data such as Personally
Identi!able Information (PII) or Personal Health Information (PHI).
While the technologies available today can safely house information
with a variety of security controls in a single system, these policies
force special data handling considerations including limited retention
periods and data access. Data restrictions impact the data size and
complexity dimensions outlined earlier, creating yet another layer of
constraints that must be considered.
</code></pre></div><div class="language-"><pre><code>» Tips From the Pros
</code></pre></div><div class="language-"><pre><code>When possible, consider approaches
that make use of previously computed
results. Your algorithm will run
much faster if you can avoid
re-computing values across
the full time horizon of data.
</code></pre></div><div class="language-"><pre><code>» Tips From the Pros
</code></pre></div><div class="language-"><pre><code>Our Data Science Product Lifecycle
has evolved to produce results quickly
and then incrementally improve
the solution.
</code></pre></div><div class="language-"><pre><code>» Tips From the Pros
</code></pre></div><div class="language-"><pre><code>Streaming approaches may be useful
for overcoming storage limitations.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h2 id="_4"><a class="header-anchor" href="#_4" aria-hidden="true">#</a> 4</h2><h5 id="data"><a class="header-anchor" href="#data" aria-hidden="true">#</a> Data</h5><h5 id="science"><a class="header-anchor" href="#science" aria-hidden="true">#</a> Science</h5><h2 id="_3"><a class="header-anchor" href="#_3" aria-hidden="true">#</a> 3</h2><div class="language-"><pre><code>DESCRIBE
</code></pre></div><div class="language-"><pre><code>DISCOVER
</code></pre></div><div class="language-"><pre><code>PREDICT
</code></pre></div><div class="language-"><pre><code>ADVISE
</code></pre></div><div class="language-"><pre><code>PAGE 67
</code></pre></div><div class="language-"><pre><code>PAGE 68
</code></pre></div><div class="language-"><pre><code>PAGE 69
</code></pre></div><div class="language-"><pre><code>PAGE 70
</code></pre></div><h2 id="_2"><a class="header-anchor" href="#_2" aria-hidden="true">#</a> 2</h2><h2 id="_1"><a class="header-anchor" href="#_1" aria-hidden="true">#</a> 1</h2><h3 id="guide-to-analytic-selection"><a class="header-anchor" href="#guide-to-analytic-selection" aria-hidden="true">#</a> Guide to Analytic Selection</h3><div class="language-"><pre><code>Your senses are incapable of perceiving the entire universe, so
we drew you a map.
</code></pre></div><div class="language-"><pre><code>#e universe of analytic techniques is vast and hard to comprehend.
We created this diagram to aid you in !nding your way from data
and goal to analytic action. Begin at the center of the universe
(Data Science) and answer questions about your analytic goals and
problem characteristics. #e answers to your questions will guide you
through the diagram to the appropriate class of analytic techniques
and provide recommendations for a few techniques to consider.
</code></pre></div><div class="language-"><pre><code>n TIP: There are several situations where dimensionality reduction may be needed:
› Models fail to converge
› Models produce results equivalent to
random chance
› You lack the computational power to
</code></pre></div><div class="language-"><pre><code>perform operations across the
feature space
› You do not know which aspects of the
data are the most important
o Feature Extraction is a broad topic and is highly dependent upon the domain area.
This topic could be the subject of an entire book. As a result, a detailed exploration
has been omitted from this diagram. See the Featuring Engineering and Feature
Selection sections in the Life in the Trenches chapter for additional information.
p TIP: Always check data labels for correctness. This is particularly true for time
stamps, which may have reverted to system default values.
q TIP: Smart enrichment can greatly speed computational time. It can also be a huge
differentiator between the accuracy of different end-to-end analytic solutions.
Source: Booz Allen Hamilton
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Data
Science
</code></pre></div><div class="language-"><pre><code>FILTERING
How do I identify
data based on
its absolute or
relative values?
</code></pre></div><div class="language-"><pre><code>IMPUTATION
How do I fill in
missing values
in my data?
</code></pre></div><div class="language-"><pre><code>DIMENSIONALITY
REDUCTION n
How do I reduce
the number of
dimensions
in my data?
</code></pre></div><div class="language-"><pre><code>NORMALIZATION &amp;
TRANSFORMATION
How do I reconcile
duplication
representations
in the data?
</code></pre></div><div class="language-"><pre><code>FEATURE
EXTRACTION o
</code></pre></div><div class="language-"><pre><code>PROCESSING p
How do I clean
and separate
my data?
</code></pre></div><div class="language-"><pre><code>If you want to add or remove data based on its value, start with:
! Relational algebra projection and selection
If early results are uninformative and duplicative, start with:
! Outlier removal^! Gaussian filter
! Exponential smoothing! Median filter
</code></pre></div><div class="language-"><pre><code>If you are unfamiliar with the data set, start with
basic statistics:
! Count! Standard deviation! Box plots
! Mean! Range! Scatter plots
If your approach assumes the data follows a
distribution, start with:
! Distribution fitting
If you want to understand all the information
available on an entity, start with:
! “Baseball card” aggregation
</code></pre></div><div class="language-"><pre><code>If you need to keep track of source information or other
user-defined parameters, start with:
! Annotation
If you often process certain data fields together or use
one field to compute the value of another, start with:
! Relational algebra rename,
! Feature addition (e.g., Geography, Technology, Weather)
</code></pre></div><div class="language-"><pre><code>If you want to generate values from other observations in your data set, start with:
! Random sampling
! Markov Chain Monte Carlo (MC)
If you want to generate values without using other observations in your data set,
start with:
! Mean! Regression models
! Statistical distributions
</code></pre></div><div class="language-"><pre><code>If you need to determine whether there is multi-dimensional correlation,
start with:
! PCA and other factor analysis
If you can represent individual observations by membership in a group, start with:
! K-means clustering
! Canopy clustering
If you have unstructured text data, start with:
! Term Frequency/Inverse Document Frequency (TF IDF)
If you have a variable number of features but your
algorithm requires a fixed number, start with:
! Feature hashing
If you are not sure which features are the most important, start with:
! Wrapper methods
! Sensitivity analysis
If you need to facilitate understanding of the
probability distribution of the space, start with:
! Self organizing maps
</code></pre></div><div class="language-"><pre><code>If you suspect duplicate data elements, start with:
! Deduplication
If you want your data to fall within a specified range, start with:
! Normalization
If your data is stored in a binary format, start with:
! Format Conversion
If you are operating in frequency space, start with:
! Fast Fourier Transform (FFT),
! Discrete wavelet transform
If you are operating in Euclidian space, start with:
! Coordinate transform
</code></pre></div><div class="language-"><pre><code>DESCRIBE
How do I develop
an understanding
of the content of
my data?
</code></pre></div><div class="language-"><pre><code>ENRICHMENT q
How do I add
new information
to my data?
</code></pre></div><div class="language-"><pre><code>AGGREGATION
How do I collect
and summarize
my data?
</code></pre></div><p><strong>1</strong></p><p><strong>2</strong></p><p><strong>4</strong></p><div class="language-"><pre><code>3
PREDICT
</code></pre></div><div class="language-"><pre><code>DISCOVER
</code></pre></div><div class="language-"><pre><code>ADVISE
</code></pre></div><p></p><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>If you want an ordered set of clusters with variable precision, start with:
! Hierarchical
r If you have an unknown number of clusters, start with:
! X-means
! Canopy
If you have text data, start with:
! Topic modeling
If you have non-elliptical clusters, start with:
! Fractal
! DB Scan
If you want soft membership in the clusters, start with:
! Gaussian mixture models
s If you have an known number of clusters, start with:
! K-means
</code></pre></div><div class="language-"><pre><code>If your data has unknown structure, start with:
! Tree-based methods
If statistical measures of importance are needed,
start with:
! Generalized linear models
If statistical measures of importance are not needed,
start with:
! Regression with shrinkage (e.g., LASSO, Elastic net)
! Stepwise regression
</code></pre></div><div class="language-"><pre><code>2
DISCOVER
What are the
key relationships
in the data?
</code></pre></div><div class="language-"><pre><code>REGRESSION
How do I
determine which
variables may be
important?
</code></pre></div><div class="language-"><pre><code>CLUSTERING
How do I segment
the data to
find natural
groupings?
</code></pre></div><div class="language-"><pre><code>1
DESCRIBE
</code></pre></div><div class="language-"><pre><code>Data
Science
</code></pre></div><p><strong>4</strong></p><div class="language-"><pre><code>3
PREDICT
</code></pre></div><div class="language-"><pre><code>ADVISE
</code></pre></div><div class="language-"><pre><code>r TIP: Canopy clustering is good when you only want to make a single pass
over the data.
s TIP: Use canopy or hierarchical clustering to estimate the number of clusters
you should generate.
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>If you are unsure of feature importance, start with:
! Neural nets,
! Random forests
If you require a highly transparent model, start with:
! Decision trees
If you have &lt;20 data dimensions, start with:
! K-nearest neighbors
If you have a large data set with an unknown classification signal, start with:
! Naive Bayes
If you want to estimate an unobservable state based on observable variables, start with:
! Hidden Markov model
u If you don&#39;t know where else to begin, start with:
! Support Vector Machines (SVM)
! Random forests
</code></pre></div><div class="language-"><pre><code>If the data structure is unknown, start with:
! Tree-based methods
If you require a highly transparent model, start with:
! Generalized linear models
If you have &lt;20 data dimensions, start with:
! K-nearest neighbors
</code></pre></div><div class="language-"><pre><code>PREDICT
What are the
likely future
outcomes?
</code></pre></div><div class="language-"><pre><code>REGRESSION
How do I predict
a future value?
</code></pre></div><div class="language-"><pre><code>If you only have knowledge of how people interact with items,
start with:
! Collaborative filtering
If you have a feature vector of item characteristics, start with:
! Content-based methods
If you only have knowledge of how items are connected to one
another, start with:
! Graph-based methods
</code></pre></div><div class="language-"><pre><code>RECOMMENDATION v
How do I predict
relevant conditions?
</code></pre></div><p><strong>CLASSIFICATION</strong> t How do I predict group membership?</p><p><strong>3</strong></p><div class="language-"><pre><code>2
DISCOVER
</code></pre></div><div class="language-"><pre><code>1
DESCRIBE
</code></pre></div><div class="language-"><pre><code>Data
Science
</code></pre></div><div class="language-"><pre><code>4
ADVISE
</code></pre></div><div class="language-"><pre><code>t TIP: It can be difficult to predict which classifier will work best on your data set.
Always try multiple classifiers. Pick the one or two that work the best to refine and
explore further.
u TIP: These are our favorite, go-to classification algorithms.
v TIP: Be careful of the &quot;recommendation bubble&quot;, the tendency of recommenders to
recommend only what has been seen in the past. You must ensure you add diversity to
avoid this phenomenon.
v TIP: SVD and PCA are good tools for creating better features for recommenders.
Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>If your problem is represented by a
non-deterministic utility function, start with:
! Stochastic search
If approximate solutions are acceptable,
start with:
! Genetic algorithms
! Simulated annealing
! Gradient search
If your problem is represented by a
deterministic utility function, start with:
! Linear programming
! Integer programming
! Non-linear programming
</code></pre></div><div class="language-"><pre><code>ADVISE
What course
of action
should I take?
</code></pre></div><div class="language-"><pre><code>OPTIMIZATION
How do I identify the
best course of action
when my objective
can be expressed as
a utility function?
</code></pre></div><div class="language-"><pre><code>If you must model discrete entities, start with:
! Discrete Event Simulation (DES)
If there are a discrete set of possible states, start with:
! Markov models
If there are actions and interactions among autonomous
entities, start with:
! Agent-based simulation
If you do not need to model discrete entities, start with:
! Monte Carlo simulation
If you are modeling a complex system with feedback
mechanisms between actions, start with:
! Systems dynamics
If you require continuous tracking of system behavior,
start with:
! Activity-based simulation
If you already have an understanding of what factors
govern the system, start with:
! ODES
! PDES
</code></pre></div><div class="language-"><pre><code>RECOMMENDATION 
How do I predict
relevant conditions?
</code></pre></div><p><strong>4</strong></p><div class="language-"><pre><code>2
DISCOVER
</code></pre></div><div class="language-"><pre><code>1
DESCRIBE
</code></pre></div><div class="language-"><pre><code>Data
Science
</code></pre></div><div class="language-"><pre><code>3
PREDICT
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Compiled by: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Algorithms or
Method Name Description Tips From the Pros
</code></pre></div><div class="language-"><pre><code>References and Papers
We Love to Read
</code></pre></div><div class="language-"><pre><code>Agent Based
Simulation
</code></pre></div><div class="language-"><pre><code>Simulates the actions
and interactions of
autonomous agents.
</code></pre></div><div class="language-"><pre><code>In many systems, complex behavior
results from surprisingly simple rules.
Keep the logic of your agents simple
and gradually build in sophistication.
</code></pre></div><div class="language-"><pre><code>Macal, Charles, and Michael
North. “Agent-based Modeling
and Simulation.” Winter
Simulation Conference. Dec.
</code></pre></div><ol start="2009"><li>Print.</li></ol><div class="language-"><pre><code>Collaborative
Filtering
</code></pre></div><div class="language-"><pre><code>Also known as
&#39;Recommendation,&#39; suggest
or eliminate items from a
set by comparing a history
of actions against items
performed by users. Finds
similar items based on who
used them or similar users
based on the items they use.
</code></pre></div><div class="language-"><pre><code>Use Singular Value Decomposition
based Recommendation for cases
where there are latent factors in your
domain, e.g., genres in movies.
</code></pre></div><div class="language-"><pre><code>Owen, Sean, Robin Anil, Ted
Dunning, and Ellen Friedman.
Mahout in Action. New Jersey:
Manning, 2012. Print.
</code></pre></div><div class="language-"><pre><code>Coordinate
Transforma-
tion
</code></pre></div><div class="language-"><pre><code>Provides a different
perspective on data.
</code></pre></div><div class="language-"><pre><code>Changing the coordinate system for data, for
example, using polar or cylindrical coordinates,
may more readily highlight key structure in the
data. A key step in coordinate transformations
is to appreciate multidimensionality and to
systematically analyze subspaces of the data.
</code></pre></div><div class="language-"><pre><code>Abbott, Edwin. A. Flatland: A
Romance of Many Dimensions.
United Kingdom: Seely &amp; Co,
</code></pre></div><ol start="1884"><li>Print.</li></ol><div class="language-"><pre><code>Design of
Experiments
</code></pre></div><div class="language-"><pre><code>Applies controlled
experiments to quantify
effects on system output
caused by changes to inputs.
</code></pre></div><div class="language-"><pre><code>Fractional factorial designs can significantly
reduce the number of different types of
experiments you must conduct.
</code></pre></div><div class="language-"><pre><code>Montgomery, Douglas. Design
and Analysis of Experiments.
New Jersey: John Wiley
&amp; Sons, 2012. Print.
</code></pre></div><h3 id="detailed-table-of-analytics"><a class="header-anchor" href="#detailed-table-of-analytics" aria-hidden="true">#</a> Detailed Table of Analytics</h3><div class="language-"><pre><code>Getting you to the right starting point isn’t enough. We also
provide a translator so you understand what you’ve been told.
</code></pre></div><div class="language-"><pre><code>Identifying several analytic techniques that can be applied to your
problem is useful, but their name alone will not be much help. #e
Detailed Table of Analytics translates the names into something more
meaningful. Once you’ve identi!ed a technique in the Guide to
Analytic Selection, !nd the corresponding row in the table. #ere you
will !nd a brief description of the techniques, tips we’ve learned and a
few references we’ve found helpful.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Compiled by: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Algorithms or
Method Name Description Tips From the Pros
</code></pre></div><div class="language-"><pre><code>References and Papers
We Love to Read
</code></pre></div><p><strong>Differential Equations</strong></p><div class="language-"><pre><code>Used to express
relationships between
functions and their
derivatives, for example,
change over time.
</code></pre></div><div class="language-"><pre><code>Differential equations can be used to formalize
models and make predictions. The equations
themselves can be solved numerically and
tested with different initial conditions to study
system trajectories.
</code></pre></div><div class="language-"><pre><code>Zill, Dennis, Warren Wright,
and Michael Cullen. Differential
Equations with Boundary-Value
Problems. Connecticut: Cengage
Learning, 2012. Print.
</code></pre></div><p><strong>Discrete Event Simulation</strong></p><div class="language-"><pre><code>Simulates a discrete
sequence of events where
each event occurs at a
particular instant in time.
The model updates its state
only at points in time when
events occur.
</code></pre></div><div class="language-"><pre><code>Discrete event simulation is useful when
analyzing event based processes such as
production lines and service centers to
determine how system level behavior changes
as different process parameters change.
Optimization can integrate with simulation to
gain efficiencies in a process.
</code></pre></div><div class="language-"><pre><code>Cassandras, Christopher, and
Stephanie Lafortune. Introduction
to Discrete Event Systems. New
York: Springer, 1999. Print.
</code></pre></div><p><strong>Discrete Wavelet Transform</strong></p><div class="language-"><pre><code>Transforms time series
data into frequency
domain preserving
locality information.
</code></pre></div><div class="language-"><pre><code>Offers very good time and frequency
localization. The advantage over Fourier
transforms is that it preserves both frequency
and locality.
</code></pre></div><div class="language-"><pre><code>Burrus, C.Sidney, Ramesh A.
Gopinath, Haitao Guo, Jan E.
Odegard, and Ivan W. Selesnick.
Introduction to Wavelets
and Wavelet Transforms:
A Primer. New Jersey:
Prentice Hall, 1998. Print.
</code></pre></div><p><strong>Exponential Smoothing</strong></p><div class="language-"><pre><code>Used to remove artifacts
expected from collection
error or outliers.
</code></pre></div><div class="language-"><pre><code>In comparison to a using moving average
where past observations are weighted equally,
exponential smoothing assigns exponentially
decreasing weights over time.
</code></pre></div><div class="language-"><pre><code>Chatfield, Chris, Anne B. Koehler,
J. Keith Ord, and Ralph D.
Snyder. “A New Look at Models
for Exponential Smoothing.”
Journal of the Royal Statistical
Society: Series D (The Statistician)
50.2 (July 2001): 147-159. Print.
</code></pre></div><p><strong>Factor Analysis</strong></p><div class="language-"><pre><code>Describes variability among
correlated variables with the
goal of lowering the number
of unobserved variables,
namely, the factors.
</code></pre></div><div class="language-"><pre><code>If you suspect there are inmeasurable
influences on your data, then you may want to
try factor analysis.
</code></pre></div><div class="language-"><pre><code>Child, Dennis. The Essentials of
Factor Analysis. United Kingdom:
Cassell Educational, 1990. Print.
</code></pre></div><p><strong>Fast Fourier Transform</strong></p><div class="language-"><pre><code>Transforms time series from
time to frequency domain
efficiently. Can also be used
for image improvement by
spatial transforms.
</code></pre></div><div class="language-"><pre><code>Filtering a time varying signal can be done
more effectively in the frequency domain. Also,
noise can often be identified in such signals by
observing power at aberrant frequencies.
</code></pre></div><div class="language-"><pre><code>Mitra, Partha P., and Hemant
Bokil. Observed Brain Dynamics.
United Kingdom: Oxford
University Press, 2008. Print.
</code></pre></div><p><strong>Format Conversion</strong></p><div class="language-"><pre><code>Creates a standard
representation of data
regardless of source format.
For example, extracting raw
UTF-8 encoded text from
binary file formats such as
Microsoft Word or PDFs.
</code></pre></div><div class="language-"><pre><code>There are a number of open source software
packages that support format conversion and
can interpret a wide variety of formats. One
notable package is Apache Tikia.
</code></pre></div><div class="language-"><pre><code>Ingersoll, Grant S., Thomas S.
Morton, and Andrew L. Farris.
Taming Text: How to Find,
Organize, and Manipulate It. New
Jersey: Manning, 2013. Print.
</code></pre></div><p><strong>Gaussian Filtering</strong></p><div class="language-"><pre><code>Acts to remove noise
or blur data.
</code></pre></div><div class="language-"><pre><code>Can be used to remove speckle
noise from images.
</code></pre></div><div class="language-"><pre><code>Parker, James R. Algorithms for
Image Processing and Computer
Vision. New Jersey: John Wiley &amp;
Sons, 2010. Print.
</code></pre></div><p><strong>Generalized Linear Models</strong></p><div class="language-"><pre><code>Expands ordinary linear
regression to allow
for error distribution
that is not normal.
</code></pre></div><div class="language-"><pre><code>Use if the observed error in your system does
not follow the normal distribution.
</code></pre></div><div class="language-"><pre><code>MacCullagh, P., and John A.
Nelder. Generalized Linear
Models. Florida: CRC Press,
</code></pre></div><ol start="1989"><li>Print.</li></ol><p><strong>Genetic Algorithms</strong></p><div class="language-"><pre><code>Evolves candidate
models over generations
by evolutionary
inspired operators of
mutation and crossover
of parameters.
</code></pre></div><div class="language-"><pre><code>Increasing the generation size adds diversity
in considering parameter combinations, but
requires more objective function evaluation.
Calculating individuals within a generation
is strongly parallelizable. Representation of
candidate solutions can impact performance.
</code></pre></div><div class="language-"><pre><code>De Jong, Kenneth A. Evolutionary
Computation - A Unified
Approach. Massachusetts:
MIT Press, 2002. Print.
</code></pre></div><p><strong>Grid Search</strong></p><div class="language-"><pre><code>Systematic search across
discrete parameter
values for parameter
exploration problems.
</code></pre></div><div class="language-"><pre><code>A grid across the parameters is used to
visualize the parameter landscape and assess
whether multiple minima are present.
</code></pre></div><div class="language-"><pre><code>Kolda, Tamara G., Robert M.
Lewis, and Virginia Torczon.
“Optimization by Direct Search:
New Perspectives on Some
Classical and Modern Methods.”
SIAM Review 45.3 (2003): 385-
</code></pre></div><ol start="482"><li>Print.</li></ol><div class="language-"><pre><code>Take off the Training Wheels 73
</code></pre></div><div class="language-"><pre><code>Compiled by: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Algorithms or
Method Name Description Tips From the Pros
</code></pre></div><div class="language-"><pre><code>References and Papers
We Love to Read
</code></pre></div><div class="language-"><pre><code>Hidden Markov
Models
</code></pre></div><div class="language-"><pre><code>Models sequential data by
determining the discrete
latent variables, but
the observables may be
continuous or discrete.
</code></pre></div><div class="language-"><pre><code>One of the most powerful properties of Hidden
Markov Models is their ability to exhibit
some degree of invariance to local warping
(compression and stretching) of the time axis.
However, a significant weakness of the Hidden
Markov Model is the way in which it represents
the distribution of times for which the system
remains in a given state.
</code></pre></div><div class="language-"><pre><code>Bishop, Christopher M. Pattern
Recognition and Machine
Learning. New York: Springer,
</code></pre></div><ol start="2006"><li>Print.</li></ol><div class="language-"><pre><code>Hierarchical
Clustering
</code></pre></div><div class="language-"><pre><code>Connectivity based
clustering approach
that sequentially builds
bigger (agglomerative)
or smaller (divisive)
clusters in the data.
</code></pre></div><div class="language-"><pre><code>Provides views of clusters at multiple
resolutions of closeness. Algorithms
begin to slow for larger data sets due
to most implementations exhibiting
O(N^3 ) or O(N^2 ) complexity.
</code></pre></div><div class="language-"><pre><code>Rui Xu, and Don Wunsch.
Clustering. New Jersey: Wiley-
IEEE Press, 2008. Print.
</code></pre></div><div class="language-"><pre><code>K-means
and X-means
Clustering
</code></pre></div><div class="language-"><pre><code>Centroid based clustering
algorithms, where with
K means the number
of clusters is set and X
means the number of
clusters is unknown.
</code></pre></div><div class="language-"><pre><code>When applying clustering techniques, make
sure to understand the shape of your data.
Clustering techniques will return poor results if
your data is not circular or ellipsoidal shaped.
</code></pre></div><div class="language-"><pre><code>Rui Xu, and Don Wunsch.
Clustering. New Jersey: Wiley-
IEEE Press, 2008. Print.
</code></pre></div><div class="language-"><pre><code>Linear,
Non-linear,
and Integer
Programming
</code></pre></div><div class="language-"><pre><code>Set of techniques for
minimizing or maximizing a
function over a constrained
set of input parameters.
</code></pre></div><div class="language-"><pre><code>Start with linear programs because algorithms
for integer and non-linear variables can take
much longer to run.
</code></pre></div><div class="language-"><pre><code>Winston, Wayne L. Operations
Research: Applications and
Algorithms. Connecticut:
Cengage Learning, 2003. Print.
</code></pre></div><div class="language-"><pre><code>Markov Chain
Monte Carlo
(MCMC)
</code></pre></div><div class="language-"><pre><code>A method of sampling
typically used in Bayesian
models to estimate the joint
distribution of parameters
given the data.
</code></pre></div><div class="language-"><pre><code>Problems that are intractable using analytic
approaches can become tractable using MCMC,
when even considering high-dimensional
problems. The tractability is a result of using
statistics on the underlying distributions of
interest, namely, sampling with Monte Carlo and
considering the stochastic sequential process of
Markov Chains.
</code></pre></div><div class="language-"><pre><code>Andrieu, Christophe, Nando
de Freitas, Amaud Doucet,
and Michael I. Jordan. “An
Introduction to MCMC for
Machine Learning.” Machine
Learning , 50.1 (January 2003):
5-43. Print.
</code></pre></div><div class="language-"><pre><code>Monte Carlo
Methods
</code></pre></div><div class="language-"><pre><code>Set of computational
techniques to generate
random numbers.
</code></pre></div><div class="language-"><pre><code>Particularly useful for numerical integration,
solutions of differential equations, computing
Bayesian posteriors, and high dimensional
multivariate sampling.
</code></pre></div><div class="language-"><pre><code>Fishman, George S. Monte
Carlo: Concepts, Algorithms, and
Applications. New York: Springer,
</code></pre></div><ol start="2003"><li>Print.</li></ol><div class="language-"><pre><code>Naïve Bayes
</code></pre></div><div class="language-"><pre><code>Predicts classes following
Bayes Theorem that
states the probability of
an outcome given a set of
features is based on the
probability of features given
an outcome.
</code></pre></div><div class="language-"><pre><code>Assumes that all variables are independent,
so it can have issues learning in the context of
highly interdependent variables. The model can
be learned on a single pass of data using simple
counts and therefore is useful in determining
whether exploitable patterns exist in large data
sets with minimal development time.
</code></pre></div><div class="language-"><pre><code>Ingersoll, Grant S., Thomas S.
Morton, and Andrew L. Farris.
Taming Text: How to Find,
Organize, and Manipulate It. New
Jersey: Manning, 2013. Print.
</code></pre></div><div class="language-"><pre><code>Neural
Networks
</code></pre></div><div class="language-"><pre><code>Learns salient features in
data by adjusting weights
between nodes through a
learning rule.
</code></pre></div><div class="language-"><pre><code>Training a neural network takes substantially
longer than evaluating new data with an already
trained network. Sparser network connectivity
can help to segment the input space and
improve performance on classification tasks.
</code></pre></div><div class="language-"><pre><code>Haykin, Simon O. Neural
Networks and Learning
Machines. New Jersey:
Prentice Hall, 2008. Print.
</code></pre></div><div class="language-"><pre><code>Outlier
Removal
</code></pre></div><div class="language-"><pre><code>Method for identifying and
removing noise or artifacts
from data.
</code></pre></div><div class="language-"><pre><code>Be cautious when removing outliers. Sometimes
the most interesting behavior of a system is at
times when there are aberrant data points.
</code></pre></div><div class="language-"><pre><code>Maimon, Oded, and Lior
Rockach. Data Mining and
Knowledge Discovery Handbook: A
Complete Guide for Practitioners
and Researchers. The
Netherlands: Kluwer Academic
Publishers, 2005. Print.
</code></pre></div><div class="language-"><pre><code>Principal
Components
Analysis
</code></pre></div><div class="language-"><pre><code>Enables dimensionality
reduction by identifying
highly correlated
dimensions.
</code></pre></div><div class="language-"><pre><code>Many large datasets contain correlations
between dimensions; therefore part of the
dataset is redundant. When analyzing the
resulting principal components, rank order
them by variance as this is the highest
information view of your data. Use skree plots to
infer the optimal number of components.
</code></pre></div><div class="language-"><pre><code>Wallisch, Pascal, Michael E.
Lusignan, Marc D. Benayoun,
Tanya I. Baker, Adam Seth
Dickey, and Nicholas G.
Hatsopoulos. Matlab for
Neuroscientists. New Jersey:
Prentice Hall, 2009. Print.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Compiled by: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Algorithms or
Method Name Description Tips From the Pros
</code></pre></div><div class="language-"><pre><code>References and Papers
We Love to Read
</code></pre></div><p><strong>Regression with Shrinkage (Lasso)</strong></p><div class="language-"><pre><code>A method of variable
selection and prediction
combined into a possibly
biased linear model.
</code></pre></div><div class="language-"><pre><code>There are different methods to select the
lambda parameter. A typical choice is cross
validation with MSE as the metric.
</code></pre></div><div class="language-"><pre><code>Tibshirani, Robert. “Regression
Shrinkage and Selection
via the Lasso.” Journal of
the Royal Statistical Society.
Series B (Methodological) 58.1
(1996): 267-288. Print.
</code></pre></div><p><strong>Sensitivity Analysis</strong></p><div class="language-"><pre><code>Involves testing individual
parameters in an analytic
or model and observing the
magnitude of the effect.
</code></pre></div><div class="language-"><pre><code>Insensitive model parameters during an
optimization are candidates for being set to
constants. This reduces the dimensionality
of optimization problems and provides an
opportunity for speed up.
</code></pre></div><div class="language-"><pre><code>Saltelli, A., Marco Ratto, Terry
Andres, Francesca Campolongo,
Jessica Cariboni, Debora Gatelli,
Michaela Saisana, and Stefano
Tarantola. Global Sensitivity
Analysis: the Primer. New Jersey:
John Wiley &amp; Sons, 2008. Print.
</code></pre></div><p><strong>Simulated Annealing</strong></p><div class="language-"><pre><code>Named after a controlled
cooling process in
metallurgy, and by
analogy using a changing
temperature or annealing
schedule to vary
algorithmic convergence.
</code></pre></div><div class="language-"><pre><code>The standard annealing function allows for
initial wide exploration of the parameter space
followed by a narrower search. Depending on
the search priority the annealing function can
be modified to allow for longer explorative
search at a high temperature.
</code></pre></div><div class="language-"><pre><code>Bertsimas, Dimitris, and John
Tsitsiklis. “Simulated Annealing.”
Statistical Science. 8.1 (1993):
10-15. Print.
</code></pre></div><p><strong>Stepwise Regression</strong></p><div class="language-"><pre><code>A method of variable
selection and prediction.
Akaike&#39;s information
criterion AIC is used as
the metric for selection.
The resulting predictive
model is based upon
ordinary least squares,
or a general linear model
with parameter estimation
via maximum likelihood.
</code></pre></div><div class="language-"><pre><code>Caution must be used when considering
Stepwise Regression, as over fitting often
occurs. To mitigate over fitting try to limit the
number of free variables used.
</code></pre></div><div class="language-"><pre><code>Hocking, R. R. “The Analysis and
Selection of Variables in Linear
Regression.” Biometrics. 32.1
(March 1976): 1-49. Print.
</code></pre></div><p><strong>Stochastic Gradient Descent</strong></p><div class="language-"><pre><code>General-purpose
optimization for learning of
neural networks, support
vector machines, and
logistic regression models.
</code></pre></div><div class="language-"><pre><code>Applied in cases where the objective
function is not completely differentiable
when using sub-gradients.
</code></pre></div><div class="language-"><pre><code>Witten, Ian H., Eibe Frank,
and Mark A. Hall. Data Mining:
Practical Machine Learning Tools
and Techniques. Massachusetts:
Morgan Kaufmann, 2011. Print.
</code></pre></div><p><strong>Support Vector Machines</strong></p><div class="language-"><pre><code>Projection of feature vectors
using a kernel function into
a space where classes are
more separable.
</code></pre></div><div class="language-"><pre><code>Try multiple kernels and use k-fold cross
validation to validate the choice of the best one.
</code></pre></div><div class="language-"><pre><code>Hsu, Chih-Wei, Chih-Chung
Chang, and Chih-Jen Lin. “A
Practical Guide to Support Vector
Classification.” National Taiwan
University. 2003. Print.
</code></pre></div><p><strong>Term Frequency Inverse Document Frequency</strong></p><div class="language-"><pre><code>A statistic that measures
the relative importance of a
term from a corpus.
</code></pre></div><div class="language-"><pre><code>Typically used in text mining. Assuming a
corpus of news articles, a term that is very
frequent such as “the” will likely appear many
times in many documents, having a low value. A
term that is infrequent such as a person’s last
name that appears in a single article will have a
higher TD IDF score.
</code></pre></div><div class="language-"><pre><code>Ingersoll, Grant S., Thomas S.
Morton, and Andrew L. Farris.
Taming Text: How to Find,
Organize, and Manipulate It. New
Jersey: Manning, 2013. Print.
</code></pre></div><p><strong>Topic Modeling (Latent Dirichlet Allocation)</strong></p><div class="language-"><pre><code>Identifies latent topics
in text by examining
word co-occurrence.
</code></pre></div><div class="language-"><pre><code>Employ part-of-speech tagging to eliminate
words other than nouns and verbs. Use raw
term counts instead of TF/IDF weighted terms.
</code></pre></div><div class="language-"><pre><code>Blei, David M., Andrew Y. Ng,
and Michael I Jordan. “Latent
Dirichlet Allocation.” Journal of
Machine Learning Research. 3
(March 2003): 993-1022. Print.
</code></pre></div><p><strong>Tree Based Methods</strong></p><div class="language-"><pre><code>Models structured as graph
trees where branches
indicate decisions.
</code></pre></div><div class="language-"><pre><code>Can be used to systematize a process or act as
a classifier.
</code></pre></div><div class="language-"><pre><code>James, G., D. Witten, T. Hastie,
and R Tibshirani. Tree-Based
Methods. In An Introduction to
Statistical Learning. New York:
Springer, 2013. Print.
</code></pre></div><p><strong>Wrapper Methods</strong></p><div class="language-"><pre><code>Feature set reduction
method that utilizes
performance of a set of
features on a model, as
a measure of the feature
set’s performance. Can
help identify combinations
of features in models that
achieve high performance.
</code></pre></div><div class="language-"><pre><code>Utilize k-fold cross validation
to control over fitting.
</code></pre></div><div class="language-"><pre><code>John, G. H, R Kohavi, and K.
Pfleger. “Irrelevant Features
and the Subset Selection
Problem.” Proceedings of
ICML-94, 11th International
Conference on Machine
Learning. New Brunswick,
New Jersey. 1994. 121-129.
</code></pre></div><ol start="59"><li>Conference Presentation.</li></ol><div class="language-"><pre><code>Take off the Training Wheels 75
</code></pre></div><h5 id="life-in-the-trenches"><a class="header-anchor" href="#life-in-the-trenches" aria-hidden="true">#</a> LIFE in THE TRENCHES</h5><div class="language-"><pre><code>NAVIGATING NECK DEEP IN DATA
Our Data Science experts have learned
and developed new solutions over the years
from properly framing or reframing analytic
solutions. In this section, we list a few
important topics to Data Science coupled
with firsthand experience from our experts.
</code></pre></div><h3 id="feature-engineering"><a class="header-anchor" href="#feature-engineering" aria-hidden="true">#</a> Feature Engineering</h3><div class="language-"><pre><code>Feature engineering is a lot like oxygen. You can’t do without
it, but you rarely give it much thought.
</code></pre></div><div class="language-"><pre><code>Feature engineering is the process by which one establishes the
representation of data in the context of an analytic approach. It is the
foundational skill in Data Science. Without feature engineering, it
would not be possible to understand and represent the world through
a mathematical model. Feature engineering is a challenging art. Like
other arts, it is a creative process that manifests uniquely in each
Data Scientist. It will be in&amp;uenced substantially by the scientist’s
experiences, tastes and understanding of the !eld.
</code></pre></div><div class="language-"><pre><code>As the name suggests, feature engineering can be a complex task
that may involve chaining and testing di$erent approaches. Features
may be simple such as “bag of words,” a popular technique in the
text processing domain, or or may be based on more complex
representations derived through activities such as machine learning.
You make use of the output of one analytic technique to create the
representation that is consumed by another. More often than not, you
will !nd yourself operating in the world of highly complex activities.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h1 id="-8"><a class="header-anchor" href="#-8" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>Ed Kohlwey
</code></pre></div><h2 id="chemoinformatic-search"><a class="header-anchor" href="#chemoinformatic-search" aria-hidden="true">#</a> Chemoinformatic Search</h2><div class="language-"><pre><code>On one assignment, my
team was confronted
with the challenge of
developing a search
engine over chemical
compounds. The goal
of chemoinformatic
search is to predict
the properties that
a molecule will exhibit as well as to
provide indices over those predicted
properties to facilitate data discovery
in chemistry-based research. These
properties may either be discreet (e.g.,
“a molecule treats disease x well”)
or continuous (e.g., “a molecule may
be dissolved up to 100.21 g/ml”).
</code></pre></div><div class="language-"><pre><code>Molecules are complex 3D structures,
which are typically represented as
a list of atoms joined by chemical
bonds of differing lengths with varying
electron domain and molecular
geometries. The structures are
specified by the 3-space coordinates
and the electrostatic potential
surface of the atoms in the molecule.
Searching this data is a daunting
task when one considers that
naïve approaches to the problem
bear significant semblance to the
Graph Isomorphism Problem.[13]
</code></pre></div><div class="language-"><pre><code>The solution we developed was
based on previous work in molecular
fingerprinting (sometimes also called
hashing or locality sensitive hashing).
Fingerprinting is a dimensionality
reduction technique that dramatically
reduces the problem space by
summarizing many features, often
with relatively little regard to the
importance of the feature. When
an exact solution is likely to be
infeasible, we often turn to heuristic
approaches such as fingerprinting.
</code></pre></div><div class="language-"><pre><code>Our approach used a training set
where all the measured properties
</code></pre></div><div class="language-"><pre><code>of the molecules were available. We
created a model of how molecular
structural similarities might affect
their properties. We began by
finding all the sub-graphs of each
molecule with length n , resulting
in a representation similar to
the bag-of-words approach from
natural language processing.
We summarized each molecule
fragment in a type of fingerprint
called a “Counting Bloom Filter.”
</code></pre></div><div class="language-"><pre><code>Next, we used several exemplars from
the set to create new features. We
found the distance from each member
of the full training set to each of the
exemplars. We fed these features into
a non-linear regression algorithm
to yield a model that could be used
on data that was not in the original
training set. This approach can be
conceptualized as a “hidden manifold,”
whereby a hidden surface or shape
defines how a molecule will exhibit a
property. We approximate this shape
using a non-linear regression and a
set of data with known properties.
Once we have the approximate
shape, we can use it to predict the
properties of new molecules.
</code></pre></div><div class="language-"><pre><code>Our approach was multi-staged and
complex – we generated sub-graphs,
created bloom filters, calculated
distance metrics and fit a linear-
regression model. This example
provides an illustration of how many
stages may be involved in producing a
sophisticated feature representation.
By creatively combining and building
“features on features,” we were able
to create new representations of data
that were richer and more descriptive,
yet were able to execute faster and
produce better results.
</code></pre></div><div class="language-"><pre><code>Life in the Trenches 79
</code></pre></div><h3 id="feature-selection"><a class="header-anchor" href="#feature-selection" aria-hidden="true">#</a> Feature Selection</h3><div class="language-"><pre><code>Models are like honored guests; you should only feed them the
good parts.
</code></pre></div><div class="language-"><pre><code>Feature selection is the process of determining the set of features with
the highest information value to the model. Two main approaches are
!ltering and wrapper methods. Filtering methods analyze features
using a test statistic and eliminate redundant or non-informative
features. As an example, a !ltering method could eliminate features
that have little correlation to the class labels. Wrapper methods
utilize a classi!cation model as part of feature selection. A model is
trained on a set of features and the classi!cation accuracy is used to
measure the information value of the feature set. One example is that
of training a neural network with a set of features and evaluating the
accuracy of the model. If the model scores highly on the test set, then
the features have high information value. All possible combinations of
features are tested to !nd the best feature set.
</code></pre></div><div class="language-"><pre><code>#ere are tradeo$s between these techniques. Filtering methods
are faster to compute since each feature only needs to be compared
against its class label. Wrapper methods, on the other hand, evaluate
feature sets by constructing models and measuring performance. #is
requires a large number of models to be trained and evaluated (a
quantity that grows exponentially in the number of features). Why
would anyone use a wrapper method? Feature sets may perform better
than individual features.[14] With !lter methods, a feature with weak
correlation to its class labels is eliminated. Some of these eliminated
features, however, may have performed well when combined with
other features.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h1 id="-9"><a class="header-anchor" href="#-9" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>Paul Yacci
</code></pre></div><h2 id="cancer-cell-classification"><a class="header-anchor" href="#cancer-cell-classification" aria-hidden="true">#</a> Cancer Cell Classification</h2><div class="language-"><pre><code>On one project,
our team was
challenged
to classify
cancer cell
profiles. The
overarching
goal was
to classify different types
of Leukemia, based on
Microarray profiles from 72
samples[15] using a small
set of features. We utilized
a hybrid Artificial Neural
Network (ANN)[16] and Genetic
Algorithm[17] to identify subsets
of 10 features selected from
thousands.[18] We trained the
ANN and tested performance
using cross-fold validation.
The performance measure
</code></pre></div><div class="language-"><pre><code>was used as feedback into
the Genetic Algorithm. When
a set of features contained
no useful information, the
model performed poorly and
a different feature set would
be explored. Over time, this
method selected a set of
features that performed with
high accuracy. The down-
selected feature set increased
speed and performance as
well as allowed for better
insight into the factors that
may govern the system. This
allowed our team to design
a diagnostic test for only a
few genetic markers instead
of thousands, substantially
reducing diagnostic test
complexity and cost.
</code></pre></div><div class="language-"><pre><code>Life in the Trenches 81
</code></pre></div><h3 id="data-veracity"><a class="header-anchor" href="#data-veracity" aria-hidden="true">#</a> Data Veracity</h3><div class="language-"><pre><code>We’re Data Scientists, not data alchemists. We can’t make
analytic gold from the lead of data.
</code></pre></div><div class="language-"><pre><code>While most people associate data volume, velocity, and variety with
big data, there is an equally important yet often overlooked dimension
</code></pre></div><ul><li>data veracity. Data veracity refers to the overall quality and correctness of the data. You must assess the truthfulness and accuracy of the data as well as identify missing or incomplete information. As the saying goes, “Garbage in, garbage out.” If your data is inaccurate or missing information, you can’t hope to make analytic gold.</li></ul><div class="language-"><pre><code>Assessing data truthfulness is often subjective. You must rely on your
experience and an understanding of the data origins and context.
Domain expertise is particularly critical for the latter. Although
data accuracy assessment may also be subjective, there are times that
quantitative methods may be used. You may be able to re-sample from
the population and conduct a statistical comparison against the stored
values, thereby providing measures of accuracy.
</code></pre></div><div class="language-"><pre><code>#e most common issues you will encounter are missing or
incomplete information. #ere are two basic strategies for dealing
with missing values – deletion and imputation. In the former, entire
observations are excluded from analysis, reducing sample size and
potentially introducing bias. Imputation, or replacement of missing
or erroneous values, uses a variety of techniques such as random
sampling (hot deck imputation) or replacement using the mean,
statistical distributions or models.
</code></pre></div><div class="language-"><pre><code>» Tips From the Pros
</code></pre></div><div class="language-"><pre><code>Find an approach that works,
implement it, and move on. You
can worry about optimization and
tuning your approaches later during
incremental improvement.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Brian Keller
</code></pre></div><h1 id="-10"><a class="header-anchor" href="#-10" aria-hidden="true">#</a> !!</h1><h2 id="time-series-modeling"><a class="header-anchor" href="#time-series-modeling" aria-hidden="true">#</a> Time Series Modeling</h2><div class="language-"><pre><code>On one of our projects, the team was faced with correlating the time
series for various parameters. Our initial analysis revealed that the
correlations were almost non-existent. We examined the data and
quickly discovered data veracity issues. There were missing and
null values, as well as negative-value observations, an impossibility
given the context of the measurements (see the figure, Time Series
Data Prior to Cleansing ). Garbage data meant garbage results.
</code></pre></div><p>Because sample size was already small, deleting observations was undesirable. The volatile nature of the time series meant that imputation through sampling could not be trusted to produce values in which the team would be confident. As a result, we quickly realized that the best strategy was an approach that could filter and correct the noise in the data.</p><p>We initially tried a simplistic approach in which we replaced each observation with a moving average. While this corrected some noise, including the outlier values in our moving-average computation shifted the time series. This caused undesirable</p><div class="language-"><pre><code>distortion in the underlying signal, and we quickly
abandoned the approach.
One of our team members who had experience in
signal processing suggested a median filter. The
median filter is a windowing technique that moves
through the data point-by-point, and replaces it
with the median value calculated for the current
window. We experimented with various window
sizes to achieve an acceptable tradeoff between
smoothing noise and smoothing away signal. The
figure, Time Series Data After Cleansing , shows the
same two time series after median filter imputation.
</code></pre></div><p>The application of the median filter approach was hugely successful. Visual inspection of the time series plots reveals smoothing of the outliers without dampening the naturally occurring peaks and troughs (no signal loss). Prior to smoothing, we saw no correlation in our data, but afterwards, Spearman’s Rho was ~0.5 for almost all parameters.</p><div class="language-"><pre><code>By addressing our data veracity issues, we were
able to create analytic gold. While other approaches
may also have been effective, implementation speed
constraints prevented us from doing any further
analysis. We achieved the success we were after and
moved on to address other aspects of the problem.
</code></pre></div><div class="language-"><pre><code>Time Series Data Prior to Cleansing
</code></pre></div><div class="language-"><pre><code>Time Series Data After Cleansing
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Life in the Trenches 83
</code></pre></div><h3 id="application-of"><a class="header-anchor" href="#application-of" aria-hidden="true">#</a> Application of</h3><h3 id="domain-knowledge"><a class="header-anchor" href="#domain-knowledge" aria-hidden="true">#</a> Domain Knowledge</h3><div class="language-"><pre><code>We are all special in our own way. Don’t discount what
you know.
</code></pre></div><div class="language-"><pre><code>Knowledge of the domain in which a problem lies is immensely
valuable and irreplaceable. It provides an in-depth understanding
of your data and the factors in&amp;uencing your analytic goal. Many
times domain knowledge is a key di$erentiator to a Data Science
team’s success. Domain knowledge in&amp;uences how we engineer and
select features, impute data, choose an algorithm, and determine
success. One person cannot possibly be a domain expert in every
!eld, however. We rely on our team, other analysts and domain
experts as well as consult research papers and publications to build
an understanding of the domain.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h1 id="-11"><a class="header-anchor" href="#-11" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>GEOSPATIAL HOTSPOTS
</code></pre></div><div class="language-"><pre><code>TEMPORAL HOTSPOT
Sun
Sat
Fri
Thu
Wed
Tue
Mon
3am 6am 9am Noon 3pm 6pm 9pm
</code></pre></div><div class="language-"><pre><code>3am 6am 9am Noon 3pm 6pm 9pm
</code></pre></div><div class="language-"><pre><code>Armen
Kherlopian
</code></pre></div><h2 id="motor-vehicle-theft"><a class="header-anchor" href="#motor-vehicle-theft" aria-hidden="true">#</a> Motor Vehicle Theft</h2><div class="language-"><pre><code>On one project,
our team explored
how Data Science
could be applied to
improve public safety.
According to the
FBI, approximately
$8 Billion is lost
annually due to
automobile theft. Recovery of the
one million vehicles stolen every
year in the U.S. is less than 60%.
Dealing with these crimes represents
a significant investment of law
enforcement resources. We wanted
to see if we could identify how to
reduce auto theft while efficiently
using law enforcement resources.
</code></pre></div><div class="language-"><pre><code>Our team began by parsing and
verifying San Francisco crime data.
We enriched stolen car reporting with
general city data. After conducting
several data experiments across both
space and time, three geospatial and
one temporal hotspot emerged (see
figure, Geospatial and Temporal Car
Theft Hotspots ). The domain expert
on the team was able to discern
that the primary geospatial hotspot
corresponded to an area surrounded
by parks. The parks created an urban
mountain with a number of over-foot
access points that were conducive to
car theft.
</code></pre></div><div class="language-"><pre><code>Our team used the temporal hotspot information in tandem with the insights
from the domain expert to develop a Monte Carlo model to predict the likelihood
of a motor vehicle theft at particular city intersections. By prioritizing the
intersections identified by the model, local governments would have the
information necessary to efficiently deploy their patrols. Motor vehicle thefts
could be reduced and law enforcement resources could be more efficiently
deployed. The analysis, enabled by domain expertise, yielded actionable insights
that could make the streets safer.
</code></pre></div><div class="language-"><pre><code>Geospatial and Temporal Car Theft Hotspots
</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton
</code></pre></div><div class="language-"><pre><code>Life in the Trenches 85
</code></pre></div><h3 id="e-curse-of"><a class="header-anchor" href="#e-curse-of" aria-hidden="true">#</a> !e Curse of</h3><h3 id="dimensionality"><a class="header-anchor" href="#dimensionality" aria-hidden="true">#</a> Dimensionality</h3><div class="language-"><pre><code>!ere is no magical potion to cure the curse, but there is PCA.
</code></pre></div><div class="language-"><pre><code>!e “curse of dimensionality” is one of the most important results
in machine learning. Most texts on machine learning mention this
phenomenon in the &quot;rst chapter or two, but it often takes many years
of practice to understand its true implications.
</code></pre></div><div class="language-"><pre><code>Classi&quot;cation methods, like most machine learning methods, are
subject to the implications of the curse of dimensionality. !e basic
intuition in this case is that as the number of data dimensions
increases, it becomes more di#cult to create generalizable
classi&quot;cation models (models that apply well over phenomena not
observed in the training set). !is di#culty is usually impossible to
overcome in real world settings. !ere are some exceptions in domains
where things happen to work out, but usually you must work to
minimize the number of dimensions. !is requires a combination
of clever feature engineering and use of dimensionality reduction
techniques (see Feature Engineering and Feature Selection Life in the
Trenches ). In our practical experience, the maximum
number of dimensions seems to be ~10 for linear model-based
approaches. !e limit seems to be in the tens of thousands for more
sophisticated methods such as support vector machines, but the limit
still exists nonetheless.
</code></pre></div><div class="language-"><pre><code>A counterintuitive consequence of the curse of dimensionality is
that it limits the amount of data needed to train a classi&quot;cation
model. !ere are roughly two reasons for this phenomenon. In one
case, the dimensionality is small enough that the model can be
trained on a single machine. In the other case, the exponentially
expanding complexity of a high-dimensionality problem makes it
(practically) computationally impossible to train a model. In our
experience, it is quite rare for a problem to fall in a “sweet spot”
between these two extremes.
</code></pre></div><div class="language-"><pre><code>!is observation is not to say that such a condition never arises. We
believe it is rare enough, however, that practitioners need not concern
themselves with how to address this case. Rather than trying to create
super-scalable algorithm implementations, focus your attention on
solving your immediate problems with basic methods. Wait until you
encounter a problem where an algorithm fails to converge or provides
poor cross-validated results, and then seek new approaches. Only
when you &quot;nd that alternate approaches don’t already exist, should you
begin building new implementations. !e expected cost of this work
pattern is lower than over-engineering right out of the gate.
</code></pre></div><div class="language-"><pre><code>Put otherwise, “Keep it simple, stupid”.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h2 id="baking-the-cake"><a class="header-anchor" href="#baking-the-cake" aria-hidden="true">#</a> Baking the Cake</h2><div class="language-"><pre><code>I was once given a time series set of roughly
1,600 predictor variables and 16 target variables
and asked to implement a number of modeling
techniques to predict the target variable
values. The client was challenged to handle the
complexity associated with the large number of
variables and needed help. Not only did I have
a case of the curse, but the predictor variables
were also quite diverse. At first glance, it looked
like trying to bake a cake with everything in the cupboard.
That is not a good way to bake or to make predictions!
</code></pre></div><div class="language-"><pre><code>The data diversity could be
partially explained by the fact
that the time series predictors
did not all have the same
periodicity. The target time
series were all daily values
whereas the predictors were
daily, weekly, quarterly, and
monthly. This was tricky to
sort out, given that imputing
zeros isn’t likely to produce
good results. For this specific
reason, I chose to use neural
networks for evaluating the
weekly variable contributions.
</code></pre></div><div class="language-"><pre><code>Using this approach, I was
able to condition upon week,
without greatly increasing
the dimensionality. For the
other predictors, I used
a variety of techniques,
including projection and
correlation, to make heads
or tails of the predictors. My
approach successfully reduced
the number of variables,
accomplishing the client’s goal
of making the problem space
tractable. As a result, the cake
turned out just fine.
</code></pre></div><h1 id="-12"><a class="header-anchor" href="#-12" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>Stephanie
Rivera
</code></pre></div><div class="language-"><pre><code>Life in the Trenches 87
</code></pre></div><h3 id="model-validation"><a class="header-anchor" href="#model-validation" aria-hidden="true">#</a> Model Validation</h3><div class="language-"><pre><code>Repeating what you just heard does not mean that you
learned anything.
</code></pre></div><div class="language-"><pre><code>Model validation is central to construction of any model. #is answers
the question “How well did my hypothesis !t the observed data?”
If we do not have enough data, our models cannot connect the dots.
On the other hand, given too much data the model cannot think
outside of the box. #e model learns speci!c details about the training
data that do not generalize to the population. #is is the problem of
model over !tting.
</code></pre></div><div class="language-"><pre><code>Many techniques exist to combat model over !tting. #e simplest
method is to split your data set into training, testing and validation
sets. #e training data is used to construct the model. #e model
constructed with the training data is then evaluated with the testing
data. #e performance of the model against the testing set is used to
further reduce model error. #is indirectly includes the testing data
within model construction, helping to reduce model over !t. Finally,
the model is evaluated on the validation data to assess how well the
model generalizes.
</code></pre></div><div class="language-"><pre><code>A few methods where the data is split into training and testing sets
include: k -fold cross-validation, Leave-One-Out cross-validation,
bootstrap methods, and resampling methods. Leave-One-Out cross-
validation can be used to get a sense of ideal model performance
over the training set. A sample is selected from the data to act as the
testing sample and the model is trained on the rest of the data. #e
error on the test sample is calculated and saved, and the sample is
returned to the data set. A di$erent sample is then selected and the
process is repeated. #is continues until all samples in the testing set
have been used. #e average error over the testing examples gives a
measure of the model’s error.
</code></pre></div><div class="language-"><pre><code>#ere are other approaches for testing how well your hypothesis
re&amp;ects the data. Statistical methods such as calculating the coe%cient
of determination, commonly called the R -squared value are used to
identify how much variation in the data your model explains. Note
that as the dimensionality of your feature space grows, the R -squared
value also grows. An adjusted R -squared value compensates for this
phenomenon by including a penalty for model complexity. When
testing the signi!cance of the regression as a whole, the F-test
compares the explained variance to unexplained variance. A regression
result with a high F-statistic and an adjusted R -squared over 0.7 is
almost surely signi!cant.
</code></pre></div><div class="language-"><pre><code>» Do we really need a case study
to know that you should
check your work?
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h5 id="putting-it-all-together"><a class="header-anchor" href="#putting-it-all-together" aria-hidden="true">#</a> PUTTING it ALL TOGETHER</h5><h2 id="consumer-behavior"><a class="header-anchor" href="#consumer-behavior" aria-hidden="true">#</a> Consumer Behavior</h2><h2 id="analysis-from-a"><a class="header-anchor" href="#analysis-from-a" aria-hidden="true">#</a> Analysis from a</h2><h2 id="multi-terabyte"><a class="header-anchor" href="#multi-terabyte" aria-hidden="true">#</a> Multi-Terabyte</h2><h2 id="dataset"><a class="header-anchor" href="#dataset" aria-hidden="true">#</a> Dataset</h2><p>Analytic Challenge</p><div class="language-"><pre><code>After storing over 10 years’ worth of retail
transaction in the natural health space, a retail
client was interested in employing advanced
machine learning techniques to mine the data for
valuable insights. The client wanted to develop a
database structure for long term implementation
of retail supply chain analytics and select the
proper algorithms needed to develop insights
into supplier, retailer, and consumer interactions.
Determining the actual worth of applying big
data analytics to the end-to-end retail supply
chain was also of particular interest.
</code></pre></div><p>Our Approach</p><div class="language-"><pre><code>The client’s data
included 3TBs of
product descriptions,
customer loyalty
information and B2B
and B2C transactions
for thousands of natural
health retailers across
North America. Because
the data had been stored
in an ad-hoc fashion, the
first step was creating
a practical database
structure to enable
analytics. We selected
a cloud environment
in order to quickly
implement analytics on
the client’s disparate
and sometimes
redundant datasets.
Once we created a
</code></pre></div><div class="language-"><pre><code>suitable analytics
architecture, we
moved on to identifying
appropriate machine
learning techniques that
would add value across
three key focus areas:
product receptivity, loyal
program analysis, and
market basket analysis.
For product receptivity,
our team used Bayesian
Belief Networks (BBN)
to develop probabilistic
models to predict the
success, failure, and
longevity of a new or
current product. We
joined transaction data
with attribute data of
both successful and
failed products to
</code></pre></div><h1 id="-13"><a class="header-anchor" href="#-13" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>» Our Case Studies
</code></pre></div><div class="language-"><pre><code>Hey, we have given you a lot of
really good technical content. We
know that this section has the look
and feel of marketing material,
but there is still a really good story
here. Remember, storytelling comes
in many forms and styles, one of
which is the marketing version. You
should read this chapter for what it
is – great information told with a
marketing voice.
</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><p>transform the data into usable form. Once we created this data file, we used it to train BBNs and create a predictive model for future products.</p><p>For loyalty program analysis, we joined transactional data and customer attribute data, which included location information and shopping trends. We used <em>k</em> -means clustering to segment customers based on their behavior over time. This allowed us to cluster and characterize groups of customers that exhibited similar loyalty patterns.</p><p>For market basket analysis, we employed Latent Dirichlet Allocation (LDA), a natural</p><div class="language-"><pre><code>language processing technique,
to create consistent product
categorization. The client’s
product categorization was
ad-hoc, having been entered
by individual suppliers and
retailers. As a result, it was
inconsistent and often contained
typographical errors or missing
values. LDA allowed our team
to use the existing text to derive
new, consistent customer
categories for the market basket
analysis. After joining the new
product categorization data
with transaction data, we used
Association Rules Learning
to identify sets of product
categories that customers
tended to purchase together at
individual retailer locations.
</code></pre></div><p>Our Impact</p><p>Our team provided key findings and recommendations to describe how the machine learning techniques could be operationalized to provide real-time reporting to retailers. The client received suggestions for improving product nomenclature, product promotions, and end-to-end visibility of the product and process lifecycle. As an example, we used our market basket analysis to create product recommendations for individual retail locations. Our recommendations have the potential to improve sales within certain product categories by up to 50% across the retail network. Together with time savings realized from automated data processing (such as a 300x increase in product categorization speed), these insights demonstrated the clear value of big data analytics to the client’s organization.</p><div class="language-"><pre><code>Putting it all Together 93
</code></pre></div><h2 id="strategic-insights"><a class="header-anchor" href="#strategic-insights" aria-hidden="true">#</a> Strategic Insights</h2><h2 id="within-terabytes-of"><a class="header-anchor" href="#within-terabytes-of" aria-hidden="true">#</a> within Terabytes of</h2><h2 id="passenger-data"><a class="header-anchor" href="#passenger-data" aria-hidden="true">#</a> Passenger Data</h2><p>Analytic Challenge</p><div class="language-"><pre><code>A commercial airline client was faced with increasing market
competition and challenges in profitability. They wanted to address
these challenges with rapid deployment of advanced, globalized
analytical tools within their private electronic data warehouse.
In the past, the client had analyzed smaller datasets in-house.
Because smaller datasets are filtered or diluted subsets of the
full data, the airline had not been able to extract the holistic
understanding it was seeking.
</code></pre></div><div class="language-"><pre><code>Booz Allen was engaged to create capabilities to analyze hundreds
of gigabytes of client data. The ultimate goal was to generate
insights into airline operations, investment decisions and consumer
preferences that may not have been apparent from studying data
subsets. Specifically, the airline wanted to be able to understand
issues such as: how they perform in different city-pair markets
relative to competitors; how booking behaviors change, based on
passenger and flight characteristics; and how connection times
impact demand.
</code></pre></div><p>Our Solution</p><div class="language-"><pre><code>Due to data privacy issues, our
team set up a cloud environment
within the client’s electronic
data warehouse. Leveraging
this analytic environment,
analysis proceeded with an
approach that focused on the
client’s three priorities: market
performance, booking behavior,
and passenger choice.
We performed probabilistic
analysis using machine learning
techniques, particularly
Bayesian Belief Networks
</code></pre></div><div class="language-"><pre><code>(BBN). We merged passenger
booking and other data to
create a BBN training file. Our
team developed and validated
comprehensive BBN models
to represent significant
customer behavior and market-
based factors that influence
passenger preference, with
respect to selection of flights
by connection times. Finally,
our team developed custom big
data visualizations to convey
the findings to technical and
non-technical audiences alike.
</code></pre></div><h1 id="-14"><a class="header-anchor" href="#-14" aria-hidden="true">#</a> !!</h1><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><p>Our Impact</p><p>We demonstrated the ability to rapidly deploy big data analytical tools and machine learning on massive datasets located inside a commercial airline’s private cloud environment. Results included insights that seemed counterintuitive, but could improve financial performance nonetheless. An example of one such finding was that under certain circumstances, passengers are willing to pay a premium to book itineraries with modified connection times. This translates into a potential revenue increase of many millions of dollars. These insights, often at the level of named customers, can be acted upon immediately to improve financial performance.</p><div class="language-"><pre><code>Putting it all Together 95
</code></pre></div><h2 id="savings-through"><a class="header-anchor" href="#savings-through" aria-hidden="true">#</a> Savings Through</h2><h2 id="better-manufacturing"><a class="header-anchor" href="#better-manufacturing" aria-hidden="true">#</a> Better Manufacturing</h2><p>Analytic Challenge</p><div class="language-"><pre><code>A manufacturing company engaged Booz Allen to explore data
related to chemical compound production. These processes are
quite complex. They involve a long chain of interconnected events,
which ultimately leads to high variability in product output. This
makes production very expensive.
</code></pre></div><div class="language-"><pre><code>Understanding the production process is not easy – sensors collect
thousands of time series variables and thousands of point-in-
time measurements, yielding terabytes of data. There was a huge
opportunity if the client could make sense of this data. Reducing the
variance and improving the product yield by even a small amount
could result in significant cost savings.
</code></pre></div><p>Our Solution</p><div class="language-"><pre><code>Due to the size and complexity of
the process data, prior analysis
efforts that focused on data
from only a single sub-process
had limited success. Our Data
Science team took a different
approach: analyzing all the data
from all the sub-processes
with the goal of identifying the
factors driving variation. Once
we understood those factors, we
could develop recommendations
on how to control them to
increase yield. The client’s
process engineers had always
wanted to pursue this approach
but lacked the tools to carry out
the analysis.
We decomposed the problem
into a series of smaller
problems. First, it was
necessary to identify which
time series parameters likely
affected product yield. We
engaged the client’s domain
experts to identify their
</code></pre></div><div class="language-"><pre><code>hypotheses surrounding the
process. Once we discerned a
set of hypotheses, we identified
the sensors that collected the
relevant data.
We began initial data processing,
which included filtering
bad values and identifying
patterns in the time series. We
then needed to segment the
data streams into individual
production runs. We identified a
sensor that stored the high-level
information indicating when a
production process began. This
sensor provided exactly what we
needed, but we quickly noticed
that half of the expected data
was missing. Examining the
data more closely, we realized
the sensor had only been used
within recent years. We had to
take a step back and reassess
our plan. After discussions
with the domain experts, we
identified a different sensor
</code></pre></div><h1 id="-15"><a class="header-anchor" href="#-15" aria-hidden="true">#</a> !!</h1><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><p>that gave us raw values directly from the production process. The raw values included a tag that indicated the start of a production run. The sensor was active for every production run and could be used reliably to segment the data streams into production runs.</p><p>Next, we had to determine which time series parameters affected product yield. Using the cleaned and processed data and a non-parametric correlation technique, we compared each time series in a production run to all other time series in that same run. Given the pair- wise similarities, we estimated correlation of the similarities to final product yield. We then used the correlations as input into a clustering algorithm to find clusters of time series parameters that correlated with each other in terms of product yield, not in terms of the time series themselves. This data</p><div class="language-"><pre><code>analysis was at a scale not
previously possible – millions
of comparisons for whole
production runs. Engineers were
able to look at all the data for the
first time, and to see impacts
of specific parameters across
different batches and sensors.
In addition to identifying the
key parameters, the engineers
needed to know how to control
the parameters to increase
product yield. Discussions
with domain experts provided
insight into which time series
parameters could be easily
controlled. This limited the
candidate parameters to only
those that the process engineers
could influence. We extracted
features from the remaining
time series signals and fed
them into our models to predict
yield. The models quantified the
correlation between the pattern
of parameter values and yield,
providing insights on how to
increase product yield.
</code></pre></div><p>Our Impact</p><p>With controls identified and desirable patterns quantified, we provided the engineers with a set of process control actions to improve product output. The raw sensor data that came directly from the production process drove our analysis and recommendations, thus providing the client with confidence in the approach. The reduction in product yield variability will enable the client to produce a better product with lower risk at a reduced cost.</p><div class="language-"><pre><code>Putting it all Together 97
</code></pre></div><h2 id="realizing-higher"><a class="header-anchor" href="#realizing-higher" aria-hidden="true">#</a> Realizing Higher</h2><h2 id="returns-through"><a class="header-anchor" href="#returns-through" aria-hidden="true">#</a> Returns Through</h2><h2 id="predictive-analytics"><a class="header-anchor" href="#predictive-analytics" aria-hidden="true">#</a> Predictive Analytics</h2><p>Analytic Challenge</p><div class="language-"><pre><code>A major investment house wanted to explore whether the
application of Data Science techniques could yield increased
investment returns. In particular, the company wanted to predict
future commodity value movements based on end-of-day and
previous-day equity metrics. The client hoped the predictions
could be used to optimize their trading activities. By translating
the approach across their entire portfolio, they could dramatically
improve the yield curve for their investors.
</code></pre></div><div class="language-"><pre><code>Several challenges were immediately apparent. The data volume
was very large, consisting of information from tens of thousands
of equities, commodities, and options across most major world
markets across multiple time intervals. The need to recommend
a predictive action (go short, go long, stay, increase position size,
or engage in a particular option play) with very low latency was
an even greater challenge. The team would need to develop an
approach that addressed both of these implicit constraints.
</code></pre></div><p>Our Solution</p><div class="language-"><pre><code>The client challenged Booz
Allen to use 3,500 independent
variables to predict the daily
price movements of 16 financial
instruments. The client hid the
meaning and context of the
independent variables, however,
forcing our team to perform
analysis without qualitative
information. The team
immediately began searching
for supplemental data sources.
We identified unstructured data
from other companies, financial
institutions, governments and
social media that could be
used in our analysis. The team
paid considerable attention to
</code></pre></div><div class="language-"><pre><code>database access efficiency and
security as well as the speed
of computation.
Our team implemented
a multifaceted approach,
including a mix of neural
network optimization and a
variety of principal component,
regression, and unsupervised
learning techniques. We were
able to infer insight into small-
scale exogenous events that
provided a richer basis for
predicting localized fluctuations
in the equity prices. Our
team was able to use these
predictions to determine the
optimal combination of actions
</code></pre></div><h1 id="-16"><a class="header-anchor" href="#-16" aria-hidden="true">#</a> !!</h1><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><p>that would generate the best aggregate return over 12 months of trading. Careful consideration of the residuals and skilled</p><div class="language-"><pre><code>modeling of the variance added
additional value to the outcome
for this client.
</code></pre></div><p>Our Impact</p><p>Our Data Science team conducted an experiment to determine the efficacy of our approach. We used our model to generate buy/sell recommendations based on training data provided by the client. The test cases converged on a set of recommendations in less than ten minutes, satisfying the solution timeliness constraint. The experiment revealed a true positive accuracy of approximately 85% with a similar outcome for true negative accuracy when compared against optimal recommendations based on perfect information. The typical return on investment was, as desired, quite large.</p><p>The ability to determine the position to take, not just for a single financial instrument but also for a complete portfolio, is invaluable for this client. Achieving this outcome required predictive analytics and the ability to rapidly ingest and process large data sets, including unstructured data. This could not have been accomplished without a diverse team of talented Data Scientists bringing the entirety of their tradecraft to bear on the problem.</p><div class="language-"><pre><code>Putting it all Together 99
</code></pre></div><h5 id="closing-time"><a class="header-anchor" href="#closing-time" aria-hidden="true">#</a> CLOSING TIME</h5><h2 id="parting"><a class="header-anchor" href="#parting" aria-hidden="true">#</a> PARTING</h2><h2 id="thoughts"><a class="header-anchor" href="#thoughts" aria-hidden="true">#</a> THOUGHTS</h2><div class="language-"><pre><code>Data Science capabilities are creating data analytics that are
improving every aspect of our lives, from life-saving disease
treatments, to national security, to economic stability, and even
the convenience of selecting a restaurant. We hope we have
helped you truly understand the potential of your data and how
to become extraordinary thinkers by asking the right questions of
your data. We hope we have helped drive forward the science and
art of Data Science. Most importantly, we hope you are leaving
with a newfound passion and excitement for Data Science.
</code></pre></div><div class="language-"><pre><code>!ank you for taking this journey with us. Please join our
conversation and let your voice be heard. Email us your ideas
and perspectives at data_science@bah.com or submit them
via a pull request on the Github repository.
</code></pre></div><div class="language-"><pre><code>Tell us and the world what you know. Join us. Become an
author of this story.
</code></pre></div><h1 id="-17"><a class="header-anchor" href="#-17" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>Closing Time 103
</code></pre></div><h2 id="references"><a class="header-anchor" href="#references" aria-hidden="true">#</a> REFERENCES</h2><ol><li>Commonly attributed to: Nye, Bill. <em>Reddit Ask Me Anything</em> (AMA). July 2012. Web. Accessed 15 October 2013. SSRN: &lt;<a href="http://www.reddit" target="_blank" rel="noopener noreferrer">http://www.reddit</a>. com/r/IAmA/comments/x9pq0/iam_bill_nye_the_science_guy_ama&gt;</li><li>Fayyad, Usama, Gregory Piatetsky-Shapiro, and Padhraic Smyth. “From Data Mining to Knowledge Discovery in Databases.” <em>AI Magazine</em> 17.3 (1996): 37-54. Print.</li><li>“Mining Data for Nuggets of Knowledge.” <em>Knowledge@Wharton</em> ,1999. Web. Accessed 16 October 2013. SSRN: &lt;<a href="http://knowledge.wharton" target="_blank" rel="noopener noreferrer">http://knowledge.wharton</a>. <a href="http://upenn.edu/article/mining-data-for-nuggets-of-knowledge" target="_blank" rel="noopener noreferrer">upenn.edu/article/mining-data-for-nuggets-of-knowledge</a>&gt;</li><li>Cleveland, William S. “Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics.” <em>International Statistical</em><em>Review</em> 69.1 (2001): 21-26. Print.</li><li>Davenport, #omas H., and D.J. Patil. “Data Scientist: #e Sexiest Job of the 21st Century.” <em>Harvard Business Review</em> 90.10 (October 2012): 70–76. Print.</li><li>Smith, David. “Statistics vs Data Science vs BI.” <em>Revolutions</em> , 15 May 2013. Web. Accessed 15 October 2013. SSRN:&lt;<a href="http://blog" target="_blank" rel="noopener noreferrer">http://blog</a>. <a href="http://revolutionanalytics.com/2013/05/statistics-vs-data-science-vs-bi.html" target="_blank" rel="noopener noreferrer">revolutionanalytics.com/2013/05/statistics-vs-data-science-vs-bi.html</a>&gt;</li><li>Brynjolfsson, Erik, Lorin M. Hitt, and Heekyung H. Kim. “Strength in Numbers: How Does Data-Driven Decision Making A$ect Firm Performance?” <em>Social Science Electronic Publishing</em> , 22 April 2011. Web. Accessed 15 October 2013. SSRN: &lt;<a href="http://ssrn.com/abstract=1819486" target="_blank" rel="noopener noreferrer">http://ssrn.com/abstract=1819486</a> or <a href="http://dx.doi.org/10.2139/ssrn.1819486%3E" target="_blank" rel="noopener noreferrer">http://dx.doi.org/10.2139/ssrn.1819486&gt;</a></li><li>“#e Stages of an Analytic Enterprise.” <em>Nucleus Research</em>. Febr uar y 2012. Whitepaper.</li><li>Barua, Anitesh, Deepa Mani, and Rajiv Mukherjee. “Measuring the Business Impacts of E$ective Data.” <em>University of Texas</em>. Web. Accessed 15 October 2013. SSRNL: &lt;<a href="http://www.sybase.com/!les/White_Papers/" target="_blank" rel="noopener noreferrer">http://www.sybase.com/!les/White_Papers/</a> E$ectiveDataStudyPt1-MeasuringtheBusinessImpactsofE$ectiveDa ta-WP.pdf&gt;</li></ol><h1 id="-18"><a class="header-anchor" href="#-18" aria-hidden="true">#</a> !!</h1><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><ol start="10"><li>Zikopoulos, Paul, Dirk deRoos, Kirshnan Parasuraman, #omas Deutsch, David Corrigan and James Giles. <em>Harness the Power of Big Data: !e IBM</em><em>Big Data Platform</em>. Ne w York: McGraw Hill, 2013. Pr int. 281pp.</li><li>Booz Allen Hamilton. <em>Cloud Analytics Playbook</em>. 2013. Web. Accessed 15 October 2013. SSRN: &lt;<a href="http://www.boozallen.com/media/!le/Cloud-" target="_blank" rel="noopener noreferrer">http://www.boozallen.com/media/!le/Cloud-</a> playbook-digital.pdf&gt;</li><li>Conway, Drew. “#e Data Science Venn Diagram.” March 2013. Web. Accessed 15 October 2013. SSRN: &lt;<a href="http://drewconway.com/" target="_blank" rel="noopener noreferrer">http://drewconway.com/</a> zia/2013/3/26/the-data-science-venn-diagram&gt;</li><li>Torán, Jacobo. “On the Hardness of Graph Isomorphism.” <em>SIAM Journal</em><em>on Computing</em>. 33.5 (2004): 1093-1108. Pr int.</li><li>Guyon, Isabelle and Andre Elissee$. “An Introduction to Variable and Feature Selection.” <em>Journal of Machine Learning Research</em> 3 (March 2003):1157-1182. Print.</li><li>Golub T., D. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J. Mesirov, H. Coller, M. Loh, J. Downing, M. Caligiuri, C. Bloom!eld, and E. Lander. “Molecular Classi!cation of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring.” <em>Science.</em> 286.5439 (1999): 531-537. Print.</li><li>Haykin, Simon O. <em>Neural Networks and Learning Machines</em>. Ne w Jersey : Prentice Hall, 2008. Print.</li><li>De Jong, Kenneth A. <em>Evolutionary Computation - A Uni#ed Approach</em>. Massachusetts: MIT Press, 2002. Print.</li><li>Yacci, Paul, Anne Haake, and Roger Gaborski. “Feature Selection of Microarray Data Using Genetic Algorithms and Arti!cial Neural Networks.” ANNIE 2009. St Louis, MO. 2-4 November 2009. Conference Presentation.</li></ol><div class="language-"><pre><code>Closing Time 105
</code></pre></div><h2 id="about"><a class="header-anchor" href="#about" aria-hidden="true">#</a> About</h2><h2 id="booz-allen"><a class="header-anchor" href="#booz-allen" aria-hidden="true">#</a> BOOZ ALLEN</h2><h2 id="hamilton"><a class="header-anchor" href="#hamilton" aria-hidden="true">#</a> HAMILTON</h2><div class="language-"><pre><code>Booz Allen Hamilton has been at the forefront of strategy and
technology consulting for nearly a century. Today, Booz Allen
is a leading provider of management consulting, technology, and
engineering services to the US government in defense, intelligence,
and civil markets, and to major corporations, institutions, and not-
for-pro!t organizations. In the commercial sector, the !rm focuses
on leveraging its existing expertise for clients in the !nancial
services, healthcare, and energy markets, and to international clients
in the Middle East. Booz Allen o&quot;ers clients deep functional
knowledge spanning consulting, mission operations, technology, and
engineering—which it combines with specialized expertise in clients’
mission and domain areas to help solve their toughest problems.
</code></pre></div><div class="language-"><pre><code>#e !rm’s management consulting heritage is the basis for its unique
collaborative culture and operating model, enabling Booz Allen
to anticipate needs and opportunities, rapidly deploy talent and
resources, and deliver enduring results. By combining a consultant’s
problem-solving orientation with deep technical knowledge and
strong execution, Booz Allen helps clients achieve success in their
most critical missions—as evidenced by the !rm’s many client
relationships that span decades. Booz Allen helps shape thinking
and prepare for future developments in areas of national importance,
including cybersecurity, homeland security, healthcare, and
information technology.
</code></pre></div><div class="language-"><pre><code>Booz Allen is headquartered in McLean, Virginia, employs more than
23,000 people, and had revenue of $5.76 billion for the 12 months
ended March 31, 2013. For over a decade, Booz Allen’s high standing
as a business and an employer has been recognized by dozens of
organizations and publications, including Fortune, Working Mother,
G.I. Jobs, and DiversityInc. More information is available at
http://www.boozallen.com. (NYSE: BAH)
</code></pre></div><h1 id="-19"><a class="header-anchor" href="#-19" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>Closing Time 107
</code></pre></div><p>© COPYRIGHT 2013 BOOZ ALLEN HAMILTON INC. ALL RIGHTS RESERVED.</p><div class="language-"><pre><code>Artwork by Rafael Esquer.
</code></pre></div></div></div><footer class="page-footer" data-v-7eddb2c4 data-v-fb8d84c6><div class="edit" data-v-fb8d84c6><div class="edit-link" data-v-fb8d84c6 data-v-1ed99556><a class="link" href="https://github.com/ink-kin/vitepress-starter/edit/master/docs/blog/The-Field-Guide-to-Data-Science.md" target="_blank" rel="noopener noreferrer" data-v-1ed99556>Внести изменение страницы на GitHub <svg class="icon outbound icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15" data-v-1ed99556><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div></div><div class="updated" data-v-fb8d84c6><p class="last-updated" data-v-fb8d84c6 data-v-5797b537><span class="prefix" data-v-5797b537>Версия от :</span><span class="datetime" data-v-5797b537></span></p></div></footer><div class="next-and-prev-link" data-v-7eddb2c4 data-v-38ede35f><div class="container" data-v-38ede35f><div class="prev" data-v-38ede35f><a class="link" href="/blog/time" data-v-38ede35f><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" class="icon icon-prev" data-v-38ede35f><path d="M19,11H7.4l5.3-5.3c0.4-0.4,0.4-1,0-1.4s-1-0.4-1.4,0l-7,7c-0.1,0.1-0.2,0.2-0.2,0.3c-0.1,0.2-0.1,0.5,0,0.8c0.1,0.1,0.1,0.2,0.2,0.3l7,7c0.2,0.2,0.5,0.3,0.7,0.3s0.5-0.1,0.7-0.3c0.4-0.4,0.4-1,0-1.4L7.4,13H19c0.6,0,1-0.4,1-1S19.6,11,19,11z"></path></svg><span class="text" data-v-38ede35f>Cмысл фильма Время</span></a></div><div class="next" data-v-38ede35f><a class="link" href="/blog/strugackiy_arkadiy-za_milliard_let_do_konca_sveta" data-v-38ede35f><span class="text" data-v-38ede35f>За миллиард лет до конца света</span><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" class="icon icon-next" data-v-38ede35f><path d="M19.9,12.4c0.1-0.2,0.1-0.5,0-0.8c-0.1-0.1-0.1-0.2-0.2-0.3l-7-7c-0.4-0.4-1-0.4-1.4,0s-0.4,1,0,1.4l5.3,5.3H5c-0.6,0-1,0.4-1,1s0.4,1,1,1h11.6l-5.3,5.3c-0.4,0.4-0.4,1,0,1.4c0.2,0.2,0.5,0.3,0.7,0.3s0.5-0.1,0.7-0.3l7-7C19.8,12.6,19.9,12.5,19.9,12.4z"></path></svg></a></div></div></div><!--[--><!--]--></div></main></div><!----><!--]--></div>
    <script>__VP_HASH_MAP__ = JSON.parse("{\"configuration.md\":\"3464168c\",\"index.md\":\"ee94c27f\",\"marketing-book.md\":\"0c9dfdc1\",\"specifics-and-structure.md\":\"05879e98\",\"todone.md\":\"130ad03c\",\"blog_12-best-charting-libraries-for-web-developers.md\":\"455619a8\",\"blog_2020-11-11-introdiction.md\":\"598fd26b\",\"blog_2063485.md\":\"adf59cd1\",\"blog_22857009.md\":\"7482824e\",\"blog_5814905.md\":\"3c1dc4cd\",\"blog_6158845.md\":\"c7244504\",\"blog_7459819.md\":\"3c3cdf1c\",\"blog_anaconda-sods-report-2020-final.md\":\"6f367373\",\"blog_bluespice_xwiki.md\":\"526b972a\",\"blog_how-do-engineering-scientists-think.md\":\"4d17ffd1\",\"blog_ny2020.md\":\"a2b3b0e2\",\"blog_nanoblogger.md\":\"075c0b14\",\"blog_statlearnsparsity.md\":\"48bd749d\",\"blog_the-field-guide-to-data-science.md\":\"e31f9edd\",\"blog_about.md\":\"0b660d3a\",\"blog_about_my_edi.md\":\"a2e2e652\",\"blog_agile.md\":\"834e5c31\",\"blog_america-smm.md\":\"55bafb69\",\"blog_analist-todo.md\":\"5f4118b6\",\"blog_analist.md\":\"46edbfd4\",\"blog_archi_modelling_tool.md\":\"786b0834\",\"blog_archimate_courseware.md\":\"2af08ac3\",\"blog_arhivatsiya-v-linux-tar.md\":\"5dbff134\",\"blog_asciinema.md\":\"acade4e2\",\"blog_avoid-burnout-live-happy.md\":\"94650bde\",\"blog_babok.md\":\"b1837349\",\"blog_best-practices.md\":\"1660183b\",\"blog_bomb.md\":\"57cf329a\",\"blog_bpm_book_cbok.md\":\"5bfbfa02\",\"blog_bpm_soft.md\":\"69e60668\",\"blog_brd-mrd-prd-fsd-psd-srs.md\":\"8f42a903\",\"blog_business-analysis.md\":\"eef7bf61\",\"blog_chetvertaya_promyshlennaya_revolyuciya.md\":\"eb71f5a0\",\"blog_chetvertaya_promyshlennaya_revolyuciya_2016.md\":\"9d5d111e\",\"blog_chrome_answer.md\":\"8a8a36dd\",\"blog_clamav.md\":\"e10bbb34\",\"blog_convert_--help.md\":\"ce85212b\",\"blog_data-analist.md\":\"b714fcbf\",\"blog_do-five-things.md\":\"b954e75f\",\"blog_git-backuper.md\":\"46c0cfcd\",\"blog_gost-34-comments.md\":\"824aef2c\",\"blog_gost-34.md\":\"13644ad4\",\"blog_intellectual-values.md\":\"692f9f4d\",\"blog_jamstack.md\":\"d53c396f\",\"blog_jupyter.md\":\"2336991c\",\"blog_kommersant-ru-669072.md\":\"369b4641\",\"blog_kotler_f_marketing_ot_a_do_ya_80_k.md\":\"3fc5a9e1\",\"blog_lessons.md\":\"3de0291f\",\"blog_linux-command-basics-7-commands-process-management.md\":\"cbe471da\",\"blog_linux-rmlint.md\":\"c1c71ec4\",\"blog_linux.md\":\"ed43ce75\",\"blog_luchshih-distributivov-linux-dlia-usilennoi-konfidencialnosti-i-bezopasnosti.md\":\"a6907e08\",\"blog_man-joe.md\":\"81428e20\",\"blog_methods-of-optimal-solutions.md\":\"7f0ef2cd\",\"blog_mongodb_7_competitive_advantages.md\":\"2f8c8902\",\"blog_mvp.md\":\"d113e726\",\"blog_o-nakonets-nastal-tot-chas.md\":\"99f2115b\",\"blog_obama_budget_proposal_2012.md\":\"a24b6c25\",\"blog_osnovy_matematicheskogo_modelirovaniya.md\":\"d51b6785\",\"blog_paketnoe-izmenenie-izobrazhenij.md\":\"1082aae6\",\"blog_philosophy.md\":\"2428fca9\",\"blog_privacy-policy.md\":\"c984eed9\",\"blog_pyton-tools.md\":\"6492d94a\",\"blog_scenic_speech.md\":\"05dc6d7c\",\"blog_service.md\":\"e9f23c04\",\"blog_shalyapin_maska-i-dusha.md\":\"5b6c0c20\",\"blog_shaping_the_fourth_industrial_revolution.md\":\"53f48aff\",\"blog_shtat.md\":\"9720be2e\",\"blog_sqlite.md\":\"80a8f32a\",\"blog_strugackiy_arkadiy-za_milliard_let_do_konca_sveta.md\":\"40a25535\",\"blog_time.md\":\"252cc284\",\"blog_todo.md\":\"8f83dfbc\",\"blog_top-9-mongodb-tools.md\":\"80ad56a0\",\"blog_top-issue-support-and-bug-tracking-tools.md\":\"3cf596be\",\"blog_vidy-elektronnoy-podpisi.md\":\"ec778d28\",\"blog_vybirayem-generator-staticheskikh-saytov.md\":\"e04bab34\",\"blog_web-servers.md\":\"90e96694\",\"blog_what-is-kubernetes.md\":\"f6d31c7d\",\"blog_why-vuejs-need.md\":\"b4005c29\",\"blog_xmlstar.md\":\"1fdb5b39\",\"books_kotler_f_marketing_ot_a_do_ya_80_k.md\":\"281c5d65\",\"books_uml.md\":\"9b4d9542\",\"buh_index.md\":\"a859c29b\",\"innovate_index.md\":\"c7f0e164\",\"law_index.md\":\"882862d7\",\"people_index.md\":\"522000ab\",\"pip_case-method.md\":\"5e91e2e9\",\"pip_index.md\":\"f8cb937e\",\"pip_marketing.md\":\"427de6c3\",\"sales_index.md\":\"30351193\"}")</script>
    <script type="module" async src="/assets/app.e2261b09.js"></script>
  </body>
</html>