import{o as e,c as a,a as n}from"./app.e2261b09.js";const t='{"title":"Statistical Learning","description":"","frontmatter":{"title":"Statistical Learning","date":"2020/12/12 15:22:40","cover_index":"http://picsum.photos/450/450?random=15","tags":["data"],"categories":["Books","BPM"]},"headers":[{"level":2,"title":"FOREWORD","slug":"foreword"},{"level":2,"title":"THE STORY of THE","slug":"the-story-of-the"},{"level":2,"title":"FIELD","slug":"field"},{"level":2,"title":"GUIDE","slug":"guide"},{"level":2,"title":"WE ARE ALL","slug":"we-are-all"},{"level":2,"title":"AUTHORS of THIS","slug":"authors-of-this"},{"level":2,"title":"STORY","slug":"story"},{"level":2,"title":"THE OUTLINE","slug":"the-outline"},{"level":2,"title":"of OUR STORY","slug":"of-our-story"},{"level":2,"title":"MEET your GUIDES","slug":"meet-your-guides"},{"level":2,"title":"The SHORT","slug":"the-short"},{"level":2,"title":"VERSION","slug":"version"},{"level":3,"title":"What do We Mean by","slug":"what-do-we-mean-by"},{"level":3,"title":"Data Science?","slug":"data-science"},{"level":3,"title":"How does Data Science","slug":"how-does-data-science"},{"level":3,"title":"Actually Work?","slug":"actually-work"},{"level":3,"title":"What does it Take to Create","slug":"what-does-it-take-to-create"},{"level":3,"title":"a Data Science Capability?","slug":"a-data-science-capability"},{"level":3,"title":"Guiding Principles","slug":"guiding-principles"},{"level":3,"title":"#e Importance of Reason","slug":"e-importance-of-reason"},{"level":2,"title":"The Dangers of Rejection","slug":"the-dangers-of-rejection"},{"level":3,"title":"Component Parts of","slug":"component-parts-of"},{"level":3,"title":"Data Science","slug":"data-science-1"},{"level":3,"title":"Fractal Analytic Model","slug":"fractal-analytic-model"},{"level":3,"title":"#e Analytic","slug":"e-analytic"},{"level":3,"title":"Selection Process","slug":"selection-process"},{"level":2,"title":"Identifying Spoofed Domains","slug":"identifying-spoofed-domains"},{"level":2,"title":"4","slug":"_4"},{"level":2,"title":"3","slug":"_3"},{"level":2,"title":"2","slug":"_2"},{"level":2,"title":"1","slug":"_1"},{"level":3,"title":"Guide to Analytic Selection","slug":"guide-to-analytic-selection"},{"level":3,"title":"Detailed Table of Analytics","slug":"detailed-table-of-analytics"},{"level":3,"title":"Feature Engineering","slug":"feature-engineering"},{"level":2,"title":"Chemoinformatic Search","slug":"chemoinformatic-search"},{"level":3,"title":"Feature Selection","slug":"feature-selection"},{"level":2,"title":"Cancer Cell Classification","slug":"cancer-cell-classification"},{"level":3,"title":"Data Veracity","slug":"data-veracity"},{"level":2,"title":"Time Series Modeling","slug":"time-series-modeling"},{"level":3,"title":"Application of","slug":"application-of"},{"level":3,"title":"Domain Knowledge","slug":"domain-knowledge"},{"level":2,"title":"Motor Vehicle Theft","slug":"motor-vehicle-theft"},{"level":3,"title":"!e Curse of","slug":"e-curse-of"},{"level":3,"title":"Dimensionality","slug":"dimensionality"},{"level":2,"title":"Baking the Cake","slug":"baking-the-cake"},{"level":3,"title":"Model Validation","slug":"model-validation"},{"level":2,"title":"Consumer Behavior","slug":"consumer-behavior"},{"level":2,"title":"Analysis from a","slug":"analysis-from-a"},{"level":2,"title":"Multi-Terabyte","slug":"multi-terabyte"},{"level":2,"title":"Dataset","slug":"dataset"},{"level":2,"title":"Strategic Insights","slug":"strategic-insights"},{"level":2,"title":"within Terabytes of","slug":"within-terabytes-of"},{"level":2,"title":"Passenger Data","slug":"passenger-data"},{"level":2,"title":"Savings Through","slug":"savings-through"},{"level":2,"title":"Better Manufacturing","slug":"better-manufacturing"},{"level":2,"title":"Realizing Higher","slug":"realizing-higher"},{"level":2,"title":"Returns Through","slug":"returns-through"},{"level":2,"title":"Predictive Analytics","slug":"predictive-analytics"},{"level":2,"title":"PARTING","slug":"parting"},{"level":2,"title":"THOUGHTS","slug":"thoughts"},{"level":2,"title":"REFERENCES","slug":"references"},{"level":2,"title":"About","slug":"about"},{"level":2,"title":"BOOZ ALLEN","slug":"booz-allen"},{"level":2,"title":"HAMILTON","slug":"hamilton"}],"relativePath":"blog/The-Field-Guide-to-Data-Science.md","lastUpdated":1628440207824}',i={},o=n('<div class="language-"><pre><code>Every aspect of our lives, from life-saving disease\ntreatments, to national security, to economic stability\nand even the convenience of selecting a restaurant,\ncan be improved by creating better data analytics\nthrough Data Science.\n</code></pre></div><p>THE FIELD GUIDE to DATA SCIENCE</p><div class="language-"><pre><code>© COPYRIGHT 2013 BOOZ ALLEN HAMILTON INC. ALL RIGHTS RESERVED.\n</code></pre></div><h2 id="foreword"><a class="header-anchor" href="#foreword" aria-hidden="true">#</a> FOREWORD</h2><div class="language-"><pre><code>Every aspect of our lives, from life-saving disease\ntreatments, to national security, to economic stability\nand even the convenience of selecting a restaurant,\ncan be improved by creating better data analytics\nthrough Data Science.\n</code></pre></div><div class="language-"><pre><code>We live in a world of incredible\nbeauty and complexity. A world\nincreasingly measured, mapped,\nand recorded into digital bits for\neternity. Our human existence\nis pouring into the digital realm\nfaster than ever. From global\nbusiness operations to simple\nexpressions of love – an essential\npart of humanity now exists in\nthe digital world.\n</code></pre></div><div class="language-"><pre><code>Data is the byproduct of our\nnew digital existence. Recorded\nbits of data from mundane\ntra!c cameras to telescopes\npeering into the depths of\nspace are propelling us into\nthe greatest age of discovery\nour species has ever known.\n</code></pre></div><div class="language-"><pre><code>As we move from isolation into\nour ever-connected and recorded\nfuture, data is becoming the\nnew currency and a vital natural\nresource. &quot;e power, importance,\n</code></pre></div><div class="language-"><pre><code>and responsibility such incredible\ndata stewardship will demand of\nus in the coming decades is hard\nto imagine – but we often fail to\nfully appreciate the insights data\ncan provide us today. Businesses\nthat do not rise to the occasion\nand garner insights from this new\nresource are destined for failure.\n</code></pre></div><div class="language-"><pre><code>An essential part of human\nnature is our insatiable curiosity\nand the need to #nd answers to\nour hardest problems. Today, the\nemerging #eld of Data Science is\nan auspicious and profound new\nway of applying our curiosity\nand technical tradecraft to create\nvalue from data that solves our\nhardest problems. Leaps in\nhuman imagination, vast amounts\nof data on hundreds of topics,\nand humble algorithms can be\ncombined to create a radical new\nway of thinking about data. Our\nfuture is inextricably tied to data.\n</code></pre></div><div class="language-"><pre><code>We want to share our passion for Data Science and start a\nconversation with you. &quot;is is a journey worth taking.\n</code></pre></div><h1 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> !!</h1><h4 id="everyone-you"><a class="header-anchor" href="#everyone-you" aria-hidden="true">#</a> Everyone you</h4><h4 id="will-ever-meet"><a class="header-anchor" href="#will-ever-meet" aria-hidden="true">#</a> will ever meet</h4><h4 id="knows-something"><a class="header-anchor" href="#knows-something" aria-hidden="true">#</a> knows something</h4><h4 id="you-don’t"><a class="header-anchor" href="#you-don’t" aria-hidden="true">#</a> you don’t.</h4><div class="language-"><pre><code>[1]\n</code></pre></div><h2 id="the-story-of-the"><a class="header-anchor" href="#the-story-of-the" aria-hidden="true">#</a> THE STORY of THE</h2><h2 id="field"><a class="header-anchor" href="#field" aria-hidden="true">#</a> FIELD</h2><h2 id="guide"><a class="header-anchor" href="#guide" aria-hidden="true">#</a> GUIDE</h2><p>While there are countless industry and academic publications describing <em>what</em> Data Science is and <em>why</em> we should care, little information is available to explain how to make use of data as a resource. At Booz Allen, we built an industry-leading team of Data Scientists. Over the course of hundreds of analytic challenges for dozens of clients, we’ve unraveled the DNA of Data Science. We mapped the Data Science DNA to unravel the <em>what</em> , the <em>why</em> , the <em>who</em> and the <em>how</em>.</p><p>Many people have put forth their thoughts on single aspects of Data Science. We believe we can o!er a broad perspective on the conceptual models, tradecraft, processes and culture of Data Science. Companies with strong Data Science teams often focus on a single class of problems – graph algorithms for social network analysis and recommender models for online shopping are two notable examples. Booz Allen is di!erent. In our role as consultants, we support a diverse set of clients across a variety of domains. &quot;is allows us to uniquely understand the DNA of Data Science. Our goal in creating <em>!e Field Guide to Data Science</em> is to capture what we have learned and to share it broadly. We want this e!ort to help drive forward the science and art of Data Science.</p><p>&quot;is #eld guide came from the passion our team feels for its work. It is not a textbook nor is it a super#cial treatment. Senior leaders will walk away with a deeper understanding of the concepts at the heart of Data Science. Practitioners will add to their toolbox. We hope everyone will enjoy the journey.</p><div class="language-"><pre><code>» Why Data Science DNA?\n</code></pre></div><div class="language-"><pre><code>We view Data Science as having\nDNA-like characteristics. Much like\nDNA, Data Science is composed\nof basic building blocks that are\nwoven into a thing of great beauty\nand complexity. &quot;e building blocks\ncreate the blueprint, but successful\nreplication also requires carefully\nbalanced processes and the right\nset of environmental conditions.\nIn the end, every instance may look\nsuper#cially di!erent, but the raw\nmaterials remain the same.\n</code></pre></div><h1 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> !!</h1><h2 id="we-are-all"><a class="header-anchor" href="#we-are-all" aria-hidden="true">#</a> WE ARE ALL</h2><h2 id="authors-of-this"><a class="header-anchor" href="#authors-of-this" aria-hidden="true">#</a> AUTHORS of THIS</h2><h2 id="story"><a class="header-anchor" href="#story" aria-hidden="true">#</a> STORY</h2><div class="language-"><pre><code>We recognize that Data Science is a team sport. !e Field Guide\nto Data Science provides Booz Allen’s perspective on the complex\nand sometimes mysterious !eld of Data Science. We cannot\ncapture all that is Data Science. Nor can we keep up - the pace at\nwhich this !eld progresses outdates work as fast as it is produced.\nAs a result, we have opened this !eld guide to the world as a\nliving document to bend and grow with technology, expertise, and\nevolving techniques. If you !nd the guide to be useful, neat, or even\nlacking, then we encourage you to add your expertise, including:\n</code></pre></div><div class="language-"><pre><code>› Case studies from which you have learned\n› Citations for journal articles or papers that inspire you\n› Algorithms and techniques that you love\n› Your thoughts and comments on other people’s additions\n</code></pre></div><div class="language-"><pre><code>Email us your ideas and perspectives at data_science@bah.com\nor submit them via a pull request on the Github repository.\n</code></pre></div><div class="language-"><pre><code>Join our conversation and take the journey with us. Tell us and\nthe world what you know. Become an author of this story.\n</code></pre></div><h1 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> !!</h1><h2 id="the-outline"><a class="header-anchor" href="#the-outline" aria-hidden="true">#</a> THE OUTLINE</h2><h2 id="of-our-story"><a class="header-anchor" href="#of-our-story" aria-hidden="true">#</a> of OUR STORY</h2><p>(^10) <strong><em>!!</em></strong> <strong>Meet Your Guides</strong> (^13) <strong><em>!!</em></strong> <strong>!e Short Version –</strong> !e Core Concepts of Data Science (^14) <strong><em>!!</em></strong> <strong>Start Here for the Basics –</strong>^ An Introduction to Data Science What Do We Mean by Data Science? How Does Data Science Actually Work? What Does It Take to Create a Data Science Capability? (^40) <strong><em>!!</em></strong> <strong>Ta k e o &quot; t h e Tr a i n i n g W h e e l s –</strong>^ !e Practitioner’s Guide to Data Science Guiding Principles !e Importance of Reason Component Parts of Data Science Fractal Analytic Model !e Analytic Selection Process Guide to Analytic Selection Detailed Table of Analytics (^76) <strong><em>!!</em></strong> <strong>Life in the Trenches –</strong> Navigating Neck Deep in Data Feature Engineering Feature Selection Data Veracity Application of Domain Knowledge !e Curse of Dimensionality Model Validation (^90) <strong><em>!!</em></strong> <strong>Putting it all Together –</strong>^ Our Case Studies Consumer Behavior Analysis from a Multi-Terabyte Data Set Strategic Insights with Terabytes of Passenger Data Savings !rough Better Manufacturing Realizing Higher Returns !rough Predictive Analytics (^100) <strong><em>!!</em></strong> <strong>Closing Time</strong>^ Parting !oughts References About Booz Allen Hamilton</p><h1 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> !!</h1><h2 id="meet-your-guides"><a class="header-anchor" href="#meet-your-guides" aria-hidden="true">#</a> MEET your GUIDES</h2><div class="language-"><pre><code>Mark Herman\n(@cloudEBITDA)\n</code></pre></div><p>End every analysis with ... ‘and therefore’</p><div class="language-"><pre><code>Josh Sullivan\n(@joshdsullivan)\n</code></pre></div><p>Leading our Data Science team shows me every day the incredible power of discovery and human curiosity. Don’t be afraid to blend art and science to advance your own view of data analytics – it can be a powerful mixture.</p><div class="language-"><pre><code>Stephanie Rivera\n(@boozallen)\n</code></pre></div><div class="language-"><pre><code>I treat Data Science like I do rock\nclimbing: awesome dedication\nleads to incremental improvement.\nPersistence leads to the top.\n</code></pre></div><div class="language-"><pre><code>Peter Guerra\n(@petrguerra)\n</code></pre></div><div class="language-"><pre><code>Data Science is the most fascinating\nblend of art and math and code\nand sweat and tears. It can take\nyou to the highest heights and the\nlowest depths in an instant, but it\nis the only way we will be able to\nunderstand and describe the why.\n</code></pre></div><div class="language-"><pre><code>Steven Mills\n(@stevndmills)\n</code></pre></div><div class="language-"><pre><code>Data Science, like life, is not linear.\nIt’s complex, intertwined, and can be\nbeautiful. Success requires the support\nof your friends and colleagues.\n</code></pre></div><div class="language-"><pre><code>Alex Cosmas\n(@boozallen)\n</code></pre></div><div class="language-"><pre><code>Data miners produce bottle cap facts\n(e.g., animals that lay eggs don’t have\nbelly buttons). Data Scientists produce\ninsights - they require the intellectual\ncuriosity to ask “why” or “so what”?\n</code></pre></div><h1 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> !!</h1><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Drew Farris\n(@drewfarris)\n</code></pre></div><p>Don’t forget to play. Play with tools, play with data, and play with algorithms. You just might discover something that will help you solve that next nagging problem.</p><div class="language-"><pre><code>Brian Keller\n(@boozallen)\n</code></pre></div><p>Grit will get you farther than talent.</p><div class="language-"><pre><code>Ed Kohlwey\n(@ekohlwey)\n</code></pre></div><div class="language-"><pre><code>Data Science is about formally\nanalyzing everything around you\nand becoming data driven.\n</code></pre></div><div class="language-"><pre><code>Armen Kherlopian\n(@akherlopian)\n</code></pre></div><div class="language-"><pre><code>A Data Scientist must\ncontinuously seek truth in spite\nof ambiguity; therein rests the\nbasis of rigor and insight.\n</code></pre></div><div class="language-"><pre><code>Paul Yacci\n(@paulyacci)\n</code></pre></div><div class="language-"><pre><code>In the jungle of data, don’t\nmiss the forest for the trees,\nor the trees for the forest.\n</code></pre></div><div class="language-"><pre><code>Michael Kim\n(@boozallen)\n</code></pre></div><div class="language-"><pre><code>Data science is both an art\nand science.\n</code></pre></div><div class="language-"><pre><code>We would like to thank the following people for their\ncontributions and edits:\nTim Andrews, Mike Delurey, Greg Dupier, Jason Escaravage,\nChristine Fantaskey, Juergen Klenk, and Mark Rockley.\n</code></pre></div><div class="language-"><pre><code>Meet Your Guides 11\n</code></pre></div><h2 id="the-short"><a class="header-anchor" href="#the-short" aria-hidden="true">#</a> The SHORT</h2><h2 id="version"><a class="header-anchor" href="#version" aria-hidden="true">#</a> VERSION</h2><div class="language-"><pre><code>› Data Science is the art of turning data into actions.\nIt’s all about the tradecraft. Tradecraft is the process, tools and\ntechnologies for humans and computers to work together to\ntransform data into insights.\n</code></pre></div><div class="language-"><pre><code>› Data Science tradecraft creates data products.\nData products provide actionable information without exposing\ndecision makers to the underlying data or analytics (e.g., buy/sell\nstrategies for !nancial instruments, a set of actions to improve\nproduct yield, or steps to improve product marketing).\n</code></pre></div><div class="language-"><pre><code>› Data Science supports and encourages shifting between\ndeductive (hypothesis-based) and inductive (pattern-\nbased) reasoning.\n&quot;is is a fundamental change from traditional analysis approaches.\nInductive reasoning and exploratory data analysis provide a means\nto form or re!ne hypotheses and discover new analytic paths.\nModels of reality no longer need to be static. &quot;ey are constantly\ntested, updated and improved until better models are found.\n</code></pre></div><div class="language-"><pre><code>› Data Science is necessary for companies to stay with the\npack and compete in the future.\nOrganizations are constantly making decisions based on gut\ninstinct, loudest voice and best argument – sometimes they are\neven informed by real information. &quot;e winners and the losers in\nthe emerging data economy are going to be determined by their\nData Science teams.\n</code></pre></div><div class="language-"><pre><code>› Data Science capabilities can be built over time.\nOrganizations mature through a series of stages – Collect,\nDescribe, Discover, Predict, Advise – as they move from data\ndeluge to full Data Science maturity. At each stage, they can\ntackle increasingly complex analytic goals with a wider breadth\nof analytic capabilities. However, organizations need not reach\nmaximum Data Science maturity to achieve success. Signi!cant\ngains can be found in every stage.\n</code></pre></div><div class="language-"><pre><code>› Data Science is a di!erent kind of team sport.\nData Science teams need a broad view of the organization. Leaders\nmust be key advocates who meet with stakeholders to ferret out\nthe hardest challenges, locate the data, connect disparate parts of\nthe business, and gain widespread buy-in.\n</code></pre></div><h1 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> &quot;&quot;</h1><div class="language-"><pre><code>The Short Version 1313\n</code></pre></div><div class="language-"><pre><code>AN INTRODUCTION TO DATA SCIENCE\nIf you haven’t heard of Data Science, you’re behind the\ntimes. Just renaming your Business Intelligence group\nthe Data Science group is not the solution.\n</code></pre></div><h5 id="start-here-for-the-basics"><a class="header-anchor" href="#start-here-for-the-basics" aria-hidden="true">#</a> START HERE for THE BASICS</h5><h3 id="what-do-we-mean-by"><a class="header-anchor" href="#what-do-we-mean-by" aria-hidden="true">#</a> What do We Mean by</h3><h3 id="data-science"><a class="header-anchor" href="#data-science" aria-hidden="true">#</a> Data Science?</h3><div class="language-"><pre><code>Describing Data Science is like trying to describe a sunset – it\nshould be easy, but somehow capturing the words is impossible.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 1717\n</code></pre></div><h6 id="data-science-de-ned"><a class="header-anchor" href="#data-science-de-ned" aria-hidden="true">#</a> Data Science De!ned</h6><p>Data Science is the art of turning data into actions. #is is accomplished through the creation of data products, which provide actionable information without exposing decision makers to the underlying data or analytics (e.g., buy/sell strategies for !nancial instruments, a set of actions to improve product yield, or steps to improve product marketing).</p><p>Performing Data Science requires the extraction of timely, actionable information from diverse data sources to drive data products. Examples of data products include answers to questions such as: “Which of my products should I advertise more heavily to increase pro!t? How can I improve my compliance program, while reducing costs? What manufacturing process change will allow me to build a better product?” #e key to answering these questions is: understand the data you have and what the data inductively tells you.</p><div class="language-"><pre><code>» Data Product\n</code></pre></div><div class="language-"><pre><code>A data product provides actionable\ninformation without exposing\ndecision makers to the underlying\ndata or analytics. Examples include:\n</code></pre></div><ul><li>Movie Recommendations</li><li>We a t h e r Fo recasts</li><li>Stock Market Predictions</li><li>Production Process Improvements</li><li>Health Diagnosis</li><li>Flu Trend Predictions</li><li>Ta r g e t e d Advertising</li></ul><p><strong><em>Read this for additional background:</em></strong></p><p>#e term Data Science appeared in the computer science literature throughout the 1960s-1980s. It was not until the late 1990s however, that the !eld as we describe it here, began to emerge from the statistics and data mining communities (e.g., [2] and [3]). Data Science was !rst introduced as an independent discipline in 2001.[4]^ Since that time, there have been countless articles advancing the discipline, culminating with Data Scientist being declared the sexiest job of the 21st century.[5]</p><div class="language-"><pre><code>We e s t a b l i s h e d o u r !r s t D a t a\nScience team at Booz Allen\nin 2010. It began as a natural\nextension of our Business\nIntelligence and cloud\n</code></pre></div><div class="language-"><pre><code>infrastructure development\nwork. We saw the need for a\nnew approach to distill value\nfrom our clients’ data. We\napproached the problem\nwith a multidisciplinary\nteam of computer scientists,\nmathematicians and domain\nexperts. #ey immediately\nproduced new insights and\nanalysis paths, solidifying the\nvalidity of the approach. Since\nthat time, our Data Science\nteam has grown to 250 sta$\nsupporting dozens of clients\nacross a variety of domains.\n#is breadth of experience\nprovides a unique perspective\non the conceptual models,\ntradecraft, processes and culture\nof Data Science.\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><h6 id="what-makes-data-science-di-erent"><a class="header-anchor" href="#what-makes-data-science-di-erent" aria-hidden="true">#</a> What makes Data Science Di&quot;erent?</h6><div class="language-"><pre><code>Data Science supports and encourages shifting between deductive\n(hypothesis-based) and inductive (pattern-based) reasoning. #is is\na fundamental change from traditional analytic approaches. Inductive\nreasoning and exploratory data analysis provide a means to form or\nre!ne hypotheses and discover new analytic paths. In fact, to do the\ndiscovery of signi!cant insights that are the hallmark of Data Science,\nyou must have the tradecraft and the interplay between inductive\nand deductive reasoning. By actively combining the ability to reason\ndeductively and inductively, Data Science creates an environment\nwhere models of reality no longer need to be static and empirically\nbased. Instead, they are constantly tested, updated and improved until\nbetter models are found. #ese concepts are summarized in the !gure,\n!e Types of Reason and !eir Role in Data Science Tradecraft.\n</code></pre></div><div class="language-"><pre><code>THE TYPES OF REASON...\n</code></pre></div><div class="language-"><pre><code>DEDUCTIVE REASONING:\n</code></pre></div><div class="language-"><pre><code>› Commonly associated\nwith “formal logic.”\n› Involves reasoning from known\npremises, or premises presumed\nto be true, to a certain conclusion.\n› The conclusions reached are\ncertain, inevitable, inescapable.\n</code></pre></div><div class="language-"><pre><code>INDUCTIVE REASONING\n</code></pre></div><div class="language-"><pre><code>› Commonly known as “informal\nlogic,” or “everyday argument.”\n› Involves drawing uncertain\ninferences, based on\nprobabilistic reasoning.\n› The conclusions reached\nare probable, reasonable,\nplausible, believable.\n</code></pre></div><div class="language-"><pre><code>...AND THEIR ROLE IN DATA SCIENCE TRADECRAFT.\n</code></pre></div><div class="language-"><pre><code>DEDUCTIVE REASONING:\n</code></pre></div><div class="language-"><pre><code>› Formulate hypotheses about\nrelationships and underlying models.\n› Carry out experiments with the data\nto test hypotheses and models.\n</code></pre></div><div class="language-"><pre><code>INDUCTIVE REASONING\n</code></pre></div><div class="language-"><pre><code>› Exploratory data analysis to\ndiscover or refine hypotheses.\n› Discover new relationships, insights\nand analytic paths from the data.\n</code></pre></div><div class="language-"><pre><code>The Types of Reason and Their Role in Data Science Tradecraft\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 1919\n</code></pre></div><p>#e di$erences between Data Science and traditional analytic approaches do not end at seamless shifting between deductive and inductive reasoning. Data Science o$ers a distinctly di$erent perspective than capabilities such as Business Intelligence. Data Science should not replace Business Intelligence functions within an organization, however. #e two capabilities are additive and complementary, each o$ering a necessary view of business operations and the operating environment. #e !gure, <em>Business Intelligence and Data Science – A Comparison,</em> highlights the di$erences between the two capabilities. Key contrasts include:</p><div class="language-"><pre><code>› Discovery vs. Pre-canned Questions: Data Science actually\nworks on discovering the question to ask as opposed to just\nasking it.\n› Power of Many vs. Ability of One: An entire team provides\na common forum for pulling together computer science,\nmathematics and domain expertise.\n› Prospective vs. Retrospective: Data Science is focused on\nobtaining actionable information from data as opposed to\nreporting historical facts.\n</code></pre></div><div class="language-"><pre><code>LOOKING BACKWARD AND FORWARD\n</code></pre></div><div class="language-"><pre><code>FIRST THERE WAS\nBUSINESS INTELLIGENCE\n</code></pre></div><div class="language-"><pre><code>Deductive Reasoning\nBackward Looking\nSlice and Dice Data\nWarehoused and Siloed Data\nAnalyze the Past, Guess the Future\nCreates Reports\nAnalytic Output\n</code></pre></div><div class="language-"><pre><code>NOW WE&#39;VE ADDED\nDATA SCIENCE\n</code></pre></div><div class="language-"><pre><code>Inductive and Deductive Reasoning\nForward Looking\nInteract with Data\nDistributed, Real Time Data\nPredict and Advise\nCreates Data Products\nAnswer Questions and Create New Ones\nActionable Answer\n</code></pre></div><div class="language-"><pre><code>Business Intelligence and Data Science - A Comparison (adapted in part from [6])\n</code></pre></div><h6 id="what-is-the-impact-of-data-science"><a class="header-anchor" href="#what-is-the-impact-of-data-science" aria-hidden="true">#</a> What is the Impact of Data Science?</h6><div class="language-"><pre><code>As we move into the data economy, Data Science is the competitive\nadvantage for organizations interested in winning – in whatever way\nwinning is de!ned. #e manner in which the advantage is de!ned\nis through improved decision-making. A former colleague liked to\ndescribe data-informed decision making like this: If you have perfect\ninformation or zero information then your task is easy – it is in between\nthose two extremes that the trouble begins. What he was highlighting is\nthe stark reality that whether or not information is available, decisions\nmust be made.\n</code></pre></div><div class="language-"><pre><code>#e way organizations make decisions has been evolving for half a\ncentury. Before the introduction of Business Intelligence, the only\noptions were gut instinct, loudest voice, and best argument. Sadly, this\nmethod still exists today, and in some pockets it is the predominant\nmeans by which the organization acts. Take our advice and never, ever\nwork for such a company!\n</code></pre></div><div class="language-"><pre><code>Fortunately for our economy, most organizations began to inform\ntheir decisions with real information through the application of\nsimple statistics. #ose that did it well were rewarded; those that did\nnot failed. We are outgrowing the ability of simple stats to keep pace\nwith market demands, however. #e rapid expansion of available data,\nand the tools to access and make use of the data at scale, are enabling\nfundamental changes to the way organizations make decisions.\n</code></pre></div><div class="language-"><pre><code>Data Science is required to maintain competitiveness in the\nincreasingly data-rich environment. Much like the application of\nsimple statistics, organizations that embrace Data Science will be\nrewarded while those that do not will be challenged to keep pace.\nAs more complex, disparate data sets become available, the chasm\nbetween these groups will only continue to widen. #e !gure,\n!e Business Impacts of Data Science, highlights the value awaiting\norganizations that embrace Data Science.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 2121\n</code></pre></div><div class="language-"><pre><code>DATA SCIENCE IS NECESSARY...\n</code></pre></div><p>17-49%^ increase in productivity when organizations increase data usability by 10%</p><p>11-42% return on assets (ROA) when organizations increase data access by 10%</p><p>241%^ increase in ROI when organizations use big data to^ improve competitiveness</p><p>1000%</p><div class="language-"><pre><code>increase in ROI when deploying analytics across most of\nthe organization, aligning daily operations with senior\nmanagement&#39;s goals, and incorporating big data\n</code></pre></div><p>5-6%^ performance improvement for organizations making^ data-driven decisions.</p><div class="language-"><pre><code>...TO COMPETE IN THE FUTURE\n</code></pre></div><div class="language-"><pre><code>The Business Impacts of Data Science (adapted from [7], [8] and [9])\n</code></pre></div><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 2323\n</code></pre></div><h6 id="what-is-di-erent-now"><a class="header-anchor" href="#what-is-di-erent-now" aria-hidden="true">#</a> What is Di&quot;erent Now?</h6><p>For 20 years IT systems were built the same way. We separated the people who ran the business from the people who managed the infrastructure (and therefore saw data as simply another thing they had to manage). With the advent of new technologies and analytic techniques, this arti!cial – and highly ine$ective – separation of critical skills is no longer necessary. For the !rst time, organizations can directly connect business decision makers to the data. #is simple step transforms data from being ‘something to be managed’ into</p><p>‘something to be valued.’</p><p>In the wake of the transformation, organizations face a stark choice: you can continue to build data silos and piece together disparate information or you can consolidate your data and distill answers.</p><p>From the Data Science perspective, this is a false choice: #e siloed approach is untenable when you consider the (a) the opportunity cost of not making maximum use of all available data to help an organization succeed, and (b) the resource and time costs of continuing down the same path with outdated processes. #e tangible bene!ts of data products include:</p><p>› <strong><em>Opportunity Costs:</em></strong> Because Data Science is an emerging !eld, opportunity costs arise when a competitor implements and generates value from data before you. Failure to learn and account for changing customer demands will inevitably drive customers away from your current o$erings. When competitors are able to successfully leverage Data Science to gain insights, they can drive di$erentiated customer value propositions and lead their industries as a result.</p><p>› <strong><em>Enhanced Processes:</em></strong> As a result of the increasingly interconnected world, huge amounts of data are being generated and stored every instant. Data Science can be used to transform data into insights that help improve existing processes. Operating costs can be driven down dramatically by e$ectively incorporating the complex interrelationships in data like never before. #is results in better quality assurance, higher product yield and more e$ective operations.</p><h3 id="how-does-data-science"><a class="header-anchor" href="#how-does-data-science" aria-hidden="true">#</a> How does Data Science</h3><h3 id="actually-work"><a class="header-anchor" href="#actually-work" aria-hidden="true">#</a> Actually Work?</h3><div class="language-"><pre><code>It’s not rocket science... it’s something better - Data Science\n</code></pre></div><div class="language-"><pre><code>Let’s not kid ourselves - Data Science is a complex !eld. It is di%cult,\nintellectually taxing work, which requires the sophisticated integration\nof talent, tools and techniques. But as a !eld guide, we need to cut\nthrough the complexity and provide a clear, yet e$ective way to\nunderstand this new world.\n</code></pre></div><div class="language-"><pre><code>To do this, we will transform the !eld of Data Science into a set of\nsimpli!ed activities as shown in the !gure, !e Four Key Activities of a\nData Science Endeavor. Data Science purists will likely disagree with\nthis approach, but then again, they probably don’t need a !eld guide,\nsitting as they do in their ivory towers! In the real world, we need\nclear and simple operating models to help drive us forward.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><p>1 2 3 4</p><p><strong>Acquire Prepare Analyze Act</strong></p><div class="language-"><pre><code>Low\n</code></pre></div><div class="language-"><pre><code>High\n</code></pre></div><p>Degree of Effort</p><div class="language-"><pre><code>Data Science Activities\n</code></pre></div><div class="language-"><pre><code>Try\n</code></pre></div><div class="language-"><pre><code>Evaluate\n</code></pre></div><div class="language-"><pre><code>Setup Do\n</code></pre></div><div class="language-"><pre><code>Evaluate\n</code></pre></div><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 2525\n</code></pre></div><div class="language-"><pre><code>Activity 1: Acquire\nThis activity focuses\non obtaining the\ndata you need.\nGiven the nature of\ndata, the details of\nthis activity depend\nheavily on who you\nare and what you\ndo. As a result, we\nwill not spend a\nlot of time on this\nactivity other than\nto emphasize its\nimportance and\nto encourage an\nexpansive view on\nwhich data can and\nshould be used.\n</code></pre></div><div class="language-"><pre><code>Activity 2: Prepare\nGreat outcomes\ndon’t just happen\nby themselves.\nA lot depends on\npreparation, and\nin Data Science,\nthat means\nmanipulating the\ndata to fit your\nanalytic needs.\nThis stage can\nconsume a great\ndeal of time, but\nit is an excellent\ninvestment. The\nbenefits are\nimmediate and\nlong term.\n</code></pre></div><div class="language-"><pre><code>Activity 3: Analyze\nThis is the activity\nthat consumes the\nlion’s share of the\nteam’s attention.\nIt is also the most\nchallenging and\nexciting (you will\nsee a lot of ‘aha\nmoments’ occur in\nthis space). As the\nmost challenging\nand vexing of the\nfour activities,\nthis field guide\nfocuses on helping\nyou do this better\nand faster.\n</code></pre></div><div class="language-"><pre><code>Activity 4: Act\nEvery effective\nData Science team\nanalyzes its data\nwith a purpose\n</code></pre></div><ul><li>that is, to turn data into actions. Actionable and impactful insights are the holy grail of Data Science. Converting insights into action can be a politically charged activity, however. This activity depends heavily on the culture and character of your organization, so we will leave you to figure out those details for yourself.</li></ul><div class="language-"><pre><code>The Four Key Activities of a Data Science Endeavor\n</code></pre></div><h6 id="acquire"><a class="header-anchor" href="#acquire" aria-hidden="true">#</a> Acquire</h6><div class="language-"><pre><code>All analysis starts with access to data, and for the Data Scientist\nthis axiom holds true. But there are some signi!cant di$erences –\nparticularly with respect to the question of who stores, maintains and\nowns the data in an organization.\n</code></pre></div><div class="language-"><pre><code>But before we go there, lets look at what is changing. Traditionally,\nrigid data silos arti!cially de!ne the data to be acquired. Stated\nanother way, the silos create a !lter that lets in a very small amount of\ndata and ignores the rest. #ese !ltered processes give us an arti!cial\nview of the world based on the ‘surviving data,’ rather than one that\nshows full reality and meaning. Without a broad and expansive data\nset, we can never immerse ourselves in the diversity of the data. We\ninstead make decisions based on limited and constrained information.\n</code></pre></div><div class="language-"><pre><code>Eliminating the need for silos gives us access to all the data at once –\nincluding data from multiple outside sources. It embraces the reality\nthat diversity is good and complexity is okay. #is mindset creates a\ncompletely di$erent way of thinking about data in an organization by\ngiving it a new and di$erentiated role. Data represents a signi!cant\nnew pro!t and mission-enhancement opportunity for organizations.\n</code></pre></div><div class="language-"><pre><code>But as mentioned earlier, this !rst activity is heavily dependent upon\nthe situation and circumstances. We can’t leave you with anything\nmore than general guidance to help ensure maximum value:\n</code></pre></div><div class="language-"><pre><code>› Look inside &quot;rst: What data do you have current access\nto that you are not using? #is is in large part the data\nbeing left behind by the !ltering process, and may be\nincredibly valuable.\n› Remove the format constraints: Stop limiting your data\nacquisition mindset to the realm of structured databases.\nInstead, think about unstructured and semi-structured data\nas viable sources.\n› Figure out what’s missing: Ask yourself what data would\nmake a big di$erence to your processes if you had access to it.\n#en go !nd it!\n› Embrace diversity: Try to engage and connect to publicly\navailable sources of data that may have relevance to your\ndomain area.\n</code></pre></div><div class="language-"><pre><code>» Not All Data Is Created Equal\n</code></pre></div><div class="language-"><pre><code>As you begin to aggregate data,\nremember that not all data is\ncreated equally. Organizations have\na tendency to collect any data that\nis available. Data that is nearby\n(readily accessible and easily\nobtained) may be cheap to collect,\nbut there is no guarantee it is the\nright data to collect. Focus on the\ndata with the highest ROI for your\norganization. Your Data Science\nteam can help identify that data.\nAlso remember that you need to\nstrike a balance between the data\nthat you need and the data that you\nhave. Collecting huge volumes of\ndata is useless and costly if it is not\nthe data that you need.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 2727\n</code></pre></div><h6 id="prepare"><a class="header-anchor" href="#prepare" aria-hidden="true">#</a> Prepare</h6><p>Once you have the data, you need to prepare it for analysis.</p><p>Organizations often make decisions based on inexact data. Data stovepipes mean that organizations may have blind spots. #ey are not able to see the whole picture and fail to look at their data and challenges holistically. #e end result is that valuable information is withheld from decision makers. Research has shown almost 33% of decisions are made without good data or information. [10]</p><p>When Data Scientists are able to explore and analyze all the data, new opportunities arise for analysis and data-driven decision making. #e insights gained from these new opportunities will signi!cantly change the course of action and decisions within an organization. Gaining access to an organization’s complete repository of data, however, requires preparation.</p><p>Our experience shows time and time again that the best tool for Data Scientists to prepare for analysis is a lake – speci!cally, the Data Lake.[11] #is is a new approach to collecting, storing and integrating data that helps organizations maximize the utility of their data. Instead of storing information in discrete data structures, the Data Lake consolidates an organization’s complete repository of data in a single, large view. It eliminates the expensive and cumbersome data-preparation process, known as Extract/Transform/Load (ETL), necessary with data silos. #e entire body of information in the Data Lake is available for every inquiry – and all at once.</p><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><h6 id="analyze"><a class="header-anchor" href="#analyze" aria-hidden="true">#</a> Analyze</h6><div class="language-"><pre><code>We have acquired the data... we have prepared it... now it is time to\nanalyze it.\n</code></pre></div><div class="language-"><pre><code>#e Analyze activity requires the greatest e$ort of all the activities\nin a Data Science endeavor. #e Data Scientist actually builds the\nanalytics that create value from data. Analytics in this context is\nan iterative application of specialized and scalable computational\nresources and tools to provide relevant insights from exponentially\ngrowing data. #is type of analysis enables real-time understanding\nof risks and opportunities by evaluating situational, operational and\nbehavioral data.\n</code></pre></div><div class="language-"><pre><code>With the totality of data fully accessible in the Data Lake,\norganizations can use analytics to !nd the kinds of connections and\npatterns that point to promising opportunities. #is high-speed\nanalytic connection is done within the Data Lake, as opposed to\nolder style sampling methods that could only make use of a narrow\nslice of the data. In order to understand what was in the lake, you had\nto bring the data out and study it. Now you can dive into the lake,\nbringing your analytics to the data. #e !gure, Analytic Connection in\nthe Data Lake, highlights the concept of diving into the Data Lake to\ndiscover new connections and patterns.\n</code></pre></div><div class="language-"><pre><code>Analytic Connection in the Data Lake\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 2929\n</code></pre></div><p>Data Scientists work across the spectrum of analytic goals – Describe, Discover, Predict and Advise. #e maturity of an analytic capability determines the analytic goals encompassed. Many variables play key roles in determining the di%culty and suitability of each goal for an organization. Some of these variables are the size and budget of an organization and the type of data products needed by the decision makers. A detailed discussion on analytic maturity can be found in <em>Data Science Maturity within an Organization</em>.</p><p>In addition to consuming the greatest e$ort, the Analyze activity is by far the most complex. #e tradecraft of Data Science is an art. While we cannot teach you how to be an artist, we can share foundational tools and techniques that can help you be successful. #e entirety of <em>Take O&quot; the Training Wheels</em> is dedicated to sharing insights we have learned over time while serving countless clients. #is includes descriptions of a Data Science product lifecycle and the <em>Fractal Analytic Model</em> (FAM). #e <em>Analytic Selection Process</em> and accompanying <em>Guide to Analytic Selection</em> provide key insights into one of the most challenging tasks in all of Data Science – selecting the right technique for the job.</p><h6 id="act"><a class="header-anchor" href="#act" aria-hidden="true">#</a> Act</h6><p>Now that we have analyzed the data, it’s time to take action.</p><p>#e ability to make use of the analysis is critical. It is also very situational. Like the Acquire activity, the best we can hope for is to provide some guiding principles to help you frame the output for maximum impact. Here are some key points to keep in mind when presenting your results:</p><ol><li>#e !nding must make sense with relatively little up-front training or preparation on the part of the decision maker.</li><li>#e !nding must make the most meaningful patterns, trends and exceptions easy to see and interpret.</li><li>Every e$ort must be made to encode quantitative data accurately so the decision maker can accurately interpret and compare the data.</li><li>#e logic used to arrive at the !nding must be clear and compelling as well as traceable back through the data.</li><li>#e !ndings must answer real business questions.</li></ol><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Proportion\nof\nEffort\n</code></pre></div><div class="language-"><pre><code>Maturity\n</code></pre></div><div class="language-"><pre><code>Stages of Maturity\n</code></pre></div><div class="language-"><pre><code>Collect\n</code></pre></div><div class="language-"><pre><code>Describe\n</code></pre></div><div class="language-"><pre><code>Discover\n</code></pre></div><div class="language-"><pre><code>Predict\n</code></pre></div><div class="language-"><pre><code>Advise\n</code></pre></div><div class="language-"><pre><code>Data Silos\n</code></pre></div><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 3131\n</code></pre></div><h6 id="data-science-maturity-within"><a class="header-anchor" href="#data-science-maturity-within" aria-hidden="true">#</a> Data Science Maturity within</h6><h6 id="an-organization"><a class="header-anchor" href="#an-organization" aria-hidden="true">#</a> an Organization</h6><p>#e four activities discussed thus far provide a simpli!ed view of Data Science. Organizations will repeat these activities with each new Data Science endeavor. Over time, however, the level of e$ort necessary for each activity will change. As more data is Acquired and Prepared in the Data Lake, for example, signi!cantly less e$ort will need to be expended on these activities. #is is indicative of a maturing Data Science capability.</p><p>Assessing the maturity of your Data Science capability calls for a slightly di$erent view. We use <em>!e Data Science Maturity Model</em> as a common framework for describing the maturity progression and components that make up a Data Science capability. #is framework can be applied to an organization’s Data Science capability or even to the maturity of a speci!c solution, namely a data product. At each stage of maturity, powerful insight can be gained.</p><div class="language-"><pre><code>The Data Science Maturity Model\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>When organizations start out, they have Data Silos. At this stage,\nthey have not carried out any broad Aggregate activities. #ey may\nnot have a sense of all the data they have or the data they need. #e\ndecision to create a Data Science capability signals the transition into\nthe Collect stage.\n</code></pre></div><div class="language-"><pre><code>All of your initial e$ort will be focused on identifying and aggregating\ndata. Over time, you will have the data you need and a smaller\nproportion of your e$ort can focus on Collect. You can now begin to\nDescribe your data. Note, however, that while the proportion of time\nspent on Collect goes down dramatically, it never goes away entirely.\n#is is indicative of the four activities outlined earlier – you will\ncontinue to Aggregate and Prepare data as new analytic questions\narise, additional data is needed and new data sources become available.\n</code></pre></div><div class="language-"><pre><code>Organizations continue to advance in maturity as they move through\nthe stages from Describe to Advise. At each stage they can tackle\nincreasingly complex analytic goals with a wider breadth of analytic\ncapabilities. As described for Collect , each stage never goes away\nentirely. Instead, the proportion of time spent focused on it goes\ndown and new, more mature activities begin. A brief description\nof each stage of maturity is shown in the table !e Stages of Data\nScience Maturity.\n</code></pre></div><div class="language-"><pre><code>The Stages of Data Science Maturity\n</code></pre></div><div class="language-"><pre><code>Stage Description Example\n</code></pre></div><div class="language-"><pre><code>Collect Focuses on collecting internal or external datasets. Gathering sales records and corresponding weather data.\n</code></pre></div><div class="language-"><pre><code>Describe\n</code></pre></div><div class="language-"><pre><code>Seeks to enhance or\nrefine raw data as well\nas leverage basic analytic\nfunctions such as counts.\n</code></pre></div><div class="language-"><pre><code>How are my customers\ndistributed with respect to\nlocation, namely zip code?\n</code></pre></div><div class="language-"><pre><code>Discover Identifies hidden relationships or patterns.\n</code></pre></div><div class="language-"><pre><code>Are there groups within\nmy regular customers that\npurchase similarly?\n</code></pre></div><div class="language-"><pre><code>Predict\n</code></pre></div><div class="language-"><pre><code>Utilizes past observations to\npredict future observations.\n</code></pre></div><div class="language-"><pre><code>Can we predict which products\nthat certain customer groups\nare more likely to purchase?\n</code></pre></div><div class="language-"><pre><code>Advise\n</code></pre></div><div class="language-"><pre><code>Defines your possible decisions,\noptimizes over those decisions,\nand advises to use the decision\nthat gives the best outcome.\n</code></pre></div><div class="language-"><pre><code>Your advice is to target advertise\nto specific groups for certain\nproducts to maximize revenue.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 3333\n</code></pre></div><p>#e maturity model provides a powerful tool for understanding and appreciating the maturity of a Data Science capability. Organizations need not reach maximum maturity to achieve success. Signi!cant gains can be found in every stage. We believe strongly that one does not engage in a Data Science e$ort, however, unless it is intended to produce an output – that is, you have the intent to <em>Advise</em>. #is means simply that each step forward in maturity drives you to the right in the model diagram. Moving to the right requires the correct processes, people, culture and operating model – a robust Data Science capability. <em>What Does it Take to Create a Data Science Capability?</em> addresses this topic.</p><p>We have observed very few organizations actually operating at the highest levels of maturity, the <em>Predict</em> and <em>Advise</em> stages. #e tradecraft of <em>Discover</em> is only now maturing to the point that organizations can focus on advanced <em>Predict</em> and <em>Advise</em> activities. #is is the new frontier of Data Science. #is is the space in which we will begin to understand how to close the cognitive gap between humans and computers. Organizations that reach <em>Advise</em> will be met with true insights and real competitive advantage.</p><div class="language-"><pre><code>» Where does your organization\nfall in analytic maturity?\n</code></pre></div><div class="language-"><pre><code>Take the quiz!\n</code></pre></div><p><strong>1. How many data sources do</strong><strong>you collect?</strong> a. Why do we need a bunch of data? <em>- 0 points, end here.</em> b. I don’t know the exact number. <em>- 5 points</em> c. We identified the required data and collect it. <em>– 10 points</em><strong>2. Do you know what questions</strong><strong>your Data Science team is trying</strong><strong>to answer?</strong> a. Why do we need questions? <em>- 0 points</em> b. No, they figure it out for themselves. <em>- 5 points</em> c. Yes, we evaluated the questions that will have the largest impact to the business. <em>– 10 points</em><strong>3. Do you know the important factors</strong><strong>driving your business?</strong> a. I have no idea. <em>– 0 points</em> b. Our quants help me figure it out. <em>- 5 points</em> c. We have a data product for that. <em>- 10 points</em><strong>4. Do you have an understanding of</strong><strong>future conditions?</strong> a. I look at the current conditions and read the tea leaves. <em>– 0 points</em> b. We have a data product for that. <em>- 5 points</em><strong>5. Do you know the best course</strong><strong>of action to take for your key</strong><strong>decisions?</strong> a. I look at the projections and plan a course. <em>– 0 points</em> b. We have a data product for that. <em>- 5 points</em></p><div class="language-"><pre><code>Check your score:\n</code></pre></div><div class="language-"><pre><code>0 – Data Silos, 5-10 – Collect,\n10-20 – Describe, 20-30 – Discover,\n30-35 – Predict, 35-40 - Advise\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>COMPUTER SCIENCE\nProvides the environment\nin which data products\nare created.\n</code></pre></div><div class="language-"><pre><code>DOMAIN EXPERTISE\nProvides understanding\nof the reality in which a\nproblem space exists.\n</code></pre></div><div class="language-"><pre><code>MATHEMATICS\nProvides the theoretical\nstructure in which Data\nScience problems\nare examined.\n</code></pre></div><h3 id="what-does-it-take-to-create"><a class="header-anchor" href="#what-does-it-take-to-create" aria-hidden="true">#</a> What does it Take to Create</h3><h3 id="a-data-science-capability"><a class="header-anchor" href="#a-data-science-capability" aria-hidden="true">#</a> a Data Science Capability?</h3><div class="language-"><pre><code>Data Science is all about building teams and culture.\n</code></pre></div><div class="language-"><pre><code>As with any team sport, Data Science depends on a diverse set of skills\nto achieve its objective – winning at the game of improved insights.\nYou need the three skill sets shown in !e Data Science Venn Diagram\nto create a winning team in the world of Data Science.\n</code></pre></div><div class="language-"><pre><code>Building Data Science teams is di%cult. It requires an understanding\nof the types of personalities that make Data Science possible, as well\nas a willingness to establish a culture of innovation and curiosity in\nyour organization. You must also consider how to deploy the team and\ngain widespread buy-in from across your organization.\n</code></pre></div><div class="language-"><pre><code>The Data Science Venn Diagram (inspired by [12])\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 3535\n</code></pre></div><h6 id="understanding-what-makes"><a class="header-anchor" href="#understanding-what-makes" aria-hidden="true">#</a> Understanding What Makes</h6><h6 id="a-data-scientist"><a class="header-anchor" href="#a-data-scientist" aria-hidden="true">#</a> a Data Scientist</h6><p>Data Science often requires a signi!cant investment of time across a variety of tasks. Hypotheses must be generated and data must be acquired, prepared, analyzed, and acted upon. Multiple techniques are often applied before one yields interesting results. If that seems daunting, it is because it is. Data Science is di%cult, intellectually taxing work, which requires lots of talent: both tangible technical skills as well as the intangible ‘x-factors.’</p><p>#e most important qualities of Data Scientists tend to be the intangible aspects of their personalities. Data Scientists are by nature curious, creative, focused, and detail-oriented.</p><p>› <strong><em>Curiosity</em></strong> is necessary to peel apart a problem and examine the interrelationships between data that may appear super!cially unrelated.</p><p>› <strong><em>Creativity</em></strong> is required to invent and try new approaches to solving a problem, which often times have never been applied in such a context before.</p><p>› <strong><em>Focus</em></strong> is required to design and test a technique over days and weeks, !nd it doesn’t work, learn from the failure, and try again.</p><p>› <strong><em>Attention to Detail</em></strong> is needed to maintain rigor, and to detect and avoid over-reliance on intuition when examining data.</p><p>Success of a Data Science team requires pro!ciency in three foundational technical skills: computer science, mathematics and domain expertise, as re&amp;ected in <em>!e Data Science Venn Diagram</em>. Computers provide the environment in which data-driven hypotheses are tested, and as such computer science is necessary for data manipulation and processing. Mathematics provides the theoretical structure in which Data Science problems are examined. A rich background in statistics, geometry, linear algebra, and calculus are all important to understand the basis for many algorithms and tools. Finally, domain expertise contributes to an understanding of what problems actually need to be solved, what kind of data exists in the domain and how the problem space may be instrumented and measured.</p><div class="language-"><pre><code>» !e Triple !reat Unicorn\n</code></pre></div><div class="language-"><pre><code>Individuals who are great at all three\nof the Data Science foundational\ntechnical skills are like unicorns –\nvery rare and if you’re ever lucky\nenough to !nd one they should be\ntreated carefully. When you manage\nthese people:\n› Encourage them to lead\nyour team, but not manage\nit. Don’t bog them down\nwith responsibilities of\nmanagement that could be\ndone by other sta$.\n› Put extra e$ort into managing\ntheir careers and interests\nwithin your organization.\nBuild opportunities for\npromotion into your\norganization that allow them\nto focus on mentoring other\nData Scientists and progressing\nthe state of the art while also\nadvancing their careers.\n› Make sure that they have the\nopportunity to present and\nspread their ideas in many\ndi$erent forums, but also be\nsensitive to their time.\n</code></pre></div><h6 id="finding-the-athletes-for-your-team"><a class="header-anchor" href="#finding-the-athletes-for-your-team" aria-hidden="true">#</a> Finding the Athletes for Your Team</h6><div class="language-"><pre><code>Building a Data Science team is complex. Organizations must\nsimultaneously engage existing internal sta$ to create an “anchor” that\ncan be used to recruit and grow the team, while at the same time\nundergo organizational change and transformation to meaningfully\nincorporate this new class of employee.\n</code></pre></div><div class="language-"><pre><code>Building a team starts with identifying existing sta$ within an\norganization who have a high aptitude for Data Science. Good\ncandidates will have a formal background in any of the three\nfoundational technical skills we mentioned, and will most importantly\nhave the personality traits necessary for Data Science. #ey may often\nhave advanced (masters or higher) degrees, but not always. #e very\n!rst sta$ you identify should also have good leadership traits and a\nsense of purpose for the organization, as they will lead subsequent\nsta%ng and recruiting e$orts. Don’t discount anyone – you will !nd\nData Scientists in the strangest places with the oddest combinations\nof backgrounds.\n</code></pre></div><h6 id="shaping-the-culture"><a class="header-anchor" href="#shaping-the-culture" aria-hidden="true">#</a> Shaping the Culture</h6><div class="language-"><pre><code>Good Data Science requires a highly academic culture of peer review,\nwhere no member of the organization is immune from constructive\ncriticism. As you build your Data Science practice, you should be\nprepared to subject all aspects of your corporate operations to the\ncurious nature of your Data Science teams. Failure to do so creates\na negative image of a culture that fails to “eat its own dog food,”\nand will invite negative re&amp;ection on the brand, both internally and\nexternally. You should be conscious of any cultural legacies existing in\nan organization that are antithetical to Data Science.\n</code></pre></div><div class="language-"><pre><code>Data Scientists are fundamentally curious and imaginative. We have\na saying on our team, “We’re not nosy, we’re Data Scientists.” #ese\nqualities are fundamental to the success of the project and to gaining\nnew dimensions on challenges and questions. Often Data Science\nprojects are hampered by the lack of the ability to imagine something\nnew and di$erent. Fundamentally, organizations must foster trust and\ntransparent communication across all levels, instead of deference to\nauthority, in order to establish a strong Data Science team. Managers\nshould be prepared to invite participation more frequently, and o$er\nexplanation or apology less frequently.\n</code></pre></div><div class="language-"><pre><code>» Don’t judge a book by its\ncover, or a Data Scientist\nby his or her degree in\nthis case. Amazing Data\nScientists can be found\nanywhere. Just look at the\ndiverse and surprising\nsampling of degrees\nheld by Our Experts:\n</code></pre></div><div class="language-"><pre><code>› Bioinformatics\n› Biomedical Engineering\n› Biophysics\n› Business\n› Computer Graphics\n› Computer Science\n› English\n› Forest Management\n› History\n› Industrial Engineering\n› Information Technology\n› Mathematics\n› National Security Studies\n› Operations Research\n› Physics\n› Wildlife &amp; Fisheries\nManagement\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 3737\n</code></pre></div><h6 id="selecting-your-operating-model"><a class="header-anchor" href="#selecting-your-operating-model" aria-hidden="true">#</a> Selecting Your Operating Model</h6><p>Depending on the size, complexity, and the business drivers, organizations should consider one of three Data Science operating models: Centralized, Deployed, or Di$used. #ese three models are shown in the !gure, <em>Data Science Operating Models.</em></p><p><strong><em>Centralized Data Science teams</em></strong> serve the organization across all business units. #e team is centralized under a Chief Data Scientist. #ey serve all the analytical needs of an organization and they all co-locate together. #e domain experts come to this organization for brief rotational stints to solve challenges around the business.</p><p><strong><em>Deployed Data Science teams</em></strong> go to the business unit or group and reside there for short- or long-term assignments. #ey are their own entity and they work with the domain experts within the group to solve hard problems. #ey may be working independently on particular challenges, but they should always collaborate with the other teams to exchange tools, techniques and war stories.</p><p><strong><em>#e Di$used Data Science team</em></strong> is one that is fully embedded with each group and becomes part of the long-term organization. #ese teams work best when the nature of the domain or business unit is already one focused on analytics. However, building a cross-cut view into the team that can collaborate with other Data Science teams is critical to the success.</p><div class="language-"><pre><code>CENTRALIZED\nBusiness units bring their\nproblems to a centralized\nData Science team.\n</code></pre></div><div class="language-"><pre><code>DIFFUSED\nData Scientists are fully\nembedded within the\nbusiness units.\n</code></pre></div><div class="language-"><pre><code>DEPLOYED\nSmall Data Science teams\nare forward deployed to\nbusiness units.\n</code></pre></div><div class="language-"><pre><code>Data Science Operating Models\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>BALANCING THE DATA\nSCIENCE TEAM EQUATION\n</code></pre></div><div class="language-"><pre><code>Balancing the composition of a Data Science team\nis much like balancing the reactants and products in\na chemical reaction. Each side of the equation must\nrepresent the same quantity of any particular element.\nIn the case of Data Science, these elements are the\nfoundational technical skills computer science (CS),\nmathematics (M) and domain expertise (DE). The\nreactants, your Data Scientists, each have their own\nunique skills composition. You must balance the staff\nmix to meet the skill requirements of the Data Science\nteam, the product in the reaction. If you don’t correctly\nbalance the equation, your Data Science team will not\nhave the desired impact on the organization.\n</code></pre></div><h6 id="_2-cs-m-2-2-cs-m-de-→-cs-4-m-5-de"><a class="header-anchor" href="#_2-cs-m-2-2-cs-m-de-→-cs-4-m-5-de" aria-hidden="true">#</a> 2 CS M 2 + 2 CS + M DE → CS 4 M 5 DE</h6><div class="language-"><pre><code>I n the example above, your project requires four parts\ncomputer science, five parts mathematics and one\npart domain expertise. Given the skills mix of the\nstaff, five people are needed to balance the equation.\nThroughout your Data Science project, the skills\nrequirements of the team will change. You will need\nto re-balance the equation to ensure the reactants\nbalance with the products.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Start Here for the BasicsStart Here for the Basics 3939\n</code></pre></div><h6 id="success-starts-at-the-top"><a class="header-anchor" href="#success-starts-at-the-top" aria-hidden="true">#</a> Success Starts at the Top</h6><p>Data Science teams, no matter how they are deployed, must have sponsorship. #ese can start as grass roots e$orts by a few folks to start tackling hard problems, or as e$orts directed by the CEO. Depending on the complexity of the organization, direction from top- down for large organizations is the best for assuaging fears and doubts of these new groups.</p><p>Data Science teams often face harder political headwinds when solving problems than any technical hurdles. To prove a Data Science team’s value, the team needs to initially focus on the hardest problems within an organization that have the highest return for key stakeholders and will change how the organization approaches challenges in the future. #is has the e$ect of keeping the team motivated and encouraged in the face of di%cult challenges. Leaders must be key advocates who meet with stakeholders to ferret out the hardest problems, locate the data, connect disparate parts of the business and gain widespread buy-in.</p><h5 id="ta-k-e-o-f-f-the-training-wheels"><a class="header-anchor" href="#ta-k-e-o-f-f-the-training-wheels" aria-hidden="true">#</a> TA K E O F F the TRAINING WHEELS</h5><div class="language-"><pre><code>THE PRACTITIONER’S GUIDE\nTO DATA SCIENCE\nRead this section to get beyond the hype and\nlearn the secrets of being a Data Scientist.\n</code></pre></div><h3 id="guiding-principles"><a class="header-anchor" href="#guiding-principles" aria-hidden="true">#</a> Guiding Principles</h3><div class="language-"><pre><code>Failing is good; failing quickly is even better.\n</code></pre></div><div class="language-"><pre><code>#e set of guiding principles that govern how we conduct the\ntradecraft of Data Science are based loosely on the central tenets\nof innovation, as the two areas are highly connected. #ese principles\nare not hard and fast rules to strictly follow, but rather key tenets\nthat have emerged in our collective consciousness. You should use\nthese to guide your decisions, from problem decomposition\nthrough implementation.\n</code></pre></div><div class="language-"><pre><code>› Be willing to fail. At the core of Data Science is the idea of\nexperimentation. Truly innovative solutions only emerge when\nyou experiment with new ideas and applications. Failure is an\nacceptable byproduct of experimentation. Failures locate regions\nthat no longer need to be considered as you search for a solution.\n› Fail often and learn quickly. In addition to a willingness to fail, be\nready to fail repeatedly. #ere are times when a dozen approaches\nmust be explored in order to !nd the one that works. While you\nshouldn’t be concerned with failing, you should strive to learn from\nthe attempt quickly. #e only way you can explore a large number\nof solutions is to do so quickly.\n› Keep the goal in mind. You can often get lost in the details and\nchallenges of an implementation. When this happens, you lose\nsight of your goal and begin to drift o$ the path from data to\nanalytic action. Periodically step back, contemplate your goal, and\nevaluate whether your current approach can really lead you where\nyou want to go.\n› Dedication and focus lead to success. Yo u m u s t o f t e n e x p l o r e\nmany approaches before !nding the one that works. It’s easy to\nbecome discouraged. You must remain dedicated to your analytic\ngoal. Focus on the details and the insights revealed by the data.\nSometimes seemingly small observations lead to big successes.\n› Complicated does not equal better. As technical practitioners, we\nhave a tendency to explore highly complex, advanced approaches.\nWhile there are times where this is necessary, a simpler approach\ncan often provide the same insight. Simpler means easier and\nfaster to prototype, implement and verify.\n</code></pre></div><div class="language-"><pre><code>» Tips From the Pros\n</code></pre></div><div class="language-"><pre><code>It can be easier to rule out a solution\nthan confirm its correctness. As a\nresult, focus on exploring obvious\nshortcomings that can quickly\ndisqualify an approach. This will allow\nyou to focus your time on exploring\ntruly viable approaches as opposed to\ndead ends.\n</code></pre></div><div class="language-"><pre><code>» Tips From the Pros\n</code></pre></div><div class="language-"><pre><code>If the first thing you try to do is to\ncreate the ultimate solution, you will\nfail, but only after banging your head\nagainst a wall for several weeks.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h3 id="e-importance-of-reason"><a class="header-anchor" href="#e-importance-of-reason" aria-hidden="true">#</a> #e Importance of Reason</h3><p>Beware: in the world of Data Science, if it walks like a duck and quacks like a duck, it might just be a moose.</p><p>Data Science supports and encourages shifting between deductive (hypothesis-based) and inductive (pattern-based) reasoning. Inductive reasoning and exploratory data analysis provide a means to form or re!ne hypotheses and discover new analytic paths. Models of reality no longer need to be static. #ey are constantly tested, updated and improved until better models are found.</p><p>#e analysis of big data has brought inductive reasoning to the forefront. Massive amounts of data are analyzed to identify correlations. However, a common pitfall to this approach is confusing correlation with causation. Correlation implies but does not prove causation. Conclusions cannot be drawn from correlations until the underlying mechanisms that relate the data elements are understood. Without a suitable model relating the data, a correlation may simply be a coincidence.</p><div class="language-"><pre><code>» Correlation without\nCausation\n</code></pre></div><div class="language-"><pre><code>A common example of this\nphenomenon is the high correlation\nbetween ice cream consumption and\nthe murder rate during the summer\nmonths. Does this mean ice cream\nconsumption causes murder or,\nconversely, murder causes ice cream\nconsumption? Most likely not, but\nyou can see the danger in mistaking\ncorrelation for causation. Our job as\nData Scientists is making sure we\nunderstand the di$erence.\n</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 43\n</code></pre></div><h1 id="-6"><a class="header-anchor" href="#-6" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>Paul Yacci\n</code></pre></div><h2 id="the-dangers-of-rejection"><a class="header-anchor" href="#the-dangers-of-rejection" aria-hidden="true">#</a> The Dangers of Rejection</h2><div class="language-"><pre><code>In the era of big\ndata, one piece\nof analysis that\nis frequently\noverlooked is\nthe problem of\nfinding patterns\nwhen there\nare actually no\napparent patterns. In statistics\nthis is referred to as Type I error.\nAs scientists, we are always\non the lookout for a new or\ninteresting breakthrough that\ncould explain a phenomenon.\nWe hope to see a pattern in our\ndata that explains something\nor that can give us an answer.\nThe primary goal of hypothesis\ntesting is to limit Type I error.\nThis is accomplished by using\nsmall Į values. For example,\na Į value of 0.05 states that\nthere is a 1 in 20 chance that\nthe test will show that there\nis something significant when\nin actuality there isn’t. This\nproblem compounds when\ntesting multiple hypotheses.\nWhen running multiple\nhypothesis tests, we are likely\nto encounter Type I error. As\nmore data becomes available\nfor analysis, Type I error\nneeds to be controlled.\n</code></pre></div><div class="language-"><pre><code>One of my projects required\ntesting the difference between\nthe means of two microarray\ndata samples. Microarray\ndata contains thousands of\nmeasurements but is limited\nin the number of observations.\nA common analysis approach\nis to measure the same genes\nunder different conditions. If\nthere is a significant enough\ndifference in the amount of\ngene expression between the\ntwo samples, we can say that\nthe gene is correlated with a\nparticular phenotype. One way\nto do this is to take the mean of\neach phenotype for a particular\n</code></pre></div><div class="language-"><pre><code>gene and formulate a hypothesis\nto test whether there is a\nsignificant difference between\nthe means. Given that we were\nrunning thousands of these tests\nat Į = 0.05, we found several\ndifferences that were significant.\nThe problem was that some\nof these could be caused by\nrandom chance.\n</code></pre></div><div class="language-"><pre><code>Many corrections exist to\ncontrol for false indications of\nsignificance. The Bonferroni\ncorrection is one of the most\nconservative. This calculation\nlowers the level below which you\nwill reject the null hypothesis\n(your p value). The formula is\nalpha/n , where n equals the\nnumber of hypothesis tests\nthat you are running. Thus, if\nyou were to run 1,000 tests of\nsignificance at Į= 0.05, your\np value should be less than\n0.00005 (0.05/1,000) to reject the\nnull hypothesis. This is obviously\na much more stringent value.\nA large number of the previously\nsignificant values were no longer\nsignificant, revealing the true\nrelationships within the data.\n</code></pre></div><div class="language-"><pre><code>The corrected significance gave\nus confidence that the observed\nexpression levels were due to\ndifferences in the cellular gene\nexpression rather than noise. We\nwere able to use this information\nto begin investigating what\nproteins and pathways were\nactive in the genes expressing\nthe phenotype of interest. By\nsolidifying our understanding\nof the causal relationships, we\nfocused our research on the\nareas that could lead to new\ndiscoveries about gene function\nand, ultimately to improved\nmedical treatments.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><p>Reason and common sense are foundational to Data Science. Without it, data is simply a collection of bits. Context, inferences and models are created by humans and carry with them biases and assumptions. Blindly trusting your analyses is a dangerous thing that can lead to erroneous conclusions. When you approach an analytic challenge, you should always pause to ask yourself the following questions:</p><p>› <strong><em>What problem are we trying to solve?</em></strong> Articulate the answer as a sentence, especially when communicating with the end- user. Make sure that it sounds like an answer. For example, “Given a !xed amount of human capital, deploying people with these priorities will generate the best return on their time.”</p><p>› <strong><em>Does the approach make sense?</em></strong> Write out your analytic plan. Embrace the discipline of writing, as it brings structure to your thinking. Back of the envelope calculations are an existence proof of your approach. Without this kind of preparation, computers are power tools that can produce lots of bad answers really fast.</p><p>› <strong><em>Does the answer make sense?</em></strong> Can you explain the answer? Computers, unlike children, do what they are told. Make sure you spoke to it clearly by validating that the instructions you provided are the ones you intended. Document your assumptions and make sure they have not introduced bias in your work.</p><p>› <strong><em>Is it a &quot;nding or a mistake?</em></strong> Be skeptical of surprise !ndings. Experience says that it if seems wrong, it probably is wrong. Before you accept that conclusion, however, make sure you understand and can clearly explain why it is wrong.</p><div class="language-"><pre><code>› Does the analysis address the\noriginal intent? Make sure\nthat you are not aligning the\nanswer with the expectations\nof the client. Always speak\nthe truth, but remember that\nanswers of “your baby is ugly”\nrequire more, not less, analysis.\n</code></pre></div><div class="language-"><pre><code>› Is the story complete? #e goal\nof your analysis is to tell an\nactionable story. You cannot\nrely on the audience to stitch\nthe pieces together. Identify\npotential holes in your\nstory and !ll them to avoid\nsurprises. Grammar, spelling\nand graphics matter; your\naudience will lose con!dence\nin your analysis if your results\nlook sloppy.\n</code></pre></div><div class="language-"><pre><code>› Where would we head next?\nNo analysis is every !nished,\nyou just run out of resources.\nUnderstand and explain what\nadditional measures could\nbe taken if more resources\nare found.\n</code></pre></div><div class="language-"><pre><code>» Tips From the Pros\n</code></pre></div><div class="language-"><pre><code>Better a short pencil than a\nlong memory. End every day by\ndocumenting where you are; you\nmay learn something along the way.\nDocument what you learned and why\nyou changed your plan.\n</code></pre></div><div class="language-"><pre><code>» Tips From the Pros\n</code></pre></div><div class="language-"><pre><code>Test your answers with a friendly\naudience to make sure your findings\nhold water. Red teams save careers.\n</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 45\n</code></pre></div><h3 id="component-parts-of"><a class="header-anchor" href="#component-parts-of" aria-hidden="true">#</a> Component Parts of</h3><h3 id="data-science-1"><a class="header-anchor" href="#data-science-1" aria-hidden="true">#</a> Data Science</h3><div class="language-"><pre><code>#ere is a web of components that interact to create your\nsolution space. Understanding how they are connected\nis critical to your ability to engineer solutions to Data\nScience problems.\n</code></pre></div><div class="language-"><pre><code>#e components involved in any Data Science project fall into a\nnumber of di$erent categories including the data types analyzed, the\nanalytic classes used, the learning models employed and the execution\nmodels used to run the analytics. #e interconnection across these\ncomponents, shown in the !gure, Interconnection Among the Component\nParts of Data Science , speaks to the complexity of engineering Data\nScience solutions. A choice made for one component exerts in&amp;uence\nover choices made for others categories. For example, data types\nlead the choices in analytic class and learning models, while latency,\ntimeliness and algorithmic parallelization strategy inform the\nexecution model. As we dive deeper into the technical aspects of\nData Science, we will begin with an exploration of these components\nand touch on examples of each.\n</code></pre></div><p><strong><em>Read this to get the quick and dirty:</em></strong></p><div class="language-"><pre><code>When engineering a Data\nScience solution, work from an\nunderstanding of the components\nthat de!ne the solution space.\nRegardless of your analytic goal,\nyou must consider the data types\nwith which you will be working,\nthe classes of analytics you will use\nto generate your data product,\n</code></pre></div><div class="language-"><pre><code>how the learning models embodied\nwill operate and evolve, and the\nexecution models that will govern\nhow the analytic will be run.\nYou will be able to articulate a\ncomplete Data Science solution\nonly after considering each of\nthese aspects.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>streamingdata\n</code></pre></div><div class="language-"><pre><code>data\n</code></pre></div><div class="language-"><pre><code>batch\n</code></pre></div><div class="language-"><pre><code>data\n</code></pre></div><div class="language-"><pre><code>structured\n</code></pre></div><div class="language-"><pre><code>data\n</code></pre></div><div class="language-"><pre><code>unstructured\n</code></pre></div><div class="language-"><pre><code>analytics\n</code></pre></div><div class="language-"><pre><code>transforming\n</code></pre></div><div class="language-"><pre><code>analytics\n</code></pre></div><div class="language-"><pre><code>learning\n</code></pre></div><div class="language-"><pre><code>analyticspredictive\nlearningsupervised\n</code></pre></div><div class="language-"><pre><code>learning\nunsupervised\n</code></pre></div><div class="language-"><pre><code>learning\n</code></pre></div><div class="language-"><pre><code>online\n</code></pre></div><div class="language-"><pre><code>learning\n</code></pre></div><div class="language-"><pre><code>offline\n</code></pre></div><p>execution</p><div class="language-"><pre><code>batch\n</code></pre></div><div class="language-"><pre><code>execution\n</code></pre></div><div class="language-"><pre><code>streaming\n</code></pre></div><div class="language-"><pre><code>executionparallel\n</code></pre></div><div class="language-"><pre><code>execution\nserial\n</code></pre></div><div class="language-"><pre><code>EXECUTION\nMODELS\n</code></pre></div><p>DATA TYPES</p><div class="language-"><pre><code>ANALYTIC\nCLASSES\n</code></pre></div><div class="language-"><pre><code>LEARNING\nMODELS\n</code></pre></div><div class="language-"><pre><code>Interconnection Among the Component Parts of Data Science\n</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 47\n</code></pre></div><h6 id="data-types"><a class="header-anchor" href="#data-types" aria-hidden="true">#</a> Data Types</h6><p>Data types and analytic goals go hand-in-hand much like the chicken and the egg; it is not always clear which comes !rst. Analytic goals are derived from business objectives, but the data type also in&amp;uences the goals. For example, the business objective of understanding consumer product perception drives the analytic goal of sentiment analysis. Similarly, the goal of sentiment analysis drives the selection of a text-like data type such as social media content. Data type also drives many other choices when engineering your solutions.</p><p>#ere are a number of ways to classify data. It is common to characterize data as <em>structured</em> or <em>unstructured</em>. Structured data exists when information is clearly broken out into !elds that have an explicit meaning and are highly categorical, ordinal or numeric. A related category, semi-structured, is sometimes used to describe structured data that does not conform to the formal structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers. Unstructured data, such as natural language text, has less clearly delineated meaning. Still images, video and audio often fall under the category of unstructured data. Data in this form requires preprocessing to identify and extract relevant ‘features.’ #e features are structured information that are used for indexing and retrieval, or training classi!cation, or clustering models.</p><p>Data may also be classi!ed by the rate at which it is generated, collected or processed. #e distinction is drawn between streaming data that arrives constantly like a torrent of water from a !re hose, and batch data, which arrives in buckets. While there is rarely a connection between data type and data rate, data rate has signi!cant in&amp;uence over the execution model chosen for analytic implementation and may also inform a decision of analytic class or learning model.</p><div class="language-"><pre><code>Take off the Training Wheels 49\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><h6 id="classes-of-analytic-techniques"><a class="header-anchor" href="#classes-of-analytic-techniques" aria-hidden="true">#</a> Classes of Analytic Techniques</h6><div class="language-"><pre><code>As a means for helping conceptualize the universe of possible analytic\ntechniques, we grouped them into nine basic classes. Note that\ntechniques from a given class may be applied in multiple ways to\nachieve various analytic goals. Membership in a class simply indicates\na similar analytic function. #e nine analytic classes are shown in the\n!gure, Classes of Analytic Techniques.\n</code></pre></div><div class="language-"><pre><code>»  Transforming Analytics\n› Aggregation : Techniques to summarize the data. #ese\ninclude basic statistics (e.g., mean, standard deviation),\ndistribution !tting, and graphical plotting.\n› Enrichment : Techniques for adding additional information\nto the data, such as source information or other labels.\n› Processing : Techniques that address data cleaning,\npreparation, and separation. #is group also includes\ncommon algorithm pre-processing activities such as\ntransformations and feature extraction.\n</code></pre></div><div class="language-"><pre><code>»  Learning Analytics\n› Regression : Techniques for estimating relationships among\nvariables, including understanding which variables are\nimportant in predicting future values.\n› Clustering : Techniques to segment the data into naturally\nsimilar groups.\n› Classi&quot;cation : Techniques to identify data element\ngroup membership.\n› Recommendation : Techniques to predict the rating or\npreference for a new entity, based on historic preference\nor behavior.\n</code></pre></div><div class="language-"><pre><code>»  Predictive Analytics\n› Simulation : Techniques to imitate the operation of a real-\nworld process or system. #ese are useful for predicting\nbehavior under new conditions.\n› Optimization : Operations Research techniques focused on\nselecting the best element from a set of available alternatives\nto maximize a utility function.\n</code></pre></div><div class="language-"><pre><code>Aggregation Enrichment Processing Regression Clustering Classification Recommend Simulation Optimization\n</code></pre></div><div class="language-"><pre><code>TRANSFORMING LEARNING PREDICTIVE\n</code></pre></div><div class="language-"><pre><code>Classes of Analytic Techniques\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><h6 id="learning-models"><a class="header-anchor" href="#learning-models" aria-hidden="true">#</a> Learning Models</h6><p>Analytic classes that perform predictions such as regression, clustering, classi!cation, and recommendation employ learning models. #ese models characterize how the analytic is trained to perform judgments on new data based on historic observation. Aspects of learning models describe both the types of judgments performed and how the models evolve over time, as shown in the !gure, <em>Analytic Learning Models</em>.</p><p>#e learning models are typically described as employing unsupervised or supervised learning. Supervised learning takes place when a model is trained using a labeled data set that has a known class or category associated with each data element. #e model relates the features found in training instances with the labels so that predictions can be made for unlabeled instances. Unsupervised learning models have no a-priori knowledge about the classes into which data can be placed. #ey use the features in the dataset to form groupings based on feature similarity.</p><p>A useful distinction of learning models is between those that are trained in a single pass, which are known as o&#39;ine models, and those that are trained incrementally over time, known as online models. Many learning approaches have online or o&#39;ine variants. #e decision to use one or another is based on the analytic goals and execution models chosen.</p><p>Generating an o&#39;ine model requires taking a pass over the entire training data set. Improving the model requires making separate passes over the data. #ese models are static in that once trained, their predictions will not change until a new model is created through a subsequent training stage. O&#39;ine model performance is easier to evaluate due to this deterministic behavior. Deployment of the model into a production environment involves swapping out the old model for the new.</p><p>Online models have both advantages and disadvantages. #ey dynamically evolve over time, meaning they only require a single deployment into a production setting. #e fact that these models do not have the entire dataset available when being trained, however, is a challenge. #ey must make assumptions about the data based</p><div class="language-"><pre><code>TRAINING STYLE\n</code></pre></div><div class="language-"><pre><code>Unsupervised Supervised Offline Online\n</code></pre></div><div class="language-"><pre><code>LEARNING STYLE\n</code></pre></div><div class="language-"><pre><code>Analytic Learning Models\n</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 51\n</code></pre></div><div class="language-"><pre><code>on the examples observed; these assumptions may be sub-optimal.\n#is can be o$set somewhat in cases where feedback on the model’s\npredictions is available since online models can rapidly incorporate\nfeedback to improve performance.\n</code></pre></div><h6 id="execution-models"><a class="header-anchor" href="#execution-models" aria-hidden="true">#</a> Execution Models</h6><div class="language-"><pre><code>Execution models describe how data is manipulated to perform\nan analytic function. #ey may be categorized across a number\nof dimensions. Execution Models are embodied by an execution\nframework, which orchestrates the sequencing of analytic\ncomputation. In this sense, a framework might be as simple as a\nprogramming language runtime, such as the Python interpreter, or\na distributed computing framework that provides a speci!c API for\none or more programming languages such as Hadoop, MapReduce\nor Spark. Grouping execution models based on how they handle data\nis common, classifying them as either batch or streaming execution\nmodels. #e categories of execution model are shown in the !gure,\nAnalytic Execution Models.\n</code></pre></div><div class="language-"><pre><code>Analytic Execution Models\n</code></pre></div><div class="language-"><pre><code>A batch execution model implies that data is analyzed in large\nsegments, that the analytic has a state where it is running and a state\nwhere it is not running and that little state is maintained in memory\nbetween executions. Batch execution may also imply that the analytic\nproduces results with a frequency on the order of several minutes or\nmore. Batch workloads tend to be fairly easy to conceptualize because\nthey represent discrete units of work. As such, it is easy to identify\na speci!c series of execution steps as well as the proper execution\nfrequency and time bounds based on the rate at which data arrives.\nDepending on the algorithm choice, batch execution models are\neasily scalable through parallelism. #ere are a number of frameworks\nthat support parallel batch analytic execution. Most famously,\nHadoop provides a distributed batch execution model in its\nMapReduce framework.\n</code></pre></div><div class="language-"><pre><code>Conversely, a streaming model analyzes data as it arrives. Streaming\nexecution models imply that under normal operation, the analytic\nis always executing. #e analytic can hold state in memory and\n</code></pre></div><div class="language-"><pre><code>SEQUENCING\n</code></pre></div><div class="language-"><pre><code>Batch Streaming Serial Parallel\n</code></pre></div><div class="language-"><pre><code>SCHEDULING\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><p>constantly deliver results as new data arrives, on the order of seconds or less. Many of the concepts in streaming are inherent in the Unix- pipeline design philosophy; processes are chained together by linking the output of one process to the input of the next. As a result, many developers are already familiar with the basic concepts of streaming. A number of frameworks are available that support the parallel execution of streaming analytics such as Storm, S4 and Samza.</p><p>#e choice between batch and streaming execution models often hinges on analytic latency and timeliness requirements. Latency refers to the amount of time required to analyze a piece of data once it arrives at the system, while timeliness refers to the average age of an answer or result generated by the analytic system. For many analytic goals, a latency of hours and timeliness of days is acceptable and thus lend themselves to the implementation enabled by the batch approach. Some analytic goals have up-to-the-second requirements where a result that is minutes old has little worth. #e streaming execution model better supports such goals.</p><p>Batch and streaming execution models are not the only dimensions within which to categorize analytic execution methods. Another distinction is drawn when thinking about scalability. In many cases, scale can be achieved by spreading computation over a number of computers. In this context, certain algorithms require a large shared memory state, while others are easily parallelizable in a context where no shared state exists between machines. #is distinction has signi!cant impacts on both software and hardware selection when building out a parallel analytic execution environment.</p><div class="language-"><pre><code>» Tips From the Pros\n</code></pre></div><div class="language-"><pre><code>In order to understand system capacity\nin the context of streaming analytic\nexecution, collect metrics including:\nthe amount of data consumed, data\nemitted, and latency. This will help\nyou understand when scale limits\nare reached.\n</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 53\n</code></pre></div><h3 id="fractal-analytic-model"><a class="header-anchor" href="#fractal-analytic-model" aria-hidden="true">#</a> Fractal Analytic Model</h3><div class="language-"><pre><code>Data Science analytics are a lot like broccoli.\n</code></pre></div><div class="language-"><pre><code>Fractals are mathematical sets that display self-similar patterns. As\nyou zoom in on a fractal, the same patterns reappear. Imagine a stalk\nof broccoli. Rip o$ a piece of broccoli and the piece looks much like\nthe original stalk. Progressively smaller pieces of broccoli still look like\nthe original stalk.\n</code></pre></div><div class="language-"><pre><code>Data Science analytics are a lot like broccoli – fractal in nature in\nboth time and construction. Early versions of an analytic follow the\nsame development process as later versions. At any given iteration, the\nanalytic itself is a collection of smaller analytics that often decompose\ninto yet smaller analytics.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Setup Try\n</code></pre></div><div class="language-"><pre><code>Evaluate\n</code></pre></div><div class="language-"><pre><code>Do\n</code></pre></div><div class="language-"><pre><code>Evaluate\n</code></pre></div><h6 id="iterative-by-nature"><a class="header-anchor" href="#iterative-by-nature" aria-hidden="true">#</a> Iterative by Nature</h6><p>Good Data Science is fractal in time — an iterative process. Getting an imperfect solution out the door quickly will gain more interest from stakeholders than a perfect solution that is never completed. #e !gure, <em>!e Data Science Product Lifecycle,</em> summarizes the lifecycle of the Data Science product.</p><p><em>Set up</em> the infrastructure, aggregate and prepare the data, and incorporate domain expert knowledge. <em>Try</em> di$erent analytic techniques and models on subsets of the data. <em>Evaluate</em> the models, re!ne, evaluate again, and select a model. <em>Do</em> something with your models and results – deploy the models to inform, inspire action, and act. <em>Evaluate</em> the business results to improve the overall product.</p><div class="language-"><pre><code>The Data Science Product Lifecycle\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 55\n</code></pre></div><h6 id="smaller-pieces-of-broccoli-a-data"><a class="header-anchor" href="#smaller-pieces-of-broccoli-a-data" aria-hidden="true">#</a> Smaller Pieces of Broccoli: A Data</h6><h6 id="science-product"><a class="header-anchor" href="#science-product" aria-hidden="true">#</a> Science Product</h6><div class="language-"><pre><code>Components inside and outside of the Data Science product will\nchange with each iteration. Let’s take a look under the hood of a\nData Science product and examine the components during one\nsuch iteration.\n</code></pre></div><div class="language-"><pre><code>In order to achieve a greater analytic goal, you need to !rst decompose\nthe problem into sub-components to divide and conquer. #e !gure,\n!e Fractal Analytic Model , shows a decomposition of the Data Science\nproduct into four component pieces.\n</code></pre></div><div class="language-"><pre><code>GOAL\n! Describe\n! Discover\n! Predict\n! Advise\n</code></pre></div><div class="language-"><pre><code>ACTION\n! Productization\n! Data Monetization\n! Insights &amp; Relationships\n</code></pre></div><div class="language-"><pre><code>DATA\n! Text\n! Imagery\n! Waveform\n! Geo\n! Time Series\n</code></pre></div><div class="language-"><pre><code>COMPUTATION\n</code></pre></div><div class="language-"><pre><code>Aggregation\n</code></pre></div><div class="language-"><pre><code>Enrichment\n</code></pre></div><div class="language-"><pre><code>Clustering\n</code></pre></div><div class="language-"><pre><code>Classification\n</code></pre></div><div class="language-"><pre><code>CLASSES OF ANALYTICS\n</code></pre></div><div class="language-"><pre><code>The Fractal Analytic Model\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><p><strong>Goal</strong></p><p>You must !rst have some idea of your analytic goal and the end state of the analysis. Is it to Discover, Describe, Predict, or Advise? It is probably a combination of several of those. Be sure that before you start, you de!ne the business value of the data and how you plan to use the insights to drive decisions, or risk ending up with interesting but non-actionable trivia.</p><p><strong>Data</strong></p><p>Data dictates the potential insights that analytics can provide. Data Science is about !nding patterns in variable data and comparing those patterns. If the data is not representative of the universe of events you wish to analyze, you will want to collect that data through carefully planned variations in events or processes through A/B testing or design of experiments. Data sets are never perfect so don’t wait for perfect data to get started. A good Data Scientist is adept at handling messy data with missing or erroneous values. Just make sure to spend the time upfront to clean the data or risk generating garbage results.</p><p><strong>Computation</strong></p><p>Computation aligns the data to goals through the process of creating insights. #rough divide and conquer, computation decomposes into several smaller analytic capabilities with their own goals, data, computation and resulting actions, just like a smaller piece of broccoli maintains the structure of the original stalk. In this way, computation itself is fractal. Capability building blocks may utilize di$erent types of execution models such as batch computation or streaming, that individually accomplish small tasks. When properly combined together, the small tasks produce complex, actionable results.</p><p><strong>Action</strong></p><p>How should engineers change the manufacturing process to generate higher product yield? How should an insurance company choose which policies to o$er to whom and at what price? #e output of computation should enable actions that align to the goals of the data product. Results that do not support or inspire action are nothing but interesting trivia.</p><p>Given the fractal nature of Data Science analytics in time and construction, there are many opportunities to choose fantastic or shoddy analytic building blocks. <em>!e Analytic Selection Process</em> o$ers some guidance.</p><div class="language-"><pre><code>Take off the Training Wheels 57\n</code></pre></div><h3 id="e-analytic"><a class="header-anchor" href="#e-analytic" aria-hidden="true">#</a> #e Analytic</h3><h3 id="selection-process"><a class="header-anchor" href="#selection-process" aria-hidden="true">#</a> Selection Process</h3><div class="language-"><pre><code>If you focus only on the science aspect of Data Science you will\nnever become a data artist.\n</code></pre></div><div class="language-"><pre><code>A critical step in Data Science is to identify an analytic technique that\nwill produce the desired action. Sometimes it is clear; a characteristic\nof the problem (e.g., data type) points to the technique you should\nimplement. Other times, however, it can be di%cult to know where\nto begin. #e universe of possible analytic techniques is large. Finding\nyour way through this universe is an art that must be practiced. We\nare going to guide you on the next portion of your journey - becoming\na data artist.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h6 id="decomposing-the-problem"><a class="header-anchor" href="#decomposing-the-problem" aria-hidden="true">#</a> Decomposing the Problem</h6><p>Decomposing the problem into manageable pieces is the !rst step in the analytic selection process. Achieving a desired analytic action often requires combining multiple analytic techniques into a holistic, end-to-end solution. Engineering the complete solution requires that the problem be decomposed into progressively smaller sub-problems.</p><p>#e <em>Fractal Analytic Model</em> embodies this approach. At any given stage, the analytic itself is a collection of smaller computations that decompose into yet smaller computations. When the problem is decomposed far enough, only a single analytic technique is needed to achieve the analytic goal. Problem decomposition creates multiple sub-problems, each with their own goals, data, computations, and actions. #e concept behind problem decomposition is shown in the !gure, <em>Problem Decomposition Using the Fractal Analytic Model</em>.</p><div class="language-"><pre><code>GOAL\n! Describe\n! Discover\n! Predict\n! Advise\n</code></pre></div><div class="language-"><pre><code>ACTION\n! Productization\n! Data Monetization\n! Insights &amp; Relationships\n</code></pre></div><div class="language-"><pre><code>DATA\n! Text\n! Imagery\n! Waveform\n! Geo\n! Time Series\n</code></pre></div><div class="language-"><pre><code>DATA\n</code></pre></div><div class="language-"><pre><code>GOAL\n</code></pre></div><div class="language-"><pre><code>ACTION\n</code></pre></div><div class="language-"><pre><code>DATA\n</code></pre></div><div class="language-"><pre><code>GOAL\n</code></pre></div><div class="language-"><pre><code>ACTION\n</code></pre></div><div class="language-"><pre><code>Aggregation\n</code></pre></div><div class="language-"><pre><code>Enrichment\n</code></pre></div><div class="language-"><pre><code>Clustering\n</code></pre></div><div class="language-"><pre><code>Classification\n</code></pre></div><div class="language-"><pre><code>CLASSES OF ANALYTICS\n</code></pre></div><div class="language-"><pre><code>Problem Decomposition Using the Fractal Analytic Model\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 59\n</code></pre></div><div class="language-"><pre><code>On the surface, problem decomposition appears to be a mechanical,\nrepeatable process. While this may be true conceptually, it is\nreally the performance of an art as opposed to the solving of an\nengineering problem. #ere may be many valid ways to decompose\nthe problem, each leading to a di$erent solution. #ere may be\nhidden dependencies or constraints that only emerge after you begin\ndeveloping a solution. #is is where art meets science. Although the\nart behind problem decomposition cannot be taught, we have distilled\nsome helpful hints to help guide you. When you begin to think about\ndecomposing your problem, look for:\n</code></pre></div><div class="language-"><pre><code>› Compound analytic goals that create natural segmentation.\nFor example, many problems focused on predicting future\nconditions include both Discover and Predict goals.\n› Natural orderings of analytic goals. For example, when extracting\nfeatures you must !rst identify candidate features and then\nselect the features set with the highest information value. #ese\ntwo activities form distinct analytic goals.\n› Data types that dictate processing activities. For example, text or\nimagery both require feature extraction.\n› Requirements for human-in-the-loop feedback. For example,\nwhen developing alerting thresholds, you might need to solicit\nanalyst feedback and update the threshold based on their\nassessment.\n› #e need to combine multiple data sources. For example, you may\nneed to correlate two data sets to achieve your broader goal.\nOften this indicates the presence of a Discover goal.\n</code></pre></div><div class="language-"><pre><code>In addition to problem decomposition providing a tractable approach\nto analytic selection, it has the added bene!t of simplifying a highly\ncomplex problem. Rather than being faced with understanding the\nentire end-to-end solution, the computations are discrete segments\nthat can be explored. Note, however, that while this technique helps\nthe Data Scientist approach the problem, it is the complete end-to-\nend solution that must be evaluated.\n</code></pre></div><div class="language-"><pre><code>» Tips From the Pros\n</code></pre></div><div class="language-"><pre><code>One of your first steps should be to\nexplore available data sources that\nhave not been previously combined.\nEmerging relationships between data\nsources often allow you to pick low\nhanging fruit.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Compare\nList of recently Datasets\nregistered\ncompany\ndomains\n</code></pre></div><div class="language-"><pre><code>List of\ncandidate\nspoofed\ndomains\nList of\nrecently\nregistered\ncompany\ndomains\n</code></pre></div><div class="language-"><pre><code>Discover spoofed\ndomains\n</code></pre></div><div class="language-"><pre><code>Test &amp;\nEvaluation\n</code></pre></div><div class="language-"><pre><code>Test &amp;\nEvaluation\n</code></pre></div><div class="language-"><pre><code>Test &amp;\nEvaluation\n</code></pre></div><div class="language-"><pre><code>Calculate\nMetric\n</code></pre></div><div class="language-"><pre><code>Set\nThreshold\n</code></pre></div><div class="language-"><pre><code>Data\nCollection\n</code></pre></div><div class="language-"><pre><code>Simulate\nSpoofed\nData\n</code></pre></div><div class="language-"><pre><code>Store\nGenerated\nDomains\nGenerate\nCandidate\nDomains\n</code></pre></div><div class="language-"><pre><code>Alert on spoofed domains to\nprovide opportuinity to\nminimize brand image and\nconsumer confidence damage\n</code></pre></div><div class="language-"><pre><code>Discover likely\ncandidates for\nspoofed domains\n</code></pre></div><div class="language-"><pre><code>List of candidate\nspoofed domains\n</code></pre></div><div class="language-"><pre><code>List of recently\nregistered\ncompany domains\n</code></pre></div><div class="language-"><pre><code>Describe closeness of\nspoof to valid domains\n</code></pre></div><div class="language-"><pre><code>Quantitative\nmeasure of feature\ninformation value\n</code></pre></div><div class="language-"><pre><code>Threshold that balances\nfalse positive and false\nnegative rate\n</code></pre></div><div class="language-"><pre><code>Quantitative threshold for\nautomated result ranking\n</code></pre></div><div class="language-"><pre><code>!\n</code></pre></div><div class="language-"><pre><code>!\n</code></pre></div><div class="language-"><pre><code>List of candidate spoofed\ndomains\nList of recently registered\ncompany domains\nQuantitative measure of\nfeature information value\n</code></pre></div><div class="language-"><pre><code>!\n!\n!\n</code></pre></div><div class="language-"><pre><code>Stephanie\nRivera\n</code></pre></div><h2 id="identifying-spoofed-domains"><a class="header-anchor" href="#identifying-spoofed-domains" aria-hidden="true">#</a> Identifying Spoofed Domains</h2><div class="language-"><pre><code>Identifying spoofed domains is important for an organization\nto preserve their brand image and to avoid eroded customer\nconfidence. Spoofed domains occur when a malicious actor\ncreates a website, URL or email address that users believe is\nassociated with a valid organization. When users click the link,\nvisit the website or receive emails, they are subjected to some\ntype of nefarious activity.\n</code></pre></div><p>Our team was faced with the problem of identifying spoofed domains for a commercial company. On the surface, the problem sounded easy; take a recently registered domain, check to see if it is similar to the company’s domain and alert when the similarity is sufficiently high. Upon decomposing the problem, however, the main computation quickly became complicated.</p><p>We needed a computation that determined similarity between two domains. As we decomposed the similarity computation, complexity and speed became a concern. As with many security-related problems, fast</p><div class="language-"><pre><code>alert speeds are vital. Result speed created\nan implementation constraint that forced us to\nre-evaluate how we decomposed the problem.\n</code></pre></div><div class="language-"><pre><code>Revisiting the decomposition process led us\nto a completely new approach. In the end,\nwe derived a list of domains similar to those\nregistered by the company. We then compared\nthat list against a list of recently registered\ndomains. The figure, Spoofed Domain Problem\nDecomposition, illustrates our approach. Upon\ntesting and initial deployment, our analytic\ndiscovered a spoofed domain within 48 hours.\n</code></pre></div><h1 id="-7"><a class="header-anchor" href="#-7" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>Spoofed Domain Problem Decomposition\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 61\n</code></pre></div><div class="language-"><pre><code>SPEED: The speed at which an\nanalytic outcome must be produced\n(e.g., near real-time, hourly, daily) or the time\nit takes to develop and implement the\nanalytic solution\n</code></pre></div><div class="language-"><pre><code>ANALYTIC COMPLEXITY:\nAlgorithmic complexity (e.g., complexity class\nand execution resources)\n</code></pre></div><div class="language-"><pre><code>ACCURACY &amp; PRECISION: The ability\nto produce exact versus approximate\nsolutions as well as the ability to provide a\nPHDVXUHRIFRQȌGHQFH\n</code></pre></div><div class="language-"><pre><code>DATA SIZE: The size of the data set\n(e.g., number of rows)\n</code></pre></div><div class="language-"><pre><code>DATA COMPLEXITY: The data\ntype, formal complexity measures\nincluding measures of overlap and\nlinear separability, number of\ndimensions /columns, and linkages\nbetween data sets\n</code></pre></div><div class="language-"><pre><code>SPEED\n</code></pre></div><div class="language-"><pre><code>ANALYTIC\nCOMPLEXITY\n</code></pre></div><div class="language-"><pre><code>DATA\nCOMPLEXITY\n</code></pre></div><div class="language-"><pre><code>ACCURACY\n&amp;\nPRECISION\n</code></pre></div><div class="language-"><pre><code>DATA SIZE\n</code></pre></div><h6 id="implementation-constraints"><a class="header-anchor" href="#implementation-constraints" aria-hidden="true">#</a> Implementation Constraints</h6><p>In the spoofed domains case study, the emergence of an implementation constraint caused the team to revisit its approach. #is demonstrates that analytic selection does not simply mean choosing an analytic technique to achieve a desired outcome. It also means ensuring that the solution is feasible to implement.</p><p>#e Data Scientist may encounter a wide variety of implementation constraints. #ey can be conceptualized, however, in the context of !ve dimensions that compete for your attention: analytic complexity, speed, accuracy &amp; precision, data size, and data complexity. Balancing these dimensions is a zero sum game - an analytic solution cannot simultaneously exhibit all !ve dimensions, but instead must make trades between them. #e !gure, <em>Balancing the Five Analytic Dimensions,</em> illustrates this relationship.</p><p>Implementation constraints occur when an aspect of the problem dictates the value for one or more of these dimensions. As soon as one dimension is !xed, the Data Scientist is forced to make trades among the others. For example, if the analytic problem requires actions to be produced in near real-time, the speed dimension is !xed and trades must be made among the other four dimensions. Understanding which trades will achieve the right balance among the !ve dimensions is an art that must be learned over time.</p><div class="language-"><pre><code>As we compiled this section, we\ntalked extensively about ways to\ngroup and classify implementation\nconstraints. After much discussion\nwe settled on these !ve dimensions.\nWe present this model in hopes\nthat others weigh in and o$er\ntheir own perspectives.\n</code></pre></div><div class="language-"><pre><code>Balancing the Five Analytic Dimensions\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Take off the Training Wheels 63\n</code></pre></div><div class="language-"><pre><code>Some common examples of implementation\nconstraints include:\n</code></pre></div><p><strong><em>- Computation frequency.</em></strong> #e solution may need to run on a regular basis (e.g., hourly), requiring that computations be completed within a speci!ed window of time. #e best analytic is useless if it cannot solve the problem within the required time. <strong><em>- Solution timeliness.</em></strong> Some applications require near real-time results, pointing toward streaming approaches. While some algorithms can be implemented within streaming frameworks, many others cannot. <strong><em>- Implementation speed.</em></strong> A project may require that you rapidly develop and implement a solution to quickly produce analytic insights. In these cases, you may need to focus on less complex techniques that can be quickly implemented and veri!ed. <strong><em>- Computational resource limitations.</em></strong> Although you may be able to store and analyze your data, data size may be su%ciently large that algorithms requiring multiple computations across the full data set are too resource intensive. #is may point toward needing approaches that only require a single pass on the data (e.g., canopy cluster as opposed to k-means clustering). <strong><em>- Data storage limitations</em></strong>. #ere are times when big data becomes so big it cannot be stored or only a short time horizon can be stored. Analytic approaches that require long time horizons may not be possible.</p><div class="language-"><pre><code>Organizational policies and regulatory requirements are a major\nsource of implicit constraints that merit a brief discussion. Policies\nare often established around speci!c classes of data such as Personally\nIdenti!able Information (PII) or Personal Health Information (PHI).\nWhile the technologies available today can safely house information\nwith a variety of security controls in a single system, these policies\nforce special data handling considerations including limited retention\nperiods and data access. Data restrictions impact the data size and\ncomplexity dimensions outlined earlier, creating yet another layer of\nconstraints that must be considered.\n</code></pre></div><div class="language-"><pre><code>» Tips From the Pros\n</code></pre></div><div class="language-"><pre><code>When possible, consider approaches\nthat make use of previously computed\nresults. Your algorithm will run\nmuch faster if you can avoid\nre-computing values across\nthe full time horizon of data.\n</code></pre></div><div class="language-"><pre><code>» Tips From the Pros\n</code></pre></div><div class="language-"><pre><code>Our Data Science Product Lifecycle\nhas evolved to produce results quickly\nand then incrementally improve\nthe solution.\n</code></pre></div><div class="language-"><pre><code>» Tips From the Pros\n</code></pre></div><div class="language-"><pre><code>Streaming approaches may be useful\nfor overcoming storage limitations.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h2 id="_4"><a class="header-anchor" href="#_4" aria-hidden="true">#</a> 4</h2><h5 id="data"><a class="header-anchor" href="#data" aria-hidden="true">#</a> Data</h5><h5 id="science"><a class="header-anchor" href="#science" aria-hidden="true">#</a> Science</h5><h2 id="_3"><a class="header-anchor" href="#_3" aria-hidden="true">#</a> 3</h2><div class="language-"><pre><code>DESCRIBE\n</code></pre></div><div class="language-"><pre><code>DISCOVER\n</code></pre></div><div class="language-"><pre><code>PREDICT\n</code></pre></div><div class="language-"><pre><code>ADVISE\n</code></pre></div><div class="language-"><pre><code>PAGE 67\n</code></pre></div><div class="language-"><pre><code>PAGE 68\n</code></pre></div><div class="language-"><pre><code>PAGE 69\n</code></pre></div><div class="language-"><pre><code>PAGE 70\n</code></pre></div><h2 id="_2"><a class="header-anchor" href="#_2" aria-hidden="true">#</a> 2</h2><h2 id="_1"><a class="header-anchor" href="#_1" aria-hidden="true">#</a> 1</h2><h3 id="guide-to-analytic-selection"><a class="header-anchor" href="#guide-to-analytic-selection" aria-hidden="true">#</a> Guide to Analytic Selection</h3><div class="language-"><pre><code>Your senses are incapable of perceiving the entire universe, so\nwe drew you a map.\n</code></pre></div><div class="language-"><pre><code>#e universe of analytic techniques is vast and hard to comprehend.\nWe created this diagram to aid you in !nding your way from data\nand goal to analytic action. Begin at the center of the universe\n(Data Science) and answer questions about your analytic goals and\nproblem characteristics. #e answers to your questions will guide you\nthrough the diagram to the appropriate class of analytic techniques\nand provide recommendations for a few techniques to consider.\n</code></pre></div><div class="language-"><pre><code>n TIP: There are several situations where dimensionality reduction may be needed:\n› Models fail to converge\n› Models produce results equivalent to\nrandom chance\n› You lack the computational power to\n</code></pre></div><div class="language-"><pre><code>perform operations across the\nfeature space\n› You do not know which aspects of the\ndata are the most important\no Feature Extraction is a broad topic and is highly dependent upon the domain area.\nThis topic could be the subject of an entire book. As a result, a detailed exploration\nhas been omitted from this diagram. See the Featuring Engineering and Feature\nSelection sections in the Life in the Trenches chapter for additional information.\np TIP: Always check data labels for correctness. This is particularly true for time\nstamps, which may have reverted to system default values.\nq TIP: Smart enrichment can greatly speed computational time. It can also be a huge\ndifferentiator between the accuracy of different end-to-end analytic solutions.\nSource: Booz Allen Hamilton\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Data\nScience\n</code></pre></div><div class="language-"><pre><code>FILTERING\nHow do I identify\ndata based on\nits absolute or\nrelative values?\n</code></pre></div><div class="language-"><pre><code>IMPUTATION\nHow do I fill in\nmissing values\nin my data?\n</code></pre></div><div class="language-"><pre><code>DIMENSIONALITY\nREDUCTION n\nHow do I reduce\nthe number of\ndimensions\nin my data?\n</code></pre></div><div class="language-"><pre><code>NORMALIZATION &amp;\nTRANSFORMATION\nHow do I reconcile\nduplication\nrepresentations\nin the data?\n</code></pre></div><div class="language-"><pre><code>FEATURE\nEXTRACTION o\n</code></pre></div><div class="language-"><pre><code>PROCESSING p\nHow do I clean\nand separate\nmy data?\n</code></pre></div><div class="language-"><pre><code>If you want to add or remove data based on its value, start with:\n! Relational algebra projection and selection\nIf early results are uninformative and duplicative, start with:\n! Outlier removal^! Gaussian filter\n! Exponential smoothing! Median filter\n</code></pre></div><div class="language-"><pre><code>If you are unfamiliar with the data set, start with\nbasic statistics:\n! Count! Standard deviation! Box plots\n! Mean! Range! Scatter plots\nIf your approach assumes the data follows a\ndistribution, start with:\n! Distribution fitting\nIf you want to understand all the information\navailable on an entity, start with:\n! “Baseball card” aggregation\n</code></pre></div><div class="language-"><pre><code>If you need to keep track of source information or other\nuser-defined parameters, start with:\n! Annotation\nIf you often process certain data fields together or use\none field to compute the value of another, start with:\n! Relational algebra rename,\n! Feature addition (e.g., Geography, Technology, Weather)\n</code></pre></div><div class="language-"><pre><code>If you want to generate values from other observations in your data set, start with:\n! Random sampling\n! Markov Chain Monte Carlo (MC)\nIf you want to generate values without using other observations in your data set,\nstart with:\n! Mean! Regression models\n! Statistical distributions\n</code></pre></div><div class="language-"><pre><code>If you need to determine whether there is multi-dimensional correlation,\nstart with:\n! PCA and other factor analysis\nIf you can represent individual observations by membership in a group, start with:\n! K-means clustering\n! Canopy clustering\nIf you have unstructured text data, start with:\n! Term Frequency/Inverse Document Frequency (TF IDF)\nIf you have a variable number of features but your\nalgorithm requires a fixed number, start with:\n! Feature hashing\nIf you are not sure which features are the most important, start with:\n! Wrapper methods\n! Sensitivity analysis\nIf you need to facilitate understanding of the\nprobability distribution of the space, start with:\n! Self organizing maps\n</code></pre></div><div class="language-"><pre><code>If you suspect duplicate data elements, start with:\n! Deduplication\nIf you want your data to fall within a specified range, start with:\n! Normalization\nIf your data is stored in a binary format, start with:\n! Format Conversion\nIf you are operating in frequency space, start with:\n! Fast Fourier Transform (FFT),\n! Discrete wavelet transform\nIf you are operating in Euclidian space, start with:\n! Coordinate transform\n</code></pre></div><div class="language-"><pre><code>DESCRIBE\nHow do I develop\nan understanding\nof the content of\nmy data?\n</code></pre></div><div class="language-"><pre><code>ENRICHMENT q\nHow do I add\nnew information\nto my data?\n</code></pre></div><div class="language-"><pre><code>AGGREGATION\nHow do I collect\nand summarize\nmy data?\n</code></pre></div><p><strong>1</strong></p><p><strong>2</strong></p><p><strong>4</strong></p><div class="language-"><pre><code>3\nPREDICT\n</code></pre></div><div class="language-"><pre><code>DISCOVER\n</code></pre></div><div class="language-"><pre><code>ADVISE\n</code></pre></div><p></p><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>If you want an ordered set of clusters with variable precision, start with:\n! Hierarchical\nr If you have an unknown number of clusters, start with:\n! X-means\n! Canopy\nIf you have text data, start with:\n! Topic modeling\nIf you have non-elliptical clusters, start with:\n! Fractal\n! DB Scan\nIf you want soft membership in the clusters, start with:\n! Gaussian mixture models\ns If you have an known number of clusters, start with:\n! K-means\n</code></pre></div><div class="language-"><pre><code>If your data has unknown structure, start with:\n! Tree-based methods\nIf statistical measures of importance are needed,\nstart with:\n! Generalized linear models\nIf statistical measures of importance are not needed,\nstart with:\n! Regression with shrinkage (e.g., LASSO, Elastic net)\n! Stepwise regression\n</code></pre></div><div class="language-"><pre><code>2\nDISCOVER\nWhat are the\nkey relationships\nin the data?\n</code></pre></div><div class="language-"><pre><code>REGRESSION\nHow do I\ndetermine which\nvariables may be\nimportant?\n</code></pre></div><div class="language-"><pre><code>CLUSTERING\nHow do I segment\nthe data to\nfind natural\ngroupings?\n</code></pre></div><div class="language-"><pre><code>1\nDESCRIBE\n</code></pre></div><div class="language-"><pre><code>Data\nScience\n</code></pre></div><p><strong>4</strong></p><div class="language-"><pre><code>3\nPREDICT\n</code></pre></div><div class="language-"><pre><code>ADVISE\n</code></pre></div><div class="language-"><pre><code>r TIP: Canopy clustering is good when you only want to make a single pass\nover the data.\ns TIP: Use canopy or hierarchical clustering to estimate the number of clusters\nyou should generate.\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>If you are unsure of feature importance, start with:\n! Neural nets,\n! Random forests\nIf you require a highly transparent model, start with:\n! Decision trees\nIf you have &lt;20 data dimensions, start with:\n! K-nearest neighbors\nIf you have a large data set with an unknown classification signal, start with:\n! Naive Bayes\nIf you want to estimate an unobservable state based on observable variables, start with:\n! Hidden Markov model\nu If you don&#39;t know where else to begin, start with:\n! Support Vector Machines (SVM)\n! Random forests\n</code></pre></div><div class="language-"><pre><code>If the data structure is unknown, start with:\n! Tree-based methods\nIf you require a highly transparent model, start with:\n! Generalized linear models\nIf you have &lt;20 data dimensions, start with:\n! K-nearest neighbors\n</code></pre></div><div class="language-"><pre><code>PREDICT\nWhat are the\nlikely future\noutcomes?\n</code></pre></div><div class="language-"><pre><code>REGRESSION\nHow do I predict\na future value?\n</code></pre></div><div class="language-"><pre><code>If you only have knowledge of how people interact with items,\nstart with:\n! Collaborative filtering\nIf you have a feature vector of item characteristics, start with:\n! Content-based methods\nIf you only have knowledge of how items are connected to one\nanother, start with:\n! Graph-based methods\n</code></pre></div><div class="language-"><pre><code>RECOMMENDATION v\nHow do I predict\nrelevant conditions?\n</code></pre></div><p><strong>CLASSIFICATION</strong> t How do I predict group membership?</p><p><strong>3</strong></p><div class="language-"><pre><code>2\nDISCOVER\n</code></pre></div><div class="language-"><pre><code>1\nDESCRIBE\n</code></pre></div><div class="language-"><pre><code>Data\nScience\n</code></pre></div><div class="language-"><pre><code>4\nADVISE\n</code></pre></div><div class="language-"><pre><code>t TIP: It can be difficult to predict which classifier will work best on your data set.\nAlways try multiple classifiers. Pick the one or two that work the best to refine and\nexplore further.\nu TIP: These are our favorite, go-to classification algorithms.\nv TIP: Be careful of the &quot;recommendation bubble&quot;, the tendency of recommenders to\nrecommend only what has been seen in the past. You must ensure you add diversity to\navoid this phenomenon.\nv TIP: SVD and PCA are good tools for creating better features for recommenders.\nSource: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>If your problem is represented by a\nnon-deterministic utility function, start with:\n! Stochastic search\nIf approximate solutions are acceptable,\nstart with:\n! Genetic algorithms\n! Simulated annealing\n! Gradient search\nIf your problem is represented by a\ndeterministic utility function, start with:\n! Linear programming\n! Integer programming\n! Non-linear programming\n</code></pre></div><div class="language-"><pre><code>ADVISE\nWhat course\nof action\nshould I take?\n</code></pre></div><div class="language-"><pre><code>OPTIMIZATION\nHow do I identify the\nbest course of action\nwhen my objective\ncan be expressed as\na utility function?\n</code></pre></div><div class="language-"><pre><code>If you must model discrete entities, start with:\n! Discrete Event Simulation (DES)\nIf there are a discrete set of possible states, start with:\n! Markov models\nIf there are actions and interactions among autonomous\nentities, start with:\n! Agent-based simulation\nIf you do not need to model discrete entities, start with:\n! Monte Carlo simulation\nIf you are modeling a complex system with feedback\nmechanisms between actions, start with:\n! Systems dynamics\nIf you require continuous tracking of system behavior,\nstart with:\n! Activity-based simulation\nIf you already have an understanding of what factors\ngovern the system, start with:\n! ODES\n! PDES\n</code></pre></div><div class="language-"><pre><code>RECOMMENDATION \nHow do I predict\nrelevant conditions?\n</code></pre></div><p><strong>4</strong></p><div class="language-"><pre><code>2\nDISCOVER\n</code></pre></div><div class="language-"><pre><code>1\nDESCRIBE\n</code></pre></div><div class="language-"><pre><code>Data\nScience\n</code></pre></div><div class="language-"><pre><code>3\nPREDICT\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Compiled by: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Algorithms or\nMethod Name Description Tips From the Pros\n</code></pre></div><div class="language-"><pre><code>References and Papers\nWe Love to Read\n</code></pre></div><div class="language-"><pre><code>Agent Based\nSimulation\n</code></pre></div><div class="language-"><pre><code>Simulates the actions\nand interactions of\nautonomous agents.\n</code></pre></div><div class="language-"><pre><code>In many systems, complex behavior\nresults from surprisingly simple rules.\nKeep the logic of your agents simple\nand gradually build in sophistication.\n</code></pre></div><div class="language-"><pre><code>Macal, Charles, and Michael\nNorth. “Agent-based Modeling\nand Simulation.” Winter\nSimulation Conference. Dec.\n</code></pre></div><ol start="2009"><li>Print.</li></ol><div class="language-"><pre><code>Collaborative\nFiltering\n</code></pre></div><div class="language-"><pre><code>Also known as\n&#39;Recommendation,&#39; suggest\nor eliminate items from a\nset by comparing a history\nof actions against items\nperformed by users. Finds\nsimilar items based on who\nused them or similar users\nbased on the items they use.\n</code></pre></div><div class="language-"><pre><code>Use Singular Value Decomposition\nbased Recommendation for cases\nwhere there are latent factors in your\ndomain, e.g., genres in movies.\n</code></pre></div><div class="language-"><pre><code>Owen, Sean, Robin Anil, Ted\nDunning, and Ellen Friedman.\nMahout in Action. New Jersey:\nManning, 2012. Print.\n</code></pre></div><div class="language-"><pre><code>Coordinate\nTransforma-\ntion\n</code></pre></div><div class="language-"><pre><code>Provides a different\nperspective on data.\n</code></pre></div><div class="language-"><pre><code>Changing the coordinate system for data, for\nexample, using polar or cylindrical coordinates,\nmay more readily highlight key structure in the\ndata. A key step in coordinate transformations\nis to appreciate multidimensionality and to\nsystematically analyze subspaces of the data.\n</code></pre></div><div class="language-"><pre><code>Abbott, Edwin. A. Flatland: A\nRomance of Many Dimensions.\nUnited Kingdom: Seely &amp; Co,\n</code></pre></div><ol start="1884"><li>Print.</li></ol><div class="language-"><pre><code>Design of\nExperiments\n</code></pre></div><div class="language-"><pre><code>Applies controlled\nexperiments to quantify\neffects on system output\ncaused by changes to inputs.\n</code></pre></div><div class="language-"><pre><code>Fractional factorial designs can significantly\nreduce the number of different types of\nexperiments you must conduct.\n</code></pre></div><div class="language-"><pre><code>Montgomery, Douglas. Design\nand Analysis of Experiments.\nNew Jersey: John Wiley\n&amp; Sons, 2012. Print.\n</code></pre></div><h3 id="detailed-table-of-analytics"><a class="header-anchor" href="#detailed-table-of-analytics" aria-hidden="true">#</a> Detailed Table of Analytics</h3><div class="language-"><pre><code>Getting you to the right starting point isn’t enough. We also\nprovide a translator so you understand what you’ve been told.\n</code></pre></div><div class="language-"><pre><code>Identifying several analytic techniques that can be applied to your\nproblem is useful, but their name alone will not be much help. #e\nDetailed Table of Analytics translates the names into something more\nmeaningful. Once you’ve identi!ed a technique in the Guide to\nAnalytic Selection, !nd the corresponding row in the table. #ere you\nwill !nd a brief description of the techniques, tips we’ve learned and a\nfew references we’ve found helpful.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Compiled by: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Algorithms or\nMethod Name Description Tips From the Pros\n</code></pre></div><div class="language-"><pre><code>References and Papers\nWe Love to Read\n</code></pre></div><p><strong>Differential Equations</strong></p><div class="language-"><pre><code>Used to express\nrelationships between\nfunctions and their\nderivatives, for example,\nchange over time.\n</code></pre></div><div class="language-"><pre><code>Differential equations can be used to formalize\nmodels and make predictions. The equations\nthemselves can be solved numerically and\ntested with different initial conditions to study\nsystem trajectories.\n</code></pre></div><div class="language-"><pre><code>Zill, Dennis, Warren Wright,\nand Michael Cullen. Differential\nEquations with Boundary-Value\nProblems. Connecticut: Cengage\nLearning, 2012. Print.\n</code></pre></div><p><strong>Discrete Event Simulation</strong></p><div class="language-"><pre><code>Simulates a discrete\nsequence of events where\neach event occurs at a\nparticular instant in time.\nThe model updates its state\nonly at points in time when\nevents occur.\n</code></pre></div><div class="language-"><pre><code>Discrete event simulation is useful when\nanalyzing event based processes such as\nproduction lines and service centers to\ndetermine how system level behavior changes\nas different process parameters change.\nOptimization can integrate with simulation to\ngain efficiencies in a process.\n</code></pre></div><div class="language-"><pre><code>Cassandras, Christopher, and\nStephanie Lafortune. Introduction\nto Discrete Event Systems. New\nYork: Springer, 1999. Print.\n</code></pre></div><p><strong>Discrete Wavelet Transform</strong></p><div class="language-"><pre><code>Transforms time series\ndata into frequency\ndomain preserving\nlocality information.\n</code></pre></div><div class="language-"><pre><code>Offers very good time and frequency\nlocalization. The advantage over Fourier\ntransforms is that it preserves both frequency\nand locality.\n</code></pre></div><div class="language-"><pre><code>Burrus, C.Sidney, Ramesh A.\nGopinath, Haitao Guo, Jan E.\nOdegard, and Ivan W. Selesnick.\nIntroduction to Wavelets\nand Wavelet Transforms:\nA Primer. New Jersey:\nPrentice Hall, 1998. Print.\n</code></pre></div><p><strong>Exponential Smoothing</strong></p><div class="language-"><pre><code>Used to remove artifacts\nexpected from collection\nerror or outliers.\n</code></pre></div><div class="language-"><pre><code>In comparison to a using moving average\nwhere past observations are weighted equally,\nexponential smoothing assigns exponentially\ndecreasing weights over time.\n</code></pre></div><div class="language-"><pre><code>Chatfield, Chris, Anne B. Koehler,\nJ. Keith Ord, and Ralph D.\nSnyder. “A New Look at Models\nfor Exponential Smoothing.”\nJournal of the Royal Statistical\nSociety: Series D (The Statistician)\n50.2 (July 2001): 147-159. Print.\n</code></pre></div><p><strong>Factor Analysis</strong></p><div class="language-"><pre><code>Describes variability among\ncorrelated variables with the\ngoal of lowering the number\nof unobserved variables,\nnamely, the factors.\n</code></pre></div><div class="language-"><pre><code>If you suspect there are inmeasurable\ninfluences on your data, then you may want to\ntry factor analysis.\n</code></pre></div><div class="language-"><pre><code>Child, Dennis. The Essentials of\nFactor Analysis. United Kingdom:\nCassell Educational, 1990. Print.\n</code></pre></div><p><strong>Fast Fourier Transform</strong></p><div class="language-"><pre><code>Transforms time series from\ntime to frequency domain\nefficiently. Can also be used\nfor image improvement by\nspatial transforms.\n</code></pre></div><div class="language-"><pre><code>Filtering a time varying signal can be done\nmore effectively in the frequency domain. Also,\nnoise can often be identified in such signals by\nobserving power at aberrant frequencies.\n</code></pre></div><div class="language-"><pre><code>Mitra, Partha P., and Hemant\nBokil. Observed Brain Dynamics.\nUnited Kingdom: Oxford\nUniversity Press, 2008. Print.\n</code></pre></div><p><strong>Format Conversion</strong></p><div class="language-"><pre><code>Creates a standard\nrepresentation of data\nregardless of source format.\nFor example, extracting raw\nUTF-8 encoded text from\nbinary file formats such as\nMicrosoft Word or PDFs.\n</code></pre></div><div class="language-"><pre><code>There are a number of open source software\npackages that support format conversion and\ncan interpret a wide variety of formats. One\nnotable package is Apache Tikia.\n</code></pre></div><div class="language-"><pre><code>Ingersoll, Grant S., Thomas S.\nMorton, and Andrew L. Farris.\nTaming Text: How to Find,\nOrganize, and Manipulate It. New\nJersey: Manning, 2013. Print.\n</code></pre></div><p><strong>Gaussian Filtering</strong></p><div class="language-"><pre><code>Acts to remove noise\nor blur data.\n</code></pre></div><div class="language-"><pre><code>Can be used to remove speckle\nnoise from images.\n</code></pre></div><div class="language-"><pre><code>Parker, James R. Algorithms for\nImage Processing and Computer\nVision. New Jersey: John Wiley &amp;\nSons, 2010. Print.\n</code></pre></div><p><strong>Generalized Linear Models</strong></p><div class="language-"><pre><code>Expands ordinary linear\nregression to allow\nfor error distribution\nthat is not normal.\n</code></pre></div><div class="language-"><pre><code>Use if the observed error in your system does\nnot follow the normal distribution.\n</code></pre></div><div class="language-"><pre><code>MacCullagh, P., and John A.\nNelder. Generalized Linear\nModels. Florida: CRC Press,\n</code></pre></div><ol start="1989"><li>Print.</li></ol><p><strong>Genetic Algorithms</strong></p><div class="language-"><pre><code>Evolves candidate\nmodels over generations\nby evolutionary\ninspired operators of\nmutation and crossover\nof parameters.\n</code></pre></div><div class="language-"><pre><code>Increasing the generation size adds diversity\nin considering parameter combinations, but\nrequires more objective function evaluation.\nCalculating individuals within a generation\nis strongly parallelizable. Representation of\ncandidate solutions can impact performance.\n</code></pre></div><div class="language-"><pre><code>De Jong, Kenneth A. Evolutionary\nComputation - A Unified\nApproach. Massachusetts:\nMIT Press, 2002. Print.\n</code></pre></div><p><strong>Grid Search</strong></p><div class="language-"><pre><code>Systematic search across\ndiscrete parameter\nvalues for parameter\nexploration problems.\n</code></pre></div><div class="language-"><pre><code>A grid across the parameters is used to\nvisualize the parameter landscape and assess\nwhether multiple minima are present.\n</code></pre></div><div class="language-"><pre><code>Kolda, Tamara G., Robert M.\nLewis, and Virginia Torczon.\n“Optimization by Direct Search:\nNew Perspectives on Some\nClassical and Modern Methods.”\nSIAM Review 45.3 (2003): 385-\n</code></pre></div><ol start="482"><li>Print.</li></ol><div class="language-"><pre><code>Take off the Training Wheels 73\n</code></pre></div><div class="language-"><pre><code>Compiled by: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Algorithms or\nMethod Name Description Tips From the Pros\n</code></pre></div><div class="language-"><pre><code>References and Papers\nWe Love to Read\n</code></pre></div><div class="language-"><pre><code>Hidden Markov\nModels\n</code></pre></div><div class="language-"><pre><code>Models sequential data by\ndetermining the discrete\nlatent variables, but\nthe observables may be\ncontinuous or discrete.\n</code></pre></div><div class="language-"><pre><code>One of the most powerful properties of Hidden\nMarkov Models is their ability to exhibit\nsome degree of invariance to local warping\n(compression and stretching) of the time axis.\nHowever, a significant weakness of the Hidden\nMarkov Model is the way in which it represents\nthe distribution of times for which the system\nremains in a given state.\n</code></pre></div><div class="language-"><pre><code>Bishop, Christopher M. Pattern\nRecognition and Machine\nLearning. New York: Springer,\n</code></pre></div><ol start="2006"><li>Print.</li></ol><div class="language-"><pre><code>Hierarchical\nClustering\n</code></pre></div><div class="language-"><pre><code>Connectivity based\nclustering approach\nthat sequentially builds\nbigger (agglomerative)\nor smaller (divisive)\nclusters in the data.\n</code></pre></div><div class="language-"><pre><code>Provides views of clusters at multiple\nresolutions of closeness. Algorithms\nbegin to slow for larger data sets due\nto most implementations exhibiting\nO(N^3 ) or O(N^2 ) complexity.\n</code></pre></div><div class="language-"><pre><code>Rui Xu, and Don Wunsch.\nClustering. New Jersey: Wiley-\nIEEE Press, 2008. Print.\n</code></pre></div><div class="language-"><pre><code>K-means\nand X-means\nClustering\n</code></pre></div><div class="language-"><pre><code>Centroid based clustering\nalgorithms, where with\nK means the number\nof clusters is set and X\nmeans the number of\nclusters is unknown.\n</code></pre></div><div class="language-"><pre><code>When applying clustering techniques, make\nsure to understand the shape of your data.\nClustering techniques will return poor results if\nyour data is not circular or ellipsoidal shaped.\n</code></pre></div><div class="language-"><pre><code>Rui Xu, and Don Wunsch.\nClustering. New Jersey: Wiley-\nIEEE Press, 2008. Print.\n</code></pre></div><div class="language-"><pre><code>Linear,\nNon-linear,\nand Integer\nProgramming\n</code></pre></div><div class="language-"><pre><code>Set of techniques for\nminimizing or maximizing a\nfunction over a constrained\nset of input parameters.\n</code></pre></div><div class="language-"><pre><code>Start with linear programs because algorithms\nfor integer and non-linear variables can take\nmuch longer to run.\n</code></pre></div><div class="language-"><pre><code>Winston, Wayne L. Operations\nResearch: Applications and\nAlgorithms. Connecticut:\nCengage Learning, 2003. Print.\n</code></pre></div><div class="language-"><pre><code>Markov Chain\nMonte Carlo\n(MCMC)\n</code></pre></div><div class="language-"><pre><code>A method of sampling\ntypically used in Bayesian\nmodels to estimate the joint\ndistribution of parameters\ngiven the data.\n</code></pre></div><div class="language-"><pre><code>Problems that are intractable using analytic\napproaches can become tractable using MCMC,\nwhen even considering high-dimensional\nproblems. The tractability is a result of using\nstatistics on the underlying distributions of\ninterest, namely, sampling with Monte Carlo and\nconsidering the stochastic sequential process of\nMarkov Chains.\n</code></pre></div><div class="language-"><pre><code>Andrieu, Christophe, Nando\nde Freitas, Amaud Doucet,\nand Michael I. Jordan. “An\nIntroduction to MCMC for\nMachine Learning.” Machine\nLearning , 50.1 (January 2003):\n5-43. Print.\n</code></pre></div><div class="language-"><pre><code>Monte Carlo\nMethods\n</code></pre></div><div class="language-"><pre><code>Set of computational\ntechniques to generate\nrandom numbers.\n</code></pre></div><div class="language-"><pre><code>Particularly useful for numerical integration,\nsolutions of differential equations, computing\nBayesian posteriors, and high dimensional\nmultivariate sampling.\n</code></pre></div><div class="language-"><pre><code>Fishman, George S. Monte\nCarlo: Concepts, Algorithms, and\nApplications. New York: Springer,\n</code></pre></div><ol start="2003"><li>Print.</li></ol><div class="language-"><pre><code>Naïve Bayes\n</code></pre></div><div class="language-"><pre><code>Predicts classes following\nBayes Theorem that\nstates the probability of\nan outcome given a set of\nfeatures is based on the\nprobability of features given\nan outcome.\n</code></pre></div><div class="language-"><pre><code>Assumes that all variables are independent,\nso it can have issues learning in the context of\nhighly interdependent variables. The model can\nbe learned on a single pass of data using simple\ncounts and therefore is useful in determining\nwhether exploitable patterns exist in large data\nsets with minimal development time.\n</code></pre></div><div class="language-"><pre><code>Ingersoll, Grant S., Thomas S.\nMorton, and Andrew L. Farris.\nTaming Text: How to Find,\nOrganize, and Manipulate It. New\nJersey: Manning, 2013. Print.\n</code></pre></div><div class="language-"><pre><code>Neural\nNetworks\n</code></pre></div><div class="language-"><pre><code>Learns salient features in\ndata by adjusting weights\nbetween nodes through a\nlearning rule.\n</code></pre></div><div class="language-"><pre><code>Training a neural network takes substantially\nlonger than evaluating new data with an already\ntrained network. Sparser network connectivity\ncan help to segment the input space and\nimprove performance on classification tasks.\n</code></pre></div><div class="language-"><pre><code>Haykin, Simon O. Neural\nNetworks and Learning\nMachines. New Jersey:\nPrentice Hall, 2008. Print.\n</code></pre></div><div class="language-"><pre><code>Outlier\nRemoval\n</code></pre></div><div class="language-"><pre><code>Method for identifying and\nremoving noise or artifacts\nfrom data.\n</code></pre></div><div class="language-"><pre><code>Be cautious when removing outliers. Sometimes\nthe most interesting behavior of a system is at\ntimes when there are aberrant data points.\n</code></pre></div><div class="language-"><pre><code>Maimon, Oded, and Lior\nRockach. Data Mining and\nKnowledge Discovery Handbook: A\nComplete Guide for Practitioners\nand Researchers. The\nNetherlands: Kluwer Academic\nPublishers, 2005. Print.\n</code></pre></div><div class="language-"><pre><code>Principal\nComponents\nAnalysis\n</code></pre></div><div class="language-"><pre><code>Enables dimensionality\nreduction by identifying\nhighly correlated\ndimensions.\n</code></pre></div><div class="language-"><pre><code>Many large datasets contain correlations\nbetween dimensions; therefore part of the\ndataset is redundant. When analyzing the\nresulting principal components, rank order\nthem by variance as this is the highest\ninformation view of your data. Use skree plots to\ninfer the optimal number of components.\n</code></pre></div><div class="language-"><pre><code>Wallisch, Pascal, Michael E.\nLusignan, Marc D. Benayoun,\nTanya I. Baker, Adam Seth\nDickey, and Nicholas G.\nHatsopoulos. Matlab for\nNeuroscientists. New Jersey:\nPrentice Hall, 2009. Print.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Compiled by: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Algorithms or\nMethod Name Description Tips From the Pros\n</code></pre></div><div class="language-"><pre><code>References and Papers\nWe Love to Read\n</code></pre></div><p><strong>Regression with Shrinkage (Lasso)</strong></p><div class="language-"><pre><code>A method of variable\nselection and prediction\ncombined into a possibly\nbiased linear model.\n</code></pre></div><div class="language-"><pre><code>There are different methods to select the\nlambda parameter. A typical choice is cross\nvalidation with MSE as the metric.\n</code></pre></div><div class="language-"><pre><code>Tibshirani, Robert. “Regression\nShrinkage and Selection\nvia the Lasso.” Journal of\nthe Royal Statistical Society.\nSeries B (Methodological) 58.1\n(1996): 267-288. Print.\n</code></pre></div><p><strong>Sensitivity Analysis</strong></p><div class="language-"><pre><code>Involves testing individual\nparameters in an analytic\nor model and observing the\nmagnitude of the effect.\n</code></pre></div><div class="language-"><pre><code>Insensitive model parameters during an\noptimization are candidates for being set to\nconstants. This reduces the dimensionality\nof optimization problems and provides an\nopportunity for speed up.\n</code></pre></div><div class="language-"><pre><code>Saltelli, A., Marco Ratto, Terry\nAndres, Francesca Campolongo,\nJessica Cariboni, Debora Gatelli,\nMichaela Saisana, and Stefano\nTarantola. Global Sensitivity\nAnalysis: the Primer. New Jersey:\nJohn Wiley &amp; Sons, 2008. Print.\n</code></pre></div><p><strong>Simulated Annealing</strong></p><div class="language-"><pre><code>Named after a controlled\ncooling process in\nmetallurgy, and by\nanalogy using a changing\ntemperature or annealing\nschedule to vary\nalgorithmic convergence.\n</code></pre></div><div class="language-"><pre><code>The standard annealing function allows for\ninitial wide exploration of the parameter space\nfollowed by a narrower search. Depending on\nthe search priority the annealing function can\nbe modified to allow for longer explorative\nsearch at a high temperature.\n</code></pre></div><div class="language-"><pre><code>Bertsimas, Dimitris, and John\nTsitsiklis. “Simulated Annealing.”\nStatistical Science. 8.1 (1993):\n10-15. Print.\n</code></pre></div><p><strong>Stepwise Regression</strong></p><div class="language-"><pre><code>A method of variable\nselection and prediction.\nAkaike&#39;s information\ncriterion AIC is used as\nthe metric for selection.\nThe resulting predictive\nmodel is based upon\nordinary least squares,\nor a general linear model\nwith parameter estimation\nvia maximum likelihood.\n</code></pre></div><div class="language-"><pre><code>Caution must be used when considering\nStepwise Regression, as over fitting often\noccurs. To mitigate over fitting try to limit the\nnumber of free variables used.\n</code></pre></div><div class="language-"><pre><code>Hocking, R. R. “The Analysis and\nSelection of Variables in Linear\nRegression.” Biometrics. 32.1\n(March 1976): 1-49. Print.\n</code></pre></div><p><strong>Stochastic Gradient Descent</strong></p><div class="language-"><pre><code>General-purpose\noptimization for learning of\nneural networks, support\nvector machines, and\nlogistic regression models.\n</code></pre></div><div class="language-"><pre><code>Applied in cases where the objective\nfunction is not completely differentiable\nwhen using sub-gradients.\n</code></pre></div><div class="language-"><pre><code>Witten, Ian H., Eibe Frank,\nand Mark A. Hall. Data Mining:\nPractical Machine Learning Tools\nand Techniques. Massachusetts:\nMorgan Kaufmann, 2011. Print.\n</code></pre></div><p><strong>Support Vector Machines</strong></p><div class="language-"><pre><code>Projection of feature vectors\nusing a kernel function into\na space where classes are\nmore separable.\n</code></pre></div><div class="language-"><pre><code>Try multiple kernels and use k-fold cross\nvalidation to validate the choice of the best one.\n</code></pre></div><div class="language-"><pre><code>Hsu, Chih-Wei, Chih-Chung\nChang, and Chih-Jen Lin. “A\nPractical Guide to Support Vector\nClassification.” National Taiwan\nUniversity. 2003. Print.\n</code></pre></div><p><strong>Term Frequency Inverse Document Frequency</strong></p><div class="language-"><pre><code>A statistic that measures\nthe relative importance of a\nterm from a corpus.\n</code></pre></div><div class="language-"><pre><code>Typically used in text mining. Assuming a\ncorpus of news articles, a term that is very\nfrequent such as “the” will likely appear many\ntimes in many documents, having a low value. A\nterm that is infrequent such as a person’s last\nname that appears in a single article will have a\nhigher TD IDF score.\n</code></pre></div><div class="language-"><pre><code>Ingersoll, Grant S., Thomas S.\nMorton, and Andrew L. Farris.\nTaming Text: How to Find,\nOrganize, and Manipulate It. New\nJersey: Manning, 2013. Print.\n</code></pre></div><p><strong>Topic Modeling (Latent Dirichlet Allocation)</strong></p><div class="language-"><pre><code>Identifies latent topics\nin text by examining\nword co-occurrence.\n</code></pre></div><div class="language-"><pre><code>Employ part-of-speech tagging to eliminate\nwords other than nouns and verbs. Use raw\nterm counts instead of TF/IDF weighted terms.\n</code></pre></div><div class="language-"><pre><code>Blei, David M., Andrew Y. Ng,\nand Michael I Jordan. “Latent\nDirichlet Allocation.” Journal of\nMachine Learning Research. 3\n(March 2003): 993-1022. Print.\n</code></pre></div><p><strong>Tree Based Methods</strong></p><div class="language-"><pre><code>Models structured as graph\ntrees where branches\nindicate decisions.\n</code></pre></div><div class="language-"><pre><code>Can be used to systematize a process or act as\na classifier.\n</code></pre></div><div class="language-"><pre><code>James, G., D. Witten, T. Hastie,\nand R Tibshirani. Tree-Based\nMethods. In An Introduction to\nStatistical Learning. New York:\nSpringer, 2013. Print.\n</code></pre></div><p><strong>Wrapper Methods</strong></p><div class="language-"><pre><code>Feature set reduction\nmethod that utilizes\nperformance of a set of\nfeatures on a model, as\na measure of the feature\nset’s performance. Can\nhelp identify combinations\nof features in models that\nachieve high performance.\n</code></pre></div><div class="language-"><pre><code>Utilize k-fold cross validation\nto control over fitting.\n</code></pre></div><div class="language-"><pre><code>John, G. H, R Kohavi, and K.\nPfleger. “Irrelevant Features\nand the Subset Selection\nProblem.” Proceedings of\nICML-94, 11th International\nConference on Machine\nLearning. New Brunswick,\nNew Jersey. 1994. 121-129.\n</code></pre></div><ol start="59"><li>Conference Presentation.</li></ol><div class="language-"><pre><code>Take off the Training Wheels 75\n</code></pre></div><h5 id="life-in-the-trenches"><a class="header-anchor" href="#life-in-the-trenches" aria-hidden="true">#</a> LIFE in THE TRENCHES</h5><div class="language-"><pre><code>NAVIGATING NECK DEEP IN DATA\nOur Data Science experts have learned\nand developed new solutions over the years\nfrom properly framing or reframing analytic\nsolutions. In this section, we list a few\nimportant topics to Data Science coupled\nwith firsthand experience from our experts.\n</code></pre></div><h3 id="feature-engineering"><a class="header-anchor" href="#feature-engineering" aria-hidden="true">#</a> Feature Engineering</h3><div class="language-"><pre><code>Feature engineering is a lot like oxygen. You can’t do without\nit, but you rarely give it much thought.\n</code></pre></div><div class="language-"><pre><code>Feature engineering is the process by which one establishes the\nrepresentation of data in the context of an analytic approach. It is the\nfoundational skill in Data Science. Without feature engineering, it\nwould not be possible to understand and represent the world through\na mathematical model. Feature engineering is a challenging art. Like\nother arts, it is a creative process that manifests uniquely in each\nData Scientist. It will be in&amp;uenced substantially by the scientist’s\nexperiences, tastes and understanding of the !eld.\n</code></pre></div><div class="language-"><pre><code>As the name suggests, feature engineering can be a complex task\nthat may involve chaining and testing di$erent approaches. Features\nmay be simple such as “bag of words,” a popular technique in the\ntext processing domain, or or may be based on more complex\nrepresentations derived through activities such as machine learning.\nYou make use of the output of one analytic technique to create the\nrepresentation that is consumed by another. More often than not, you\nwill !nd yourself operating in the world of highly complex activities.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h1 id="-8"><a class="header-anchor" href="#-8" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>Ed Kohlwey\n</code></pre></div><h2 id="chemoinformatic-search"><a class="header-anchor" href="#chemoinformatic-search" aria-hidden="true">#</a> Chemoinformatic Search</h2><div class="language-"><pre><code>On one assignment, my\nteam was confronted\nwith the challenge of\ndeveloping a search\nengine over chemical\ncompounds. The goal\nof chemoinformatic\nsearch is to predict\nthe properties that\na molecule will exhibit as well as to\nprovide indices over those predicted\nproperties to facilitate data discovery\nin chemistry-based research. These\nproperties may either be discreet (e.g.,\n“a molecule treats disease x well”)\nor continuous (e.g., “a molecule may\nbe dissolved up to 100.21 g/ml”).\n</code></pre></div><div class="language-"><pre><code>Molecules are complex 3D structures,\nwhich are typically represented as\na list of atoms joined by chemical\nbonds of differing lengths with varying\nelectron domain and molecular\ngeometries. The structures are\nspecified by the 3-space coordinates\nand the electrostatic potential\nsurface of the atoms in the molecule.\nSearching this data is a daunting\ntask when one considers that\nnaïve approaches to the problem\nbear significant semblance to the\nGraph Isomorphism Problem.[13]\n</code></pre></div><div class="language-"><pre><code>The solution we developed was\nbased on previous work in molecular\nfingerprinting (sometimes also called\nhashing or locality sensitive hashing).\nFingerprinting is a dimensionality\nreduction technique that dramatically\nreduces the problem space by\nsummarizing many features, often\nwith relatively little regard to the\nimportance of the feature. When\nan exact solution is likely to be\ninfeasible, we often turn to heuristic\napproaches such as fingerprinting.\n</code></pre></div><div class="language-"><pre><code>Our approach used a training set\nwhere all the measured properties\n</code></pre></div><div class="language-"><pre><code>of the molecules were available. We\ncreated a model of how molecular\nstructural similarities might affect\ntheir properties. We began by\nfinding all the sub-graphs of each\nmolecule with length n , resulting\nin a representation similar to\nthe bag-of-words approach from\nnatural language processing.\nWe summarized each molecule\nfragment in a type of fingerprint\ncalled a “Counting Bloom Filter.”\n</code></pre></div><div class="language-"><pre><code>Next, we used several exemplars from\nthe set to create new features. We\nfound the distance from each member\nof the full training set to each of the\nexemplars. We fed these features into\na non-linear regression algorithm\nto yield a model that could be used\non data that was not in the original\ntraining set. This approach can be\nconceptualized as a “hidden manifold,”\nwhereby a hidden surface or shape\ndefines how a molecule will exhibit a\nproperty. We approximate this shape\nusing a non-linear regression and a\nset of data with known properties.\nOnce we have the approximate\nshape, we can use it to predict the\nproperties of new molecules.\n</code></pre></div><div class="language-"><pre><code>Our approach was multi-staged and\ncomplex – we generated sub-graphs,\ncreated bloom filters, calculated\ndistance metrics and fit a linear-\nregression model. This example\nprovides an illustration of how many\nstages may be involved in producing a\nsophisticated feature representation.\nBy creatively combining and building\n“features on features,” we were able\nto create new representations of data\nthat were richer and more descriptive,\nyet were able to execute faster and\nproduce better results.\n</code></pre></div><div class="language-"><pre><code>Life in the Trenches 79\n</code></pre></div><h3 id="feature-selection"><a class="header-anchor" href="#feature-selection" aria-hidden="true">#</a> Feature Selection</h3><div class="language-"><pre><code>Models are like honored guests; you should only feed them the\ngood parts.\n</code></pre></div><div class="language-"><pre><code>Feature selection is the process of determining the set of features with\nthe highest information value to the model. Two main approaches are\n!ltering and wrapper methods. Filtering methods analyze features\nusing a test statistic and eliminate redundant or non-informative\nfeatures. As an example, a !ltering method could eliminate features\nthat have little correlation to the class labels. Wrapper methods\nutilize a classi!cation model as part of feature selection. A model is\ntrained on a set of features and the classi!cation accuracy is used to\nmeasure the information value of the feature set. One example is that\nof training a neural network with a set of features and evaluating the\naccuracy of the model. If the model scores highly on the test set, then\nthe features have high information value. All possible combinations of\nfeatures are tested to !nd the best feature set.\n</code></pre></div><div class="language-"><pre><code>#ere are tradeo$s between these techniques. Filtering methods\nare faster to compute since each feature only needs to be compared\nagainst its class label. Wrapper methods, on the other hand, evaluate\nfeature sets by constructing models and measuring performance. #is\nrequires a large number of models to be trained and evaluated (a\nquantity that grows exponentially in the number of features). Why\nwould anyone use a wrapper method? Feature sets may perform better\nthan individual features.[14] With !lter methods, a feature with weak\ncorrelation to its class labels is eliminated. Some of these eliminated\nfeatures, however, may have performed well when combined with\nother features.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h1 id="-9"><a class="header-anchor" href="#-9" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>Paul Yacci\n</code></pre></div><h2 id="cancer-cell-classification"><a class="header-anchor" href="#cancer-cell-classification" aria-hidden="true">#</a> Cancer Cell Classification</h2><div class="language-"><pre><code>On one project,\nour team was\nchallenged\nto classify\ncancer cell\nprofiles. The\noverarching\ngoal was\nto classify different types\nof Leukemia, based on\nMicroarray profiles from 72\nsamples[15] using a small\nset of features. We utilized\na hybrid Artificial Neural\nNetwork (ANN)[16] and Genetic\nAlgorithm[17] to identify subsets\nof 10 features selected from\nthousands.[18] We trained the\nANN and tested performance\nusing cross-fold validation.\nThe performance measure\n</code></pre></div><div class="language-"><pre><code>was used as feedback into\nthe Genetic Algorithm. When\na set of features contained\nno useful information, the\nmodel performed poorly and\na different feature set would\nbe explored. Over time, this\nmethod selected a set of\nfeatures that performed with\nhigh accuracy. The down-\nselected feature set increased\nspeed and performance as\nwell as allowed for better\ninsight into the factors that\nmay govern the system. This\nallowed our team to design\na diagnostic test for only a\nfew genetic markers instead\nof thousands, substantially\nreducing diagnostic test\ncomplexity and cost.\n</code></pre></div><div class="language-"><pre><code>Life in the Trenches 81\n</code></pre></div><h3 id="data-veracity"><a class="header-anchor" href="#data-veracity" aria-hidden="true">#</a> Data Veracity</h3><div class="language-"><pre><code>We’re Data Scientists, not data alchemists. We can’t make\nanalytic gold from the lead of data.\n</code></pre></div><div class="language-"><pre><code>While most people associate data volume, velocity, and variety with\nbig data, there is an equally important yet often overlooked dimension\n</code></pre></div><ul><li>data veracity. Data veracity refers to the overall quality and correctness of the data. You must assess the truthfulness and accuracy of the data as well as identify missing or incomplete information. As the saying goes, “Garbage in, garbage out.” If your data is inaccurate or missing information, you can’t hope to make analytic gold.</li></ul><div class="language-"><pre><code>Assessing data truthfulness is often subjective. You must rely on your\nexperience and an understanding of the data origins and context.\nDomain expertise is particularly critical for the latter. Although\ndata accuracy assessment may also be subjective, there are times that\nquantitative methods may be used. You may be able to re-sample from\nthe population and conduct a statistical comparison against the stored\nvalues, thereby providing measures of accuracy.\n</code></pre></div><div class="language-"><pre><code>#e most common issues you will encounter are missing or\nincomplete information. #ere are two basic strategies for dealing\nwith missing values – deletion and imputation. In the former, entire\nobservations are excluded from analysis, reducing sample size and\npotentially introducing bias. Imputation, or replacement of missing\nor erroneous values, uses a variety of techniques such as random\nsampling (hot deck imputation) or replacement using the mean,\nstatistical distributions or models.\n</code></pre></div><div class="language-"><pre><code>» Tips From the Pros\n</code></pre></div><div class="language-"><pre><code>Find an approach that works,\nimplement it, and move on. You\ncan worry about optimization and\ntuning your approaches later during\nincremental improvement.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><div class="language-"><pre><code>Brian Keller\n</code></pre></div><h1 id="-10"><a class="header-anchor" href="#-10" aria-hidden="true">#</a> !!</h1><h2 id="time-series-modeling"><a class="header-anchor" href="#time-series-modeling" aria-hidden="true">#</a> Time Series Modeling</h2><div class="language-"><pre><code>On one of our projects, the team was faced with correlating the time\nseries for various parameters. Our initial analysis revealed that the\ncorrelations were almost non-existent. We examined the data and\nquickly discovered data veracity issues. There were missing and\nnull values, as well as negative-value observations, an impossibility\ngiven the context of the measurements (see the figure, Time Series\nData Prior to Cleansing ). Garbage data meant garbage results.\n</code></pre></div><p>Because sample size was already small, deleting observations was undesirable. The volatile nature of the time series meant that imputation through sampling could not be trusted to produce values in which the team would be confident. As a result, we quickly realized that the best strategy was an approach that could filter and correct the noise in the data.</p><p>We initially tried a simplistic approach in which we replaced each observation with a moving average. While this corrected some noise, including the outlier values in our moving-average computation shifted the time series. This caused undesirable</p><div class="language-"><pre><code>distortion in the underlying signal, and we quickly\nabandoned the approach.\nOne of our team members who had experience in\nsignal processing suggested a median filter. The\nmedian filter is a windowing technique that moves\nthrough the data point-by-point, and replaces it\nwith the median value calculated for the current\nwindow. We experimented with various window\nsizes to achieve an acceptable tradeoff between\nsmoothing noise and smoothing away signal. The\nfigure, Time Series Data After Cleansing , shows the\nsame two time series after median filter imputation.\n</code></pre></div><p>The application of the median filter approach was hugely successful. Visual inspection of the time series plots reveals smoothing of the outliers without dampening the naturally occurring peaks and troughs (no signal loss). Prior to smoothing, we saw no correlation in our data, but afterwards, Spearman’s Rho was ~0.5 for almost all parameters.</p><div class="language-"><pre><code>By addressing our data veracity issues, we were\nable to create analytic gold. While other approaches\nmay also have been effective, implementation speed\nconstraints prevented us from doing any further\nanalysis. We achieved the success we were after and\nmoved on to address other aspects of the problem.\n</code></pre></div><div class="language-"><pre><code>Time Series Data Prior to Cleansing\n</code></pre></div><div class="language-"><pre><code>Time Series Data After Cleansing\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Life in the Trenches 83\n</code></pre></div><h3 id="application-of"><a class="header-anchor" href="#application-of" aria-hidden="true">#</a> Application of</h3><h3 id="domain-knowledge"><a class="header-anchor" href="#domain-knowledge" aria-hidden="true">#</a> Domain Knowledge</h3><div class="language-"><pre><code>We are all special in our own way. Don’t discount what\nyou know.\n</code></pre></div><div class="language-"><pre><code>Knowledge of the domain in which a problem lies is immensely\nvaluable and irreplaceable. It provides an in-depth understanding\nof your data and the factors in&amp;uencing your analytic goal. Many\ntimes domain knowledge is a key di$erentiator to a Data Science\nteam’s success. Domain knowledge in&amp;uences how we engineer and\nselect features, impute data, choose an algorithm, and determine\nsuccess. One person cannot possibly be a domain expert in every\n!eld, however. We rely on our team, other analysts and domain\nexperts as well as consult research papers and publications to build\nan understanding of the domain.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h1 id="-11"><a class="header-anchor" href="#-11" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>GEOSPATIAL HOTSPOTS\n</code></pre></div><div class="language-"><pre><code>TEMPORAL HOTSPOT\nSun\nSat\nFri\nThu\nWed\nTue\nMon\n3am 6am 9am Noon 3pm 6pm 9pm\n</code></pre></div><div class="language-"><pre><code>3am 6am 9am Noon 3pm 6pm 9pm\n</code></pre></div><div class="language-"><pre><code>Armen\nKherlopian\n</code></pre></div><h2 id="motor-vehicle-theft"><a class="header-anchor" href="#motor-vehicle-theft" aria-hidden="true">#</a> Motor Vehicle Theft</h2><div class="language-"><pre><code>On one project,\nour team explored\nhow Data Science\ncould be applied to\nimprove public safety.\nAccording to the\nFBI, approximately\n$8 Billion is lost\nannually due to\nautomobile theft. Recovery of the\none million vehicles stolen every\nyear in the U.S. is less than 60%.\nDealing with these crimes represents\na significant investment of law\nenforcement resources. We wanted\nto see if we could identify how to\nreduce auto theft while efficiently\nusing law enforcement resources.\n</code></pre></div><div class="language-"><pre><code>Our team began by parsing and\nverifying San Francisco crime data.\nWe enriched stolen car reporting with\ngeneral city data. After conducting\nseveral data experiments across both\nspace and time, three geospatial and\none temporal hotspot emerged (see\nfigure, Geospatial and Temporal Car\nTheft Hotspots ). The domain expert\non the team was able to discern\nthat the primary geospatial hotspot\ncorresponded to an area surrounded\nby parks. The parks created an urban\nmountain with a number of over-foot\naccess points that were conducive to\ncar theft.\n</code></pre></div><div class="language-"><pre><code>Our team used the temporal hotspot information in tandem with the insights\nfrom the domain expert to develop a Monte Carlo model to predict the likelihood\nof a motor vehicle theft at particular city intersections. By prioritizing the\nintersections identified by the model, local governments would have the\ninformation necessary to efficiently deploy their patrols. Motor vehicle thefts\ncould be reduced and law enforcement resources could be more efficiently\ndeployed. The analysis, enabled by domain expertise, yielded actionable insights\nthat could make the streets safer.\n</code></pre></div><div class="language-"><pre><code>Geospatial and Temporal Car Theft Hotspots\n</code></pre></div><div class="language-"><pre><code>Source: Booz Allen Hamilton\n</code></pre></div><div class="language-"><pre><code>Life in the Trenches 85\n</code></pre></div><h3 id="e-curse-of"><a class="header-anchor" href="#e-curse-of" aria-hidden="true">#</a> !e Curse of</h3><h3 id="dimensionality"><a class="header-anchor" href="#dimensionality" aria-hidden="true">#</a> Dimensionality</h3><div class="language-"><pre><code>!ere is no magical potion to cure the curse, but there is PCA.\n</code></pre></div><div class="language-"><pre><code>!e “curse of dimensionality” is one of the most important results\nin machine learning. Most texts on machine learning mention this\nphenomenon in the &quot;rst chapter or two, but it often takes many years\nof practice to understand its true implications.\n</code></pre></div><div class="language-"><pre><code>Classi&quot;cation methods, like most machine learning methods, are\nsubject to the implications of the curse of dimensionality. !e basic\nintuition in this case is that as the number of data dimensions\nincreases, it becomes more di#cult to create generalizable\nclassi&quot;cation models (models that apply well over phenomena not\nobserved in the training set). !is di#culty is usually impossible to\novercome in real world settings. !ere are some exceptions in domains\nwhere things happen to work out, but usually you must work to\nminimize the number of dimensions. !is requires a combination\nof clever feature engineering and use of dimensionality reduction\ntechniques (see Feature Engineering and Feature Selection Life in the\nTrenches ). In our practical experience, the maximum\nnumber of dimensions seems to be ~10 for linear model-based\napproaches. !e limit seems to be in the tens of thousands for more\nsophisticated methods such as support vector machines, but the limit\nstill exists nonetheless.\n</code></pre></div><div class="language-"><pre><code>A counterintuitive consequence of the curse of dimensionality is\nthat it limits the amount of data needed to train a classi&quot;cation\nmodel. !ere are roughly two reasons for this phenomenon. In one\ncase, the dimensionality is small enough that the model can be\ntrained on a single machine. In the other case, the exponentially\nexpanding complexity of a high-dimensionality problem makes it\n(practically) computationally impossible to train a model. In our\nexperience, it is quite rare for a problem to fall in a “sweet spot”\nbetween these two extremes.\n</code></pre></div><div class="language-"><pre><code>!is observation is not to say that such a condition never arises. We\nbelieve it is rare enough, however, that practitioners need not concern\nthemselves with how to address this case. Rather than trying to create\nsuper-scalable algorithm implementations, focus your attention on\nsolving your immediate problems with basic methods. Wait until you\nencounter a problem where an algorithm fails to converge or provides\npoor cross-validated results, and then seek new approaches. Only\nwhen you &quot;nd that alternate approaches don’t already exist, should you\nbegin building new implementations. !e expected cost of this work\npattern is lower than over-engineering right out of the gate.\n</code></pre></div><div class="language-"><pre><code>Put otherwise, “Keep it simple, stupid”.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h2 id="baking-the-cake"><a class="header-anchor" href="#baking-the-cake" aria-hidden="true">#</a> Baking the Cake</h2><div class="language-"><pre><code>I was once given a time series set of roughly\n1,600 predictor variables and 16 target variables\nand asked to implement a number of modeling\ntechniques to predict the target variable\nvalues. The client was challenged to handle the\ncomplexity associated with the large number of\nvariables and needed help. Not only did I have\na case of the curse, but the predictor variables\nwere also quite diverse. At first glance, it looked\nlike trying to bake a cake with everything in the cupboard.\nThat is not a good way to bake or to make predictions!\n</code></pre></div><div class="language-"><pre><code>The data diversity could be\npartially explained by the fact\nthat the time series predictors\ndid not all have the same\nperiodicity. The target time\nseries were all daily values\nwhereas the predictors were\ndaily, weekly, quarterly, and\nmonthly. This was tricky to\nsort out, given that imputing\nzeros isn’t likely to produce\ngood results. For this specific\nreason, I chose to use neural\nnetworks for evaluating the\nweekly variable contributions.\n</code></pre></div><div class="language-"><pre><code>Using this approach, I was\nable to condition upon week,\nwithout greatly increasing\nthe dimensionality. For the\nother predictors, I used\na variety of techniques,\nincluding projection and\ncorrelation, to make heads\nor tails of the predictors. My\napproach successfully reduced\nthe number of variables,\naccomplishing the client’s goal\nof making the problem space\ntractable. As a result, the cake\nturned out just fine.\n</code></pre></div><h1 id="-12"><a class="header-anchor" href="#-12" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>Stephanie\nRivera\n</code></pre></div><div class="language-"><pre><code>Life in the Trenches 87\n</code></pre></div><h3 id="model-validation"><a class="header-anchor" href="#model-validation" aria-hidden="true">#</a> Model Validation</h3><div class="language-"><pre><code>Repeating what you just heard does not mean that you\nlearned anything.\n</code></pre></div><div class="language-"><pre><code>Model validation is central to construction of any model. #is answers\nthe question “How well did my hypothesis !t the observed data?”\nIf we do not have enough data, our models cannot connect the dots.\nOn the other hand, given too much data the model cannot think\noutside of the box. #e model learns speci!c details about the training\ndata that do not generalize to the population. #is is the problem of\nmodel over !tting.\n</code></pre></div><div class="language-"><pre><code>Many techniques exist to combat model over !tting. #e simplest\nmethod is to split your data set into training, testing and validation\nsets. #e training data is used to construct the model. #e model\nconstructed with the training data is then evaluated with the testing\ndata. #e performance of the model against the testing set is used to\nfurther reduce model error. #is indirectly includes the testing data\nwithin model construction, helping to reduce model over !t. Finally,\nthe model is evaluated on the validation data to assess how well the\nmodel generalizes.\n</code></pre></div><div class="language-"><pre><code>A few methods where the data is split into training and testing sets\ninclude: k -fold cross-validation, Leave-One-Out cross-validation,\nbootstrap methods, and resampling methods. Leave-One-Out cross-\nvalidation can be used to get a sense of ideal model performance\nover the training set. A sample is selected from the data to act as the\ntesting sample and the model is trained on the rest of the data. #e\nerror on the test sample is calculated and saved, and the sample is\nreturned to the data set. A di$erent sample is then selected and the\nprocess is repeated. #is continues until all samples in the testing set\nhave been used. #e average error over the testing examples gives a\nmeasure of the model’s error.\n</code></pre></div><div class="language-"><pre><code>#ere are other approaches for testing how well your hypothesis\nre&amp;ects the data. Statistical methods such as calculating the coe%cient\nof determination, commonly called the R -squared value are used to\nidentify how much variation in the data your model explains. Note\nthat as the dimensionality of your feature space grows, the R -squared\nvalue also grows. An adjusted R -squared value compensates for this\nphenomenon by including a penalty for model complexity. When\ntesting the signi!cance of the regression as a whole, the F-test\ncompares the explained variance to unexplained variance. A regression\nresult with a high F-statistic and an adjusted R -squared over 0.7 is\nalmost surely signi!cant.\n</code></pre></div><div class="language-"><pre><code>» Do we really need a case study\nto know that you should\ncheck your work?\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><h5 id="putting-it-all-together"><a class="header-anchor" href="#putting-it-all-together" aria-hidden="true">#</a> PUTTING it ALL TOGETHER</h5><h2 id="consumer-behavior"><a class="header-anchor" href="#consumer-behavior" aria-hidden="true">#</a> Consumer Behavior</h2><h2 id="analysis-from-a"><a class="header-anchor" href="#analysis-from-a" aria-hidden="true">#</a> Analysis from a</h2><h2 id="multi-terabyte"><a class="header-anchor" href="#multi-terabyte" aria-hidden="true">#</a> Multi-Terabyte</h2><h2 id="dataset"><a class="header-anchor" href="#dataset" aria-hidden="true">#</a> Dataset</h2><p>Analytic Challenge</p><div class="language-"><pre><code>After storing over 10 years’ worth of retail\ntransaction in the natural health space, a retail\nclient was interested in employing advanced\nmachine learning techniques to mine the data for\nvaluable insights. The client wanted to develop a\ndatabase structure for long term implementation\nof retail supply chain analytics and select the\nproper algorithms needed to develop insights\ninto supplier, retailer, and consumer interactions.\nDetermining the actual worth of applying big\ndata analytics to the end-to-end retail supply\nchain was also of particular interest.\n</code></pre></div><p>Our Approach</p><div class="language-"><pre><code>The client’s data\nincluded 3TBs of\nproduct descriptions,\ncustomer loyalty\ninformation and B2B\nand B2C transactions\nfor thousands of natural\nhealth retailers across\nNorth America. Because\nthe data had been stored\nin an ad-hoc fashion, the\nfirst step was creating\na practical database\nstructure to enable\nanalytics. We selected\na cloud environment\nin order to quickly\nimplement analytics on\nthe client’s disparate\nand sometimes\nredundant datasets.\nOnce we created a\n</code></pre></div><div class="language-"><pre><code>suitable analytics\narchitecture, we\nmoved on to identifying\nappropriate machine\nlearning techniques that\nwould add value across\nthree key focus areas:\nproduct receptivity, loyal\nprogram analysis, and\nmarket basket analysis.\nFor product receptivity,\nour team used Bayesian\nBelief Networks (BBN)\nto develop probabilistic\nmodels to predict the\nsuccess, failure, and\nlongevity of a new or\ncurrent product. We\njoined transaction data\nwith attribute data of\nboth successful and\nfailed products to\n</code></pre></div><h1 id="-13"><a class="header-anchor" href="#-13" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>» Our Case Studies\n</code></pre></div><div class="language-"><pre><code>Hey, we have given you a lot of\nreally good technical content. We\nknow that this section has the look\nand feel of marketing material,\nbut there is still a really good story\nhere. Remember, storytelling comes\nin many forms and styles, one of\nwhich is the marketing version. You\nshould read this chapter for what it\nis – great information told with a\nmarketing voice.\n</code></pre></div><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><p>transform the data into usable form. Once we created this data file, we used it to train BBNs and create a predictive model for future products.</p><p>For loyalty program analysis, we joined transactional data and customer attribute data, which included location information and shopping trends. We used <em>k</em> -means clustering to segment customers based on their behavior over time. This allowed us to cluster and characterize groups of customers that exhibited similar loyalty patterns.</p><p>For market basket analysis, we employed Latent Dirichlet Allocation (LDA), a natural</p><div class="language-"><pre><code>language processing technique,\nto create consistent product\ncategorization. The client’s\nproduct categorization was\nad-hoc, having been entered\nby individual suppliers and\nretailers. As a result, it was\ninconsistent and often contained\ntypographical errors or missing\nvalues. LDA allowed our team\nto use the existing text to derive\nnew, consistent customer\ncategories for the market basket\nanalysis. After joining the new\nproduct categorization data\nwith transaction data, we used\nAssociation Rules Learning\nto identify sets of product\ncategories that customers\ntended to purchase together at\nindividual retailer locations.\n</code></pre></div><p>Our Impact</p><p>Our team provided key findings and recommendations to describe how the machine learning techniques could be operationalized to provide real-time reporting to retailers. The client received suggestions for improving product nomenclature, product promotions, and end-to-end visibility of the product and process lifecycle. As an example, we used our market basket analysis to create product recommendations for individual retail locations. Our recommendations have the potential to improve sales within certain product categories by up to 50% across the retail network. Together with time savings realized from automated data processing (such as a 300x increase in product categorization speed), these insights demonstrated the clear value of big data analytics to the client’s organization.</p><div class="language-"><pre><code>Putting it all Together 93\n</code></pre></div><h2 id="strategic-insights"><a class="header-anchor" href="#strategic-insights" aria-hidden="true">#</a> Strategic Insights</h2><h2 id="within-terabytes-of"><a class="header-anchor" href="#within-terabytes-of" aria-hidden="true">#</a> within Terabytes of</h2><h2 id="passenger-data"><a class="header-anchor" href="#passenger-data" aria-hidden="true">#</a> Passenger Data</h2><p>Analytic Challenge</p><div class="language-"><pre><code>A commercial airline client was faced with increasing market\ncompetition and challenges in profitability. They wanted to address\nthese challenges with rapid deployment of advanced, globalized\nanalytical tools within their private electronic data warehouse.\nIn the past, the client had analyzed smaller datasets in-house.\nBecause smaller datasets are filtered or diluted subsets of the\nfull data, the airline had not been able to extract the holistic\nunderstanding it was seeking.\n</code></pre></div><div class="language-"><pre><code>Booz Allen was engaged to create capabilities to analyze hundreds\nof gigabytes of client data. The ultimate goal was to generate\ninsights into airline operations, investment decisions and consumer\npreferences that may not have been apparent from studying data\nsubsets. Specifically, the airline wanted to be able to understand\nissues such as: how they perform in different city-pair markets\nrelative to competitors; how booking behaviors change, based on\npassenger and flight characteristics; and how connection times\nimpact demand.\n</code></pre></div><p>Our Solution</p><div class="language-"><pre><code>Due to data privacy issues, our\nteam set up a cloud environment\nwithin the client’s electronic\ndata warehouse. Leveraging\nthis analytic environment,\nanalysis proceeded with an\napproach that focused on the\nclient’s three priorities: market\nperformance, booking behavior,\nand passenger choice.\nWe performed probabilistic\nanalysis using machine learning\ntechniques, particularly\nBayesian Belief Networks\n</code></pre></div><div class="language-"><pre><code>(BBN). We merged passenger\nbooking and other data to\ncreate a BBN training file. Our\nteam developed and validated\ncomprehensive BBN models\nto represent significant\ncustomer behavior and market-\nbased factors that influence\npassenger preference, with\nrespect to selection of flights\nby connection times. Finally,\nour team developed custom big\ndata visualizations to convey\nthe findings to technical and\nnon-technical audiences alike.\n</code></pre></div><h1 id="-14"><a class="header-anchor" href="#-14" aria-hidden="true">#</a> !!</h1><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><p>Our Impact</p><p>We demonstrated the ability to rapidly deploy big data analytical tools and machine learning on massive datasets located inside a commercial airline’s private cloud environment. Results included insights that seemed counterintuitive, but could improve financial performance nonetheless. An example of one such finding was that under certain circumstances, passengers are willing to pay a premium to book itineraries with modified connection times. This translates into a potential revenue increase of many millions of dollars. These insights, often at the level of named customers, can be acted upon immediately to improve financial performance.</p><div class="language-"><pre><code>Putting it all Together 95\n</code></pre></div><h2 id="savings-through"><a class="header-anchor" href="#savings-through" aria-hidden="true">#</a> Savings Through</h2><h2 id="better-manufacturing"><a class="header-anchor" href="#better-manufacturing" aria-hidden="true">#</a> Better Manufacturing</h2><p>Analytic Challenge</p><div class="language-"><pre><code>A manufacturing company engaged Booz Allen to explore data\nrelated to chemical compound production. These processes are\nquite complex. They involve a long chain of interconnected events,\nwhich ultimately leads to high variability in product output. This\nmakes production very expensive.\n</code></pre></div><div class="language-"><pre><code>Understanding the production process is not easy – sensors collect\nthousands of time series variables and thousands of point-in-\ntime measurements, yielding terabytes of data. There was a huge\nopportunity if the client could make sense of this data. Reducing the\nvariance and improving the product yield by even a small amount\ncould result in significant cost savings.\n</code></pre></div><p>Our Solution</p><div class="language-"><pre><code>Due to the size and complexity of\nthe process data, prior analysis\nefforts that focused on data\nfrom only a single sub-process\nhad limited success. Our Data\nScience team took a different\napproach: analyzing all the data\nfrom all the sub-processes\nwith the goal of identifying the\nfactors driving variation. Once\nwe understood those factors, we\ncould develop recommendations\non how to control them to\nincrease yield. The client’s\nprocess engineers had always\nwanted to pursue this approach\nbut lacked the tools to carry out\nthe analysis.\nWe decomposed the problem\ninto a series of smaller\nproblems. First, it was\nnecessary to identify which\ntime series parameters likely\naffected product yield. We\nengaged the client’s domain\nexperts to identify their\n</code></pre></div><div class="language-"><pre><code>hypotheses surrounding the\nprocess. Once we discerned a\nset of hypotheses, we identified\nthe sensors that collected the\nrelevant data.\nWe began initial data processing,\nwhich included filtering\nbad values and identifying\npatterns in the time series. We\nthen needed to segment the\ndata streams into individual\nproduction runs. We identified a\nsensor that stored the high-level\ninformation indicating when a\nproduction process began. This\nsensor provided exactly what we\nneeded, but we quickly noticed\nthat half of the expected data\nwas missing. Examining the\ndata more closely, we realized\nthe sensor had only been used\nwithin recent years. We had to\ntake a step back and reassess\nour plan. After discussions\nwith the domain experts, we\nidentified a different sensor\n</code></pre></div><h1 id="-15"><a class="header-anchor" href="#-15" aria-hidden="true">#</a> !!</h1><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><p>that gave us raw values directly from the production process. The raw values included a tag that indicated the start of a production run. The sensor was active for every production run and could be used reliably to segment the data streams into production runs.</p><p>Next, we had to determine which time series parameters affected product yield. Using the cleaned and processed data and a non-parametric correlation technique, we compared each time series in a production run to all other time series in that same run. Given the pair- wise similarities, we estimated correlation of the similarities to final product yield. We then used the correlations as input into a clustering algorithm to find clusters of time series parameters that correlated with each other in terms of product yield, not in terms of the time series themselves. This data</p><div class="language-"><pre><code>analysis was at a scale not\npreviously possible – millions\nof comparisons for whole\nproduction runs. Engineers were\nable to look at all the data for the\nfirst time, and to see impacts\nof specific parameters across\ndifferent batches and sensors.\nIn addition to identifying the\nkey parameters, the engineers\nneeded to know how to control\nthe parameters to increase\nproduct yield. Discussions\nwith domain experts provided\ninsight into which time series\nparameters could be easily\ncontrolled. This limited the\ncandidate parameters to only\nthose that the process engineers\ncould influence. We extracted\nfeatures from the remaining\ntime series signals and fed\nthem into our models to predict\nyield. The models quantified the\ncorrelation between the pattern\nof parameter values and yield,\nproviding insights on how to\nincrease product yield.\n</code></pre></div><p>Our Impact</p><p>With controls identified and desirable patterns quantified, we provided the engineers with a set of process control actions to improve product output. The raw sensor data that came directly from the production process drove our analysis and recommendations, thus providing the client with confidence in the approach. The reduction in product yield variability will enable the client to produce a better product with lower risk at a reduced cost.</p><div class="language-"><pre><code>Putting it all Together 97\n</code></pre></div><h2 id="realizing-higher"><a class="header-anchor" href="#realizing-higher" aria-hidden="true">#</a> Realizing Higher</h2><h2 id="returns-through"><a class="header-anchor" href="#returns-through" aria-hidden="true">#</a> Returns Through</h2><h2 id="predictive-analytics"><a class="header-anchor" href="#predictive-analytics" aria-hidden="true">#</a> Predictive Analytics</h2><p>Analytic Challenge</p><div class="language-"><pre><code>A major investment house wanted to explore whether the\napplication of Data Science techniques could yield increased\ninvestment returns. In particular, the company wanted to predict\nfuture commodity value movements based on end-of-day and\nprevious-day equity metrics. The client hoped the predictions\ncould be used to optimize their trading activities. By translating\nthe approach across their entire portfolio, they could dramatically\nimprove the yield curve for their investors.\n</code></pre></div><div class="language-"><pre><code>Several challenges were immediately apparent. The data volume\nwas very large, consisting of information from tens of thousands\nof equities, commodities, and options across most major world\nmarkets across multiple time intervals. The need to recommend\na predictive action (go short, go long, stay, increase position size,\nor engage in a particular option play) with very low latency was\nan even greater challenge. The team would need to develop an\napproach that addressed both of these implicit constraints.\n</code></pre></div><p>Our Solution</p><div class="language-"><pre><code>The client challenged Booz\nAllen to use 3,500 independent\nvariables to predict the daily\nprice movements of 16 financial\ninstruments. The client hid the\nmeaning and context of the\nindependent variables, however,\nforcing our team to perform\nanalysis without qualitative\ninformation. The team\nimmediately began searching\nfor supplemental data sources.\nWe identified unstructured data\nfrom other companies, financial\ninstitutions, governments and\nsocial media that could be\nused in our analysis. The team\npaid considerable attention to\n</code></pre></div><div class="language-"><pre><code>database access efficiency and\nsecurity as well as the speed\nof computation.\nOur team implemented\na multifaceted approach,\nincluding a mix of neural\nnetwork optimization and a\nvariety of principal component,\nregression, and unsupervised\nlearning techniques. We were\nable to infer insight into small-\nscale exogenous events that\nprovided a richer basis for\npredicting localized fluctuations\nin the equity prices. Our\nteam was able to use these\npredictions to determine the\noptimal combination of actions\n</code></pre></div><h1 id="-16"><a class="header-anchor" href="#-16" aria-hidden="true">#</a> !!</h1><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><p>that would generate the best aggregate return over 12 months of trading. Careful consideration of the residuals and skilled</p><div class="language-"><pre><code>modeling of the variance added\nadditional value to the outcome\nfor this client.\n</code></pre></div><p>Our Impact</p><p>Our Data Science team conducted an experiment to determine the efficacy of our approach. We used our model to generate buy/sell recommendations based on training data provided by the client. The test cases converged on a set of recommendations in less than ten minutes, satisfying the solution timeliness constraint. The experiment revealed a true positive accuracy of approximately 85% with a similar outcome for true negative accuracy when compared against optimal recommendations based on perfect information. The typical return on investment was, as desired, quite large.</p><p>The ability to determine the position to take, not just for a single financial instrument but also for a complete portfolio, is invaluable for this client. Achieving this outcome required predictive analytics and the ability to rapidly ingest and process large data sets, including unstructured data. This could not have been accomplished without a diverse team of talented Data Scientists bringing the entirety of their tradecraft to bear on the problem.</p><div class="language-"><pre><code>Putting it all Together 99\n</code></pre></div><h5 id="closing-time"><a class="header-anchor" href="#closing-time" aria-hidden="true">#</a> CLOSING TIME</h5><h2 id="parting"><a class="header-anchor" href="#parting" aria-hidden="true">#</a> PARTING</h2><h2 id="thoughts"><a class="header-anchor" href="#thoughts" aria-hidden="true">#</a> THOUGHTS</h2><div class="language-"><pre><code>Data Science capabilities are creating data analytics that are\nimproving every aspect of our lives, from life-saving disease\ntreatments, to national security, to economic stability, and even\nthe convenience of selecting a restaurant. We hope we have\nhelped you truly understand the potential of your data and how\nto become extraordinary thinkers by asking the right questions of\nyour data. We hope we have helped drive forward the science and\nart of Data Science. Most importantly, we hope you are leaving\nwith a newfound passion and excitement for Data Science.\n</code></pre></div><div class="language-"><pre><code>!ank you for taking this journey with us. Please join our\nconversation and let your voice be heard. Email us your ideas\nand perspectives at data_science@bah.com or submit them\nvia a pull request on the Github repository.\n</code></pre></div><div class="language-"><pre><code>Tell us and the world what you know. Join us. Become an\nauthor of this story.\n</code></pre></div><h1 id="-17"><a class="header-anchor" href="#-17" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>Closing Time 103\n</code></pre></div><h2 id="references"><a class="header-anchor" href="#references" aria-hidden="true">#</a> REFERENCES</h2><ol><li>Commonly attributed to: Nye, Bill. <em>Reddit Ask Me Anything</em> (AMA). July 2012. Web. Accessed 15 October 2013. SSRN: &lt;<a href="http://www.reddit" target="_blank" rel="noopener noreferrer">http://www.reddit</a>. com/r/IAmA/comments/x9pq0/iam_bill_nye_the_science_guy_ama&gt;</li><li>Fayyad, Usama, Gregory Piatetsky-Shapiro, and Padhraic Smyth. “From Data Mining to Knowledge Discovery in Databases.” <em>AI Magazine</em> 17.3 (1996): 37-54. Print.</li><li>“Mining Data for Nuggets of Knowledge.” <em>Knowledge@Wharton</em> ,1999. Web. Accessed 16 October 2013. SSRN: &lt;<a href="http://knowledge.wharton" target="_blank" rel="noopener noreferrer">http://knowledge.wharton</a>. <a href="http://upenn.edu/article/mining-data-for-nuggets-of-knowledge" target="_blank" rel="noopener noreferrer">upenn.edu/article/mining-data-for-nuggets-of-knowledge</a>&gt;</li><li>Cleveland, William S. “Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics.” <em>International Statistical</em><em>Review</em> 69.1 (2001): 21-26. Print.</li><li>Davenport, #omas H., and D.J. Patil. “Data Scientist: #e Sexiest Job of the 21st Century.” <em>Harvard Business Review</em> 90.10 (October 2012): 70–76. Print.</li><li>Smith, David. “Statistics vs Data Science vs BI.” <em>Revolutions</em> , 15 May 2013. Web. Accessed 15 October 2013. SSRN:&lt;<a href="http://blog" target="_blank" rel="noopener noreferrer">http://blog</a>. <a href="http://revolutionanalytics.com/2013/05/statistics-vs-data-science-vs-bi.html" target="_blank" rel="noopener noreferrer">revolutionanalytics.com/2013/05/statistics-vs-data-science-vs-bi.html</a>&gt;</li><li>Brynjolfsson, Erik, Lorin M. Hitt, and Heekyung H. Kim. “Strength in Numbers: How Does Data-Driven Decision Making A$ect Firm Performance?” <em>Social Science Electronic Publishing</em> , 22 April 2011. Web. Accessed 15 October 2013. SSRN: &lt;<a href="http://ssrn.com/abstract=1819486" target="_blank" rel="noopener noreferrer">http://ssrn.com/abstract=1819486</a> or <a href="http://dx.doi.org/10.2139/ssrn.1819486%3E" target="_blank" rel="noopener noreferrer">http://dx.doi.org/10.2139/ssrn.1819486&gt;</a></li><li>“#e Stages of an Analytic Enterprise.” <em>Nucleus Research</em>. Febr uar y 2012. Whitepaper.</li><li>Barua, Anitesh, Deepa Mani, and Rajiv Mukherjee. “Measuring the Business Impacts of E$ective Data.” <em>University of Texas</em>. Web. Accessed 15 October 2013. SSRNL: &lt;<a href="http://www.sybase.com/!les/White_Papers/" target="_blank" rel="noopener noreferrer">http://www.sybase.com/!les/White_Papers/</a> E$ectiveDataStudyPt1-MeasuringtheBusinessImpactsofE$ectiveDa ta-WP.pdf&gt;</li></ol><h1 id="-18"><a class="header-anchor" href="#-18" aria-hidden="true">#</a> !!</h1><p><strong>THE FIELD GUIDE</strong> <em>to</em> <strong>DATA SCIENCE</strong></p><ol start="10"><li>Zikopoulos, Paul, Dirk deRoos, Kirshnan Parasuraman, #omas Deutsch, David Corrigan and James Giles. <em>Harness the Power of Big Data: !e IBM</em><em>Big Data Platform</em>. Ne w York: McGraw Hill, 2013. Pr int. 281pp.</li><li>Booz Allen Hamilton. <em>Cloud Analytics Playbook</em>. 2013. Web. Accessed 15 October 2013. SSRN: &lt;<a href="http://www.boozallen.com/media/!le/Cloud-" target="_blank" rel="noopener noreferrer">http://www.boozallen.com/media/!le/Cloud-</a> playbook-digital.pdf&gt;</li><li>Conway, Drew. “#e Data Science Venn Diagram.” March 2013. Web. Accessed 15 October 2013. SSRN: &lt;<a href="http://drewconway.com/" target="_blank" rel="noopener noreferrer">http://drewconway.com/</a> zia/2013/3/26/the-data-science-venn-diagram&gt;</li><li>Torán, Jacobo. “On the Hardness of Graph Isomorphism.” <em>SIAM Journal</em><em>on Computing</em>. 33.5 (2004): 1093-1108. Pr int.</li><li>Guyon, Isabelle and Andre Elissee$. “An Introduction to Variable and Feature Selection.” <em>Journal of Machine Learning Research</em> 3 (March 2003):1157-1182. Print.</li><li>Golub T., D. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J. Mesirov, H. Coller, M. Loh, J. Downing, M. Caligiuri, C. Bloom!eld, and E. Lander. “Molecular Classi!cation of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring.” <em>Science.</em> 286.5439 (1999): 531-537. Print.</li><li>Haykin, Simon O. <em>Neural Networks and Learning Machines</em>. Ne w Jersey : Prentice Hall, 2008. Print.</li><li>De Jong, Kenneth A. <em>Evolutionary Computation - A Uni#ed Approach</em>. Massachusetts: MIT Press, 2002. Print.</li><li>Yacci, Paul, Anne Haake, and Roger Gaborski. “Feature Selection of Microarray Data Using Genetic Algorithms and Arti!cial Neural Networks.” ANNIE 2009. St Louis, MO. 2-4 November 2009. Conference Presentation.</li></ol><div class="language-"><pre><code>Closing Time 105\n</code></pre></div><h2 id="about"><a class="header-anchor" href="#about" aria-hidden="true">#</a> About</h2><h2 id="booz-allen"><a class="header-anchor" href="#booz-allen" aria-hidden="true">#</a> BOOZ ALLEN</h2><h2 id="hamilton"><a class="header-anchor" href="#hamilton" aria-hidden="true">#</a> HAMILTON</h2><div class="language-"><pre><code>Booz Allen Hamilton has been at the forefront of strategy and\ntechnology consulting for nearly a century. Today, Booz Allen\nis a leading provider of management consulting, technology, and\nengineering services to the US government in defense, intelligence,\nand civil markets, and to major corporations, institutions, and not-\nfor-pro!t organizations. In the commercial sector, the !rm focuses\non leveraging its existing expertise for clients in the !nancial\nservices, healthcare, and energy markets, and to international clients\nin the Middle East. Booz Allen o&quot;ers clients deep functional\nknowledge spanning consulting, mission operations, technology, and\nengineering—which it combines with specialized expertise in clients’\nmission and domain areas to help solve their toughest problems.\n</code></pre></div><div class="language-"><pre><code>#e !rm’s management consulting heritage is the basis for its unique\ncollaborative culture and operating model, enabling Booz Allen\nto anticipate needs and opportunities, rapidly deploy talent and\nresources, and deliver enduring results. By combining a consultant’s\nproblem-solving orientation with deep technical knowledge and\nstrong execution, Booz Allen helps clients achieve success in their\nmost critical missions—as evidenced by the !rm’s many client\nrelationships that span decades. Booz Allen helps shape thinking\nand prepare for future developments in areas of national importance,\nincluding cybersecurity, homeland security, healthcare, and\ninformation technology.\n</code></pre></div><div class="language-"><pre><code>Booz Allen is headquartered in McLean, Virginia, employs more than\n23,000 people, and had revenue of $5.76 billion for the 12 months\nended March 31, 2013. For over a decade, Booz Allen’s high standing\nas a business and an employer has been recognized by dozens of\norganizations and publications, including Fortune, Working Mother,\nG.I. Jobs, and DiversityInc. More information is available at\nhttp://www.boozallen.com. (NYSE: BAH)\n</code></pre></div><h1 id="-19"><a class="header-anchor" href="#-19" aria-hidden="true">#</a> !!</h1><div class="language-"><pre><code>Closing Time 107\n</code></pre></div><p>© COPYRIGHT 2013 BOOZ ALLEN HAMILTON INC. ALL RIGHTS RESERVED.</p><div class="language-"><pre><code>Artwork by Rafael Esquer.\n</code></pre></div>',983);i.render=function(n,t,i,s,r,d){return e(),a("div",null,[o])};export{t as __pageData,i as default};
