import{o as e,c as a,a as i}from"./app.e2261b09.js";const r='{"title":"Statistical Learning","description":"","frontmatter":{"title":"Statistical Learning","date":"2020/12/12 15:22:40","cover_index":"http://picsum.photos/450/450?random=14","tags":["data"],"categories":["Books","BPM"]},"headers":[{"level":2,"title":"Contents","slug":"contents"},{"level":2,"title":"Preface","slug":"preface"},{"level":2,"title":"Introduction","slug":"introduction"},{"level":2,"title":"The Lasso for Linear Models","slug":"the-lasso-for-linear-models"},{"level":3,"title":"2.1 Introduction","slug":"_2-1-introduction"},{"level":3,"title":"2.2 The Lasso Estimator","slug":"_2-2-the-lasso-estimator"},{"level":3,"title":"2.3 Cross-Validation and Inference","slug":"_2-3-cross-validation-and-inference"},{"level":3,"title":"2.4 Computation of the Lasso Solution","slug":"_2-4-computation-of-the-lasso-solution"},{"level":3,"title":"2.5 Degrees of Freedom","slug":"_2-5-degrees-of-freedom"},{"level":3,"title":"2.6 Uniqueness of the Lasso Solutions","slug":"_2-6-uniqueness-of-the-lasso-solutions"},{"level":3,"title":"2.7 A Glimpse at the Theory","slug":"_2-7-a-glimpse-at-the-theory"},{"level":3,"title":"2.8 The Nonnegative Garrote","slug":"_2-8-the-nonnegative-garrote"},{"level":3,"title":"2.9 `q Penalties and Bayes Estimates","slug":"_2-9-q-penalties-and-bayes-estimates"},{"level":3,"title":"2.10 Some Perspective","slug":"_2-10-some-perspective"},{"level":3,"title":"Exercises","slug":"exercises"},{"level":2,"title":"Generalized Linear Models","slug":"generalized-linear-models"},{"level":3,"title":"3.1 Introduction","slug":"_3-1-introduction"},{"level":3,"title":"3.2 Logistic Regression","slug":"_3-2-logistic-regression"},{"level":3,"title":"3.3 Multiclass Logistic Regression","slug":"_3-3-multiclass-logistic-regression"},{"level":3,"title":"3.4 Log-Linear Models and the Poisson GLM","slug":"_3-4-log-linear-models-and-the-poisson-glm"},{"level":3,"title":"3.5 Cox Proportional Hazards Models","slug":"_3-5-cox-proportional-hazards-models"},{"level":3,"title":"3.6 Support Vector Machines","slug":"_3-6-support-vector-machines"},{"level":3,"title":"Bibliographic Notes","slug":"bibliographic-notes"},{"level":3,"title":"Exercises","slug":"exercises-1"},{"level":2,"title":"Generalizations of the Lasso Penalty","slug":"generalizations-of-the-lasso-penalty"},{"level":3,"title":"4.1 Introduction","slug":"_4-1-introduction"},{"level":3,"title":"4.2 The Elastic Net","slug":"_4-2-the-elastic-net"},{"level":3,"title":"4.3 The Group Lasso","slug":"_4-3-the-group-lasso"},{"level":3,"title":"4.4 Sparse Additive Models and the Group Lasso","slug":"_4-4-sparse-additive-models-and-the-group-lasso"},{"level":3,"title":"4.5 The Fused Lasso","slug":"_4-5-the-fused-lasso"},{"level":3,"title":"4.6 Nonconvex Penalties","slug":"_4-6-nonconvex-penalties"},{"level":3,"title":"Bibliographic Notes","slug":"bibliographic-notes-1"},{"level":3,"title":"Exercises","slug":"exercises-2"},{"level":2,"title":"Optimization Methods","slug":"optimization-methods"},{"level":3,"title":"5.1 Introduction","slug":"_5-1-introduction"},{"level":3,"title":"5.2 Convex Optimality Conditions","slug":"_5-2-convex-optimality-conditions"},{"level":3,"title":"5.3 Gradient Descent","slug":"_5-3-gradient-descent"},{"level":3,"title":"5.4 Coordinate Descent","slug":"_5-4-coordinate-descent"},{"level":3,"title":"5.5 A Simulation Study","slug":"_5-5-a-simulation-study"},{"level":3,"title":"5.6 Least Angle Regression","slug":"_5-6-least-angle-regression"},{"level":3,"title":"5.7 Alternating Direction Method of Multipliers","slug":"_5-7-alternating-direction-method-of-multipliers"},{"level":3,"title":"5.9 Biconvexity and Alternating Minimization","slug":"_5-9-biconvexity-and-alternating-minimization"},{"level":3,"title":"5.10 Screening Rules","slug":"_5-10-screening-rules"},{"level":3,"title":"Bibliographic Notes","slug":"bibliographic-notes-2"},{"level":3,"title":"Exercises","slug":"exercises-3"},{"level":2,"title":"Statistical Inference","slug":"statistical-inference"},{"level":3,"title":"6.1 The Bayesian Lasso","slug":"_6-1-the-bayesian-lasso"},{"level":3,"title":"6.2 The Bootstrap","slug":"_6-2-the-bootstrap"},{"level":3,"title":"6.3 Post-Selection Inference for the Lasso","slug":"_6-3-post-selection-inference-for-the-lasso"},{"level":3,"title":"6.4 Inference via a Debiased Lasso","slug":"_6-4-inference-via-a-debiased-lasso"},{"level":3,"title":"6.5 Other Proposals for Post-Selection Inference","slug":"_6-5-other-proposals-for-post-selection-inference"},{"level":3,"title":"Bibliographic Notes","slug":"bibliographic-notes-3"},{"level":3,"title":"Exercises","slug":"exercises-4"},{"level":2,"title":"Matrix Decompositions,","slug":"matrix-decompositions"},{"level":2,"title":"Approximations, and Completion","slug":"approximations-and-completion"},{"level":3,"title":"7.1 Introduction","slug":"_7-1-introduction"},{"level":3,"title":"7.2 The Singular Value Decomposition","slug":"_7-2-the-singular-value-decomposition"},{"level":3,"title":"7.3 Missing Data and Matrix Completion","slug":"_7-3-missing-data-and-matrix-completion"},{"level":3,"title":"7.4 Reduced-Rank Regression","slug":"_7-4-reduced-rank-regression"},{"level":3,"title":"7.5 A General Matrix Regression Framework","slug":"_7-5-a-general-matrix-regression-framework"},{"level":3,"title":"7.6 Penalized Matrix Decomposition","slug":"_7-6-penalized-matrix-decomposition"},{"level":3,"title":"7.7 Additive Matrix Decomposition","slug":"_7-7-additive-matrix-decomposition"},{"level":3,"title":"Bibliographic Notes","slug":"bibliographic-notes-4"},{"level":3,"title":"Exercises","slug":"exercises-5"},{"level":2,"title":"Sparse Multivariate Methods","slug":"sparse-multivariate-methods"},{"level":3,"title":"8.1 Introduction","slug":"_8-1-introduction"},{"level":3,"title":"8.2 Sparse Principal Components Analysis","slug":"_8-2-sparse-principal-components-analysis"},{"level":3,"title":"8.3 Sparse Canonical Correlation Analysis","slug":"_8-3-sparse-canonical-correlation-analysis"},{"level":3,"title":"8.4 Sparse Linear Discriminant Analysis","slug":"_8-4-sparse-linear-discriminant-analysis"},{"level":3,"title":"8.5 Sparse Clustering","slug":"_8-5-sparse-clustering"},{"level":3,"title":"Bibliographic Notes","slug":"bibliographic-notes-5"},{"level":3,"title":"Exercises","slug":"exercises-6"},{"level":2,"title":"Graphs and Model Selection","slug":"graphs-and-model-selection"},{"level":3,"title":"9.1 Introduction","slug":"_9-1-introduction"},{"level":3,"title":"9.2 Basics of Graphical Models","slug":"_9-2-basics-of-graphical-models"},{"level":3,"title":"9.3 Graph Selection via Penalized Likelihood","slug":"_9-3-graph-selection-via-penalized-likelihood"},{"level":3,"title":"9.4 Graph Selection via Conditional Inference","slug":"_9-4-graph-selection-via-conditional-inference"},{"level":3,"title":"9.5 Graphical Models with Hidden Variables","slug":"_9-5-graphical-models-with-hidden-variables"},{"level":3,"title":"Bibliographic Notes","slug":"bibliographic-notes-6"},{"level":2,"title":"Signal Approximation and Compressed","slug":"signal-approximation-and-compressed"},{"level":2,"title":"Sensing","slug":"sensing"},{"level":3,"title":"10.1 Introduction","slug":"_10-1-introduction"},{"level":3,"title":"10.2 Signals and Sparse Representations","slug":"_10-2-signals-and-sparse-representations"},{"level":3,"title":"10.3 Random Projection and Approximation","slug":"_10-3-random-projection-and-approximation"},{"level":3,"title":"10.4 Equivalence between  0 and  1 Recovery","slug":"_10-4-equivalence-between-0-and-1-recovery"},{"level":3,"title":"Bibliographic Notes","slug":"bibliographic-notes-7"},{"level":3,"title":"Exercises","slug":"exercises-7"},{"level":2,"title":"Theoretical Results for the Lasso","slug":"theoretical-results-for-the-lasso"},{"level":3,"title":"11.1 Introduction","slug":"_11-1-introduction"},{"level":3,"title":"11.2 Bounds on Lasso ` 2 -Error","slug":"_11-2-bounds-on-lasso-2-error"},{"level":3,"title":"11.3 Bounds on Prediction Error","slug":"_11-3-bounds-on-prediction-error"},{"level":3,"title":"11.4 Support Recovery in Linear Regression","slug":"_11-4-support-recovery-in-linear-regression"},{"level":3,"title":"11.5 Beyond the Basic Lasso","slug":"_11-5-beyond-the-basic-lasso"},{"level":3,"title":"Bibliographic Notes","slug":"bibliographic-notes-8"},{"level":3,"title":"Exercises","slug":"exercises-8"},{"level":2,"title":"Bibliography","slug":"bibliography"},{"level":2,"title":"Author Index","slug":"author-index"},{"level":2,"title":"Index","slug":"index"}],"relativePath":"blog/StatLearnSparsity.md","lastUpdated":1628427393571}',n={},o=i('<p>copy to come from copywriter for review Statistics</p><p><a href="http://www.crcpress.com" target="_blank" rel="noopener noreferrer">http://www.crcpress.com</a></p><h2 id="contents"><a class="header-anchor" href="#contents" aria-hidden="true">#</a> Contents</h2><ul><li>1 Introduction Preface xv</li><li>2 The Lasso for Linear Models <ul><li>2.1 Introduction</li><li>2.2 The Lasso Estimator</li><li>2.3 Cross-Validation and Inference</li><li>2.4 Computation of the Lasso Solution <ul><li>2.4.1 Single Predictor: Soft Thresholding</li><li>2.4.2 Multiple Predictors: Cyclic Coordinate Descent</li><li>2.4.3 Soft-Thresholding and Orthogonal Bases</li></ul></li><li>2.5 Degrees of Freedom</li><li>2.6 Uniqueness of the Lasso Solutions</li><li>2.7 A Glimpse at the Theory</li><li>2.8 The Nonnegative Garrote</li><li>2.9 `q Penalties and Bayes Estimates</li><li>2.10 Some Perspective</li><li>Exercises</li></ul></li><li>3 Generalized Linear Models <ul><li>3.1 Introduction</li><li>3.2 Logistic Regression <ul><li>3.2.1 Example: Document Classification</li><li>3.2.2 Algorithms</li></ul></li><li>3.3 Multiclass Logistic Regression <ul><li>3.3.1 Example: Handwritten Digits</li><li>3.3.2 Algorithms</li><li>3.3.3 Grouped-Lasso Multinomial</li></ul></li><li>3.4 Log-Linear Models and the Poisson GLM <ul><li>3.4.1 Example: Distribution Smoothing</li></ul></li><li>3.5 Cox Proportional Hazards Models <ul><li>3.5.1 Cross-Validation</li><li>3.5.2 Pre-Validation</li></ul></li><li>3.6 Support Vector Machines <ul><li>3.6.1 Logistic Regression with Separable Data</li></ul></li><li>3.7 Computational Details andglmnet x</li><li>Bibliographic Notes</li><li>Exercises</li></ul></li><li>4 Generalizations of the Lasso Penalty <ul><li>4.1 Introduction</li><li>4.2 The Elastic Net</li><li>4.3 The Group Lasso <ul><li>4.3.1 Computation for the Group Lasso</li><li>4.3.2 Sparse Group Lasso</li><li>4.3.3 The Overlap Group Lasso</li></ul></li><li>4.4 Sparse Additive Models and the Group Lasso <ul><li>4.4.1 Additive Models and Backfitting</li><li>4.4.2 Sparse Additive Models and Backfitting</li><li>4.4.3 Approaches Using Optimization and the Group Lasso</li><li>4.4.4 Multiple Penalization for Sparse Additive Models</li></ul></li><li>4.5 The Fused Lasso <ul><li>4.5.1 Fitting the Fused Lasso <ul><li>4.5.1.1 Reparametrization</li><li>4.5.1.2 A Path Algorithm</li><li>4.5.1.3 A Dual Path Algorithm</li><li>4.5.1.4 Dynamic Programming for the Fused Lasso</li></ul></li><li>4.5.2 Trend Filtering</li><li>4.5.3 Nearly Isotonic Regression</li></ul></li><li>4.6 Nonconvex Penalties</li><li>Bibliographic Notes</li><li>Exercises</li></ul></li><li>5 Optimization Methods <ul><li>5.1 Introduction</li><li>5.2 Convex Optimality Conditions <ul><li>5.2.1 Optimality for Differentiable Problems</li><li>5.2.2 Nondifferentiable Functions and Subgradients</li></ul></li><li>5.3 Gradient Descent <ul><li>5.3.1 Unconstrained Gradient Descent</li><li>5.3.2 Projected Gradient Methods</li><li>5.3.3 Proximal Gradient Methods</li><li>5.3.4 Accelerated Gradient Methods</li></ul></li><li>5.4 Coordinate Descent <ul><li>5.4.1 Separability and Coordinate Descent</li><li>5.4.2 Linear Regression and the Lasso</li><li>5.4.3 Logistic Regression and Generalized Linear Models</li></ul></li><li>5.5 A Simulation Study</li><li>5.6 Least Angle Regression</li><li>5.7 Alternating Direction Method of Multipliers</li><li>5.8 Minorization-Maximization Algorithms xi</li><li>5.9 Biconvexity and Alternating Minimization</li><li>5.10 Screening Rules</li><li>Bibliographic Notes</li><li>Appendix</li><li>Exercises</li></ul></li><li>6 Statistical Inference <ul><li>6.1 The Bayesian Lasso</li><li>6.2 The Bootstrap</li><li>6.3 Post-Selection Inference for the Lasso <ul><li>6.3.1 The Covariance Test</li><li>6.3.2 A General Scheme for Post-Selection Inference <ul><li>6.3.2.1 Fixed- λ Inference for the Lasso</li><li>6.3.2.2 The Spacing Test for LAR</li></ul></li><li>6.3.3 What Hypothesis Is Being Tested?</li><li>6.3.4 Back to Forward Stepwise Regression</li></ul></li><li>6.4 Inference via a Debiased Lasso</li><li>6.5 Other Proposals for Post-Selection Inference</li><li>Bibliographic Notes</li><li>Exercises</li></ul></li><li>7 Matrix Decompositions, Approximations, and Completion <ul><li>7.1 Introduction</li><li>7.2 The Singular Value Decomposition</li><li>7.3 Missing Data and Matrix Completion <ul><li>7.3.1 The Netflix Movie Challenge</li><li>7.3.2 Matrix Completion Using Nuclear Norm</li><li>7.3.3 Theoretical Results for Matrix Completion</li><li>7.3.4 Maximum Margin Factorization and Related Methods</li></ul></li><li>7.4 Reduced-Rank Regression</li><li>7.5 A General Matrix Regression Framework</li><li>7.6 Penalized Matrix Decomposition</li><li>7.7 Additive Matrix Decomposition</li><li>Bibliographic Notes</li><li>Exercises</li></ul></li><li>8 Sparse Multivariate Methods <ul><li>8.1 Introduction</li><li>8.2 Sparse Principal Components Analysis <ul><li>8.2.1 Some Background</li><li>8.2.2 Sparse Principal Components <ul><li>8.2.2.1 Sparsity from Maximum Variance</li><li>8.2.2.2 Methods Based on Reconstruction</li></ul></li><li>8.2.3 Higher-Rank Solutions <ul><li>8.2.3.1 Illustrative Application of Sparse PCA xii</li></ul></li><li>8.2.4 Sparse PCA via Fantope Projection</li><li>8.2.5 Sparse Autoencoders and Deep Learning</li><li>8.2.6 Some Theory for Sparse PCA</li></ul></li><li>8.3 Sparse Canonical Correlation Analysis <ul><li>8.3.1 Example: Netflix Movie Rating Data</li></ul></li><li>8.4 Sparse Linear Discriminant Analysis <ul><li>8.4.1 Normal Theory and Bayes’ Rule</li><li>8.4.2 Nearest Shrunken Centroids</li><li>8.4.3 Fisher’s Linear Discriminant Analysis <ul><li>8.4.3.1 Example: Simulated Data with Five Classes</li></ul></li><li>8.4.4 Optimal Scoring <ul><li>8.4.4.1 Example: Face Silhouettes</li></ul></li></ul></li><li>8.5 Sparse Clustering <ul><li>8.5.1 Some Background on Clustering <ul><li>8.5.1.1 Example: Simulated Data with Six Classes</li></ul></li><li>8.5.2 Sparse Hierarchical Clustering</li><li>8.5.3 Sparse K -Means Clustering</li><li>8.5.4 Convex Clustering</li></ul></li><li>Bibliographic Notes</li><li>Exercises</li></ul></li><li>9 Graphs and Model Selection <ul><li>9.1 Introduction</li><li>9.2 Basics of Graphical Models <ul><li>9.2.1 Factorization and Markov Properties <ul><li>9.2.1.1 Factorization Property</li><li>9.2.1.2 Markov Property <ul><li>Properties 9.2.1.3 Equivalence of Factorization and Markov</li></ul></li></ul></li><li>9.2.2 Some Examples <ul><li>9.2.2.1 Discrete Graphical Models</li><li>9.2.2.2 Gaussian Graphical Models</li></ul></li></ul></li><li>9.3 Graph Selection via Penalized Likelihood <ul><li>9.3.1 Global Likelihoods for Gaussian Models</li><li>9.3.2 Graphical Lasso Algorithm</li><li>9.3.3 Exploiting Block-Diagonal Structure</li><li>9.3.4 Theoretical Guarantees for the Graphical Lasso</li><li>9.3.5 Global Likelihood for Discrete Models</li></ul></li><li>9.4 Graph Selection via Conditional Inference <ul><li>9.4.1 Neighborhood-Based Likelihood for Gaussians</li><li>9.4.2 Neighborhood-Based Likelihood for Discrete Models</li><li>9.4.3 Pseudo-Likelihood for Mixed Models</li></ul></li><li>9.5 Graphical Models with Hidden Variables</li><li>Bibliographic Notes</li><li>Exercises xiii</li></ul></li><li>10 Signal Approximation and Compressed Sensing <ul><li>10.1 Introduction</li><li>10.2 Signals and Sparse Representations <ul><li>10.2.1 Orthogonal Bases</li><li>10.2.2 Approximation in Orthogonal Bases</li><li>10.2.3 Reconstruction in Overcomplete Bases</li></ul></li><li>10.3 Random Projection and Approximation <ul><li>10.3.1 Johnson–Lindenstrauss Approximation</li><li>10.3.2 Compressed Sensing</li></ul></li><li>10.4 Equivalence between <code>0 and</code> 1 Recovery <ul><li>10.4.1 Restricted Nullspace Property</li><li>10.4.2 Sufficient Conditions for Restricted Nullspace</li><li>10.4.3 Proofs <ul><li>10.4.3.1 Proof of Theorem 10.1</li><li>10.4.3.2 Proof of Proposition 10.1</li></ul></li></ul></li><li>Bibliographic Notes</li><li>Exercises</li></ul></li><li>11 Theoretical Results for the Lasso <ul><li>11.1 Introduction <ul><li>11.1.1 Types of Loss Functions</li><li>11.1.2 Types of Sparsity Models</li></ul></li><li>11.2 Bounds on Lasso ` 2 -Error <ul><li>11.2.1 Strong Convexity in the Classical Setting</li><li>11.2.2 Restricted Eigenvalues for Regression</li><li>11.2.3 A Basic Consistency Result</li></ul></li><li>11.3 Bounds on Prediction Error</li><li>11.4 Support Recovery in Linear Regression <ul><li>11.4.1 Variable-Selection Consistency for the Lasso <ul><li>11.4.1.1 Some Numerical Studies</li></ul></li><li>11.4.2 Proof of Theorem 11.3</li></ul></li><li>11.5 Beyond the Basic Lasso</li><li>Bibliographic Notes</li><li>Exercises</li></ul></li><li>Bibliography</li><li>Author Index</li><li>Index</li></ul><h2 id="preface"><a class="header-anchor" href="#preface" aria-hidden="true">#</a> Preface</h2><p>In this monograph, we have attempted to summarize the actively developing field of statistical learning with sparsity. A sparse statistical model is one having only a small number of nonzero parameters or weights. It represents a classic case of “ <em>less is more</em> ”: a sparse model can be much easier to estimate and interpret than a dense model. In this age of big data, the number of features measured on a person or object can be large, and might be larger than the number of observations. The sparsity assumption allows us to tackle such problems and extract useful and reproducible patterns from big datasets. The ideas described here represent the work of an entire community of researchers in statistics and machine learning, and we thank everyone for their continuing contributions to this exciting area. We particularly thank our colleagues at Stanford, Berkeley and elsewhere; our collaborators, and our past and current students working in this area. These include Alekh Agarwal, Arash Amini, Francis Bach, Jacob Bien, Stephen Boyd, Andreas Buja, Em- manuel Candes, Alexandra Chouldechova, David Donoho, John Duchi, Brad Efron, Will Fithian, Jerome Friedman, Max G’Sell, Iain Johnstone, Michael Jordan, Ping Li, Po-Ling Loh, Michael Lim, Jason Lee, Richard Lockhart, Rahul Mazumder, Balasubramanian Narashimhan, Sahand Negahban, Guil- laume Obozinski, Mee-Young Park, Junyang Qian, Garvesh Raskutti, Pradeep Ravikumar, Saharon Rosset, Prasad Santhanam, Noah Simon, Dennis Sun, Yukai Sun, Jonathan Taylor, Ryan Tibshirani,^1 Stefan Wager, Daniela Wit- ten, Bin Yu, Yuchen Zhang, Ji Zhou, and Hui Zou. We also thank our editor John Kimmel for his advice and support.</p><p>Stanford University Trevor Hastie and Robert Tibshirani University of California, Berkeley Martin Wainwright</p><p>(^1) Some of the bibliographic references, for example in Chapters 4 and 6, are to Tibshirani 2 , R.J., rather than Tibshirani, R.; the former is Ryan Tibshirani, the latter is Robert (son and father). xv</p><div class="language-"><pre><code>Chapter 1\n</code></pre></div><h2 id="introduction"><a class="header-anchor" href="#introduction" aria-hidden="true">#</a> Introduction</h2><div class="language-"><pre><code>“I never keep a scorecard or the batting averages. I hate statistics. What\nI got to know, I keep in my head.”\n</code></pre></div><p>This is a quote from baseball pitcher Dizzy Dean, who played in the major leagues from 1930 to 1947. How the world has changed in the 75 or so years since that time! Now large quantities of data are collected and mined in nearly every area of science, en- tertainment, business, and industry. Medical scientists study the genomes of patients to choose the best treatments, to learn the underlying causes of their disease. Online movie and book stores study customer ratings to recommend or sell them new movies or books. Social networks mine information about members and their friends to try to enhance their online experience. And yes, most major league baseball teams have statisticians who collect and ana- lyze detailed information on batters and pitchers to help team managers and players make better decisions. Thus the world is awash with data. But as Rutherford D. Roger (and others) has said:</p><div class="language-"><pre><code>“We are drowning in information and starving for knowledge.”\n</code></pre></div><p>There is a crucial need to sort through this mass of information, and pare it down to its bare essentials. For this process to be successful, we need to hope that the world is not as complex as it might be. For example, we hope that not all of the 30 <em>,</em> 000 or so genes in the human body are directly involved in the process that leads to the development of cancer. Or that the ratings by a customer on perhaps 50 or 100 different movies are enough to give us a good idea of their tastes. Or that the success of a left-handed pitcher against left-handed batters will be fairly consistent for different batters. This points to an underlying assumption of simplicity. One form of sim- plicity is <em>sparsity</em> , the central theme of this book. Loosely speaking, a sparse statistical model is one in which only a relatively small number of parameters (or predictors) play an important role. In this book we study methods that exploit sparsity to help recover the underlying signal in a set of data. The leading example is linear regression, in which we observe <em>N</em> obser- vations of an outcome variable <em>yi</em> and <em>p</em> associated predictor variables (or features) <em>xi</em> = ( <em>xi</em> 1 <em>,...xip</em> ) <em>T</em>. The goal is to predict the outcome from the</p><div class="language-"><pre><code>1\n</code></pre></div><h6 id="_2-introduction"><a class="header-anchor" href="#_2-introduction" aria-hidden="true">#</a> 2 INTRODUCTION</h6><p>predictors, both for actual prediction with future data and also to discover which predictors play an important role. A linear regression model assumes that</p><div class="language-"><pre><code>yi = β 0 +\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =\n</code></pre></div><div class="language-"><pre><code>xijβj + ei, (1.1)\n</code></pre></div><p>where <em>β</em> 0 and <em>β</em> = ( <em>β</em> 1 <em>,β</em> 2 <em>,...βp</em> ) are unknown parameters and <em>ei</em> is an error term. The method of least squares provides estimates of the parameters by minimization of the least-squares objective function</p><div class="language-"><pre><code>minimize\nβ 0 ,β\n</code></pre></div><h6 id="∑-n"><a class="header-anchor" href="#∑-n" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =\n</code></pre></div><div class="language-"><pre><code>( yi − β 0 −\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =\n</code></pre></div><div class="language-"><pre><code>xijβj )^2. (1.2)\n</code></pre></div><p>Typically all of the least-squares estimates from (1.2) will be nonzero. This will make interpretation of the final model challenging if <em>p</em> is large. In fact, if <em>p &gt; N</em> , the least-squares estimates are not unique. There is an infinite set of solutions that make the objective function equal to zero, and these solutions almost surely overfit the data as well. Thus there is a need to constrain, or <em>regularize</em> the estimation process. In the <em>lasso</em> or <em>`</em> 1 <em>-regularized regression</em> , we estimate the parameters by solving the problem</p><div class="language-"><pre><code>minimize\nβ 0 ,β\n</code></pre></div><h6 id="∑-n-1"><a class="header-anchor" href="#∑-n-1" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =\n</code></pre></div><div class="language-"><pre><code>( yi − β 0 −\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =\n</code></pre></div><div class="language-"><pre><code>xijβj )^2 subject to‖ β ‖ 1 ≤ t (1.3)\n</code></pre></div><p>where‖ <em>β</em> ‖ 1 =</p><p>∑ <em>p j</em> =1| <em>βj</em> |is the <em><code>_^1 norm of _β_ , and _t_ is a user-specified parameter. We can think of _t_ as a budget on the total _</code></em> 1 norm of the parameter vector, and the lasso finds the best fit within this budget. Why do we use the <em><code>_ 1 norm? Why not use the _</code></em> 2 norm or any <em><code>q_ norm? It turns out that the _</code></em> 1 norm is special. If the budget <em>t</em> is small enough, the lasso yields sparse solution vectors, having only some coordinates that are nonzero. This does not occur for <em>`q</em> norms with <em>q &gt;</em> 1; for <em>q &lt;</em> 1, the solutions are sparse but the problem is not convex and this makes the minimization very challenging computationally. The value <em>q</em> = 1 is the smallest value that yields a convex problem. Convexity greatly simplifies the computation, as does the sparsity assumption itself. They allow for scalable algorithms that can handle problems with even millions of parameters. Thus the advantages of sparsity are interpretation of the fitted model and computational convenience. But a third advantage has emerged in the last few years from some deep mathematical analyses of this area. This has been termed the “bet on sparsity” principle:</p><div class="language-"><pre><code>Use a procedure that does wel l in sparse problems, since no procedure\ndoes well in dense problems.\n</code></pre></div><h6 id="statistical-learning-with-sparsity-3"><a class="header-anchor" href="#statistical-learning-with-sparsity-3" aria-hidden="true">#</a> STATISTICAL LEARNING WITH SPARSITY 3</h6><p>We can think of this in terms of the amount of information <em>N/p</em> per param- eter. If <em>p</em>  <em>N</em> and the true model is not sparse, then the number of samples <em>N</em> is too small to allow for accurate estimation of the parameters. But if the true model is sparse, so that only <em>k &lt; N</em> parameters are actually nonzero in the true underlying model, then it turns out that we can estimate the parameters effectively, using the lasso and related methods that we discuss in this book. This may come as somewhat of a surprise, because we are able to do this even though we are not told <em>whichk</em> of the <em>p</em> parameters are actually nonzero. Of course we cannot do as well as we could if we had that information, but it turns out that we can still do reasonably well.</p><div class="language-"><pre><code>BladderBreast\nCNSColonKidneyLiverLungLymphNormalOvary\nPancreasProstate\n</code></pre></div><div class="language-"><pre><code>Soft\nStomach\nTestis\n</code></pre></div><p><strong>Figure 1.1</strong> <em>15-class gene expression cancer data: estimated nonzero feature weights from a lasso-regularized multinomial classifier. Shown are the 254 genes (out of 4718) with at least one nonzero weight among the 15 classes. The genes (unlabel led) run from top to bottom. Line segments pointing to the right indicate positive weights, and to the left, negative weights. We see that only a handful of genes are needed to characterize each class.</em></p><p>For all of these reasons, the area of sparse statistical modelling is exciting— for data analysts, computer scientists, and theorists—and practically useful. Figure 1.1 shows an example. The data consists of quantitative gene expression measurements of 4718 genes on samples from 349 cancer patients. The cancers have been categorized into 15 different types such as “Bladder,” “Breast”,</p><h6 id="_4-introduction"><a class="header-anchor" href="#_4-introduction" aria-hidden="true">#</a> 4 INTRODUCTION</h6><p>“CNS,” etc. The goal is to build a classifier to predict cancer class based on some or all of the 4718 features. We want the classifier to have a low error rate on independent samples and would prefer that it depend only on a subset of the genes, to aid in our understanding of the underlying biology. For this purpose we applied a lasso-regularized multinomial classifier to these data, as described in Chapter 3. This produces a set of 4718 weights or coefficients for each of the 15 classes, for discriminating each class from the rest. Because of the _<code>_ 1 penalty, only some of these weights may be nonzero (depending on the choice of the regularization parameter). We used cross- validation to estimate the optimal choice of regularization parameter, and display the resulting weights in Figure 1.1. Only 254 genes have at least one nonzero weight, and these are displayed in the figure. The cross-validated error rate for this classifier is about 10%, so the procedure correctly predicts the class of about 90% of the samples. By comparison, a standard support vector classifier had a slightly higher error rate (13%) using all of the features. Using sparsity, the lasso procedure has dramatically reduced the number of features without sacrificing accuracy. Sparsity has also brought computational efficiency: although there are potentially 4718× 15 ≈ 70 _,_ 000 parameters to estimate, the entire calculation for Figure 1.1 was done on a standard laptop computer in less than a minute. For this computation we used theglmnet procedure described in Chapters 3 and 5. Figure 1.2 shows another example taken from an article by Cand</code>es and Wakin (2008) in the field of <em>compressed sensing</em>. On the left is a megapixel image. In order to reduce the amount of space needed to store the image, we represent it in a wavelet basis, whose coefficients are shown in the middle panel. The largest 25 <em>,</em> 000 coefficients are then retained and the rest zeroed out, yielding the excellent reconstruction in the right image. This all works because of sparsity: although the image seems complex, in the wavelet basis it is simple and hence only a relatively small number of coefficients are nonzero. The original image can be perfectly recovered from just 96 <em>,</em> 000 incoherent measurements. Compressed sensing is a powerful tool for image analysis, and is described in Chapter 10. In this book we have tried to summarize the hot and rapidly evolving field of sparse statistical modelling. In Chapter 2 we describe and illustrate the lasso for linear regression, and a simple coordinate descent algorithm for its computation. Chapter 3 covers the application of <em>`</em> 1 penalties to generalized linear models such as multinomial and survival models, as well as support vector machines. Generalized penalties such as the elastic net and group lasso are discussed in Chapter 4. Chapter 5 reviews numerical methods for opti- mization, with an emphasis on first-order methods that are useful for the large-scale problems that are discussed in this book. In Chapter 6, we dis- cuss methods for statistical inference for fitted (lasso) models, including the bootstrap, Bayesian methods and some more recently developed approaches. Sparse matrix decomposition is the topic of Chapter 7, and we apply these methods in the context of sparse multivariate analysis in Chapter 8. Graph-</p><h6 id="statistical-learning-with-sparsity-5"><a class="header-anchor" href="#statistical-learning-with-sparsity-5" aria-hidden="true">#</a> STATISTICAL LEARNING WITH SPARSITY 5</h6><p>theory tells us that, if <em>f</em> ( <em>t</em> )actually has very low band- width, then a small number of (uniform) samples will suf- fice for recovery. As we will see in the remainder of this article, signal recovery can actually be made possible for a much broader class of signal models.</p><p><strong>INCOHERENCE AND THE SENSING OF SPARSE SIGNALS</strong> This section presents the two fundamental premises underlying CS: sparsity and incoherence.</p><p><strong><em>SPARSITY</em></strong> Many natural signals have concise representations when expressed in a convenient basis. Consider, for example, the image in Figure 1(a) and its wavelet transform in (b). Although nearly all the image pixels have nonzero values, the wavelet coefficients offer a concise summary: most coeffi- cients are small, and the relatively few large coefficients cap- ture most of the information. Mathematically speaking, we have a vector <em>f</em> ∈R <em>n</em> (such as the <em>n</em> -pixel image in Figure 1) which we expand in an orthonor- mal basis (such as a wavelet basis) =[ψ 1 ψ 2 ···ψ <em>n</em> ]as follows:</p><div class="language-"><pre><code>f ( t )=\n</code></pre></div><div class="language-"><pre><code> n\n</code></pre></div><div class="language-"><pre><code>i = 1\n</code></pre></div><div class="language-"><pre><code>xi ψ i ( t ), ( 2 )\n</code></pre></div><p>where <em>x</em> is the coefficient sequence of <em>f</em> , <em>xi</em> = <em>f</em> ,ψ <em>i</em> . It will be convenient to express <em>f</em> as  <em>x</em> (where is the <em>n</em> × <em>n</em> matrix with ψ 1 ,... ,ψ <em>n</em> as columns). The implication of sparsity is now clear: when a signal has a sparse expansion, one can dis- card the small coefficients without much perceptual loss. Formally, consider <em>fS</em> ( <em>t</em> )obtained by keeping only the terms corresponding to the <em>S</em> largest values of ( <em>xi</em> )in the expansion (2). By definition, <em>fS</em> := <em>xS</em> , where here and below, <em>xS</em> is the vector of coefficients ( <em>xi</em> )with all but the largest <em>S</em> set to zero. This vector is sparse in a strict sense since all but a few of its entries are zero; we will call <em>S</em> - <em>sparse</em> such objects with at most <em>S</em> nonzero entries. Since is an orthonormal basis (or “orthobasis”), we have  <em>f</em> − <em>fS</em>  2 = <em>x</em> − <em>xS</em>  2 , and if <em>x</em> is sparse or <em>compressible</em> in the sense that the sorted magnitudes of the ( <em>xi</em> ) decay quickly, then <em>x</em> is well approxi- mated by <em>xS</em> and, therefore, the error  <em>f</em> − <em>fS</em>  2 is small. In plain terms, one can “throw away” a large fraction of the coefficients without much loss. Figure 1(c) shows an example where the perceptual loss is hardly noticeable from a megapixel image to its approxi- mation obtained by throwing away 97.5% of the coefficients. This principle is, of course, what underlies most modern lossy coders such as JPEG-2000 [4] and many</p><div class="language-"><pre><code>others, since a simple method for data compression would be to\ncompute x from f and then (adaptively) encode the locations\nand values of the S significant coefficients. Such a process\nrequires knowledge of all the n coefficients x , as the locations\nof the significant pieces of information may not be known in\nadvance (they are signal dependent); in our example, they tend\nto be clustered around edges in the image. More generally,\nsparsity is a fundamental modeling tool which permits efficient\nfundamental signal processing; e.g., accurate statistical estima-\ntion and classification, efficient data compression, and so on.\nThis article is about a more surprising and far-reaching impli-\ncation, however, which is that sparsity has significant bearings\non the acquisition process itself. Sparsity determines how effi-\nciently one can acquire signals nonadaptively.\n</code></pre></div><div class="language-"><pre><code>INCOHERENT SAMPLING\nSuppose we are given a pair (, )of orthobases of R n. The first\nbasis is used for sensing the object f as in (1) and the second is\nused to represent f. The restriction to pairs of orthobases is not\nessential and will merely simplify our treatment.\n</code></pre></div><div class="language-"><pre><code>DEFINITION 1\nThe coherence between the sensing basis and the representa-\ntion basis is\n</code></pre></div><div class="language-"><pre><code>μ(, )=\n</code></pre></div><div class="language-"><pre><code>√\nn · max\n1 ≤ k , j ≤ n\n</code></pre></div><div class="language-"><pre><code>|φ k ,ψ j |.( 3 )\n</code></pre></div><div class="language-"><pre><code>In plain English, the coherence measures the largest correlation\nbetween any two elements of and ; see also [5]. If and \ncontain correlated elements, the coherence is large. Otherwise,\nit is small. As for how large and how small, it follows from linear\nalgebra that μ(, )∈[1,\n</code></pre></div><div class="language-"><pre><code>√\nn ].\nCompressive sampling is mainly concerned with low coher-\nence pairs, and we now give examples of such pairs. In our first\nexample, is the canonical or spike basis φ k ( t )=δ( t − k )and\n</code></pre></div><div class="language-"><pre><code>[FIG1](a) Original megapixel image with pixel values in the range [0,255] and (b) its\nwavelet transform coefficients (arranged in random order for enhanced visibility).\nRelatively few wavelet coefficients capture most of the signal energy; many such images\nare highly compressible. (c) The reconstruction obtained by zeroing out all the coefficients\nin the wavelet expansion but the 25,000 largest (pixel values are thresholded to the range\n[0,255]). The difference with the original picture is hardly noticeable. As we describe in\n“Undersampling and Sparse Signal Recovery,” this image can be perfectly recovered from\njust 96,000 incoherent measurements.\n</code></pre></div><div class="language-"><pre><code>(a) (b)\n</code></pre></div><div class="language-"><pre><code>− 1\n0246810\n</code></pre></div><div class="language-"><pre><code>−0.\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>0.\n</code></pre></div><div class="language-"><pre><code>1.\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>Wavelet\n</code></pre></div><p>× (^104) Coefficients 1 (c) × 105 IEEE SIGNAL PROCESSING MAGAZINE[ 23 ]MARCH 2008 <strong>Figure 1.2</strong> <em>(a) Original megapixel image with pixel values in the range</em> [0 <em>,</em> 255] <em>and (b) its wavelet transform coefficients (arranged in random order for enhanced visibility). Relatively few wavelet coefficients capture most of the signal energy; many such images are highly compressible. (c) The reconstruction obtained by zeroing out al l the coefficients in the wavelet expansion but the</em> 25 <em>,</em> 000 <em>largest (pixel values are thresholded to the range</em> [0 <em>,</em> 255] <em>). The differences from the original picture are hardly noticeable.</em> ical models and their selection are discussed in Chapter 9 while compressed sensing is the topic of Chapter 10. Finally, a survey of theoretical results for the lasso is given in Chapter 11. We note that both <em>supervised</em> and <em>unsupervised</em> learning problems are dis- cussed in this book, the former in Chapters 2, 3, 4, and 10, and the latter in Chapters 7 and 8. <em>Notation</em> We have adopted a notation to reduce mathematical clutter. Vectors are col- umn vectors by default; hence <em>β</em> ∈R <em>p</em> is a column vector, and its transpose <em>βT</em> is a row vector. All vectors are lower case and non-bold, except <em>N</em> -vectors which are bold, where <em>N</em> is the sample size. For example <strong>x</strong> <em>j</em> might be the <em>N</em> -vector of observed values for the <em>jth</em> variable, and <strong>y</strong> the response <em>N</em> -vector. All matrices are bold; hence <strong>X</strong> might represent the <em>N</em> × <em>p</em> matrix of observed predictors, and <strong>Θ</strong> a <em>p</em> × <em>p</em> precision matrix. This allows us to use <em>xi</em> ∈R <em>p</em> to represent the vector of <em>p</em> features for observation <em>i</em> (i.e., <em>xTi</em> is the <em>ith</em> row of <strong>X</strong> ), while <strong>x</strong> <em>k</em> is the <em>kth</em> column of <strong>X</strong> , without ambiguity.</p><div class="language-"><pre><code>Chapter 2\n</code></pre></div><h2 id="the-lasso-for-linear-models"><a class="header-anchor" href="#the-lasso-for-linear-models" aria-hidden="true">#</a> The Lasso for Linear Models</h2><p>In this chapter, we introduce the lasso estimator for linear regression. We describe the basic lasso method, and outline a simple approach for its im- plementation. We relate the lasso to ridge regression, and also view it as a Bayesian estimator.</p><h3 id="_2-1-introduction"><a class="header-anchor" href="#_2-1-introduction" aria-hidden="true">#</a> 2.1 Introduction</h3><p>In the linear regression setting, we are given <em>N</em> samples{( <em>xi,yi</em> )} <em>Ni</em> =1, where each <em>xi</em> = ( <em>xi</em> 1 <em>,...,xip</em> ) is a <em>p</em> -dimensional vector of features or predictors, and each <em>yi</em> ∈Ris the associated response variable. Our goal is to approximate the response variable <em>yi</em> using a linear combination of the predictors</p><div class="language-"><pre><code>η ( xi ) = β 0 +\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =\n</code></pre></div><div class="language-"><pre><code>xijβj. (2.1)\n</code></pre></div><p>The model is parametrized by the vector of regression weights <em>β</em> = ( <em>β</em> 1 <em>,...,βp</em> )∈R <em>p</em> and an intercept (or “bias”) term <em>β</em> 0 ∈R. The usual “least-squares” estimator for the pair ( <em>β</em> 0 <em>,β</em> ) is based on mini- mizing squared-error loss:</p><div class="language-"><pre><code>minimize\nβ 0 ,β\n</code></pre></div><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id="_1"><a class="header-anchor" href="#_1" aria-hidden="true">#</a> 1</h6><h6 id="_2-n"><a class="header-anchor" href="#_2-n" aria-hidden="true">#</a> 2 N</h6><h6 id="∑-n-2"><a class="header-anchor" href="#∑-n-2" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =\n</code></pre></div><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>yi − β 0 −\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =\n</code></pre></div><div class="language-"><pre><code>xijβj\n</code></pre></div><h6 id="_2"><a class="header-anchor" href="#_2" aria-hidden="true">#</a> ) 2</h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id="_2-2"><a class="header-anchor" href="#_2-2" aria-hidden="true">#</a> . (2.2)</h6><p>There are two reasons why we might consider an alternative to the least- squares estimate. The first reason is <em>prediction accuracy</em> : the least-squares estimate often has low bias but large variance, and prediction accuracy can sometimes be improved by shrinking the values of the regression coefficients, or setting some coefficients to zero. By doing so, we introduce some bias but reduce the variance of the predicted values, and hence may improve the overall prediction accuracy (as measured in terms of the mean-squared error). The second reason is for the purposes of <em>interpretation</em>. With a large number of predictors, we often would like to identify a smaller subset of these predictors that exhibit the strongest effects.</p><div class="language-"><pre><code>7\n</code></pre></div><h6 id="_8-the-lasso-for-linear-models"><a class="header-anchor" href="#_8-the-lasso-for-linear-models" aria-hidden="true">#</a> 8 THE LASSO FOR LINEAR MODELS</h6><p>This chapter is devoted to discussion of the <em>lasso</em> , a method that combines the least-squares loss (2.2) with an <em>`</em> 1 -constraint, or bound on the sum of the absolute values of the coefficients. Relative to the least-squares solution, this constraint has the effect of shrinking the coefficients, and even setting some to zero.^1 In this way it provides an automatic way for doing model selection in linear regression. Moreover, unlike some other criteria for model selection, the resulting optimization problem is convex, and can be solved efficiently for large problems.</p><h3 id="_2-2-the-lasso-estimator"><a class="header-anchor" href="#_2-2-the-lasso-estimator" aria-hidden="true">#</a> 2.2 The Lasso Estimator</h3><p>Given a collection of <em>N</em> predictor-response pairs{( <em>xi,yi</em> )} <em>Ni</em> =1, the lasso finds</p><p>the solution ( <em>β</em> ̂ 0 <em>,β</em> ̂) to the optimization problem</p><div class="language-"><pre><code>minimize\nβ 0 ,β\n</code></pre></div><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="_1-1"><a class="header-anchor" href="#_1-1" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-1"><a class="header-anchor" href="#_2-n-1" aria-hidden="true">#</a> 2 N</h6><h6 id="∑-n-3"><a class="header-anchor" href="#∑-n-3" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =\n</code></pre></div><div class="language-"><pre><code>( yi − β 0 −\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =\n</code></pre></div><div class="language-"><pre><code>xijβj )^2\n</code></pre></div><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>subject to\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =\n</code></pre></div><div class="language-"><pre><code>| βj |≤ t.\n</code></pre></div><h6 id="_2-3"><a class="header-anchor" href="#_2-3" aria-hidden="true">#</a> (2.3)</h6><p>The constraint</p><p>∑ <em>p j</em> =1| <em>βj</em> | ≤ <em>t</em> can be written more compactly as the <em>`</em>^1 -norm constraint‖ <em>β</em> ‖ 1 ≤ <em>t</em>. Furthermore, (2.3) is often represented using matrix- vector notation. Let <strong>y</strong> = ( <em>y</em> 1 <em>,...,yN</em> ) denote the <em>N</em> -vector of responses, and <strong>X</strong> be an <em>N</em> × <em>p</em> matrix with <em>xi</em> ∈R <em>p</em> in its <em>ith</em> row, then the optimization problem (2.3) can be re-expressed as</p><div class="language-"><pre><code>minimize\nβ 0 ,β\n</code></pre></div><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> {</h6><h6 id="_1-2"><a class="header-anchor" href="#_1-2" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-2"><a class="header-anchor" href="#_2-n-2" aria-hidden="true">#</a> 2 N</h6><div class="language-"><pre><code>‖ y − β 0 1 − X β ‖^22\n</code></pre></div><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>subject to‖ β ‖ 1 ≤ t ,\n</code></pre></div><h6 id="_2-4"><a class="header-anchor" href="#_2-4" aria-hidden="true">#</a> (2.4)</h6><p>where <strong>1</strong> is the vector of <em>N</em> ones, and‖·‖ 2 denotes the usual Euclidean norm on vectors. The bound <em>t</em> is a kind of “budget”: it limits the sum of the abso- lute values of the parameter estimates. Since a shrunken parameter estimate corresponds to a more heavily-constrained model, this budget limits how well we can fit the data. It must be specified by an external procedure such as cross-validation, which we discuss later in the chapter. Typically, we first standardize the predictors <strong>X</strong> so that each column is centered ( <em>N</em>^1</p><h6 id="∑-n-4"><a class="header-anchor" href="#∑-n-4" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1 xij = 0) and has unit variance (\n</code></pre></div><div class="language-"><pre><code>1\nN\n</code></pre></div><h6 id="∑-n-5"><a class="header-anchor" href="#∑-n-5" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1 x\n2\nij = 1). Without\n</code></pre></div><p>(^1) A <em>lasso</em> is a long rope with a noose at one end, used to catch horses and cattle. In a figurative sense, the method “lassos” the coefficients of the model. In the original lasso paper (Tibshirani 1996), the name “lasso” was also introduced as an acronym for “Least Absolute Selection and Shrinkage Operator.” Pronunciation: in the US “lasso” tends to be pronounced “lass-oh” (oh as in goat), while in the UK “lass-oo.” In the OED (2nd edition, 1965): “lasso is pronounced l ̆asoo by those who use it, and by most English people too.”</p><h6 id="the-lasso-estimator-9"><a class="header-anchor" href="#the-lasso-estimator-9" aria-hidden="true">#</a> THE LASSO ESTIMATOR 9</h6><p>standardization, the lasso solutions would depend on the units (e.g., feet ver- sus meters) used to measure the predictors. On the other hand, we typically would not standardize if the features were measured in the same units. For convenience, we also assume that the outcome values <em>yi</em> have been centered, meaning that <em>N</em>^1</p><h6 id="∑-n-6"><a class="header-anchor" href="#∑-n-6" aria-hidden="true">#</a> ∑ N</h6><p><em>i</em> =1 <em>yi</em> = 0. These centering conditions are convenient, since they mean that we can omit the intercept term <em>β</em> 0 in the lasso optimization. Given an optimal lasso solution <em>β</em> ̂on the centered data, we can recover the optimal solutions for the uncentered data: <em>β</em> ̂is the same, and the intercept <em>β</em> ̂ 0 is given by</p><div class="language-"><pre><code>β ̂ 0 = ̄ y −\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =\n</code></pre></div><div class="language-"><pre><code>̄ xjβ ̂ j,\n</code></pre></div><p>where ̄ <em>y</em> and{ <em>x</em> ̄ <em>j</em> } <em>p</em> 1 are the original means.^2 For this reason, we omit the intercept <em>β</em> 0 from the lasso for the remainder of this chapter. It is often convenient to rewrite the lasso problem in the so-called La- grangian form</p><div class="language-"><pre><code>minimize\nβ ∈R p\n</code></pre></div><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> {</h6><h6 id="_1-3"><a class="header-anchor" href="#_1-3" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-3"><a class="header-anchor" href="#_2-n-3" aria-hidden="true">#</a> 2 N</h6><div class="language-"><pre><code>‖ y − X β ‖^22 + λ ‖ β ‖ 1\n</code></pre></div><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> }</h6><h6 id="_2-5"><a class="header-anchor" href="#_2-5" aria-hidden="true">#</a> , (2.5)</h6><p>for some <em>λ</em> ≥0. By Lagrangian duality, there is a one-to-one correspondence between the constrained problem (2.3) and the Lagrangian form (2.5): for each value of <em>t</em> in the range where the constraint‖ <em>β</em> ‖ 1 ≤ <em>t</em> is active, there is a corresponding value of <em>λ</em> that yields the same solution from the Lagrangian form (2.5). Conversely, the solution <em>β</em> ̂ <em>λ</em> to problem (2.5) solves the bound problem with <em>t</em> =‖ <em>β</em> ̂ <em>λ</em> ‖ 1. We note that in many descriptions of the lasso, the factor 1 <em>/</em> 2 <em>N</em> appearing in (2.3) and (2.5) is replaced by 1 <em>/</em> 2 or simply 1. Although this makes no difference in (2.3), and corresponds to a simple reparametrization of <em>λ</em> in (2.5), this kind of standardization makes <em>λ</em> values comparable for different sample sizes (useful for cross-validation). The theory of convex analysis tells us that necessary and sufficient condi- tions for a solution to problem (2.5) take the form</p><h6 id="−"><a class="header-anchor" href="#−" aria-hidden="true">#</a> −</h6><h6 id="_1-4"><a class="header-anchor" href="#_1-4" aria-hidden="true">#</a> 1</h6><h6 id="n"><a class="header-anchor" href="#n" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>〈 x j, y − X β 〉+ λsj = 0 , j = 1 ,...,p. (2.6)\n</code></pre></div><p>Here each <em>sj</em> is an unknown quantity equal to sign( <em>βj</em> ) if <em>βj</em> 6 = 0 and some value lying in [− 1 <em>,</em> 1] otherwise—that is, it is a subgradient for the absolute value function (see Chapter 5 for details). In other words, the solutions <em>β</em> ˆ to problem (2.5) are the same as solutions ( <em>β,</em> ˆ <em>s</em> ˆ) to (2.6). This system is a form of the so-called Karush–Kuhn–Tucker (KKT) conditions for problem (2.5). Expressing a problem in subgradient form can be useful for designing</p><p>(^2) This is typically only true for linear regression with squared-error loss; it’s not true, for example, for lasso logistic regression.</p><h6 id="_10-the-lasso-for-linear-models"><a class="header-anchor" href="#_10-the-lasso-for-linear-models" aria-hidden="true">#</a> 10 THE LASSO FOR LINEAR MODELS</h6><p>algorithms for finding its solutions. More details are given in Exercises (2.3) and (2.4). As an example of the lasso, let us consider the data given in Table 2.1, taken from Thomas (1990). The outcome is the total overall reported crime rate per</p><div class="language-"><pre><code>Table 2.1 Crime data: Crime rate and five predictors, forN = 50 U.S. cities.\ncity funding hs not-hs college college4 crime rate\n1 40 74 11 31 20 478\n2 32 72 11 43 18 494\n3 57 70 18 16 16 643\n4 31 71 11 25 19 341\n5 67 72 9 29 24 773\n..\n.\n</code></pre></div><div class="language-"><pre><code>..\n.\n</code></pre></div><div class="language-"><pre><code>..\n.\n</code></pre></div><div class="language-"><pre><code>..\n.\n</code></pre></div><div class="language-"><pre><code>..\n.\n50 66 67 26 18 16 940\n</code></pre></div><p>one million residents in 50 U.S cities. There are five predictors: annual police funding in dollars per resident, percent of people 25 years and older with four years of high school, percent of 16- to 19-year olds not in high school and not high school graduates, percent of 18- to 24-year olds in college, and percent of people 25 years and older with at least four years of college. This small example is for illustration only, but helps to demonstrate the nature of the lasso solutions. Typically the lasso is most useful for much larger problems, including “wide” data for which <em>p</em>  <em>N</em>. The left panel of Figure 2.1 shows the result of applying the lasso with the bound <em>t</em> varying from zero on the left, all the way to a large value on the right, where it has no effect. The horizontal axis has been scaled so that the maximal bound, corresponding to the least-squares estimates <em>β</em> ̃, is one. We see that for much of the range of the bound, many of the estimates are exactly zero and hence the corresponding predictor(s) would be excluded from the model. Why does the lasso have this model selection property? It is due to the geometry that underlies the <em>`</em> 1 constraint‖ <em>β</em> ‖ 1 ≤ <em>t</em>. To understand this better, the right panel shows the estimates from <em>ridge regression</em> , a technique that predates the lasso. It solves a criterion very similar to (2.3):</p><div class="language-"><pre><code>minimize\nβ 0 ,β\n</code></pre></div><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="_1-5"><a class="header-anchor" href="#_1-5" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-4"><a class="header-anchor" href="#_2-n-4" aria-hidden="true">#</a> 2 N</h6><h6 id="∑-n-7"><a class="header-anchor" href="#∑-n-7" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi − β 0 −\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>xijβj )^2\n</code></pre></div><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>subject to\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>β^2 j ≤ t^2.\n</code></pre></div><h6 id="_2-7"><a class="header-anchor" href="#_2-7" aria-hidden="true">#</a> (2.7)</h6><p>The ridge profiles in the right panel have roughly the same shape as the lasso profiles, but are not equal to zero except at the left end. Figure 2.2 contrasts the two constraints used in the lasso and ridge regression. The residual sum</p><h6 id="the-lasso-estimator-11"><a class="header-anchor" href="#the-lasso-estimator-11" aria-hidden="true">#</a> THE LASSO ESTIMATOR 11</h6><div class="language-"><pre><code>0.0 0.2 0.4 0.6 0.8 1.0\n</code></pre></div><div class="language-"><pre><code>−5\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>5\n</code></pre></div><div class="language-"><pre><code>10\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>hs\n</code></pre></div><div class="language-"><pre><code>college\n</code></pre></div><div class="language-"><pre><code>college4not−hs\n</code></pre></div><div class="language-"><pre><code>funding\n</code></pre></div><div class="language-"><pre><code>Lasso\n</code></pre></div><div class="language-"><pre><code>0.0 0.2 0.4 0.6 0.8 1.0\n</code></pre></div><div class="language-"><pre><code>−5\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>5\n</code></pre></div><div class="language-"><pre><code>10\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>hs\n</code></pre></div><div class="language-"><pre><code>college\n</code></pre></div><div class="language-"><pre><code>college4not−hs\n</code></pre></div><div class="language-"><pre><code>funding\n</code></pre></div><div class="language-"><pre><code>Ridge Regression\n</code></pre></div><div class="language-"><pre><code>‖βˆ‖ 1 /‖β ̃‖ 1 ‖βˆ‖ 2 /‖β ̃‖ 2\n</code></pre></div><p><strong>Figure 2.1</strong> <em>Left: Coefficient path for the lasso, plotted versus the<code>_ 1 _norm of the coefficient vector, relative to the norm of the unrestricted least-squares estimateβ_ ̃_. Right: Same for ridge regression, plotted against the relative</code></em> 2 <em>norm.</em></p><p>(^2) β^ β^ β**..** 1 β 2 β 1 β <strong>Figure 2.2</strong> <em>Estimation picture for the lasso (left) and ridge regression (right). The solid blue areas are the constraint regions</em> | <em>β</em> 1 |+| <em>β</em> 2 |≤ <em>tandβ</em> 12 + <em>β</em>^22 ≤ <em>t</em>^2 <em>, respectively, while the red ellipses are the contours of the residual-sum-of-squares function. The pointβ</em> ̂ <em>depicts the usual (unconstrained) least-squares estimate.</em></p><h6 id="_12-the-lasso-for-linear-models"><a class="header-anchor" href="#_12-the-lasso-for-linear-models" aria-hidden="true">#</a> 12 THE LASSO FOR LINEAR MODELS</h6><p><strong>Table 2.2</strong> <em>Results from analysis of the crime data. Left panel shows the least-squares estimates, standard errors, and their ratio (Z-score). Middle and right panels show the corresponding results for the lasso, and the least-squares estimates applied to the subset of predictors chosen by the lasso.</em></p><div class="language-"><pre><code>LS coef SE Z Lasso SE Z LS SE Z\nfunding 10.98 3.08 3.6 8.84 3.55 2.5 11.29 2.90 3.9\nhs -6.09 6.54 -0.9 -1.41 3.73 -0.4 -4.76 4.53 -1.1\nnot-hs 5.48 10.05 0.5 3.12 5.05 0.6 3.44 7.83 0.4\ncollege 0.38 4.42 0.1 0.0 - - 0.0 - -\ncollege4 5.50 13.75 0.4 0.0 - - 0.0 - -\n</code></pre></div><p>of squares has elliptical contours, centered at the full least-squares estimates. The constraint region for ridge regression is the disk <em>β</em> 12 + <em>β</em>^22 ≤ <em>t</em>^2 , while that for lasso is the diamond| <em>β</em> 1 |+| <em>β</em> 2 |≤ <em>t</em>. Both methods find the first point where the elliptical contours hit the constraint region. Unlike the disk, the diamond has corners; if the solution occurs at a corner, then it has one parameter <em>βj</em> equal to zero. When <em>p &gt;</em> 2, the diamond becomes a rhomboid, and has many corners, flat edges, and faces; there are many more opportunities for the estimated parameters to be zero (see Figure 4.2 on page 58.) We use the term <em>sparse</em> for a model with few nonzero coefficients. Hence a key property of the <em>`</em> 1 -constraint is its ability to yield sparse solutions. This idea can be applied in many different statistical models, and is the central theme of this book. Table 2.2 shows the results of applying three fitting procedures to the crime data. The lasso bound <em>t</em> was chosen by cross-validation, as described in Section 2.3. The left panel corresponds to the full least-squares fit, while the middle panel shows the lasso fit. On the right, we have applied least- squares estimation to the subset of three predictors with nonzero coefficients in the lasso. The standard errors for the least-squares estimates come from the usual formulas. No such simple formula exists for the lasso, so we have used the bootstrap to obtain the estimate of standard errors in the middle panel (see Exercise 2.6; Chapter 6 discusses some promising new approaches for post-selection inference). Overall it appears thatfundinghas a large effect, probably indicating that police resources have been focused on higher crime areas. The other predictors have small to moderate effects. Note that the lasso sets two of the five coefficients to zero, and tends to shrink the coefficients of the others toward zero relative to the full least-squares estimate. In turn, the least-squares fit on the subset of the three predictors tends to expand the lasso estimates away from zero. The nonzero estimates from the lasso tend to be biased toward zero, so the debiasing in the right panel can often improve the prediction error of the model. This two-stage process is also known as the <em>relaxed lasso</em> (Meinshausen 2007).</p><h6 id="cross-validation-and-inference-13"><a class="header-anchor" href="#cross-validation-and-inference-13" aria-hidden="true">#</a> CROSS-VALIDATION AND INFERENCE 13</h6><h3 id="_2-3-cross-validation-and-inference"><a class="header-anchor" href="#_2-3-cross-validation-and-inference" aria-hidden="true">#</a> 2.3 Cross-Validation and Inference</h3><p>The bound <em>t</em> in the lasso criterion (2.3) controls the complexity of the model; larger values of <em>t</em> free up more parameters and allow the model to adapt more closely to the training data. Conversely, smaller values of <em>t</em> restrict the parameters more, leading to sparser, more interpretable models that fit the data less closely. Forgetting about interpretability, we can ask for the value of <em>t</em> that gives the most accurate model for predicting independent test data from the same population. Such accuracy is called the <em>generalization</em> ability of the model. A value of <em>t</em> that is too small can prevent the lasso from capturing the main signal in the data, while too large a value can lead to overfitting. In this latter case, the model adapts to the noise as well as the signal that is present in the training data. In both cases, the prediction error on a test set will be inflated. There is usually an intermediate value of <em>t</em> that strikes a good balance between these two extremes, and in the process, produces a model with some coefficients equal to zero. In order to estimate this best value for <em>t</em> , we can create artificial training and test sets by splitting up the given dataset at random, and estimating performance on the test data, using a procedure known as <em>cross-validation</em>. In more detail, we first randomly divide the full dataset into some number of groups <em>K &gt;</em> 1. Typical choices of <em>K</em> might be 5 or 10, and sometimes <em>N</em>. We fix one group as the test set, and designate the remaining <em>K</em> −1 groups as the training set. We then apply the lasso to the training data for a range of different <em>t</em> values, and we use each fitted model to predict the responses in the test set, recording the mean-squared prediction errors for each value of <em>t</em>. This process is repeated a total of <em>K</em> times, with each of the <em>K</em> groups getting the chance to play the role of the test data, with the remaining <em>K</em> −1 groups used as training data. In this way, we obtain <em>K</em> different estimates of the prediction error over a range of values of <em>t</em>. These <em>K</em> estimates of prediction error are averaged for each value of <em>t</em> , thereby producing a <em>cross-validation error curve</em>. Figure 2.3 shows the cross-validation error curve for the crime-data ex- ample, obtained using <em>K</em> = 10 splits. We plot the estimated mean-squared prediction error versus the relative bound ̃ <em>t</em> =‖ <em>β</em> ̂( <em>t</em> )‖ 1 <em>/</em> ‖ <em>β</em> ̃‖ 1 , where the esti-</p><p>mate <em>β</em> ̂( <em>t</em> ) correspond to the lasso solution for bound <em>t</em> and <em>β</em> ̃is the ordinary least-squares solution. The error bars in Figure 2.3 indicate plus and minus one standard error in the cross-validated estimates of the prediction error. A vertical dashed line is drawn at the position of the minimum ( ̃ <em>t</em> = 0_._ 56) while a dotted line is drawn at the “one-standard-error rule” choice ( <em>t</em> ̃= 0_._ 03). This is the smallest value of <em>t</em> yielding a CV error no more than one standard error above its minimum value. The number of nonzero coefficients in each model is shown along the top. Hence the model that minimizes the CV error has three predictors, while the one-standard-error-rule model has just one. We note that the cross-validation process above focused on the bound parameter <em>t</em>. One can just as well carry out cross-validation in the Lagrangian</p><h6 id="_14-the-lasso-for-linear-models"><a class="header-anchor" href="#_14-the-lasso-for-linear-models" aria-hidden="true">#</a> 14 THE LASSO FOR LINEAR MODELS</h6><div class="language-"><pre><code>0.0 0.2 0.4 0.6 0.8 1.0\n</code></pre></div><div class="language-"><pre><code>5e+04\n</code></pre></div><div class="language-"><pre><code>7e+04\n</code></pre></div><div class="language-"><pre><code>9e+04\n</code></pre></div><div class="language-"><pre><code>Relative Bound\n</code></pre></div><div class="language-"><pre><code>CV Mean−Squared Error\n</code></pre></div><div class="language-"><pre><code>0 1 1 1 1 2 2 3 3 3 3 4 4 5 5\n</code></pre></div><p><strong>Figure 2.3</strong> <em>Cross-validated estimate of mean-squared prediction error, as a function of the relative<code>_ 1 _bound_ ̃ _t_ =‖ _β_ ̂( _t_ )‖ 1 _/_ ‖ _β_ ̃‖ 1_. Hereβ_ ̂( _t_ ) _is the lasso estimate correspond- ing to the</code></em> 1 <em>boundtandβ</em> ̃ <em>is the ordinary least-squares solution. Included are the location of the minimum, pointwise standard-error bands, and the “one-standard- error” location. The standard errors are large since the sample sizeNis only 50.</em></p><p>form (2.5), focusing on the parameter <em>λ</em>. The two methods will give similar but not identical results, since the mapping between <em>t</em> and <em>λ</em> is data-dependent.</p><h3 id="_2-4-computation-of-the-lasso-solution"><a class="header-anchor" href="#_2-4-computation-of-the-lasso-solution" aria-hidden="true">#</a> 2.4 Computation of the Lasso Solution</h3><p>The lasso problem is a convex program, specifically a quadratic program (QP) with a convex constraint. As such, there are many sophisticated QP meth- ods for solving the lasso. However there is a particularly simple and effective computational algorithm, that gives insight into how the lasso works. For convenience, we rewrite the criterion in Lagrangian form:</p><div class="language-"><pre><code>minimize\nβ ∈R p\n</code></pre></div><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="_1-6"><a class="header-anchor" href="#_1-6" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-5"><a class="header-anchor" href="#_2-n-5" aria-hidden="true">#</a> 2 N</h6><h6 id="∑-n-8"><a class="header-anchor" href="#∑-n-8" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi −\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>xijβj )^2 + λ\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>| βj |\n</code></pre></div><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="_2-8"><a class="header-anchor" href="#_2-8" aria-hidden="true">#</a> . (2.8)</h6><p>As before, we will assume that both <em>yi</em> and the features <em>xij</em> have been stan- dardized so that <em>N</em>^1</p><h6 id="∑"><a class="header-anchor" href="#∑" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>iyi = 0,\n</code></pre></div><div class="language-"><pre><code>1\nN\n</code></pre></div><h6 id="∑-1"><a class="header-anchor" href="#∑-1" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>ixij = 0, and\n</code></pre></div><div class="language-"><pre><code>1\nN\n</code></pre></div><h6 id="∑-2"><a class="header-anchor" href="#∑-2" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>ix\n</code></pre></div><p>2 <em>ij</em> = 1. In this case, the intercept term <em>β</em> 0 can be omitted. The Lagrangian form is especially convenient for numerical computation of the solution by a simple procedure known as <em>coordinate descent</em>.</p><h6 id="computation-of-the-lasso-solution-15"><a class="header-anchor" href="#computation-of-the-lasso-solution-15" aria-hidden="true">#</a> COMPUTATION OF THE LASSO SOLUTION 15</h6><h4 id="_2-4-1-single-predictor-soft-thresholding"><a class="header-anchor" href="#_2-4-1-single-predictor-soft-thresholding" aria-hidden="true">#</a> 2.4.1 Single Predictor: Soft Thresholding</h4><p>Let’s first consider a single predictor setting, based on samples{( <em>zi,yi</em> )} <em>Ni</em> =1 (for convenience we have given the name <em>zi</em> to this single <em>xi</em> 1 ). The problem then is to solve</p><div class="language-"><pre><code>minimize\nβ\n</code></pre></div><h6 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> {</h6><h6 id="_1-7"><a class="header-anchor" href="#_1-7" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-6"><a class="header-anchor" href="#_2-n-6" aria-hidden="true">#</a> 2 N</h6><h6 id="∑-n-9"><a class="header-anchor" href="#∑-n-9" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi − ziβ )^2 + λ | β |\n</code></pre></div><h6 id="-6"><a class="header-anchor" href="#-6" aria-hidden="true">#</a> }</h6><h6 id="_2-9"><a class="header-anchor" href="#_2-9" aria-hidden="true">#</a> . (2.9)</h6><p>The standard approach to this univariate minimization problem would be to take the gradient (first derivative) with respect to <em>β</em> , and set it to zero. There is a complication, however, because the absolute value function| <em>β</em> |does not have a derivative at <em>β</em> = 0. However we can proceed by direct inspection of the function (2.9), and find that</p><div class="language-"><pre><code>β ̂=\n</code></pre></div><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>1\nN 〈 z , y 〉− λ if\n</code></pre></div><div class="language-"><pre><code>1\nN 〈 z , y 〉 &gt; λ,\n0 if N^1 |〈 z , y 〉| ≤ λ,\n1\nN 〈 z , y 〉+ λ if\n</code></pre></div><div class="language-"><pre><code>1\nN 〈 z , y 〉 &lt; − λ.\n</code></pre></div><h6 id="_2-10"><a class="header-anchor" href="#_2-10" aria-hidden="true">#</a> (2.10)</h6><p>(Exercise 2.2), which we can write succinctly as</p><div class="language-"><pre><code>β ̂=S λ (^1\nN 〈 z , y 〉\n</code></pre></div><h6 id="-7"><a class="header-anchor" href="#-7" aria-hidden="true">#</a> )</h6><h6 id="_2-11"><a class="header-anchor" href="#_2-11" aria-hidden="true">#</a> . (2.11)</h6><p>Here the <em>soft-thresholding operator</em></p><div class="language-"><pre><code>S λ ( x ) = sign( x )\n</code></pre></div><h6 id="-8"><a class="header-anchor" href="#-8" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>| x |− λ\n</code></pre></div><h6 id="-9"><a class="header-anchor" href="#-9" aria-hidden="true">#</a> )</h6><h6 id="_2-12"><a class="header-anchor" href="#_2-12" aria-hidden="true">#</a> + (2.12)</h6><p>translates its argument <em>x</em> toward zero by the amount <em>λ</em> , and sets it to zero if| <em>x</em> | ≤ <em>λ</em>.^3 See Figure 2.4 for an illustration. Notice that for standardized data with <em>N</em>^1</p><h6 id="∑-3"><a class="header-anchor" href="#∑-3" aria-hidden="true">#</a> ∑</h6><p><em>iz</em> 2 <em>i</em> = 1, (2.11) is just a soft-thresholded version of the usual least-squares estimate <em>β</em> ̃= <em>N</em>^1 〈 <strong>z</strong> <em>,</em> <strong>y</strong> 〉. One can also derive these results using the notion of subgradients (Exercise 2.3).</p><h4 id="_2-4-2-multiple-predictors-cyclic-coordinate-descent"><a class="header-anchor" href="#_2-4-2-multiple-predictors-cyclic-coordinate-descent" aria-hidden="true">#</a> 2.4.2 Multiple Predictors: Cyclic Coordinate Descent</h4><p>Using this intuition from the univariate case, we can now develop a simple coordinatewise scheme for solving the full lasso problem (2.5). More precisely, we repeatedly cycle through the predictors in some fixed (but arbitrary) order (say <em>j</em> = 1 <em>,</em> 2 <em>,...,p</em> ), where at the <em>jth</em> step, we update the coefficient <em>βj</em> by minimizing the objective function in this coordinate while holding fixed all other coefficients{ <em>β</em> ̂ <em>k,k</em> 6 = <em>j</em> }at their current values. Writing the objective in (2.5) as</p><div class="language-"><pre><code>1\n2 N\n</code></pre></div><h6 id="∑-n-10"><a class="header-anchor" href="#∑-n-10" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi −\n</code></pre></div><h6 id="∑-4"><a class="header-anchor" href="#∑-4" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>k 6 = j\n</code></pre></div><div class="language-"><pre><code>xikβk − xijβj )^2 + λ\n</code></pre></div><h6 id="∑-5"><a class="header-anchor" href="#∑-5" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>k 6 = j\n</code></pre></div><div class="language-"><pre><code>| βk |+ λ | βj | , (2.13)\n</code></pre></div><p>(^3) <em>t</em> +denotes the positive part of <em>t</em> ∈R, equal to <em>t</em> if <em>t &gt;</em> 0 and 0 otherwise.</p><h6 id="_16-the-lasso-for-linear-models"><a class="header-anchor" href="#_16-the-lasso-for-linear-models" aria-hidden="true">#</a> 16 THE LASSO FOR LINEAR MODELS</h6><div class="language-"><pre><code>Sλ(x) λ\n</code></pre></div><div class="language-"><pre><code>(0,0) x\n</code></pre></div><p><strong>Figure 2.4</strong> <em>Soft thresholding function</em> S <em>λ</em> ( <em>x</em> ) = sign( <em>x</em> ) (| <em>x</em> |− <em>λ</em> )+ <em>is shown in blue (broken lines), along with the</em> 45 ◦ <em>line in black.</em></p><p>we see that solution for each <em>βj</em> can be expressed succinctly in terms of the</p><p><em>partial residualr</em> ( <em>ij</em> )= <em>yi</em> −</p><h6 id="∑-6"><a class="header-anchor" href="#∑-6" aria-hidden="true">#</a> ∑</h6><p><em>k</em> 6 = <em>jxik β</em> ̂ <em>k</em> , which removes from the outcome the current fit from all but the <em>jth</em> predictor. In terms of this partial residual, the <em>jth</em> coefficient is updated as</p><div class="language-"><pre><code>β ̂ j =S λ\n</code></pre></div><h6 id="-10"><a class="header-anchor" href="#-10" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>1\nN 〈 x j, r\n</code></pre></div><div class="language-"><pre><code>( j )〉\n</code></pre></div><h6 id="-11"><a class="header-anchor" href="#-11" aria-hidden="true">#</a> )</h6><h6 id="_2-14"><a class="header-anchor" href="#_2-14" aria-hidden="true">#</a> . (2.14)</h6><p>Equivalently, the update can be written as</p><div class="language-"><pre><code>β ̂ j ←S λ\n</code></pre></div><h6 id="-12"><a class="header-anchor" href="#-12" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>β ̂ j +^1\nN 〈 x j, r 〉\n</code></pre></div><h6 id="-13"><a class="header-anchor" href="#-13" aria-hidden="true">#</a> )</h6><h6 id="_2-15"><a class="header-anchor" href="#_2-15" aria-hidden="true">#</a> , (2.15)</h6><p>where <em>ri</em> = <em>yi</em> −</p><p>∑ <em>p j</em> =1 <em>xij β</em> ̂ <em>j</em> are the full residuals (Exercise 2.4). The overall algorithm operates by applying this soft-thresholding update (2.14) repeatedly in a cyclical manner, updating the coordinates of <em>β</em> ̂(and hence the residual vectors) along the way. Why does this algorithm work? The criterion (2.5) is a convex function of <em>β</em> and so has no local minima. The algorithm just described corresponds to the method of <em>cyclical coordinate descent</em> , which minimizes this convex objec- tive along each coordinate at a time. Under relatively mild conditions (which apply here), such coordinate-wise minimization schemes applied to a convex function converge to a global optimum. It is important to note that some conditions are required, because there are instances, involving nonseparable penalty functions, in which coordinate descent schemes can become “jammed.” Further details are in given in Chapter 5. Note that the choice <em>λ</em> = 0 in (2.5) delivers the solution to the ordinary least-squares problem. From the update (2.14), we see that the algorithm does a univariate regression of the partial residual onto each predictor, cycling through the predictors until convergence. When the data matrix <strong>X</strong> is of full</p><h6 id="degrees-of-freedom-17"><a class="header-anchor" href="#degrees-of-freedom-17" aria-hidden="true">#</a> DEGREES OF FREEDOM 17</h6><p>rank, this point of convergence is the least-squares solution. However, it is not a particularly efficient method for computing it. In practice, one is often interested in finding the lasso solution not just for a single fixed value of <em>λ</em> , but rather the entire path of solutions over a range of possible <em>λ</em> values (as in Figure 2.1). A reasonable method for doing so is to begin with a value of <em>λ</em> just large enough so that the only optimal solution is the all-zeroes vector. As shown in Exercise 2.1, this value is equal to <em>λmax</em> = max <em>j</em> | <em>N</em>^1 〈 <strong>x</strong> <em>j,</em> <strong>y</strong> 〉|. Then we decrease <em>λ</em> by a small amount and run coordinate descent until convergence. Decreasing <em>λ</em> again and using the previous solution as a “warm start,” we then run coordinate descent until convergence. In this way we can efficiently compute the solutions over a grid of <em>λ</em> values. We refer to this method as <em>pathwise coordinate descent</em>. Coordinate descent is especially fast for the lasso because the coordinate- wise minimizers are explicitly available (Equation (2.14)), and thus an iter- ative search along each coordinate is not needed. Secondly, it exploits the sparsity of the problem: for large enough values of <em>λ</em> most coefficients will be zero and will not be moved from zero. In Section 5.4, we discuss computational hedges for guessing the active set, which speed up the algorithm dramatically.</p><p><em>Homotopy methods</em> are another class of techniques for solving the lasso. They produce the entire path of solutions in a sequential fashion, starting at zero. This path is actually piecewise linear, as can be seen in Figure 2.1 (as a function of <em>t</em> or <em>λ</em> ). The <em>least angle regression</em> (LARS) algorithm is a homotopy method that efficiently constructs the piecewise linear path, and is described in Chapter 5.</p><h4 id="_2-4-3-soft-thresholding-and-orthogonal-bases"><a class="header-anchor" href="#_2-4-3-soft-thresholding-and-orthogonal-bases" aria-hidden="true">#</a> 2.4.3 Soft-Thresholding and Orthogonal Bases</h4><p>The soft-thresholding operator plays a central role in the lasso and also in signal denoising. To see this, notice that the coordinate minimization scheme above takes an especially simple form if the predictors are orthogonal, mean- ing that <em>N</em>^1 〈 <strong>x</strong> <em>j,</em> <strong>x</strong> <em>k</em> 〉= 0 for each <em>j</em> 6 = <em>k</em>. In this case, the update (2.14) sim-</p><p>plifies dramatically, since <em>N</em>^1 〈 <strong>x</strong> <em>j,</em> <strong><em>r</em></strong> ( <em>j</em> )〉= <em>N</em>^1 〈 <strong>x</strong> <em>j,</em> <strong>y</strong> 〉so that <em>β</em> ̂ <em>j</em> is simply the soft-thresholded version of the univariate least-squares estimate of <strong>y</strong> regressed against <strong>x</strong> <em>j</em>. Thus, in the special case of an orthogonal design, the lasso has an explicit closed-form solution, and no iterations are required. Wavelets are a popular form of orthogonal bases, used for smoothing and compression of signals and images. In wavelet smoothing one represents the data in a wavelet basis, and then denoises by soft-thresholding the wavelet coefficients. We discuss this further in Section 2.10 and in Chapter 10.</p><h3 id="_2-5-degrees-of-freedom"><a class="header-anchor" href="#_2-5-degrees-of-freedom" aria-hidden="true">#</a> 2.5 Degrees of Freedom</h3><p>Suppose we have <em>p</em> predictors, and fit a linear regression model using only a subset of <em>k</em> of these predictors. Then if these <em>k</em> predictors were chosen without</p><h6 id="_18-the-lasso-for-linear-models"><a class="header-anchor" href="#_18-the-lasso-for-linear-models" aria-hidden="true">#</a> 18 THE LASSO FOR LINEAR MODELS</h6><p>regard to the response variable, the fitting procedure “spends” <em>k</em> degrees of freedom. This is a loose way of saying that the standard test statistic for testing the hypothesis that all <em>k</em> coefficients are zero has a Chi-squared distribution with <em>k</em> degrees of freedom (with the error variance <em>σ</em>^2 assumed to be known) However if the <em>k</em> predictors were chosen using knowledge of the response variable, for example to yield the smallest training error among all subsets of size <em>k</em> , then we would expect that the fitting procedure spends more than <em>k</em> degrees of freedom. We call such a fitting procedure <em>adaptive</em> , and clearly the lasso is an example of one. Similarly, a forward-stepwise procedure in which we sequentially add the predictor that most decreases the training error is adaptive, and we would expect that the resulting model uses more than <em>k</em> degrees of freedom after <em>k</em> steps. For these reasons and in general, one cannot simply count as degrees of freedom the number of nonzero coefficients in the fitted model. However, it turns out that for the lasso, one <em>can</em> count degrees of freedom by the number of nonzero coefficients, as we now describe. First we need to define precisely what we mean by the degrees of freedom of an adaptively fitted model. Suppose we have an additive-error model, with</p><div class="language-"><pre><code>yi = f ( xi ) + i, i = 1 ,...,N, (2.16)\n</code></pre></div><p>for some unknown <em>f</em> and with the errors <em>i</em> iid (0 <em>,σ</em>^2 ). If the <em>N</em> sample pre- dictions are denoted by <strong>y</strong> ̂, then we define</p><div class="language-"><pre><code>df( y ̂) : =\n</code></pre></div><h6 id="_1-8"><a class="header-anchor" href="#_1-8" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>σ^2\n</code></pre></div><h6 id="∑-n-11"><a class="header-anchor" href="#∑-n-11" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>Cov\n</code></pre></div><h6 id="-14"><a class="header-anchor" href="#-14" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>y ̂ i,yi\n</code></pre></div><h6 id="-15"><a class="header-anchor" href="#-15" aria-hidden="true">#</a> )</h6><h6 id="_2-17"><a class="header-anchor" href="#_2-17" aria-hidden="true">#</a> . (2.17)</h6><p>The covariance here is taken over the randomness in the response vari- ables{ <em>yi</em> } <em>Ni</em> =1 with the predictors held fixed. Thus, the degrees of freedom corresponds to the total amount of <em>self-influence</em> that each response measure- ment has on its prediction. The more the model fits—that is, adapts—to the data, the larger the degrees of freedom. In the case of a fixed linear model, using <em>k</em> predictors chosen independently of the response variable, it is easy to show that df(̂ <strong>y</strong> ) = <em>k</em> (Exercise 2.7). However, under adaptive fitting, it is typically the case that the degrees of freedom is larger than <em>k</em>. Somewhat miraculously, one can show that for the lasso, with a fixed penalty parameter <em>λ</em> , the number of nonzero coefficients <em>kλ</em> is an unbiased esti- mate of the degrees of freedom^4 (Zou, Hastie and Tibshirani 2007, Tibshirani 2 and Taylor 2012). As discussed earlier, a variable-selection method like forward-stepwise regression uses more than <em>k</em> degrees of freedom after <em>k</em> steps. Given the apparent similarity between forward-stepwise regression and the lasso, how can the lasso have this simple degrees of freedom property? The</p><p>(^4) An even stronger statement holds for the LAR path, where the degrees of freedom after <em>k</em> steps is exactly <em>k</em> , under some conditions on <strong>X</strong>. The LAR path relates closely to the lasso, and is described in Section 5.6.</p><h6 id="uniqueness-of-the-lasso-solutions-19"><a class="header-anchor" href="#uniqueness-of-the-lasso-solutions-19" aria-hidden="true">#</a> UNIQUENESS OF THE LASSO SOLUTIONS 19</h6><p>reason is that the lasso not only selects predictors (which inflates the degrees of freedom), but also shrinks their coefficients toward zero, relative to the usual least-squares estimates. This shrinkage turns out to be just the right amount to bring the degrees of freedom down to <em>k</em>. This result is useful be- cause it gives us a qualitative measure of the amount of fitting that we have done at any point along the lasso path. In the general setting, a proof of this result is quite difficult. In the special case of an orthogonal design, it is relatively easy to prove, using the fact that the lasso estimates are simply soft-thresholded versions of the univariate regression coefficients for the orthogonal design. We explore the details of this argument in Exercise 2.8. This idea is taken one step further in Section 6.3.1 where we describe the <em>covariance test</em> for testing the significance of predictors in the context of the lasso.</p><h3 id="_2-6-uniqueness-of-the-lasso-solutions"><a class="header-anchor" href="#_2-6-uniqueness-of-the-lasso-solutions" aria-hidden="true">#</a> 2.6 Uniqueness of the Lasso Solutions</h3><p>We first note that the theory of convex duality can be used to show that when the columns of <strong>X</strong> are in general position, then for <em>λ &gt;</em> 0 the solution to the lasso problem (2.5) is unique. This holds even when <em>p</em> ≥ <em>N</em> , although then the number of nonzero coefficients in any lasso solution is at most <em>N</em> (Rosset, Zhu and Hastie 2004, Tibshirani 2 2013). Now when the predictor matrix <strong>X</strong> is not of full column rank, the least squares fitted values are unique, but the parameter estimates themselves are not. The non-full-rank case can occur when <em>p</em> ≤ <em>N</em> due to collinearity, and always occurs when <em>p &gt; N</em>. In the latter scenario, there are an infinite number of solutions <em>β</em> ̂that yield a perfect fit with zero training error. Now consider the lasso problem in Lagrange form (2.5) for <em>λ &gt;</em> 0. As shown in Exercise 2.5, the fitted values <strong>X</strong> <em>β</em> ̂are unique. But it turns out that the solution <em>β</em> ̂may not be unique. Consider a simple example with two predictors <strong>x</strong> 1 and <strong>x</strong> 2 and response <strong>y</strong> , and suppose the lasso solution coefficients <em>β</em> ̂at <em>λ</em> are ( <em>β</em> ̂ 1 <em>,β</em> ̂ 2 ). If we now include a third predictor <strong>x</strong> 3 = <strong>x</strong> 2 into the mix, an identical copy of the second, then for any <em>α</em> ∈[0 <em>,</em> 1], the vector <em>β</em> ̃( <em>α</em> ) = ( <em>β</em> ̂ 1 <em>, α</em> · <em>β</em> ̂ 2 <em>,</em> (1− <em>α</em> )· <em>β</em> ̂ 2 ) produces an identical fit, and has <em>`</em> 1 norm</p><p>‖ <em>β</em> ̃( <em>α</em> )‖ 1 =‖ <em>β</em> ̂‖ 1. Consequently, for this model (in which we might have either <em>p</em> ≤ <em>N</em> or <em>p &gt; N</em> ), there is an infinite family of solutions. In general, when <em>λ &gt;</em> 0, one can show that if the columns of the model matrix <strong>X</strong> are in <em>general position</em> , then the lasso solutions are unique. To be precise, we say the columns{ <strong>x</strong> <em>j</em> } <em>pj</em> =1are in general position if any affine sub- spaceL⊂R <em>N</em> of dimension <em>k &lt; N</em> contains at most <em>k</em> + 1 elements of the set{± <strong>x</strong> 1 <em>,</em> ± <strong>x</strong> 2 <em>,...</em> ± <strong>x</strong> <em>p</em> }, excluding antipodal pairs of points (that is, points differing only by a sign flip). We note that the data in the example in the previous paragraph are not in general position. If the <em>X</em> data are drawn from a continuous probability distribution, then with probability one the data are in general position and hence the lasso solutions will be unique. As a re- sult, non-uniqueness of the lasso solutions can only occur with discrete-valued data, such as those arising from dummy-value coding of categorical predic-</p><h6 id="_20-the-lasso-for-linear-models"><a class="header-anchor" href="#_20-the-lasso-for-linear-models" aria-hidden="true">#</a> 20 THE LASSO FOR LINEAR MODELS</h6><p>tors. These results have appeared in various forms in the literature, with a summary given by Tibshirani 2 (2013). We note that numerical algorithms for computing solutions to the lasso will typically yield valid solutions in the non-unique case. However, the particular solution that they deliver can depend on the specifics of the algorithm. For example with coordinate descent, the choice of starting values can affect the final solution.</p><h3 id="_2-7-a-glimpse-at-the-theory"><a class="header-anchor" href="#_2-7-a-glimpse-at-the-theory" aria-hidden="true">#</a> 2.7 A Glimpse at the Theory</h3><p>There is a large body of theoretical work on the behavior of the lasso. It is largely focused on the mean-squared-error consistency of the lasso, and recov- ery of the nonzero support set of the true regression parameters, sometimes called <em>sparsistency</em>. For MSE consistency, if <em>β</em> ∗and <em>β</em> ˆare the true and lasso- estimated parameters, it can be shown that as <em>p,n</em> →∞</p><div class="language-"><pre><code>‖ X ( β ˆ− β ∗)‖^22 /N ≤ C ·‖ β ∗‖ 1\n</code></pre></div><h6 id="√"><a class="header-anchor" href="#√" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>log( p ) /N (2.18)\n</code></pre></div><p>with high probability (Greenshtein and Ritov 2004, B ̈uhlmann and van de Geer 2011, Chapter 6). Thus if‖ <em>β</em> ∗‖ 1 = <em>o</em> (</p><h6 id="√-1"><a class="header-anchor" href="#√-1" aria-hidden="true">#</a> √</h6><p><em>N/</em> log( <em>p</em> )) then the lasso is consistent for prediction. This means that the true parameter vector must be sparse relative to the ratio <em>N/</em> log( <em>p</em> ). The result only assumes that the design <strong>X</strong> is fixed and has no other conditions on <strong>X</strong>. Consistent recovery of the nonzero support set requires more stringent assumptions on the level of cross-correlation between the predictors inside and outside of the support set. Details are given in Chapter 11.</p><h3 id="_2-8-the-nonnegative-garrote"><a class="header-anchor" href="#_2-8-the-nonnegative-garrote" aria-hidden="true">#</a> 2.8 The Nonnegative Garrote</h3><p>The <em>nonnegative garrote</em> (Breiman 1995)^5 is a two-stage procedure, with a close relationship to the lasso.^6 Given an initial estimate of the regression coefficients <em>β</em> ̃∈R <em>p</em> , we then solve the optimization problem</p><div class="language-"><pre><code>minimize\nc ∈R p\n</code></pre></div><h6 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> </h6><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><h6 id="∑-n-12"><a class="header-anchor" href="#∑-n-12" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><h6 id="-16"><a class="header-anchor" href="#-16" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>yi −\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>cjxijβ ̃ j\n</code></pre></div><h6 id="_2-1"><a class="header-anchor" href="#_2-1" aria-hidden="true">#</a> ) 2</h6><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>subject to c 0 and‖ c ‖ 1 ≤ t,\n</code></pre></div><h6 id="_2-19"><a class="header-anchor" href="#_2-19" aria-hidden="true">#</a> (2.19)</h6><p>where <em>c</em> 0 means the vector has nonnegative coordinates. Finally, we set <em>β</em> ̂ <em>j</em> = ˆ <em>cj</em> · <em>β</em> ̃ <em>j, j</em> = 1 <em>,...,p.</em> There is an equivalent Lagrangian form for this procedure, using a penalty <em>λ</em> ‖ <em>c</em> ‖ 1 for some regularization weight <em>λ</em> ≥0, plus the nonnegativity constraints.</p><p>(^5) A garrote is a device used for execution by strangulation or by breaking the neck. It is a Spanish word, and is alternately spelled <em>garrotte</em> or <em>garotte</em>. We are using the spelling in the original paper of Breiman (1995). (^6) Breiman’s paper was the inspiration for Tibshirani’s 1996 lasso paper.</p><h6 id="the-nonnegative-garrote-21"><a class="header-anchor" href="#the-nonnegative-garrote-21" aria-hidden="true">#</a> THE NONNEGATIVE GARROTE 21</h6><p>In the original paper (Breiman 1995), the initial <em>β</em> ̃ was chosen to be the ordinary-least-squares (OLS) estimate. Of course, when <em>p &gt; N</em> , these estimates are not unique; since that time, other authors (Yuan and Lin 2007 <em>b</em> , Zou 2006) have shown that the nonnegative garrote has attrac- tive properties when we use other initial estimators such as the lasso, ridge regression or the elastic net.</p><div class="language-"><pre><code>(0,0) β\n</code></pre></div><div class="language-"><pre><code>Lasso\nGarrote\n</code></pre></div><p><strong>Figure 2.5</strong> <em>Comparison of the shrinkage behavior of the lasso and the nonnegative garrote for a single variable. Since theirλs are on different scales, we used 2 for the lasso and 7 for the garrote to make them somewhat comparable. The garrote shrinks smal ler values ofβmore severely than lasso, and the opposite for larger values.</em></p><p>The nature of the nonnegative garrote solutions can be seen when the columns of <strong>X</strong> are orthogonal. Assuming that <em>t</em> is in the range where the equality constraint‖ <em>c</em> ‖ 1 = <em>t</em> can be satisfied, the solutions have the explicit form</p><div class="language-"><pre><code>̂ cj =\n</code></pre></div><h6 id="-17"><a class="header-anchor" href="#-17" aria-hidden="true">#</a> (</h6><h6 id="_1-−"><a class="header-anchor" href="#_1-−" aria-hidden="true">#</a> 1 −</h6><div class="language-"><pre><code>λ\nβ ̃^2\nj\n</code></pre></div><h6 id="-18"><a class="header-anchor" href="#-18" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>+\n</code></pre></div><div class="language-"><pre><code>, j = 1 ,...,p, (2.20)\n</code></pre></div><p>where <em>λ</em> is chosen so that‖ˆ <em>c</em> ‖ 1 = <em>t</em>. Hence if the coefficient <em>β</em> ̃ <em>j</em> is large, the shrinkage factor will be close to 1 (no shrinkage), but if it is small the estimate will be shrunk toward zero. Figure 2.5 compares the shrinkage behavior of the lasso and nonnegative garrote. The latter exhibits the shrinkage behavior of the nonconvex penalties (next section and Section 4.6). There is also a close relationship between the nonnegative garrote and the <em>adaptive lasso</em> , discussed in Section 4.6; see Exercise 4.26. Following this, Yuan and Lin (2007 <em>b</em> ) and Zou (2006) have shown that the nonnegative garrote is <em>path-consistent</em> under less stringent conditions than the lasso. This holds if the initial estimates are</p><h6 id="√-2"><a class="header-anchor" href="#√-2" aria-hidden="true">#</a> √</h6><p><em>N</em> -consistent, for example those based on least squares (when <em>p &lt; N</em> ), the lasso, or the elastic net. “Path- consistent” means that the solution path contains the true model somewhere in its path indexed by <em>t</em> or <em>λ</em>. On the other hand, the convergence of the parameter estimates from the nonnegative garrote tends to be slower than that of the initial estimate.</p><h6 id="_22-the-lasso-for-linear-models"><a class="header-anchor" href="#_22-the-lasso-for-linear-models" aria-hidden="true">#</a> 22 THE LASSO FOR LINEAR MODELS</h6><div class="language-"><pre><code>Table 2.3 Estimators ofβj from (2.21) in the\ncase of an orthonormal model matrix X.\nq Estimator Formula\n</code></pre></div><div class="language-"><pre><code>0 Best subset β ̃ j ·I[| β ̃ j | &gt;\n</code></pre></div><h6 id="√-3"><a class="header-anchor" href="#√-3" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>2 λ ]\n1 Lasso sign( β ̃ j )(| β ̃ j |− λ )+\n2 Ridge β ̃ j/ (1 + λ )\n</code></pre></div><h3 id="_2-9-q-penalties-and-bayes-estimates"><a class="header-anchor" href="#_2-9-q-penalties-and-bayes-estimates" aria-hidden="true">#</a> 2.9 `q Penalties and Bayes Estimates</h3><p>For a fixed real number <em>q</em> ≥0, consider the criterion</p><div class="language-"><pre><code>minimize\nβ ∈R p\n</code></pre></div><h6 id="-6"><a class="header-anchor" href="#-6" aria-hidden="true">#</a> </h6><h6 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> </h6><h6 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> </h6><h6 id="_1-9"><a class="header-anchor" href="#_1-9" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-7"><a class="header-anchor" href="#_2-n-7" aria-hidden="true">#</a> 2 N</h6><h6 id="∑-n-13"><a class="header-anchor" href="#∑-n-13" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi −\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>xijβj )^2 + λ\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>| βj | q\n</code></pre></div><h6 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> </h6><h6 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> </h6><h6 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> </h6><h6 id="_2-21"><a class="header-anchor" href="#_2-21" aria-hidden="true">#</a> . (2.21)</h6><p>This is the lasso for∑ <em>q</em> = 1 and ridge regression for <em>q</em> = 2. For <em>q</em> = 0, the term <em>p j</em> =1| <em>βj</em> | <em>q</em> counts the number of nonzero elements in <em>β</em> , and so solving (2.21)</p><p>amounts to best-subset selection. Figure 2.6 displays the constraint regions corresponding to these penalties for the case of two predictors ( <em>p</em> = 2). Both</p><div class="language-"><pre><code>q= 4 q= 2 q= 1 q= 0. 5 q= 0. 1\n</code></pre></div><p><strong>Figure 2.6</strong> <em>Constraint regions</em></p><div class="language-"><pre><code>∑ p\nj =1| βj |\n</code></pre></div><div class="language-"><pre><code>q ≤ 1 for different values ofq. Forq &lt; 1 ,\n</code></pre></div><p><em>the constraint region is nonconvex.</em></p><p>the lasso and ridge regression versions of (2.21) amount to solving convex programs, and so scale well to large problems. Best subset selection leads to a nonconvex and combinatorial optimization problem, and is typically not feasible with more than say <em>p</em> = 50 predictors. In the special case of an orthonormal model matrix <strong>X</strong> , all three proce- dures have explicit solutions. Each method applies a simple coordinate-wise transformation to the least-squares estimate <em>β</em> ̃, as detailed in Table 2.9. Ridge regression does a proportional shrinkage. The lasso translates each coefficient by a constant factor <em>λ</em> and truncates at zero, otherwise known as soft thresh- olding. Best-subset selection applies the hard thresholding operator: it leaves the coefficient alone if it is bigger than</p><h6 id="√-4"><a class="header-anchor" href="#√-4" aria-hidden="true">#</a> √</h6><p>2 <em>λ</em> , and otherwise sets it to zero. The lasso is special in that the choice <em>q</em> = 1 is the smallest value of <em>q</em> (closest to best-subset) that leads to a convex constraint region and hence a</p><h6 id="some-perspective-23"><a class="header-anchor" href="#some-perspective-23" aria-hidden="true">#</a> SOME PERSPECTIVE 23</h6><p>convex optimization problem. In this sense, it is the closest convex relaxation of the best-subset selection problem. There is also a Bayesian view of these estimators. Thinking of| <em>βj</em> | <em>q</em> as proportional to the negative log-prior density for <em>βj</em> , the constraint contours represented in Figure 2.6 have the same shape as the equi-contours of the prior distribution of the parameters. Notice that for <em>q</em> ≤1, the prior concentrates more mass in the coordinate directions. The prior corresponding to the <em>q</em> = 1 case is an independent double exponential (or Laplace) distribution for each parameter, with joint density (1 <em>/</em> 2 <em>τ</em> ) exp(−‖ <em>β</em> ‖ 1 ) <em>/τ</em> ) and <em>τ</em> = 1 <em>/λ</em>. This means that the lasso estimate is the Bayesian MAP (maximum <em>aposteriori</em> ) estimator using a Laplacian prior, as opposed to the mean of the posterior distribution, which is not sparse. Similarly, if we sample from the posterior distribution corresponding to the Laplace prior, we do not obtain sparse vectors. In order to obtain sparse vectors via posterior sampling, one needs to start with a prior distribution that puts a point mass at zero. Bayesian approaches to the lasso are explored in Section 6.1.</p><h3 id="_2-10-some-perspective"><a class="header-anchor" href="#_2-10-some-perspective" aria-hidden="true">#</a> 2.10 Some Perspective</h3><p>The lasso uses an <em><code>_ 1 -penalty, and such penalties are now widely used in statis- tics, machine learning, engineering, finance, and other fields. The lasso was proposed by Tibshirani (1996), and was directly inspired by the nonnegative garrote of Breiman (1995). Soft thresholding was popularized earlier in the context of wavelet filtering by Donoho and Johnstone (1994); this is a popular alternative to Fourier filtering in signal processing, being both “local in time and frequency.” Since wavelet bases are orthonormal, wavelet filtering corre- sponds to the lasso in the orthogonal **X** case (Section 2.4.1). Around the same time as the advent of the lasso, Chen, Donoho and Saunders (1998) proposed the closely related _basis pursuit_ method, which extends the ideas of wavelet fil- tering to search for a sparse representation of a signal in over-complete bases using an _</code></em> 1 -penalty. These are unions of orthonormal frames and hence no longer completely mutually orthonormal. Taking a broader perspective, <em><code>_ 1 -regularization has a pretty lengthy his- tory. For example Donoho and Stark (1989) discussed _</code></em> 1 -based recovery in detail, and provided some guarantees for incoherent bases. Even earlier (and mentioned in Donoho and Stark (1989)) there are related works from the 1980s in the geosciences community, for example Oldenburg, Scheuer and Levy (1983) and Santosa and Symes (1986). In the signal processing world, Alliney and Ruzinsky (1994) investigated some algorithmic issues associated with <em><code>_ 1 regularization. And there surely are many other authors who have proposed similar ideas, such as Fuchs (2000). Rish and Grabarnik (2014) pro- vide a modern introduction to sparse methods for machine learning and signal processing. In the last 10–15 years, it has become clear that the _</code></em> 1 -penalty has a number of good properties, which can be summarized as follows:</p><h6 id="_24-the-lasso-for-linear-models"><a class="header-anchor" href="#_24-the-lasso-for-linear-models" aria-hidden="true">#</a> 24 THE LASSO FOR LINEAR MODELS</h6><p><em>Interpretation of the final model:</em> The <em><code>_ 1 -penalty provides a natural way to encourage or enforce sparsity and simplicity in the solution. _Statistical efficiency:_ In the book _The Elements of Statistical Learning_ (Hastie, Tibshirani and Friedman 2009), the authors discuss an informal “ _bet-on-sparsity principle_ .” Assume that the underlying true signal is sparse and we use an _</code></em> 1 penalty to try to recover it. If our assumption is correct, we can do a good job in recovering the true signal. Note that sparsity can hold in the given bases (set of features) or a transformation of the features (e.g., a wavelet bases). But if we are wrong—the underlying truth is not sparse in the chosen bases—then the <em><code>_ 1 penalty will not work well. However in that instance, no method can do well, relative to the Bayes error. There is now a large body of theoretical support for these loose statements: see Chapter 11 for some results. _Computational efficiency:</code></em> 1 -based penalties are convex and this fact and the assumed sparsity can lead to significant computational advantages. If we have 100 observations and one million features, and we have to estimate one million nonzero parameters, then the computation is very challenging. However, if we apply the lasso, then at most 100 parameters can be nonzero in the solution, and this makes the computation much easier. More details are given in Chapter 5.^7 In the remainder of this book, we describe many of the exciting develop- ments in this field.</p><h3 id="exercises"><a class="header-anchor" href="#exercises" aria-hidden="true">#</a> Exercises</h3><p>Ex. 2.1 Show that the smallest value of <em>λ</em> such that the regression coefficients estimated by the lasso are all equal to zero is given by</p><div class="language-"><pre><code>λ max= max\nj\n</code></pre></div><h6 id="-19"><a class="header-anchor" href="#-19" aria-hidden="true">#</a> |</h6><h6 id="_1-10"><a class="header-anchor" href="#_1-10" aria-hidden="true">#</a> 1</h6><h6 id="n-1"><a class="header-anchor" href="#n-1" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>〈 x j, y 〉|.\n</code></pre></div><p>Ex. 2.2 Show that the soft-threshold estimator (2.12) yields the solution to the single predictor lasso problem (2.9). (Do not make use of subgradients, and note that <strong>z</strong> is standardized).</p><p>Ex. 2.3 <em>Soft thresholding and subgradients.</em> Since (2.9) is a convex function, it is guaranteed to have a subgradient (see Chapter 5 for more details), and any optimal solution must satisfy the <em>subgradient</em> equation</p><h6 id="−-1"><a class="header-anchor" href="#−-1" aria-hidden="true">#</a> −</h6><h6 id="_1-11"><a class="header-anchor" href="#_1-11" aria-hidden="true">#</a> 1</h6><h6 id="n-2"><a class="header-anchor" href="#n-2" aria-hidden="true">#</a> N</h6><h6 id="∑-n-14"><a class="header-anchor" href="#∑-n-14" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi − ziβ ) zi + λs = 0 , where s is a subgradient of| β |. (2.22)\n</code></pre></div><p>For the absolute value function, subgradients take the form <em>s</em> ∈sign( <em>β</em> ), mean- ing that <em>s</em> = sign( <em>β</em> ) when <em>β</em> 6 = 0 and <em>s</em> ∈[− 1 <em>,</em> +1] when <em>β</em> = 0. The general</p><p>(^7) Ridge regression also enjoys a similar efficiency in the <em>p</em>  <em>N</em> case.</p><h6 id="exercises-25"><a class="header-anchor" href="#exercises-25" aria-hidden="true">#</a> EXERCISES 25</h6><p>theory of convex optimization, as discussed in Chapter 5, guarantees that any pair ( <em>β,</em> ̂̂ <em>s</em> ) that is a solution to the zero subgradient Equation (2.22) with ̂ <em>s</em> ∈sign( <em>β</em> ̂) defines an optimal solution to the original minimization prob- lem (2.9). Solve Equation (2.22) and hence arrive at solutions (2.10) and (2.11).</p><p>Ex. 2.4 Show that the subgradient equations for Problem (2.5) take the form given in (2.6). Hence derive expressions for coordinate descent steps (2.14) and (2.15).</p><p>Ex. 2.5 <em>Uniqueness of fitted values from the lasso.</em> For some <em>λ</em> ≥0, suppose that we have two lasso solutions <em>β,</em> ̂̂ <em>γ</em> with common optimal value <em>c</em> ∗.</p><div class="language-"><pre><code>(a) Show that it must be the case that X β ̂= X ̂ γ , meaning that the two\nsolutions must yield the same predicted values. ( Hint: If not, then use the\nstrict convexity of the function f ( u ) =‖ y − u ‖^22 and convexity of the ` 1 -\nnorm to establish a contradiction.)\n(b) If λ &gt; 0, show that we must have‖ β ̂‖ 1 =‖̂ γ ‖ 1.\n</code></pre></div><p>(Tibshirani 2 2013).</p><p>Ex. 2.6 Here we use the bootstrap as the basis for inference with the lasso.</p><div class="language-"><pre><code>(a) For the crime data, apply the bootstrap to estimate the standard errors\nof the estimated lasso coefficients, as in the middle section of Table 2.2. Use\nthe nonparametric bootstrap, sampling features and outcome values ( xi,yi )\nwith replacement from the observed data. Keep the bound t fixed at its\nestimated value from the original lasso fit. Estimate as well the probability\nthat an estimated coefficient is zero.\n(b) Repeat part (a), but now re-estimate λ ˆfor each bootstrap replication.\nCompare the results to those in part (a).\n</code></pre></div><p>Ex. 2.7 Consider a fixed linear model based on <em>k</em> predictors and fit by least squares. Show that its degrees of freedom (2.17) is equal to <em>k</em>.</p><p>Ex. 2.8 <em>Degrees of freedom for lasso in the orthogonal case.</em> Suppose that <em>yi</em> = <em>β</em> 0 +</p><h6 id="∑-7"><a class="header-anchor" href="#∑-7" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>jxijβj + i where i ∼ N (0 ,σ\n</code></pre></div><p>(^2) ), with the <em>xij</em> fixed (non-random). Assume that the features are centered and also assume they are uncorrelated, so that</p><h6 id="∑-8"><a class="header-anchor" href="#∑-8" aria-hidden="true">#</a> ∑</h6><p><em>ixijxik</em> = 0 for all <em>j,k</em>. Stein’s lemma (Stein 1981) states that for <em>Y</em> ∼ <em>N</em> ( <em>μ,σ</em>^2 ) and all absolutely continuous functions <em>g</em> such thatE| <em>g</em> ′( <em>Y</em> )| <em>&lt;</em> ∞,</p><div class="language-"><pre><code>E( g ( Y )( Y − μ )) = σ^2 E( g ′( Y )). (2.23)\n</code></pre></div><p>Use this to show that the degrees of freedom (2.17) for the lasso in the or- thogonal case is equal to <em>k</em> , the number of nonzero estimated coefficients in the solution.</p><h6 id="_26-the-lasso-for-linear-models"><a class="header-anchor" href="#_26-the-lasso-for-linear-models" aria-hidden="true">#</a> 26 THE LASSO FOR LINEAR MODELS</h6><p>Ex. 2.9 Derive the solutions (2.20) to the nonnegative garrote criterion (2.19).</p><p>Ex. 2.10 <em>Robust regression view of lasso.</em> Consider a robust version of the standard linear regression problem, in which we wish to protect ourselves against perturbations of the features. In order to do so, we consider the min- max criterion</p><div class="language-"><pre><code>minimize\nβ\nmax\n∆ ∈U\n</code></pre></div><h6 id="-20"><a class="header-anchor" href="#-20" aria-hidden="true">#</a> {</h6><h6 id="_1-12"><a class="header-anchor" href="#_1-12" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-8"><a class="header-anchor" href="#_2-n-8" aria-hidden="true">#</a> 2 N</h6><div class="language-"><pre><code>‖ y −( X + ∆ ) β ‖^22\n</code></pre></div><h6 id="-21"><a class="header-anchor" href="#-21" aria-hidden="true">#</a> }</h6><h6 id="_2-24"><a class="header-anchor" href="#_2-24" aria-hidden="true">#</a> , (2.24)</h6><p>where the allowable perturbations <strong>∆</strong> : = ( <strong><em>δ</em></strong> 1 <em>,...,</em> <strong><em>δ</em></strong> <em>p</em> ) belong to the subset of R <em>N</em> × <em>p</em></p><div class="language-"><pre><code>U: =\n</code></pre></div><h6 id="-22"><a class="header-anchor" href="#-22" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>( δ 1 , δ 2 ,... δ p ) | ‖ δ j ‖ 2 ≤ cj for all j = 1 , 2 ,...,p\n</code></pre></div><h6 id="-23"><a class="header-anchor" href="#-23" aria-hidden="true">#</a> }</h6><h6 id="_2-25"><a class="header-anchor" href="#_2-25" aria-hidden="true">#</a> . (2.25)</h6><p>Hence each feature value <em>xij</em> can be perturbed by a maximum amount <em>cj</em> , with the <em>`</em> 2 -norm of the overall perturbation vector for that feature bounded by <em>cj</em>. The perturbations for different features also act independently of one another. We seek the coefficients that minimize squared error under the “worst” allow- able perturbation of the features. We assume that both <strong>y</strong> and the columns of <strong>X</strong> have been standardized, and have not included an intercept. Show that the solution to this problem is equivalent to</p><div class="language-"><pre><code>min\nβ ∈R p\n</code></pre></div><h6 id="-7"><a class="header-anchor" href="#-7" aria-hidden="true">#</a> </h6><h6 id="-6"><a class="header-anchor" href="#-6" aria-hidden="true">#</a> </h6><h6 id="-6"><a class="header-anchor" href="#-6" aria-hidden="true">#</a> </h6><h6 id="_1-13"><a class="header-anchor" href="#_1-13" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-9"><a class="header-anchor" href="#_2-n-9" aria-hidden="true">#</a> 2 N</h6><div class="language-"><pre><code>‖ y − X β ‖^22 +\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>cj | βj |\n</code></pre></div><h6 id="-6"><a class="header-anchor" href="#-6" aria-hidden="true">#</a> </h6><h6 id="-6"><a class="header-anchor" href="#-6" aria-hidden="true">#</a> </h6><h6 id="-6"><a class="header-anchor" href="#-6" aria-hidden="true">#</a> </h6><h6 id="_2-26"><a class="header-anchor" href="#_2-26" aria-hidden="true">#</a> . (2.26)</h6><p>In the special case <em>cj</em> = <em>λ</em> for all <em>j</em> = 1 <em>,</em> 2 <em>,...,p</em> , we thus obtain the lasso, so that it can be viewed as a method for guarding against uncertainty in the measured predictor values, with more uncertainty leading to a greater amount of shrinkage. (See Xu, Caramanis and Mannor (2010) for further details.)</p><p>Ex. 2.11 <em>Robust regression and constrained optimization.</em> This exercise doesn’t involve the lasso itself, but rather a related use of the <em>`</em> 1 -norm in regression. We consider the model</p><div class="language-"><pre><code>yi =\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>xijβj + γi + i\n</code></pre></div><p>with <em>i</em> ∼ <em>N</em> (0 <em>,σ</em>^2 ) and <em>γi, i</em> = 1 <em>,</em> 2 <em>,...,N</em> are unknown constants. Let <strong><em>γ</em></strong> = ( <em>γ</em> 1 <em>,γ</em> 2 <em>,...,γN</em> ) and consider minimization of</p><div class="language-"><pre><code>minimize\nβ ∈R p, γ ∈R N\n</code></pre></div><h6 id="_1-14"><a class="header-anchor" href="#_1-14" aria-hidden="true">#</a> 1</h6><h6 id="_2-6"><a class="header-anchor" href="#_2-6" aria-hidden="true">#</a> 2</h6><h6 id="∑-n-15"><a class="header-anchor" href="#∑-n-15" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi −\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>xijβj − γi )^2 + λ\n</code></pre></div><h6 id="∑-n-16"><a class="header-anchor" href="#∑-n-16" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>| γi |. (2.27)\n</code></pre></div><p>The idea is that for each <em>i</em> , <em>γi</em> allows <em>yi</em> to be an outlier; setting <em>γi</em> = 0 means that the observation is not deemed an outlier. The penalty term effectively limits the number of outliers.</p><h6 id="exercises-27"><a class="header-anchor" href="#exercises-27" aria-hidden="true">#</a> EXERCISES 27</h6><div class="language-"><pre><code>(a) Show this problem is jointly convex in β and γ.\n(b) Consider Huber’s loss function\n</code></pre></div><div class="language-"><pre><code>ρ ( t ; λ ) =\n</code></pre></div><h6 id="-24"><a class="header-anchor" href="#-24" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>λ | t |− λ^2 / 2 if| t | &gt; λ\nt^2 / 2 if| t |≤ λ.\n</code></pre></div><h6 id="_2-28"><a class="header-anchor" href="#_2-28" aria-hidden="true">#</a> (2.28)</h6><div class="language-"><pre><code>This is a tapered squared-error loss; it is quadratic for| t | ≤ λ but linear\noutside of that range, to reduce the effect of outliers on the estimation of β.\nWith the scale parameter σ fixed at one, Huber’s robust regression method\nsolves\n</code></pre></div><div class="language-"><pre><code>minimize\nβ ∈R p\n</code></pre></div><h6 id="∑-n-17"><a class="header-anchor" href="#∑-n-17" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>ρ ( yi −\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>xijβj ; λ ). (2.29)\n</code></pre></div><div class="language-"><pre><code>Show that problems (2.27) and (2.29) have the same solutions β ̂. (Antoniadis\n2007, Gannaz 2007, She and Owen 2011).\n</code></pre></div><div class="language-"><pre><code>Chapter 3\n</code></pre></div><h2 id="generalized-linear-models"><a class="header-anchor" href="#generalized-linear-models" aria-hidden="true">#</a> Generalized Linear Models</h2><p>In Chapter 2, we focused exclusively on linear regression models fit by least squares. Such linear models are suitable when the response variable is quan- titative, and ideally when the error distribution is Gaussian. However, other types of response arise in practice. For instance, binary variables can be used to indicate the presence or absence of some attribute (e.g., “cancerous” versus “normal” cells in a biological assay, or “clicked” versus “not clicked” in web browsing analysis); here the binomial distribution is more appropriate. Some- times the response occurs as counts (e.g., number of arrivals in a queue, or number of photons detected); here the Poisson distribution might be called for. In this chapter, we discuss generalizations of simple linear models and the lasso that are suitable for such applications.</p><h3 id="_3-1-introduction"><a class="header-anchor" href="#_3-1-introduction" aria-hidden="true">#</a> 3.1 Introduction</h3><p>With a binary response coded in the form <em>Y</em> ∈{ 0 <em>,</em> 1 }, the linear logistic model is often used: it models the log-likelihood ratio as the linear combination</p><div class="language-"><pre><code>log\n</code></pre></div><div class="language-"><pre><code>Pr( Y = 1| X = x )\nPr( Y = 0| X = x )\n</code></pre></div><div class="language-"><pre><code>= β 0 + βTx, (3.1)\n</code></pre></div><p>where <em>X</em> = ( <em>X</em> 1 <em>,X</em> 2 <em>,...Xp</em> ) is a vector of predictors, <em>β</em> 0 ∈Ris an intercept term, and <em>β</em> ∈R <em>p</em> is a vector of regression coefficients. Inverting this transfor- mation yields an expression for the conditional probability</p><div class="language-"><pre><code>Pr( Y = 1| X = x ) =\neβ^0 + β\n</code></pre></div><div class="language-"><pre><code>Tx\n</code></pre></div><div class="language-"><pre><code>1 + eβ^0 + βTx\n</code></pre></div><h6 id="_3-2"><a class="header-anchor" href="#_3-2" aria-hidden="true">#</a> . (3.2)</h6><p>By inspection, without any restriction on the parameters ( <em>β</em> 0 <em>,β</em> ), the model specifies probabilities lying in (0 <em>,</em> 1). We typically fit logistic models by maxi- mizing the binomial log-likelihood of the data. The logit transformation (3.1) of the conditional probabilities is an exam- ple of a <em>link function</em>. In general, a link function is a transformation of the conditional meanE[ <em>Y</em> | <em>X</em> = <em>x</em> ]—in this case, the conditional probability that <em>Y</em> = 1—to a more natural scale on which the parameters can be fit without constraints. As another example, if the response <em>Y</em> represents counts, taking</p><div class="language-"><pre><code>29\n</code></pre></div><h6 id="_30-generalized-linear-models"><a class="header-anchor" href="#_30-generalized-linear-models" aria-hidden="true">#</a> 30 GENERALIZED LINEAR MODELS</h6><p>values in{ 0 <em>,</em> 1 <em>,</em> 2 <em>,...</em> }, then we need to ensure that the conditional mean is positive. A natural choice is the log-linear model</p><div class="language-"><pre><code>logE[ Y | X = x ] = β 0 + βTx, (3.3)\n</code></pre></div><p>with its log link function. Here we fit the parameters by maximizing the Pois- son log-likelihood of the data. The models (3.1) and (3.3) are both special cases of <em>generalized linear mod- els</em> (McCullagh and Nelder 1989). These models describe the response variable using a member of the <em>exponential family</em> , which includes the Bernoulli, Pois- son, and Gaussian as particular cases. A transformed version of the response meanE[ <em>Y</em> | <em>X</em> = <em>x</em> ] is then approximated by a linear model. In detail, if we use <em>μ</em> ( <em>x</em> ) =E[ <em>Y</em> | <em>X</em> = <em>x</em> ] to denote the conditional mean of <em>Y</em> given <em>X</em> = <em>x</em> , then a GLM is based on a model of the form</p><div class="language-"><pre><code>g\n</code></pre></div><h6 id="-25"><a class="header-anchor" href="#-25" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>μ ( x )\n</code></pre></div><h6 id="-26"><a class="header-anchor" href="#-26" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>= β 0 + βTx\n︸ ︷︷ ︸\nη ( x )\n</code></pre></div><h6 id="_3-4"><a class="header-anchor" href="#_3-4" aria-hidden="true">#</a> , (3.4)</h6><p>where <em>g</em> :R→Ris a strictly monotonic link function. For example, for a bi- nary response <em>Y</em> ∈{ 0 <em>,</em> 1 }, the logistic regression model is based on the choices <em>μ</em> ( <em>x</em> ) = Pr[ <em>Y</em> = 1| <em>X</em> = <em>x</em> ] and <em>g</em> ( <em>μ</em> ) = logit( <em>μ</em> ) = log</p><h6 id="-27"><a class="header-anchor" href="#-27" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>μ/ (1− μ )\n</code></pre></div><h6 id="-28"><a class="header-anchor" href="#-28" aria-hidden="true">#</a> )</h6><p>. When the response variable is modeled as a Gaussian, the choices <em>μ</em> ( <em>x</em> ) = <em>β</em> 0 + <em>βTx</em> and <em>g</em> ( <em>μ</em> ) = <em>μ</em> recover the standard linear model, as discussed in the previous chapter. Generalized linear models can also be used to model the multicategory responses that occur in many problems, including handwritten digit classifi- cation, speech-recognition, document classification, and cancer classification. The multinomial replaces the binomial distribution here, and we use a sym- metric log-linear representation:</p><div class="language-"><pre><code>Pr[ Y = k | X = x ] =\n</code></pre></div><div class="language-"><pre><code>eβ^0 k + β\n</code></pre></div><div class="language-"><pre><code>Tkx\n∑ K\n` =1 e\n</code></pre></div><div class="language-"><pre><code>β 0 ` + βT`x. (3.5)\n</code></pre></div><p>Here there are <em>K</em> coefficients for each variable (one per class). In this chapter, we discuss approaches to fitting generalized linear models that are based on maximizing the likelihood, or equivalently minimizing the negative log-likelihood along with an <em>`</em> 1 -penalty</p><div class="language-"><pre><code>minimize\nβ 0 ,β\n</code></pre></div><h6 id="-29"><a class="header-anchor" href="#-29" aria-hidden="true">#</a> {</h6><h6 id="−-2"><a class="header-anchor" href="#−-2" aria-hidden="true">#</a> −</h6><h6 id="_1-15"><a class="header-anchor" href="#_1-15" aria-hidden="true">#</a> 1</h6><h6 id="n-3"><a class="header-anchor" href="#n-3" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>L( β 0 ,β ; y , X ) + λ ‖ β ‖ 1\n</code></pre></div><h6 id="-30"><a class="header-anchor" href="#-30" aria-hidden="true">#</a> }</h6><h6 id="_3-6"><a class="header-anchor" href="#_3-6" aria-hidden="true">#</a> . (3.6)</h6><p>Here <strong>y</strong> is the <em>N</em> -vector of outcomes and <strong>X</strong> is the <em>N</em> × <em>p</em> matrix of predictors, and the specific form of the log-likelihoodLvaries according to the GLM. In the special case of Gaussian responses and the standard linear model, we have L( <em>β</em> 0 <em>,β</em> ; <strong>y</strong> <em>,</em> <strong>X</strong> ) = 21 <em>σ</em> 2 ‖ <strong>y</strong> − <em>β</em> 0 <strong>1</strong> − <strong>X</strong> <em>β</em> ‖^22 + <em>c</em> , where <em>c</em> is a constant independent of ( <em>β</em> 0 <em>,β</em> ), so that the optimization problem (3.6) corresponds to the ordinary linear least-squares lasso.</p><h6 id="logistic-regression-31"><a class="header-anchor" href="#logistic-regression-31" aria-hidden="true">#</a> LOGISTIC REGRESSION 31</h6><p>Similar forms of <em>`</em> 1 -regularization are also useful for related models. With survival models, the response is the time to failure (death), with possible censoring if subjects are lost to followup. In this context, a popular choice is the Cox proportional hazards model, which takes the form</p><div class="language-"><pre><code>h ( t | x ) = h 0 ( t ) eβ\n</code></pre></div><div class="language-"><pre><code>Tx\n</code></pre></div><p><em>.</em> (3.7)</p><p>Here <em>t</em> 7→ <em>h</em> ( <em>t</em> | <em>x</em> ) is the <em>hazard function</em> for an individual with covariates <em>x</em> : the value <em>h</em> ( <em>t</em> | <em>x</em> ) corresponds to the instantaneous probability of failure at time <em>Y</em> = <em>t</em> , given survival up to time <em>t</em>. The function <em>h</em> 0 specifies the baseline hazard, corresponding to <em>x</em> = 0. As another example, the support-vector machine (SVM) is a popular clas- sifier in the machine-learning community. Here the goal is to predict a two- class response <em>y</em> ∈{− 1 <em>,</em> +1},^1 in the simplest case using a linear classification boundary of the form <em>f</em> ( <em>x</em> ) = <em>β</em> 0 + <em>βTx</em> , with the predicted class given by sign( <em>f</em> ( <em>x</em> )). Thus, the correctness of a given decision can be determined by checking whether or not the margin <em>yf</em> ( <em>x</em> ) is positive. The traditional <em>soft- margin</em> linear SVM is fit by solving the optimization problem^2</p><div class="language-"><pre><code>minimize\nβ 0 ,β\n</code></pre></div><h6 id="-8"><a class="header-anchor" href="#-8" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="_1-16"><a class="header-anchor" href="#_1-16" aria-hidden="true">#</a> 1</h6><h6 id="n-4"><a class="header-anchor" href="#n-4" aria-hidden="true">#</a> N</h6><h6 id="∑-n-18"><a class="header-anchor" href="#∑-n-18" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>[1− yif ( xi )]+\n︸ ︷︷ ︸\nφ ( yif ( xi ))\n</code></pre></div><div class="language-"><pre><code>+ λ ‖ β ‖^22\n</code></pre></div><h6 id="-7"><a class="header-anchor" href="#-7" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id="_3-8"><a class="header-anchor" href="#_3-8" aria-hidden="true">#</a> . (3.8)</h6><p>The first term, known as <em>hinge loss</em> , is designed to penalize the negative mar- gins that represent incorrect classifications. In general, an optimal solution vector <em>β</em> ∈R <em>p</em> to the standard linear SVM (3.8) is not sparse, since the quadratic penalty has no sparsity-enforcing properties. However, replacing the quadratic penalty by the <em><code>_ 1 -norm‖ _β_ ‖ 1 leads to an _</code></em> 1 linear SVM, which does produce sparse solutions. In the following sections, we discuss each of these models in more detail. In each case, we provide examples of their applications, discuss some of the issues that arise, as well as computational approaches for fitting the models.</p><h3 id="_3-2-logistic-regression"><a class="header-anchor" href="#_3-2-logistic-regression" aria-hidden="true">#</a> 3.2 Logistic Regression</h3><p>Logistic regression has been popular in biomedical research for half a century, and has recently gained popularity for modeling a wider range of data. In the high-dimensional setting, in which the number of features <em>p</em> is larger than the sample size, it cannot be used without modification. When <em>p &gt; N</em> , any linear model is over-parametrized, and regularization is needed to achieve a stable fit. Such high-dimensional models arise in various applications. For example, document classification problems can involve binary features (presence versus</p><p>(^1) For SVMs, it is convenient to code the binary response via the sign function. (^2) This is not the most standard way to introduce the support vector machine. We discuss this topic in more detail in Section 3.6.</p><h6 id="_32-generalized-linear-models"><a class="header-anchor" href="#_32-generalized-linear-models" aria-hidden="true">#</a> 32 GENERALIZED LINEAR MODELS</h6><p>absence) over a predefined dictionary of <em>p</em> = 20 <em>,</em> 000 or more words and tokens. Another example is genome-wide association studies (GWAS), where we have genotype measurements at <em>p</em> = 500 <em>,</em> 000 or more “SNPs,” and the response is typically the presence/absence of a disease. A SNP (pronounced “snip”) is a single-nucleotide polymorphism, and is typically represented as a three-level factor with possible values{AA <em>,</em> Aa <em>,</em> aa}, where “A” refers to the wild-type, and “a” the mutation. When the response is binary, it is typically coded as 0/1. Attention then focuses on estimating the conditional probability Pr( <em>Y</em> = 1| <em>X</em> = <em>x</em> ) =E[ <em>Y</em> | <em>X</em> = <em>x</em> ]. Given the logistic model (3.1), the negative log likelihood with <em>`</em> 1 - regularization takes the form</p><h6 id="−-3"><a class="header-anchor" href="#−-3" aria-hidden="true">#</a> −</h6><h6 id="_1-17"><a class="header-anchor" href="#_1-17" aria-hidden="true">#</a> 1</h6><h6 id="n-5"><a class="header-anchor" href="#n-5" aria-hidden="true">#</a> N</h6><h6 id="∑-n-19"><a class="header-anchor" href="#∑-n-19" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>{ yi log Pr( Y = 1| xi ) + (1− yi ) log Pr( Y = 0| xi )}+ λ ‖ β ‖ 1\n</code></pre></div><h6 id="−-4"><a class="header-anchor" href="#−-4" aria-hidden="true">#</a> =−</h6><h6 id="_1-18"><a class="header-anchor" href="#_1-18" aria-hidden="true">#</a> 1</h6><h6 id="n-6"><a class="header-anchor" href="#n-6" aria-hidden="true">#</a> N</h6><h6 id="∑-n-20"><a class="header-anchor" href="#∑-n-20" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><h6 id="-31"><a class="header-anchor" href="#-31" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>yi ( β 0 + βTxi )−log(1 + eβ^0 + β\n</code></pre></div><div class="language-"><pre><code>Txi\n)\n</code></pre></div><h6 id="-32"><a class="header-anchor" href="#-32" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>+ λ ‖ β ‖ 1. (3.9)\n</code></pre></div><p>In the machine-learning community, it is more common to code the response <em>Y</em> in terms of sign variables{− 1 <em>,</em> +1}rather than{ 0 <em>,</em> 1 }values; when using sign variables, the penalized (negative) log-likelihood has the form</p><h6 id="_1-19"><a class="header-anchor" href="#_1-19" aria-hidden="true">#</a> 1</h6><h6 id="n-7"><a class="header-anchor" href="#n-7" aria-hidden="true">#</a> N</h6><h6 id="∑-n-21"><a class="header-anchor" href="#∑-n-21" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>log(1 + e − yif ( xi ; β^0 ,β )) + λ ‖ β ‖ 1 , (3.10)\n</code></pre></div><p>where <em>f</em> ( <em>xi</em> ; <em>β</em> 0 <em>,β</em> ) : = <em>β</em> 0 + <em>βTxi</em>. For a given covariate-response pair ( <em>x,y</em> ), the product <em>yf</em> ( <em>x</em> ) is referred to as the <em>margin</em> : a positive margin means a correct classification, whereas a negative margin means an incorrect classifi- cation. From the form of the log-likelihood (3.10), we see that maximizing the likelihood amounts to minimizing a loss function monotone decreasing in the margins. We discuss the interplay of the margin and the penalty in Section 3.6.1.</p><h4 id="_3-2-1-example-document-classification"><a class="header-anchor" href="#_3-2-1-example-document-classification" aria-hidden="true">#</a> 3.2.1 Example: Document Classification</h4><p>We illustrate <em>`</em> 1 -regularized logistic regression in a domain where it has gained popularity, namely document classification using the 20-Newsgroups corpus (Lang 1995). We use the particular feature set and class definition defined by Koh, Kim and Boyd (2007).^3 There are <em>N</em> = 11 <em>,</em> 314 documents and <em>p</em> = 777 <em>,</em> 811 features, with 52% in the positive class. Only 0_._ 05% of the features are nonzero for any given document.</p><p>(^3) The positive class consists of the 10 groups with names of the formsci.*,comp.*and misc.forsale, and the rest are the negative class. The feature set consists of trigrams, with message headers skipped, no stoplist, and features with less than two documents omitted.</p><h6 id="logistic-regression-33"><a class="header-anchor" href="#logistic-regression-33" aria-hidden="true">#</a> LOGISTIC REGRESSION 33</h6><div class="language-"><pre><code>0.0 0.2 0.4 0.6 0.8\n</code></pre></div><div class="language-"><pre><code>−4\n</code></pre></div><div class="language-"><pre><code>−2\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>Fraction Deviance Explained\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>0 66 246 1061 3176\n</code></pre></div><p><strong>Figure 3.1</strong> <em>Coefficient paths for an`</em> 1 <em>-regularized logistic regression for a document- classification task—the “NewsGroup” data. There are 11K documents roughly divided into two classes, and</em> 0_._ 78 <em>M features. Only</em> 0_._ 05 <em>% of the features are nonzero. The coefficients are plotted as a function of the fraction of nul l deviance explained.</em></p><p>Figure 3.1 shows the coefficient profile, computed using the R package glmnet. Although the solutions were computed at 100 values of <em>λ</em> , uniformly spaced on the log scale, we have indexed the solutions by the <em>fraction of deviance explained</em>^4 on the training data:</p><div class="language-"><pre><code>D^2 λ =\n</code></pre></div><div class="language-"><pre><code>Devnull−Dev λ\nDevnull\n</code></pre></div><h6 id="_3-11"><a class="header-anchor" href="#_3-11" aria-hidden="true">#</a> . (3.11)</h6><p>Here the deviance Dev <em>λ</em> is defined as minus twice the difference in the log- likelihood for a model fit with parameter <em>λ</em> and the “saturated” model (having ̂ <em>y</em> = <em>yi</em> ). Devnullis the null deviance computed at the constant (mean) model. Since for these data the classes are separable, the range of <em>λ</em> is chosen so as not to get too close to the saturated fit (where the coefficients would be undefined; see the next section). The maximum number of nonzero coefficients in any of these models can be shown to be min( <em>N,p</em> ), which is equal 11 <em>,</em> 314 in this case. In Figure 3.1, the largest model actually had only 5 <em>,</em> 277 coefficients sinceglmnetdid not go to the very end of the solution path. Although it might seem more natural to plot against the log( <em>λ</em> ) sequence, or perhaps‖ <em>β</em> ˆ( <em>λ</em> )‖ 1 , there are problems with both in the <em>p</em>  <em>N</em> setting. The former quantity is data and problem dependent,</p><p>(^4) the name D (^2) is by analogy with R (^2) , the fraction of variance explained in regression.</p><h6 id="_34-generalized-linear-models"><a class="header-anchor" href="#_34-generalized-linear-models" aria-hidden="true">#</a> 34 GENERALIZED LINEAR MODELS</h6><div class="language-"><pre><code>0.0 0.2 0.4 0.6 0.8\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.1\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.3\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.5\n</code></pre></div><div class="language-"><pre><code>Fraction Deviance Explained\n</code></pre></div><div class="language-"><pre><code>Misclassification Error\n</code></pre></div><div class="language-"><pre><code>0 7 28 95 169 352 756 1785 3502\n10−fold CV\nTrain\n</code></pre></div><div class="language-"><pre><code>0.0 0.2 0.4 0.6 0.8\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>1.2\n</code></pre></div><div class="language-"><pre><code>1.4\n</code></pre></div><div class="language-"><pre><code>Fraction Deviance Explained\n</code></pre></div><div class="language-"><pre><code>Binomial Deviance\n</code></pre></div><div class="language-"><pre><code>0 7 28 95 169 352 756 1785 3502\n</code></pre></div><p><strong>Figure 3.2</strong> <em>Lasso (`</em> 1 <em>)-penalized logistic regression. Tenfold cross-validation curves for the Newsgroup data are shown in red, along with pointwise standard-error bands (not visible). The left plot shows misclassification error; the right plot shows deviance. Also shown in blue is the training error for each of these measures. The number of nonzero coefficients in each model is shown along the top of each plot.</em></p><p>and gives no indication of the amount of overfitting, whereas for the latter measure, the graph would be dominated by the less interesting right-hand side, in which the coefficients and hence their norm explode. Figure 3.2 shows the results of tenfold cross-validation for these data, as well as training error. These are also indexed by the fraction of deviance explained on the training data. Figure 3.3 shows the analogous results to those in Figure 3.2, for ridge regression. The cross-validated error rates are about the same as for the lasso. The number of nonzero coefficients in every model is <em>p</em> = 777 <em>,</em> 811 compared to a maximum of 5 <em>,</em> 277 in Figure 3.2. However the rank of the ridge regression fitted values is actually min( <em>N,p</em> ) which equals 11 <em>,</em> 314 in this case, not much different from that of the lasso fit. Nonetheless, ridge regression might be more costly from a computational viewpoint. We produced the cross-validation results in Figure 3.3 using theglmnetpackage; for ridge the tenfold cross-validation took 8.3 minutes, while for lasso under one minute. A different approach would be to use the kernel trick (Hastie and Tibshirani 2004, for example), but this requires a singular value or similar decomposition of an 11 <em>,</em> 314 × 11 <em>,</em> 314 matrix. For this example, using the packageglmnet, we fit the regularization path in Figure 3.1 at 100 values of <em>λ</em> in 5 secs on a 2.6 GHz Macbook Pro. In ex- amples like this with so many features, dramatic speedups can be achieved by screening the features. For example, the first feature to enter the regulariza- tion path achieves <em>λ</em> max= max <em>j</em> |〈 <em>xj,</em> <strong>y</strong> − <strong>p</strong> ̄〉|, where <strong>y</strong> is the vector of binary</p><h6 id="logistic-regression-35"><a class="header-anchor" href="#logistic-regression-35" aria-hidden="true">#</a> LOGISTIC REGRESSION 35</h6><div class="language-"><pre><code>0.3 0.4 0.5 0.6 0.7 0.8 0.9\n</code></pre></div><div class="language-"><pre><code>0.00\n</code></pre></div><div class="language-"><pre><code>0.02\n</code></pre></div><div class="language-"><pre><code>0.04\n</code></pre></div><div class="language-"><pre><code>0.06\n</code></pre></div><div class="language-"><pre><code>0.08\n</code></pre></div><div class="language-"><pre><code>0.10\n</code></pre></div><div class="language-"><pre><code>0.12\n</code></pre></div><div class="language-"><pre><code>Fraction Deviance Explained\n</code></pre></div><div class="language-"><pre><code>Misclassification Error\n</code></pre></div><div class="language-"><pre><code>10−fold CV\nTrain\n</code></pre></div><div class="language-"><pre><code>0.3 0.4 0.5 0.6 0.7 0.8 0.9\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>Fraction Deviance Explained\n</code></pre></div><div class="language-"><pre><code>Binomial Deviance\n</code></pre></div><p><strong>Figure 3.3</strong> <em>Ridge (`</em> 2 <em>)-penalized logistic regression: tenfold cross validation curves for the Newsgroup data are shown in red, along with pointwise standard-error bands. The left plot shows misclassification error; the right plot shows deviance. Also shown in blue is the training error for each of these measures.</em></p><p>outcomes, and <strong>p</strong> ̄= 0_._ 52 <strong>1</strong> is a vector of the overall mean. This is the entry value for <em>λ</em> ; that is the smallest value for which all coefficients are zero. When computing the solution path from <em>λ</em> maxdown to a slightly lower value <em>λ</em> 1 , we can screen out the vast majority of variables for which this inner-product is substantially lower than <em>λ</em> 1. Once we have computed the solution with the much smaller subset, we can check if any those screened were omitted in er- ror. This can be repeated as we move down the path, using inner-products with the current residuals. This “strong-rule” screening is implemented in theglmnetpackage that we used for the computations in the above example. We discuss strong rules and other computational speedups in more detail in Section 5.10.</p><h4 id="_3-2-2-algorithms"><a class="header-anchor" href="#_3-2-2-algorithms" aria-hidden="true">#</a> 3.2.2 Algorithms</h4><p>Two-class logistic regression is a popular generalization of linear regression, and as a consequence much effort has gone into fitting lasso-penalized logistic models. The objective (3.9) is convex and the likelihood part is differentiable, so in principle finding a solution is a standard task in convex optimization (Koh et al. 2007). Coordinate descent is both attractive and efficient for this problem, and in the bibliographic notes we give a partial account of the large volume of research on this approach; see also Sections 2.4.2 and 5.4. Theglmnetpackage uses a proximal-Newton iterative approach, which repeatedly approximates the negative log-likelihood by a quadratic function (Lee, Sun and Saunders 2014).</p><h6 id="_36-generalized-linear-models"><a class="header-anchor" href="#_36-generalized-linear-models" aria-hidden="true">#</a> 36 GENERALIZED LINEAR MODELS</h6><p>In detail, with the current estimate ( <em>β</em> ̃ 0 <em>,β</em> ̃), we form the quadratic function</p><div class="language-"><pre><code>Q ( β 0 ,β ) =\n</code></pre></div><h6 id="_1-20"><a class="header-anchor" href="#_1-20" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-10"><a class="header-anchor" href="#_2-n-10" aria-hidden="true">#</a> 2 N</h6><h6 id="∑-n-22"><a class="header-anchor" href="#∑-n-22" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>wi ( zi − β 0 − βTxi )^2 + C ( β ̃ 0 ,β ̃) , (3.12)\n</code></pre></div><p>where <em>C</em> denotes a constant independent of ( <em>β</em> 0 <em>,β</em> ), and</p><div class="language-"><pre><code>zi = β ̃ 0 + β ̃ Txi +\n</code></pre></div><div class="language-"><pre><code>yi − p ̃( xi )\np ̃( xi )(1− p ̃( xi ))\n</code></pre></div><div class="language-"><pre><code>, and wi = ̃ p ( xi )(1− ̃ p ( xi )) , (3.13)\n</code></pre></div><p>with ̃ <em>p</em> ( <em>xi</em> ) being the current estimate for Pr( <em>Y</em> = 1| <em>X</em> = <em>xi</em> ). Each outer loop then amounts to a weighted lasso regression. By using warm starts on a fine grid of values for <em>λ</em> , typically only a few outer-loop iterations are required, because locally the quadratic approximation is very good. We discuss some of the features ofglmnetin Sections 3.7 and 5.4.2.</p><h3 id="_3-3-multiclass-logistic-regression"><a class="header-anchor" href="#_3-3-multiclass-logistic-regression" aria-hidden="true">#</a> 3.3 Multiclass Logistic Regression</h3><p>Some classification and discrimination problems have <em>K &gt;</em> 2 output classes. In machine learning a popular approach is to build all</p><h6 id="k"><a class="header-anchor" href="#k" aria-hidden="true">#</a> ( K</h6><div class="language-"><pre><code>2\n</code></pre></div><h6 id="-33"><a class="header-anchor" href="#-33" aria-hidden="true">#</a> )</h6><p>classifiers (“one versus one” or OvO), and then classify to the class that wins the most competitions. Another approach is “one versus all” (OvA) which treats all but one class as the negative examples. Both of these methods can be put on firm theoretical grounds, but also have limitations. OvO can be computationally wasteful, and OvA can suffer from certain masking effects (Hastie et al. 2009, Chapter 4). With multiclass logistic regression, a more natural approach is available. We use the multinomial likelihood and represent the probabilities using the log-linear representation</p><div class="language-"><pre><code>Pr( Y = k | X = x ) =\n</code></pre></div><div class="language-"><pre><code>eβ^0 k + β\n</code></pre></div><div class="language-"><pre><code>T\nkx\n∑ K\n` =1 e\n</code></pre></div><div class="language-"><pre><code>β 0 ` + βT`x. (3.14)\n</code></pre></div><p>This model is over specified, since we can add the linear term <em>γ</em> 0 + <strong><em>γ</em></strong> <em>Tx</em> to the linear model for each class, and the probabilities are unchanged. For this reason, it is customary to set one of the class models to zero—often the last class—leading to a model with <em>K</em> −1 linear functions to estimate (each a contrast with the last class). The model fit by maximum-likelihood is invariant to the choice of this base class, and the parameter estimates are equivariant (the solution for one base can be obtained from the solution for another). Here we prefer the redundant but symmetric approach (3.14), because</p><ul><li>we regularize the coefficients, and the regularized solutions are not equiv- ariant under base changes, and</li><li>the regularization automatically eliminates the redundancy (details below).</li></ul><p>For observations{( <em>xi,yi</em> } <em>Ni</em> =1, we can write the regularized form of the negative</p><h6 id="multiclass-logistic-regression-37"><a class="header-anchor" href="#multiclass-logistic-regression-37" aria-hidden="true">#</a> MULTICLASS LOGISTIC REGRESSION 37</h6><p>log-likelihood as</p><h6 id="−-5"><a class="header-anchor" href="#−-5" aria-hidden="true">#</a> −</h6><h6 id="_1-21"><a class="header-anchor" href="#_1-21" aria-hidden="true">#</a> 1</h6><h6 id="n-8"><a class="header-anchor" href="#n-8" aria-hidden="true">#</a> N</h6><h6 id="∑-n-23"><a class="header-anchor" href="#∑-n-23" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>log Pr( Y = yi | xi ;{ β 0 k,βk } Kk =1) + λ\n</code></pre></div><h6 id="∑-k"><a class="header-anchor" href="#∑-k" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1\n</code></pre></div><div class="language-"><pre><code>‖ βk ‖ 1. (3.15)\n</code></pre></div><p>Denote by <strong>R</strong> the <em>N</em> × <em>K indicator response</em> matrix with elements <em>rik</em> = I( <em>yi</em> = <em>k</em> ). Then we can write the log-likelihood part of the objective (3.15) in the more explicit form</p><div class="language-"><pre><code>1\nN\n</code></pre></div><h6 id="∑-n-24"><a class="header-anchor" href="#∑-n-24" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>wi\n</code></pre></div><h6 id="k-1"><a class="header-anchor" href="#k-1" aria-hidden="true">#</a> [ K</h6><h6 id="∑-9"><a class="header-anchor" href="#∑-9" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>k =1\n</code></pre></div><div class="language-"><pre><code>rik ( β 0 k + βkTxi )−log\n</code></pre></div><h6 id="k-2"><a class="header-anchor" href="#k-2" aria-hidden="true">#</a> { K</h6><h6 id="∑-10"><a class="header-anchor" href="#∑-10" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>k =1\n</code></pre></div><div class="language-"><pre><code>eβ^0 k + β\nkTxi\n</code></pre></div><h6 id="-34"><a class="header-anchor" href="#-34" aria-hidden="true">#</a> }]</h6><h6 id="_3-16"><a class="header-anchor" href="#_3-16" aria-hidden="true">#</a> . (3.16)</h6><p>We have included a weight <em>wi</em> per observation, where the setting <em>wi</em> = 1 <em>/N</em> is the default. This form allows for <em>grouped</em> response data: at each value <em>xi</em> we have a collection of <em>ni</em> multicategory responses, with <em>rik</em> in category <em>k</em>. Alternatively, the rows of <strong>R</strong> can be a vector of class proportions, and we can provide <em>wi</em> = <em>ni</em> as the observation weights. As mentioned, the model probabilities and hence the log-likelihood are in- variant under a constant shift in the <em>K</em> coefficients for each variable <em>xj</em> —in other words{ <em>βkj</em> + <em>cj</em> } <em>Kk</em> =1and{ <em>βkj</em> } <em>Kk</em> =1produce exactly the same probabil- ities. It is therefore up to the penalty in the criterion (3.15) to resolve the choice of <em>cj</em>. Clearly, for any candidate set{ <em>β</em> ̃ <em>kj</em> } <em>Kk</em> =1, the optimal <em>cj</em> should satisfy</p><div class="language-"><pre><code>cj = arg min\nc ∈R\n</code></pre></div><h6 id="k-3"><a class="header-anchor" href="#k-3" aria-hidden="true">#</a> { K</h6><h6 id="∑-11"><a class="header-anchor" href="#∑-11" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>k =1\n</code></pre></div><div class="language-"><pre><code>| β ̃ kj − c |\n</code></pre></div><h6 id="-35"><a class="header-anchor" href="#-35" aria-hidden="true">#</a> }</h6><h6 id="_3-17"><a class="header-anchor" href="#_3-17" aria-hidden="true">#</a> . (3.17)</h6><p>Consequently, as shown in Exercise 3.3, for each <em>j</em> = 1 <em>,...,p</em> , the maximizer of the objective (3.17) is given by the median of{ <em>β</em> ̃ 1 <em>j,...,β</em> ̃ <em>Kj</em> }. Since the inter- cepts{ <em>β</em> 0 <em>k</em> } <em>Kk</em> =1are not penalized, we do need to resolve their indeterminacy; in theglmnetpackage, they are constrained to sum to zero.</p><h4 id="_3-3-1-example-handwritten-digits"><a class="header-anchor" href="#_3-3-1-example-handwritten-digits" aria-hidden="true">#</a> 3.3.1 Example: Handwritten Digits</h4><p>As an illustration, we consider the US post-office handwritten digits data (Le Cun, Boser, Denker, Henderson, Howard, Hubbard and Jackel 1990). There are <em>N</em> = 7291 training images of the digits{ 0 <em>,</em> 1 <em>,...,</em> 9 }, each digitized to a 16×16 gray-scale image. Using the <em>p</em> = 256 pixels as features, we fit a 10-class lasso multinomial model. Figure 3.4 shows the training and test mis- classification error as a function of the sequence of <em>λ</em> values used. In Figure 3.5 we display the coefficients as images (on average about 25% are nonzero). Some of these can be identified as appropriate contrast functionals for highlighting each digit.</p><h6 id="_38-generalized-linear-models"><a class="header-anchor" href="#_38-generalized-linear-models" aria-hidden="true">#</a> 38 GENERALIZED LINEAR MODELS</h6><div class="language-"><pre><code>5e−05 5e−04 5e−03 5e−02\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.1\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.3\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>Misclassification Error\n</code></pre></div><div class="language-"><pre><code>train\ntest\n</code></pre></div><div class="language-"><pre><code>Multinomial Lasso on Zip Code Data\n</code></pre></div><div class="language-"><pre><code>log(λ)\n</code></pre></div><p><strong>Figure 3.4</strong> <em>Training and test misclassification errors of a multinomial lasso model fit to the zip code data, plotted as a function of</em> log( <em>λ</em> )<em>. The minimum test error here is around</em> 0_._ 086 <em>, while the minimum training error is</em> 0_. We highlight the value λ_ = 0_._ 001 <em>, where we examine the individual class coefficients in Figure 3.5.</em></p><p>(^01234) 5 6 7 8 9 <strong>Figure 3.5</strong> <em>Coefficients of the multinomial lasso, displayed as images for each digit class. The gray background image is the average training example for that class. Superimposed in two colors (yel low for positive, blue for negative) are the nonzero coefficients for each class. We notice that they are nonzero in different places, and create discriminant scores for each class. Not all of these are interpretable.</em></p><h6 id="multiclass-logistic-regression-39"><a class="header-anchor" href="#multiclass-logistic-regression-39" aria-hidden="true">#</a> MULTICLASS LOGISTIC REGRESSION 39</h6><h4 id="_3-3-2-algorithms"><a class="header-anchor" href="#_3-3-2-algorithms" aria-hidden="true">#</a> 3.3.2 Algorithms</h4><p>Although one could tackle this problem with standard convex-optimization software, we have found coordinate-descent to be particularly effective (Friedman, Hastie, Simon and Tibshirani 2015). In the two-class case, there is an outer Newton loop and an inner weighted least-squares step. The outer loop can be seen as making a quadratic approximation to the log-likelihood, centered at the current estimates ( <em>β</em> ̃ 0 <em>k,β</em> ̃ <em>k</em> } <em>Kk</em> =1. Here we do the same, except we hold all but one class’s parameters fixed when making this approxima- tion. In detail, when updating the parameters ( <em>β</em> 0 <em><code>,β</code></em> ), we form the quadratic function</p><div class="language-"><pre><code>Q` ( β 0 `,β` ) =−\n</code></pre></div><h6 id="_1-22"><a class="header-anchor" href="#_1-22" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-11"><a class="header-anchor" href="#_2-n-11" aria-hidden="true">#</a> 2 N</h6><h6 id="∑-n-25"><a class="header-anchor" href="#∑-n-25" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>wi` ( zi` − β 0 ` − β`Txi )^2 + C ({ β ̃ 0 k,β ̃ k } Kk =1) , (3.18)\n</code></pre></div><p>where <em>C</em> denotes a constant independent of ( <em>β</em> 0 <em><code>,β</code></em> ), and</p><div class="language-"><pre><code>zi` = β ̃ 0 ` + β ̃ T`xi +\nri` − p ̃ ` ( xi )\np ̃ ` ( xi )(1− p ̃ ` ( xi ))\n</code></pre></div><div class="language-"><pre><code>, and wi` = ̃ p` ( xi )(1− p ̃ ` ( xi ))\n</code></pre></div><p>where ̃ <em>p<code>_ ( _xi_ ) is the current estimate for the conditional probability Pr( _Y_ = _</code></em> | <em>xi</em> ). Our approach is similar to the two-class case, except now we have to cycle over the classes as well in the outer loop. For each value of <em>λ</em> , we create an outer loop which cycles over <em><code>_ ∈{ 1 _,...,K_ }and computes the par- tial quadratic approximation _Q</code></em> about the current parameters ( <em>β</em> ̃ 0 <em>,β</em> ̃). Then we use coordinate descent to solve the weighted lasso problem problem</p><div class="language-"><pre><code>minimize\n( β 0 `,β` )∈R p +1\n{ Q ( β 0 `,β` ) + λ ‖ β` ‖ 1 }. (3.19)\n</code></pre></div><h4 id="_3-3-3-grouped-lasso-multinomial"><a class="header-anchor" href="#_3-3-3-grouped-lasso-multinomial" aria-hidden="true">#</a> 3.3.3 Grouped-Lasso Multinomial</h4><p>As can be seen in Figure 3.5, the lasso penalty will select different variables for different classes. This can mean that although individual coefficient vectors are sparse, the overall model may not be. In this example, on average there are 25% of the coefficients nonzero per class, while overall 81% of the variables are used. An alternative approach is to use a grouped-lasso penalty (see Section 4.3) for the set of coefficients <em>βj</em> = ( <em>β</em> 1 <em>j,β</em> 2 <em>j,...,βKj</em> ), and hence replace the cri- terion (3.15) with the regularized objective</p><h6 id="−-6"><a class="header-anchor" href="#−-6" aria-hidden="true">#</a> −</h6><h6 id="_1-23"><a class="header-anchor" href="#_1-23" aria-hidden="true">#</a> 1</h6><h6 id="n-9"><a class="header-anchor" href="#n-9" aria-hidden="true">#</a> N</h6><h6 id="∑-n-26"><a class="header-anchor" href="#∑-n-26" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>log Pr( Y = yi | X = xi ;{ βj } pj =1) + λ\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>‖ βj ‖ 2. (3.20)\n</code></pre></div><p>It is important that this criterion involves the sum of the ordinary <em><code>_ 2 -norms ‖·‖ 2 , as opposed to the squared _</code></em> 2 -norms. In this way, it amounts to im- posing a block <em><code>_ 1 _/</code></em> 2 constraint on the overall collection of coefficients: the</p><h6 id="_40-generalized-linear-models"><a class="header-anchor" href="#_40-generalized-linear-models" aria-hidden="true">#</a> 40 GENERALIZED LINEAR MODELS</h6><p>sum of the <em>`</em> 2 -norms over the groups. The effect of this group penalty is to select all the coefficients for a particular variable to be in or out of the model. When included, they are all nonzero in general, and as shown in Exercise 3.6, they will automatically satisfy the constraint</p><h6 id="∑-k-1"><a class="header-anchor" href="#∑-k-1" aria-hidden="true">#</a> ∑ K</h6><p><em>k</em> =1 <em>βkj</em> = 0. Criterion (3.20) is convex, so standard methods can be used to find the optimum. As before, coordinate descent techniques are one reasonable choice, in this case block coordinate descent on each vector <em>βj</em> , holding all the others fixed; see Exer- cise 3.7 for the details. The group lasso and variants are discussed in more detail in Chapter 4.3.</p><h3 id="_3-4-log-linear-models-and-the-poisson-glm"><a class="header-anchor" href="#_3-4-log-linear-models-and-the-poisson-glm" aria-hidden="true">#</a> 3.4 Log-Linear Models and the Poisson GLM</h3><p>When the response variable <em>Y</em> is nonnegative and represents a count, its mean will be positive and the Poisson likelihood is often used for inference. In this case we typically use the log-linear model (3.3) to enforce the positivity. We assume that for each <em>X</em> = <em>x</em> , the response <em>Y</em> follows a Poisson distribution with mean <em>μ</em> satisfying</p><div class="language-"><pre><code>log μ ( x ) = β 0 + βTx. (3.21)\n</code></pre></div><p>The <em>`</em> 1 -penalized negative log-likelihood is given by</p><h6 id="−-7"><a class="header-anchor" href="#−-7" aria-hidden="true">#</a> −</h6><h6 id="_1-24"><a class="header-anchor" href="#_1-24" aria-hidden="true">#</a> 1</h6><h6 id="n-10"><a class="header-anchor" href="#n-10" aria-hidden="true">#</a> N</h6><h6 id="∑-n-27"><a class="header-anchor" href="#∑-n-27" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><h6 id="-36"><a class="header-anchor" href="#-36" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>yi ( β 0 + βTxi )− eβ^0 + β\n</code></pre></div><div class="language-"><pre><code>Txi }\n+ λ ‖ β ‖ 1. (3.22)\n</code></pre></div><p>As with other GLMs, we can fit this model by iteratively reweighted least squares, which amounts to fitting a weighted lasso regression at each outer iteration. Typically, we do not penalize the intercept <em>β</em> 0. It is easy to see that this enforces the constraint that the average fitted value is equal to the mean</p><p>response—namely, that <em>N</em>^1</p><h6 id="∑-n-28"><a class="header-anchor" href="#∑-n-28" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1 μ ˆ i = ̄ y , where ˆ μi : = e\n</code></pre></div><p><em>η</em> ˆ( <em>xi</em> )= <em>e</em> ̂ <em>β</em> 0 + <em>β</em> ̂ <em>Txi</em>. Poisson models are often used to model rates, such as death rates. If the length <em>Ti</em> of the observation window is different for each observation, then the mean count isE( <em>yi</em> | <em>Xi</em> = <em>xi</em> ) = <em>Tiμ</em> ( <em>xi</em> ) where <em>μ</em> ( <em>xi</em> ) is the rate per unit time interval. In this case, our model takes the form</p><div class="language-"><pre><code>log(E( Y | X = x, T )) = log( T ) + β 0 + βTx. (3.23)\n</code></pre></div><p>The terms log( <em>Ti</em> ) for each observation require no fitting, and are called an <em>offset</em>. Offsets play a role in the following example as well.</p><h4 id="_3-4-1-example-distribution-smoothing"><a class="header-anchor" href="#_3-4-1-example-distribution-smoothing" aria-hidden="true">#</a> 3.4.1 Example: Distribution Smoothing</h4><p>The Poisson model is a useful tool for estimating distributions. The fol- lowing example was brought to our attention by Yoram Singer (Singer and Dubiner 2011). Suppose that we have a sample of <em>N</em> counts{ <em>yk</em> } <em>Nk</em> =1from an</p><h6 id="log-linear-models-and-the-poisson-glm-41"><a class="header-anchor" href="#log-linear-models-and-the-poisson-glm-41" aria-hidden="true">#</a> LOG-LINEAR MODELS AND THE POISSON GLM 41</h6><p><em>N</em> -cell multinomial distribution, and let <em>rk</em> = <em>yk/</em></p><h6 id="∑-n-29"><a class="header-anchor" href="#∑-n-29" aria-hidden="true">#</a> ∑ N</h6><p><em><code>_ =1 _y</code></em> be the correspond- ing vector of proportions. For example, in large-scale web applications, these counts might represent the number of people in each county in the USA that visited a particular website during a given week. This vector could be sparse, depending on the specifics, so there is a desire to regularize toward a broader, more stable distribution <strong>u</strong> ={ <em>uk</em> } <em>Nk</em> =1(for example, the same demographic, except measured over a year). Singer and Dubiner (2011) posed the following problem</p><div class="language-"><pre><code>minimize\nq ∈R N, qk ≥ 0\n</code></pre></div><h6 id="∑-n-30"><a class="header-anchor" href="#∑-n-30" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>k =1\n</code></pre></div><div class="language-"><pre><code>qk log\n</code></pre></div><h6 id="-37"><a class="header-anchor" href="#-37" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>qk\nuk\n</code></pre></div><h6 id="-38"><a class="header-anchor" href="#-38" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>such that‖ q − r ‖∞≤ δ and\n</code></pre></div><h6 id="∑-n-31"><a class="header-anchor" href="#∑-n-31" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>k =1 qk = 1.\n</code></pre></div><p>(3.24) In words, we find the distribution, within a <em>δ</em> tolerance in the <em>`</em> ∞-norm from the observed distribution, that is as close as possible to the nominal distri- bution <strong>u</strong> in terms of Kullback–Leibler (KL) divergence. It can be shown (see</p><div class="language-"><pre><code>−6 −4 −2 0 2 4 6\n</code></pre></div><div class="language-"><pre><code>0.00 0.01 0.02 0.03 0.04\n</code></pre></div><div class="language-"><pre><code>X\n</code></pre></div><div class="language-"><pre><code>−6 −4 −2 0 2 4 6\n</code></pre></div><div class="language-"><pre><code>5e−07 5e−06 5e−05 5e−04\n</code></pre></div><div class="language-"><pre><code>X\n</code></pre></div><div class="language-"><pre><code>Differences\n</code></pre></div><div class="language-"><pre><code>f\n(x\n</code></pre></div><div class="language-"><pre><code>)\n</code></pre></div><p><strong>Figure 3.6</strong> <em>Estimating distributions via the Poisson. In the left panel, the solid black curve is the parent distribution</em> <strong>u</strong> <em>, here represented as a discretization of a one- dimensional distributionf</em> ( <em>x</em> ) <em>into 100 cel ls. The blue points represent the observed distribution, and the orange points represent the distribution recovered by the model. While the observed distribution may have many zero counts, the modeled distribution has the same support as</em> <strong>u</strong><em>. The right plot shows theN</em> = 100 <em>differences</em> |ˆ <em>qk</em> − <em>rk</em> | <em>, which are constrained to be less thanδ</em> = 0_._ 001 <em>, which is the horizontal orange line.</em></p><p>Exercise 3.4) that the Lagrange-dual to the optimization problem (3.24) has the form</p><div class="language-"><pre><code>maximize\nβ 0 , α\n</code></pre></div><h6 id="n-11"><a class="header-anchor" href="#n-11" aria-hidden="true">#</a> { N</h6><h6 id="∑-12"><a class="header-anchor" href="#∑-12" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>k =1\n</code></pre></div><div class="language-"><pre><code>[ rk log qk ( β 0 ,αk )− qk ( β 0 ,αk )]− δ ‖ α ‖ 1\n</code></pre></div><h6 id="-39"><a class="header-anchor" href="#-39" aria-hidden="true">#</a> }</h6><h6 id="_3-25"><a class="header-anchor" href="#_3-25" aria-hidden="true">#</a> , (3.25)</h6><p>where <em>qk</em> ( <em>β</em> 0 <em>,αk</em> ) : = <em>ukeβ</em>^0 + <em>αk</em>. This is equivalent to fitting a Poisson GLM with offset log( <em>uk</em> ), individual parameter <em>αk</em> per observation, and the ex- tremely sparse design matrix <strong>X</strong> = <strong>I</strong> <em>N</em> × <em>N</em>. Consequently, it can be fit very efficiently using sparse-matrix methods (see Section 3.7 below). Figure 3.6</p><h6 id="_42-generalized-linear-models"><a class="header-anchor" href="#_42-generalized-linear-models" aria-hidden="true">#</a> 42 GENERALIZED LINEAR MODELS</h6><p>shows a simulation example, where the distribution <em>uk</em> is a discretized contin- uous distribution (mixture of Gaussians). There are <em>N</em> = 100 cells, and a total of</p><h6 id="∑-n-32"><a class="header-anchor" href="#∑-n-32" aria-hidden="true">#</a> ∑ N</h6><p><em>k</em> =1 <em>yk</em> = 1000 observations distributed to these cells. As discussed above, the presence of the unpenalized <em>β</em> 0 ensures that</p><h6 id="∑-n-33"><a class="header-anchor" href="#∑-n-33" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>k =1 q ˆ k =\n</code></pre></div><h6 id="∑-n-34"><a class="header-anchor" href="#∑-n-34" aria-hidden="true">#</a> ∑ N</h6><p><em>k</em> =1 <em>rk</em> = 1 (see also Exercise 3.5). Although we only show one solution in Figure 3.6, the path gives solutions ˆ <em>qk</em> ( <em>δ</em> ) that vary smoothly between the background distribution <em>uk</em> and the observed distribution <em>rk</em>.</p><h3 id="_3-5-cox-proportional-hazards-models"><a class="header-anchor" href="#_3-5-cox-proportional-hazards-models" aria-hidden="true">#</a> 3.5 Cox Proportional Hazards Models</h3><p>In medical studies, the outcome of interest after treatment is often time to death or time to recurrence of the disease. Patients are followed after their treatment, and some drop out because they move away, or perhaps die from an independent cause. Such outcomes are called <em>right censored</em>. Denoting by <em>T</em> the underlying survival time, for each patient we observe the quantity <em>Y</em> = min( <em>T,C</em> ) where <em>C</em> is a <em>censoring time</em>. Interest tends to focus on the survivor function <em>S</em> ( <em>t</em> ) : = Pr( <em>T &gt; t</em> ), the probability of surviving beyond a certain time <em>t</em>.</p><div class="language-"><pre><code>0 5 10 15 20\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>Kaplan−Meier Estimates (naive)\n</code></pre></div><div class="language-"><pre><code>Time\n</code></pre></div><div class="language-"><pre><code>Survival Probability\n</code></pre></div><div class="language-"><pre><code>Pred &gt; 0\nPred&lt;=0\nOverall\n</code></pre></div><div class="language-"><pre><code>0 5 10 15 20\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>Kaplan−Meier Estimates (prevalidated)\n</code></pre></div><div class="language-"><pre><code>Time\n</code></pre></div><div class="language-"><pre><code>Survival Probability\n</code></pre></div><div class="language-"><pre><code>p=0.00012\n</code></pre></div><p><strong>Figure 3.7</strong> <em>The black curves are the Kaplan–Meier estimates ofS</em> ( <em>t</em> ) <em>for the Lym- phoma data. In the left plot, we segment the data based on the predictions from the Cox proportional hazards lasso model, selected by cross-validation. Although the tuning parameter is chosen by cross-validation, the predictions are based on the ful l training set, and are overly optimistic. The right panel uses</em> prevalidation <em>to build a prediction on the entire dataset, with this training-set bias removed. Although the separation is not as strong, it is still significant. The spikes indicate censoring times. The p-value in the right panel comes from the log-rank test.</em></p><p>The black curves in Figure 3.7 show estimates of <em>S</em> ( <em>t</em> ) for a population of <em>N</em> = 240 Lymphoma patients (Alizadeh et al. 2000). Each of the spikes in the</p><h6 id="cox-proportional-hazards-models-43"><a class="header-anchor" href="#cox-proportional-hazards-models-43" aria-hidden="true">#</a> COX PROPORTIONAL HAZARDS MODELS 43</h6><p>plot indicates a censoring point, meaning a time at which a patient was lost for follow-ups. Although survival curves are useful summaries of such data, when incorporating covariates it is more common to model the <em>hazard function</em> , a monotone transformation of <em>S</em>. More specifically, the hazard at time <em>t</em> is given by</p><div class="language-"><pre><code>h ( t ) = lim\nδ → 0\n</code></pre></div><div class="language-"><pre><code>Pr( Y ∈( t,t + δ )| Y ≥ t )\nδ\n</code></pre></div><h6 id="-40"><a class="header-anchor" href="#-40" aria-hidden="true">#</a> =</h6><div class="language-"><pre><code>f ( t )\nS ( t )\n</code></pre></div><h6 id="_3-26"><a class="header-anchor" href="#_3-26" aria-hidden="true">#</a> , (3.26)</h6><p>and corresponds to the instantaneous probability of death at time <em>t</em> , given survival up till <em>t</em>. We now discuss Cox’s proportional hazards model that was used to produce the blue and orange survival curves in Figure 3.7. The proportional hazards model (CPH) is based on the hazard function</p><div class="language-"><pre><code>h ( t ; x ) = h 0 ( t ) eβ\n</code></pre></div><div class="language-"><pre><code>Tx\n, (3.27)\n</code></pre></div><p>where <em>h</em> 0 ( <em>t</em> ) is a baseline hazard (the hazard for an individual with <em>x</em> = 0). We have data of the form ( <em>xi,yi,δi</em> ), where <em>δi</em> is binary-valued indicator of whether <em>yi</em> is a death time or censoring time. For the lymphoma data, there are <em>p</em> = 7399 variables, each a measure of gene expression. Of the <em>N</em> = 240 samples, a total of 102 samples are right censored. Here we fit an <em>`</em> 1 -penalized CPH by solving</p><div class="language-"><pre><code>minimize\nβ\n</code></pre></div><h6 id="-9"><a class="header-anchor" href="#-9" aria-hidden="true">#</a> </h6><h6 id="-7"><a class="header-anchor" href="#-7" aria-hidden="true">#</a> </h6><h6 id="-7"><a class="header-anchor" href="#-7" aria-hidden="true">#</a> </h6><h6 id="−-8"><a class="header-anchor" href="#−-8" aria-hidden="true">#</a> −</h6><h6 id="∑-13"><a class="header-anchor" href="#∑-13" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>{ i | δi =1}\n</code></pre></div><div class="language-"><pre><code>log\n</code></pre></div><h6 id="-41"><a class="header-anchor" href="#-41" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>eβ\n</code></pre></div><div class="language-"><pre><code>Txi\n∑\nj ∈ Rie\nβTxj\n</code></pre></div><h6 id="-42"><a class="header-anchor" href="#-42" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>+ λ ‖ β ‖ 1\n</code></pre></div><h6 id="-8"><a class="header-anchor" href="#-8" aria-hidden="true">#</a> </h6><h6 id="-7"><a class="header-anchor" href="#-7" aria-hidden="true">#</a> </h6><h6 id="-7"><a class="header-anchor" href="#-7" aria-hidden="true">#</a> </h6><h6 id="_3-28"><a class="header-anchor" href="#_3-28" aria-hidden="true">#</a> , (3.28)</h6><p>where for each <em>i</em> = 1 <em>,...,N</em> , <em>Ri</em> is the <em>risk set</em> of individuals who are alive and in the study at time <em>yi</em>. The first term is the log of the <em>partial likelihood</em> , corresponding to the conditional probability in the risk set of the observed death. Note that the baseline hazard does not play a role, an attractive feature of this approach. Here we have assumed that there are no ties, that is, the survival times are all unique. Modification of the partial likelihood is needed in the event of ties. Figure 3.8 shows the coefficients obtained in fitting the model (3.28) to the Lymphoma data. Since <em>p</em>  <em>N</em> , the model would “saturate” as <em>λ</em> ↓0, meaning that some parameters would diverge to±∞, and the log partial likelihood would approach 0. We see evidence of this undesirable behavior as <em>λ</em> gets small. The computations for the Cox model are similar to those for the multino- mial model but slightly more complex. Simon, Friedman, Hastie and Tibshi- rani (2011) give details for an algorithm based on coordinate-descent.</p><h4 id="_3-5-1-cross-validation"><a class="header-anchor" href="#_3-5-1-cross-validation" aria-hidden="true">#</a> 3.5.1 Cross-Validation</h4><p>All the models in this chapter require a choice of <em>λ</em> , and we typically use <em>K</em> −fold cross-validation where <em>K</em> equal to 5 or 10, as in Figure 3.2. For the</p><h6 id="_44-generalized-linear-models"><a class="header-anchor" href="#_44-generalized-linear-models" aria-hidden="true">#</a> 44 GENERALIZED LINEAR MODELS</h6><div class="language-"><pre><code>−6 −5 −4 −3 −2\n</code></pre></div><div class="language-"><pre><code>−4\n</code></pre></div><div class="language-"><pre><code>−2\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>219 202 188 126 3115\n</code></pre></div><div class="language-"><pre><code>−2.8 −2.6 −2.4 −2.2 −2.0 −1.8 −1.6\n</code></pre></div><div class="language-"><pre><code>−0.4\n</code></pre></div><div class="language-"><pre><code>−0.2\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>15\n</code></pre></div><div class="language-"><pre><code>Zoomed in\n31\n</code></pre></div><div class="language-"><pre><code>log(λ) log(λ)\n</code></pre></div><p><strong>Figure 3.8</strong> <em>The`</em> 1 <em>-regularized coefficient path for the Cox model fit to the Lym- phoma data. Sincep</em>  <em>N, the plot has a trumpet shape near the end, corresponding to a saturated model with partial likelihood equal to one. The right-hand plot zooms in on the area of interest, a fairly heavily regularized solution with</em> 31 <em>nonzero coef- ficients.</em></p><p>Cox model, we compute the cross-validated deviance, which is minus twice the log partial likelihood. An issue arises in computing the deviance, since if <em>N/K</em> is small, there will not be sufficient observations to compute the risk sets. Here we use a trick due to van Houwelingen et al. (2006). When fold <em>k</em> is left out, we compute the coefficients <em>β</em> ̂− <em>k</em> ( <em>λ</em> ), and then compute</p><div class="language-"><pre><code>Dev̂ kλ : = Dev[ β ̂− k ( λ )]−Dev− k [ β ̂− k ( λ )]. (3.29)\n</code></pre></div><div class="language-"><pre><code>The first term on the right uses all N samples in computing the deviance,\n</code></pre></div><p>while the second term omits the fold- <em>k</em> samples. Finally Dev <em>CVλ</em> =</p><h6 id="∑-k-2"><a class="header-anchor" href="#∑-k-2" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1Dev̂\n</code></pre></div><p><em>k λ</em> is obtained by subtraction. The point is that each of these terms has sufficient data to compute the deviance, and in the standard cases (that is, any of the other generalized linear models), the estimate would be precisely the deviance on the left-out set. The deviance in Figure 3.9 was computed in this fashion; we zoom in on the right-hand section. We see that the minimum is achieved at 31 nonzero coefficients. Figure 3.7 shows the effect of the chosen model. We compute <em>η</em> ˆ( <em>xi</em> ) = <em>xTiβ</em> ˆ( <em>λ</em> min) for each observation, and then create two groups by thresh- olding these scores at zero. The two colored survival curves in the left-hand plot show the difference in survival for the two groups thus formed. They are well separated, which suggests we have derived a powerful signature. However, these scores are biased: we are evaluating their performance on the same data for which they were computed.</p><h6 id="cox-proportional-hazards-models-45"><a class="header-anchor" href="#cox-proportional-hazards-models-45" aria-hidden="true">#</a> COX PROPORTIONAL HAZARDS MODELS 45</h6><div class="language-"><pre><code>−6 −5 −4 −3 −2\n</code></pre></div><div class="language-"><pre><code>50\n</code></pre></div><div class="language-"><pre><code>100\n</code></pre></div><div class="language-"><pre><code>150\n</code></pre></div><div class="language-"><pre><code>Partial Likelihood Deviance\n</code></pre></div><div class="language-"><pre><code>230 218 212 216 204 186 135 68 25 7\n</code></pre></div><div class="language-"><pre><code>−2.8 −2.6 −2.4 −2.2 −2.0 −1.8 −1.6\n</code></pre></div><div class="language-"><pre><code>11.0\n</code></pre></div><div class="language-"><pre><code>11.5\n</code></pre></div><div class="language-"><pre><code>12.0\n</code></pre></div><div class="language-"><pre><code>12.5\n</code></pre></div><div class="language-"><pre><code>13.0\n</code></pre></div><div class="language-"><pre><code>Partial Likelihood Deviance\n</code></pre></div><div class="language-"><pre><code>114 93 75 55 41 28 20 14 7 4 3 0\n</code></pre></div><div class="language-"><pre><code>Zoomed in\n</code></pre></div><div class="language-"><pre><code>log(λ) log(λ)\n</code></pre></div><p><strong>Figure 3.9</strong> <em>Cross-validated deviance for the lymphoma data, computed by subtrac- tions, as described in the text. The right-hand plot zooms in on the area of interest. The dotted vertical line on the left corresponds to the minimum, and the model we chose in this case; the one on the right corresponds to the rightmost point on the curve (simplest model) within one standard error of the minimum. This is a basis for a more conservative approach to selection. The number of nonzero coefficients is shown along the top of each plot.</em></p><h4 id="_3-5-2-pre-validation"><a class="header-anchor" href="#_3-5-2-pre-validation" aria-hidden="true">#</a> 3.5.2 Pre-Validation</h4><p>In Figure 3.7, we used a variant of cross-validation, known as <em>pre-validation</em> (Tibshirani and Efron 2002), in order to obtain a fair evaluation of the model. Cross-validation leaves out data in order to obtain a reasonably unbiased estimate of the error rate of a model. But the error rate is not a very inter- pretable measure in some settings such as survival modelling. The method of <em>pre-validation</em> is similar to cross-validation, but instead produces a new set of “unbiased data” that mimics the performance of the model applied to inde- pendent data. The pre-validated data can then be analyzed and displayed. In computing the score ˆ <em>η</em> ( <em>xi</em> )( <em>k</em> )for the observations in fold <em>k</em> , we use the coeffi-</p><p>cient vector <em>β</em> ̂(− <em>k</em> )computed with those observations omitted.^5 Doing this for all <em>K</em> folds, we obtain the “pre-validated” dataset{(ˆ <em>η</em> ( <em>xi</em> )( <em>k</em> ) <em>,yi,δi</em> )} <em>Ni</em> =1. The key aspect of this pre-validated data is that each score ˆ <em>η</em> ( <em>xi</em> )( <em>k</em> )is derived in- dependently of its response value ( <em>yi,δi</em> ). Hence we can essentially treat these scores as if they were derived from a dataset completely separate from the “test data”{( <em>xi,yi,δi</em> )} <em>Ni</em> =1. In the right-hand panel of Figure 3.7, we have split the pre-validated scores into two groups and plotted the corresponding</p><p>(^5) Strictly speaking <em>λ</em> should be chosen each time as well, but we did not do that here.</p><h6 id="_46-generalized-linear-models"><a class="header-anchor" href="#_46-generalized-linear-models" aria-hidden="true">#</a> 46 GENERALIZED LINEAR MODELS</h6><p>survival curves. Although the curves are not as spread out as in the left-hand plot, they are still significantly different.</p><h3 id="_3-6-support-vector-machines"><a class="header-anchor" href="#_3-6-support-vector-machines" aria-hidden="true">#</a> 3.6 Support Vector Machines</h3><p>We now turn to a method for binary classification known as the support vector machine (SVM). The idea is shown in Figure 3.10. The decision boundary is the solid line in the middle of the yellow slab. The <em>margin</em> is the half-width of the yellow slab. Ideally, all of the blue data points should lie above the slab on the right, and the red points should lie below it on the left. However in the picture, three red points and two blue points lie on the wrong side of their margin. These correspond to the “errors” <em>ξi</em>. The SVM decision boundary is chosen to maximize the margin, subject to a fixed budget on the total error∑ <em>N i</em> =1 <em>ξi</em>. The idea is that a decision boundary achieving the largest margin has more space between the classes and will generalize better to test data. This leads to the optimization problem</p><div class="language-"><pre><code>maximize\nβ 0 , β, { ξi } N 1\n</code></pre></div><div class="language-"><pre><code>M subject to yi ( β 0 + βTxi )\n︸ ︷︷ ︸\nf ( xi ; β 0 ,β )\n</code></pre></div><div class="language-"><pre><code>≥ M (1− ξi )∀ i, (3.30)\n</code></pre></div><div class="language-"><pre><code>and ξi ≥ 0 ∀ i,\n</code></pre></div><h6 id="∑-n-35"><a class="header-anchor" href="#∑-n-35" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>ξi ≤ C, and‖ β ‖ 2 = 1. (3.31)\n</code></pre></div><p>(See Section 3.6.1 for an explanation of this particular form.) This problem involves a linear cost function subject to convex constraints, and many efficient algorithms have been designed for its solution. It can be shown to be equivalent to the penalized form (3.8) previously specified on page 31, which we restate here:</p><div class="language-"><pre><code>minimize\nβ 0 ,β\n</code></pre></div><h6 id="-43"><a class="header-anchor" href="#-43" aria-hidden="true">#</a> {</h6><h6 id="_1-25"><a class="header-anchor" href="#_1-25" aria-hidden="true">#</a> 1</h6><h6 id="n-12"><a class="header-anchor" href="#n-12" aria-hidden="true">#</a> N</h6><h6 id="∑-n-36"><a class="header-anchor" href="#∑-n-36" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>[1− yif ( x ; β 0 ,β )]++ λ ‖ β ‖^22\n</code></pre></div><h6 id="-44"><a class="header-anchor" href="#-44" aria-hidden="true">#</a> }</h6><h6 id="_3-32"><a class="header-anchor" href="#_3-32" aria-hidden="true">#</a> . (3.32)</h6><p>Decreasing <em>λ</em> has a similar effect to decreasing <em>C</em>.^6 The linear SVM can be generalized using a <em>kernel</em> to create nonlinear boundaries; it involves replacing the squared <em>`</em> 2 -norm in the objective (3.32) by the squared Hilbert norm de- fined by a symmetric bivariate kernel. Details on this extension can be found elsewhere—for instance, see Hastie et al. (2009), Section 5.8. Since the criterion (3.32) involves a quadratic penalty, the estimated coef- ficient vector will not be sparse. However, because the hinge loss function is piecewise linear, it introduces a different kind of sparsity. It can be shown via the dual formulation of the SVM that the solution <em>β</em> ˆhas the form</p><div class="language-"><pre><code>β ˆ=\n</code></pre></div><h6 id="∑-n-37"><a class="header-anchor" href="#∑-n-37" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>α ˆ iyixi ; (3.33)\n</code></pre></div><p>(^6) Solutions to (3.32) do not have‖ <em>β</em> ˆ‖ 2 = 1, but since a linear classifier is scale invariant, the solution coefficients can be rescaled.</p><h6 id="support-vector-machines-47"><a class="header-anchor" href="#support-vector-machines-47" aria-hidden="true">#</a> SUPPORT VECTOR MACHINES 47</h6><ul><li><ul><li></li></ul></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li>•</li></ul><div class="language-"><pre><code>margin\n</code></pre></div><div class="language-"><pre><code>ξξξ∗ 1 ∗ 1 ∗ 1\n</code></pre></div><div class="language-"><pre><code>ξξξ 222 ∗∗∗\n</code></pre></div><div class="language-"><pre><code>ξξ 33 ∗∗\n</code></pre></div><div class="language-"><pre><code>ξξξ 444 ∗∗∗ ξ 5 ∗\n</code></pre></div><div class="language-"><pre><code>M=kβ^1 k 2\n</code></pre></div><div class="language-"><pre><code>M=kβ^1 k 2\n</code></pre></div><div class="language-"><pre><code>β 0 +βTx=0\n</code></pre></div><p><strong>Figure 3.10</strong> <em>Support vector classifier: The decision boundary is the solid line, while broken lines bound the shaded maximal margin of width</em> 2 <em>M</em> = 2 <em>/</em> ‖ <em>β</em> ‖ 2_. The points labelledξ_ ∗ <em>jare on the wrong side of their margin by an amountξ</em> ∗ <em>j</em> = <em>Mξj; points on the correct side have</em> ∑ <em>ξ</em> ∗ <em>j</em> = 0_. The margin is maximized subject to a total budget N i_ =1 <em>ξi</em> ≤ <em>C. Hence</em></p><h6 id="∑-n-38"><a class="header-anchor" href="#∑-n-38" aria-hidden="true">#</a> ∑ N</h6><p><em>i</em> =1 <em>ξ j</em> ∗ <em>is the total distance of points on the wrong side of their margin.</em></p><p>each observation <em>i</em> ∈ { 1 <em>,...,N</em> }is associated with a nonnegative weight ˆ <em>αi</em> , and only a subsetV <em>λ</em> , referred to as the <em>support set</em> , will be associated with nonzero weights. SVMs are popular in high-dimensional classification problems with <em>p</em>  <em>N</em> , since the computations areO( <em>pN</em>^2 ) for both linear and nonlinear kernels. Ad- ditional efficiencies can be realized for linear SVMs, using stochastic subgradi- ent methods (Shalev-Shwartz, Singer and Srebro 2007). They are not, however, sparse in the features. Replacing the <em><code>_ 2 penalty in the objective (3.32) with an _</code></em> 1 penalty promotes such sparsity, and yields the <em>`</em> 1 <em>-regularized linear SVM</em> :</p><div class="language-"><pre><code>minimize\nβ 0 ,β\n</code></pre></div><h6 id="-45"><a class="header-anchor" href="#-45" aria-hidden="true">#</a> {</h6><h6 id="_1-26"><a class="header-anchor" href="#_1-26" aria-hidden="true">#</a> 1</h6><h6 id="n-13"><a class="header-anchor" href="#n-13" aria-hidden="true">#</a> N</h6><h6 id="∑-n-39"><a class="header-anchor" href="#∑-n-39" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>[1− yif ( xi ; β 0 ,β )]++ λ ‖ β ‖ 1\n</code></pre></div><h6 id="-46"><a class="header-anchor" href="#-46" aria-hidden="true">#</a> }</h6><h6 id="_3-34"><a class="header-anchor" href="#_3-34" aria-hidden="true">#</a> . (3.34)</h6><p>The optimization problem (3.34) is a linear program with many con- straints (Zhu, Rosset, Hastie and Tibshirani 2004, Wang, Zhu and Zou 2006), and efficient algorithms can be complex (Exercise 3.9). The solution paths (in fine detail) can have many jumps, and show many discontinuities. For this reason, some authors prefer to replace the usual hinge loss <em>φ</em> hin= (1− <em>t</em> )+</p><h6 id="_48-generalized-linear-models"><a class="header-anchor" href="#_48-generalized-linear-models" aria-hidden="true">#</a> 48 GENERALIZED LINEAR MODELS</h6><div class="language-"><pre><code>0.0 0.5 1.0 1.5 2.0 2.5\n</code></pre></div><div class="language-"><pre><code>−0.2\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>SA Heart Disease p &lt; N\n</code></pre></div><div class="language-"><pre><code>L1 norm\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>Hinge Loss versus Binomial with L1 Regularization\n</code></pre></div><div class="language-"><pre><code>0 1 2 3 4 5 6\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.5\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>1.5\n</code></pre></div><div class="language-"><pre><code>Leukemia p &gt;&gt; N\n</code></pre></div><div class="language-"><pre><code>L1 norm\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><p><strong>Figure 3.11</strong> <em>A comparison of the coefficient paths for the`</em> 1 <em>-regularized SVM versus logistic regression on two examples. In the left we have the South African heart disease data (N</em> = 462 <em>andp</em> = 9 <em>), and on the right the Leukemia data (N</em> = 38 <em>andp</em> = 6087 <em>). The dashed lines are the SVM coefficients, the solid lines logistic regression. The similarity is striking in the left example, and strong in the right.</em></p><p>with <em>squared hinge lossφ</em> sqh( <em>t</em> ) = (1− <em>t</em> )^2 +, which is differentiable everywhere (see Exercise 3.8). The SVM loss function shares many similarities with the binomial loss (Hastie et al. 2009, Section 12.3), and their solutions are not too dif- ferent. Figure 3.11 compares their <em><code>_ 1 regularization paths on two examples, and supports this claim. In the left-hand plot, they are virtually identical. In the right-hand plot, for more than half of the path, the training data are separated by the solution. As we proceed to the end of the path, the logistic coefficients become less stable than those of the SVM, and can account for the bigger discrepancies. The support vector machine, on the other hand, is designed for finding maximal-margin solutions for separable data, and its coefficients do not blow up at the least-regularized end of the path. However, in terms of the _</code></em> 1 penalty, this is at the nonsparse end of the path. In light of this, we do not recommend the <em>`</em> 1 regularized linear SVM as a variable selector, because the corresponding logistic regression problem (3.6) gives very similar solutions when the penalty is active, and the algorithms are more stable.</p><h6 id="support-vector-machines-49"><a class="header-anchor" href="#support-vector-machines-49" aria-hidden="true">#</a> SUPPORT VECTOR MACHINES 49</h6><h4 id="_3-6-1-logistic-regression-with-separable-data"><a class="header-anchor" href="#_3-6-1-logistic-regression-with-separable-data" aria-hidden="true">#</a> 3.6.1 Logistic Regression with Separable Data</h4><p>It is a well-known fact that without a penalty on the coefficients, the linear logistic regression model fails when the two classes are linearly separable (Ex- ercise 3.1); the maximum-likelihood estimates for the coefficients are infinite. The problem in this case is that the likelihood is trying to make the probabil- ities all 1s and 0s, and inspection of (3.2) shows that this cannot be achieved with finite parameters. Once we penalize the criterion as in (3.6) the problem goes away, for as long as <em>λ &gt;</em> 0, very large coefficients will not be tolerated. With wide data ( <em>N</em>  <em>p</em> ), the classes are almost always separable, unless there are exact ties in covariate space for the two classes. Figure 3.1 shows the logistic-regression coefficient path for a wide-data situation; notice how the coefficients start to fan out near the end of the path. One has to take care at this end of the path, and not allow <em>λ</em> to get too small. In many situations, this end represents the overfit situation, which is not of primary interest. It appears not to be the case in this example, as can be seen in the cross-validation plots in Figure 3.2. The ends of the path have special meaning in the machine-learning com- munity, since we will see they amount to maximal-margin classifiers. Before giving the details, we review some geometry associated with linear classifica- tion. Consider the boundaryB: ={ <em>x</em> ∈R <em>p</em> | <em>f</em> ( <em>x</em> ) = 0}associated with a linear classifier <em>f</em> ( <em>x</em> )≡ <em>f</em> ( <em>x</em> ; <em>β</em> 0 <em>,β</em> ) = <em>β</em> 0 + <em>βTx</em>. The Euclidean distance from a point <em>x</em> 0 to the boundary is given by</p><div class="language-"><pre><code>dist 2 ( x 0 , B) : = inf\nz ∈B\n‖ z − x 0 ‖ 2 =\n</code></pre></div><div class="language-"><pre><code>| f ( x 0 )|\n‖ β ‖ 2\n</code></pre></div><h6 id="_3-35"><a class="header-anchor" href="#_3-35" aria-hidden="true">#</a> (3.35)</h6><p>(Exercise 3.2). Consequently, for a given predictor-response pair ( <em>x,y</em> ), the quantity <em>y f</em> ‖ <em>β</em> (‖ <em>x</em> 2 )is the signed Euclidean distance to the boundary: it will be</p><p>negative if the sign of <em>y</em> disagrees with that of <em>f</em> ( <em>x</em> ). For separable data, the optimal separating hyperplane <em>f</em> ∗( <em>x</em> ) = 0 solves the optimization problem</p><div class="language-"><pre><code>M 2 ∗= max\nβ 0 ,β\n</code></pre></div><h6 id="-47"><a class="header-anchor" href="#-47" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>min\ni ∈{ 1 ,...,N }\n</code></pre></div><div class="language-"><pre><code>yif ( xi ; β 0 ,β )\n‖ β ‖ 2\n</code></pre></div><h6 id="-48"><a class="header-anchor" href="#-48" aria-hidden="true">#</a> }</h6><h6 id="_3-36"><a class="header-anchor" href="#_3-36" aria-hidden="true">#</a> . (3.36)</h6><p>In words, it maximizes the Euclidean distance of the closest sample to the boundary. Rosset et al. (2004) establish an interesting connection between this op- timal separating hyperplane and a certain limiting case of ridge-regularized logistic regression. In particular, suppose that we replace the <em><code>_ 1 -penalty in the objective (3.10) with a squared _</code></em> 2 -penalty, and solve the problem</p><div class="language-"><pre><code>minimize\nβ 0 ,β\n</code></pre></div><h6 id="-49"><a class="header-anchor" href="#-49" aria-hidden="true">#</a> {</h6><h6 id="_1-27"><a class="header-anchor" href="#_1-27" aria-hidden="true">#</a> 1</h6><h6 id="n-14"><a class="header-anchor" href="#n-14" aria-hidden="true">#</a> N</h6><h6 id="∑-n-40"><a class="header-anchor" href="#∑-n-40" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>log(1 + e − yif ( xi ; β^0 ,β )) + λ ‖ β ‖^22\n</code></pre></div><h6 id="-50"><a class="header-anchor" href="#-50" aria-hidden="true">#</a> }</h6><h6 id="_3-37"><a class="header-anchor" href="#_3-37" aria-hidden="true">#</a> ; (3.37)</h6><p>let ( <em>β</em> ̃ 0 ( <em>λ</em> ) <em>,β</em> ̃( <em>λ</em> )) be the optimal solution, specifying a particular linear classi- fier. We then consider the behavior of this linear classifier as the regularization</p><h6 id="_50-generalized-linear-models"><a class="header-anchor" href="#_50-generalized-linear-models" aria-hidden="true">#</a> 50 GENERALIZED LINEAR MODELS</h6><p>weight <em>λ</em> vanishes: in particular, it can be shown (Rosset et al. 2004) that</p><div class="language-"><pre><code>lim\nλ → 0\n</code></pre></div><h6 id="-51"><a class="header-anchor" href="#-51" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>min\ni ∈{ 1 ,...,N }\n</code></pre></div><div class="language-"><pre><code>yif ( xi ; β ̃ 0 ( λ ) ,β ̃( λ ))\n‖ β ̃( λ )‖ 2\n</code></pre></div><h6 id="-52"><a class="header-anchor" href="#-52" aria-hidden="true">#</a> }</h6><h6 id="m-2-∗-3-38"><a class="header-anchor" href="#m-2-∗-3-38" aria-hidden="true">#</a> = M 2 ∗. (3.38)</h6><p>Thus, the end of the <em>`</em> 2 -regularized logistic regression path corresponds to the SVM solution. In particular, if ( <em>β</em> ̆ 0 <em>,β</em> ̆) solves the SVM objective (3.30) with <em>C</em> = 0, then</p><div class="language-"><pre><code>lim\nλ → 0\n</code></pre></div><div class="language-"><pre><code>β ̃( λ )\n‖ β ̃( λ )‖ 2\n</code></pre></div><div class="language-"><pre><code>= β. ̆ (3.39)\n</code></pre></div><p>How does this translate to the setting of <em><code>_ 1 -regularized models? Matters get a little more complicated, since we move into the territory of general projec- tions and dual norms (Mangasarian 1999). The analog of the _</code></em> 2 -distance (3.35) is the quantity</p><div class="language-"><pre><code>dist∞( x 0 , B) : = inf\nz ∈B\n‖ z − x 0 ‖∞ =\n</code></pre></div><div class="language-"><pre><code>| f ( x 0 )|\n‖ β ‖ 1\n</code></pre></div><h6 id="_3-40"><a class="header-anchor" href="#_3-40" aria-hidden="true">#</a> , (3.40)</h6><p>For a given <em>λ</em> ≥0, let ( <em>β</em> ̂ 0 ( <em>λ</em> ) <em>,β</em> ̂( <em>λ</em> )) denote an optimal solution to the <em>`</em> 1 - regularized logistic regression objective (3.10). Then as <em>λ</em> decreases toward zero, we have</p><div class="language-"><pre><code>lim\nλ → 0\n</code></pre></div><h6 id="-53"><a class="header-anchor" href="#-53" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>min\ni ∈{ 1 ,...,N }\n</code></pre></div><div class="language-"><pre><code>yif ( xi ; β ̂ 0 ( λ ) ,β ̂( λ ))\n‖ β ̂( λ )‖ 1\n</code></pre></div><h6 id="-54"><a class="header-anchor" href="#-54" aria-hidden="true">#</a> ]</h6><h6 id="m-∞∗-3-41"><a class="header-anchor" href="#m-∞∗-3-41" aria-hidden="true">#</a> = M ∞∗ , (3.41)</h6><p>so that the worst-case margin of the <em><code>_ 1 -regularized logistic regression converges to the _</code></em> 1 -regularized version of the support vector machine, which maximizes the <em>`</em> ∞margin (3.40). In summary, then, we can make the following observations:</p><ul><li>At the end of the path, where the solution is most dense, the logistic re- gression solution coincides with the SVM solution.</li><li>The SVM approach leads to a more stable numerical method for computing the solution in this region.</li><li>In contrast, logistic regression is most useful in the sparser part of the solution path.</li></ul><p><strong>3.7 Computational Details and</strong> glmnet</p><p>Most of the examples in this chapter were fit using the R packageglmnet (Friedman et al. 2015). Here we detail some of the options and features in glmnet. Although these are specific to this package, they also would be natural requirements in any other similar software.</p><h6 id="computational-details-andglmnet-51"><a class="header-anchor" href="#computational-details-andglmnet-51" aria-hidden="true">#</a> COMPUTATIONAL DETAILS ANDGLMNET 51</h6><p><em>Family:</em> The family option allows one to pick the loss-function and the asso- ciated model. As of version 1.7, these aregaussian,binomial,multinomial (grouped or not),poisson, andcox. Thegaussianfamily allows for multiple responses (multitask learning), in which case a group lasso is used to select coefficients for each variable, as in the grouped multinomial. Associated with each family is a <em>deviance</em> measure, the analog of the residual sum-of-squares for Gaussian errors. Denote by <strong><em>μ</em></strong> ˆ <em>λ</em> the <em>N</em> -vector of fitted mean values when the parameter is <em>λ</em> , and <strong><em>μ</em></strong> ̃the unrestricted or <em>saturated</em> fit. Then</p><div class="language-"><pre><code>Dev λ = 2[. ` ( y , μ ̃)− ` ( y , μ ˆ λ )]. (3.42)\n</code></pre></div><p>Here <em>`</em> ( <strong>y</strong> <em>,</em> <strong><em>μ</em></strong> ) is the log-likelihood of the model <strong><em>μ</em></strong> , a sum of <em>N</em> terms. The <em>nul l deviance</em> is Devnull= Dev∞; typically this means <strong><em>μ</em></strong> ˆ∞= ̄ <em>y</em> <strong>1</strong> , or in the case of thecoxfamily <strong><em>μ</em></strong> ˆ∞= <strong>0</strong> .Glmnetreports D^2 , the fraction of deviance explained, as defined in (3.11) on page 33.</p><p><em>Penalties:</em> For all models, theglmnetalgorithm admits a range of elastic-net penalties ranging from <em><code>_ 2 to _</code></em> 1. The general form of the penalized optimization problem is</p><div class="language-"><pre><code>minimize\nβ 0 ,β\n</code></pre></div><h6 id="-10"><a class="header-anchor" href="#-10" aria-hidden="true">#</a> </h6><h6 id="-8"><a class="header-anchor" href="#-8" aria-hidden="true">#</a> </h6><h6 id="-8"><a class="header-anchor" href="#-8" aria-hidden="true">#</a> </h6><h6 id="−-9"><a class="header-anchor" href="#−-9" aria-hidden="true">#</a> −</h6><h6 id="_1-28"><a class="header-anchor" href="#_1-28" aria-hidden="true">#</a> 1</h6><h6 id="n-15"><a class="header-anchor" href="#n-15" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>` ( y ; β 0 ,β ) + λ\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>γj\n</code></pre></div><h6 id="-55"><a class="header-anchor" href="#-55" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>(1− α ) βj^2 + α | βj |\n</code></pre></div><h6 id="-56"><a class="header-anchor" href="#-56" aria-hidden="true">#</a> }</h6><h6 id="-9"><a class="header-anchor" href="#-9" aria-hidden="true">#</a> </h6><h6 id="-8"><a class="header-anchor" href="#-8" aria-hidden="true">#</a> </h6><h6 id="-8"><a class="header-anchor" href="#-8" aria-hidden="true">#</a> </h6><h6 id="_3-43"><a class="header-anchor" href="#_3-43" aria-hidden="true">#</a> . (3.43)</h6><p>This family of penalties is specified by three sets of real-valued parameters:</p><ul><li>The parameter <em>λ</em> determines the overall complexity of the model. By de- fault, theglmnetalgorithm generates a sequence of 100 values for <em>λ</em> that cover the whole path (on the log scale), with care taken at the lower end for saturated fits.</li><li>The elastic-net parameter <em>α</em> ∈[0 <em>,</em> 1] provides a mix between ridge regression and the lasso. Although one can select <em>α</em> via cross-validation, we typically try a course grid of around three to five values of <em>α</em>.</li><li>For each <em>j</em> = 1 <em>,</em> 2 <em>,...,p</em> , the quantity <em>γj</em> ≥0 is a penalty modifier. When <em>γj</em> = 0, the <em>jth</em> variable is always included; when <em>γj</em> =infit is always excluded. Typically <em>γj</em> = 1 (the default), and all variables are treated as equals.</li></ul><p><em>Coefficient bounds:</em> With coordinate descent, it is very easy to allow for upper and lower bounds on each coefficient in the model. For example, we might ask for a nonnegative lasso. In this case, if a coefficient exceeds an upper or lower bound during the coordinate-descent cycle, it is simply set to the bound.</p><p><em>Offset:</em> All the models allow for an <em>offset</em> term. This is a real valued number <em>oi</em> for each observation, that gets added to the linear predictor, and is not associated with any parameter:</p><div class="language-"><pre><code>η ( xi ) = oi + β 0 + βTxi. (3.44)\n</code></pre></div><h6 id="_52-generalized-linear-models"><a class="header-anchor" href="#_52-generalized-linear-models" aria-hidden="true">#</a> 52 GENERALIZED LINEAR MODELS</h6><p>The offset has many uses. Sometimes we have a previously-fit model <em>h</em> ( <em>z</em> ) (where <em>z</em> might include or coincide with <em>x</em> ), and we wish to see if augmenting it with a linear model offers improvement. We would supply <em>oi</em> = <em>h</em> ( <em>zi</em> ) for each observation. For Poisson models the offset allows us to model rates rather than mean counts, if the observation period differs for each observation. Suppose we ob- serve a count <em>Y</em> over period <em>t</em> , thenE[ <em>Y</em> | <em>T</em> = <em>t,X</em> = <em>x</em> ] = <em>tμ</em> ( <em>x</em> ), where <em>μ</em> ( <em>x</em> ) is the rate per unit time. Using the log link, we would supply <em>oi</em> = log( <em>ti</em> ) for each observation. See Section 3.4.1 for an example.</p><p><em>Matrix input and weights:</em> Binomial and multinomial responses are typically supplied as a 2 or <em>K</em> -level factor. As an alternativeglmnetallows the response to be supplied in matrix form. This allows for <em>grouped</em> data, where at each <em>xi</em> we see a multinomial sample. In this case the rows of the <em>N</em> × <em>K</em> response matrix represent counts in each category. Alternatively the rows can be pro- portions summing to one. For the latter case, supplying an observation weight equal to the total count for each observation is equivalent to the first form. Trivially an indicator response matrix is equivalent to supplying the data as a factor, in <em>ungrouped</em> form.</p><p><em>Sparse model matrices</em> <strong>X</strong> <em>:</em> Often when <em>p</em>  <em>N</em> is very large, there are many zeros in the input matrix <strong>X</strong>. For example, in document models, each feature vector <em>xi</em> ∈R <em>p</em> might count the number of times each word in a very large dictionary occurs in a document. Such vectors and matrices can be stored efficiently by only storing the nonzero values, and then row and column indices of where they occur. Coordinate descent is ideally suited to capitalize on such sparsity, since it handles the variables one-at-a-time, and the principal operation is an inner-product. For example, in Section 3.4.1, the model-matrix <strong>X</strong> = <strong>I</strong> is the extremely sparse <em>N</em> × <em>N</em> identity matrix. Even with <em>N</em> = 10^6 , the program can compute the relaxation path at 100 values of <em>δ</em> in only 27 seconds.</p><h3 id="bibliographic-notes"><a class="header-anchor" href="#bibliographic-notes" aria-hidden="true">#</a> Bibliographic Notes</h3><p>Generalized linear models were proposed as a comprehensive class of models by Nelder and Wedderburn (1972); see the book by McCullagh and Nelder (1989) for a thorough account. Application of the lasso to logistic regression was proposed in Tibshirani (1996); coordinate descent methods for logistic, multinomial, and Poisson regression were developed in Friedman, Hastie, Hoe- fling and Tibshirani (2007), Friedman, Hastie and Tibshirani (2010 <em>b</em> ), Wu and Lange (2008), and Wu, Chen, Hastie, Sobel and Lange (2009). Pre-validation was proposed by Tibshirani and Efron (2002). Boser, Guyon and Vapnik (1992) described the support vector machine, with a thorough treatment in Vapnik (1996).</p><h6 id="exercises-53"><a class="header-anchor" href="#exercises-53" aria-hidden="true">#</a> EXERCISES 53</h6><h3 id="exercises-1"><a class="header-anchor" href="#exercises-1" aria-hidden="true">#</a> Exercises</h3><p>Ex. 3.1 Consider a linear logistic regression model with separable data, mean- ing that the data can be correctly separated into two classes by a hyperplane. Show that the likelihood estimates are unbounded, and that the log-likelihood objective reaches its maximal value of zero. Are the fitted probabilities well- defined?</p><p>Ex. 3.2 For a response variable <em>y</em> ∈{− 1 <em>,</em> +1}and a linear classification func- tion <em>f</em> ( <em>x</em> ) = <em>β</em> 0 + <em>βTx</em> , suppose that we classify according to sign( <em>f</em> ( <em>x</em> )). Show that the signed Euclidean distance of the point <em>x</em> with label <em>y</em> to the decision boundary is given by 1 ‖ <em>β</em> ‖ 2</p><div class="language-"><pre><code>y f ( x ). (3.45)\n</code></pre></div><p>Ex. 3.3 Here we show that for the multinomial model, the penalty used auto- matically imposes a normalization on the parameter estimates. We solve this problem for a general elastic-net penalty (Section 4.2). For some parameter <em>α</em> ∈[0 <em>,</em> 1] consider the problem</p><div class="language-"><pre><code>cj ( α ) = arg min\nt ∈R\n</code></pre></div><h6 id="k-4"><a class="header-anchor" href="#k-4" aria-hidden="true">#</a> { K</h6><h6 id="∑-14"><a class="header-anchor" href="#∑-14" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>` =1\n</code></pre></div><h6 id="-57"><a class="header-anchor" href="#-57" aria-hidden="true">#</a> [</h6><h6 id="_1-29"><a class="header-anchor" href="#_1-29" aria-hidden="true">#</a> 1</h6><h6 id="_2-13"><a class="header-anchor" href="#_2-13" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>(1− α )( βj` − t )^2 + α | βj` − t |\n</code></pre></div><h6 id="-58"><a class="header-anchor" href="#-58" aria-hidden="true">#</a> ]}</h6><h6 id="_3-46"><a class="header-anchor" href="#_3-46" aria-hidden="true">#</a> . (3.46)</h6><p>Let <em>β</em> ̄ <em>j</em> = <em>K</em>^1</p><h6 id="∑-k-3"><a class="header-anchor" href="#∑-k-3" aria-hidden="true">#</a> ∑ K</h6><p><em><code>_ =1 _βj</code></em> be the sample mean, and let <em>β</em> ̃ <em>j</em> be a sample median. (For simplicity, assume that <em>β</em> ̄ <em>j</em> ≤ <em>β</em> ̃ <em>j</em> ). Show that</p><div class="language-"><pre><code>β ̄ j ≤ cj ( α )≤ β ̃ j for all α ∈[0 , 1] (3.47)\n</code></pre></div><p>with the lower inequality achieved if <em>α</em> = 0, and the upper inequality achieved if <em>α</em> = 1.</p><p>Ex. 3.4 Derive the Lagrange dual (3.25) of the <em>maximum-entropy</em> prob- lem (3.24). Note that positivity is automatically enforced, since the log func- tion in the objective (3.24) serves as a barrier. ( <em>Hint:</em> It may help to intro- duce additional variables <em>wi</em> = <em>pi</em> − <em>ri</em> , and now minimize the criterion (3.24) with respect to both{ <em>pi,wi</em> } <em>Ni</em> =1, subject to the additional constraints that <em>wi</em> = <em>pi</em> − <em>ri</em> .)</p><p>Ex. 3.5 Recall the dual (3.25) of the maximum entropy problem, and the associated example motivating it. Suppose that for each cell, we also measure the value <em>xk</em> corresponding to the mid-cell ordinate on the continuous domain <em>x</em>. Consider the model</p><div class="language-"><pre><code>qk = ukeβ^0 +\n</code></pre></div><h6 id="∑-m"><a class="header-anchor" href="#∑-m" aria-hidden="true">#</a> ∑ M</h6><div class="language-"><pre><code>m =1 βmx\n</code></pre></div><div class="language-"><pre><code>mk + αk\n, (3.48)\n</code></pre></div><p>and suppose that we fit it using the penalized log-likelihood (3.25) without penalizing any of the coefficients. Show that for the estimated distribution ˆ <strong>q</strong> ={ <em>q</em> ˆ <em>k</em> } <em>Nk</em> =1, the moments of <em>X</em> up to order <em>M</em> match those of the empirical distribution <strong><em>r</em></strong> ={ <em>rk</em> } <em>Nk</em> =1.</p><h6 id="_54-generalized-linear-models"><a class="header-anchor" href="#_54-generalized-linear-models" aria-hidden="true">#</a> 54 GENERALIZED LINEAR MODELS</h6><p>Ex. 3.6 Consider the group-lasso-regularized version of multinomial regres- sion (3.20). Suppose that for a particular value of <em>λ</em> , the coefficient <em>β</em> ̂ <em>kj</em> is <em>not equal</em> to 0. Show that <em>β</em> ̂ <em><code>j_ 6 = 0 for all _</code></em> ∈(1 <em>,...,K</em> ), and moreover that ∑ <em>K <code>_ =1 _β_ ̂ _</code>j</em> = 0.</p><p>Ex. 3.7 This problem also applies to the group-lasso-regularized form of multi- nomial regression (3.20). Suppose that for a particular value of <em>λ</em> , and the fitted probabilities are ˆ <em>πi</em> = (ˆ <em>πi</em> 1 <em>,...,</em> ˆ <em>πiK</em> ) <em>T</em>. Similarly let <em>ri</em> = ( <em>ri</em> 1 <em>,...,riK</em> ) <em>T</em> be the observed proportions. Suppose we consider including an additional variable (vector) <em>Z</em> with observed values <em>zi</em> , and wish to update the fit. Let</p><p><em>g</em> =</p><h6 id="∑-n-41"><a class="header-anchor" href="#∑-n-41" aria-hidden="true">#</a> ∑ N</h6><p><em>i</em> =1 <em>zi</em> ( <em>ri</em> − <em>π</em> ˆ <em>i</em> ). Show that if‖ <em>g</em> ‖^2 <em>&lt; λ</em> , then the coefficients of <em>Z</em> are zero, and the model remains unchanged.</p><p>Ex. 3.8 The <em>squared hinge loss functionφ</em> sqh( <em>t</em> ) : = (1− <em>t</em> )^2 +can be used as a margin-based loss function <em>φ</em> ( <em>y f</em> ( <em>x</em> )) for binary classification problems.</p><div class="language-"><pre><code>(a) Show that φ sqhis differentiable everywhere.\n(b) Suppose Y ∈{− 1 , +1}with Pr( Y = +1) = π ∈(0 , 1). Find the function\nf :R p →Rthat minimizes (for each x ∈R p ) the criterion\n</code></pre></div><div class="language-"><pre><code>minimize\nf\n</code></pre></div><h6 id="e-y"><a class="header-anchor" href="#e-y" aria-hidden="true">#</a> E Y</h6><h6 id="-59"><a class="header-anchor" href="#-59" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>φ sqh\n</code></pre></div><h6 id="-60"><a class="header-anchor" href="#-60" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Y f ( x )\n</code></pre></div><h6 id="-61"><a class="header-anchor" href="#-61" aria-hidden="true">#</a> )]</h6><h6 id="_3-49"><a class="header-anchor" href="#_3-49" aria-hidden="true">#</a> (3.49)</h6><div class="language-"><pre><code>(c) Repeat part (b) using the usual hinge loss φ hin( t ) = (1− t )+.\n</code></pre></div><p>Ex. 3.9 Given binary responses <em>yi</em> ∈ {− 1 <em>,</em> +1}, consider the <em>`</em> 1 -regularized SVM problem</p><div class="language-"><pre><code>( β ̂ 0 ,β ̂) = arg min\nβ 0 ,β\n</code></pre></div><h6 id="-11"><a class="header-anchor" href="#-11" aria-hidden="true">#</a> </h6><h6 id="-9"><a class="header-anchor" href="#-9" aria-hidden="true">#</a> </h6><h6 id="-9"><a class="header-anchor" href="#-9" aria-hidden="true">#</a> </h6><h6 id="∑-n-42"><a class="header-anchor" href="#∑-n-42" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>{ 1 − yif ( xi ; β 0 ,β )}++ λ\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>| βj |\n</code></pre></div><h6 id="-10"><a class="header-anchor" href="#-10" aria-hidden="true">#</a> </h6><h6 id="-9"><a class="header-anchor" href="#-9" aria-hidden="true">#</a> </h6><h6 id="-9"><a class="header-anchor" href="#-9" aria-hidden="true">#</a> </h6><h6 id="_3-50"><a class="header-anchor" href="#_3-50" aria-hidden="true">#</a> , (3.50)</h6><p>where <em>f</em> ( <em>x</em> ; <em>β</em> 0 <em>,β</em> ) : = <em>β</em> 0 + <em>βTx</em>. In this exercise, we compare solutions of this problem to those of weighted <em>`</em> 2 -regularized SVM problem: given nonnegative weights{ <em>wj</em> } <em>pj</em> =1, we solve</p><div class="language-"><pre><code>( β ̃ 0 ,β ̃) = arg min\nβ 0 ,β\n</code></pre></div><h6 id="-12"><a class="header-anchor" href="#-12" aria-hidden="true">#</a> </h6><h6 id="-10"><a class="header-anchor" href="#-10" aria-hidden="true">#</a> </h6><h6 id="-10"><a class="header-anchor" href="#-10" aria-hidden="true">#</a> </h6><h6 id="∑-n-43"><a class="header-anchor" href="#∑-n-43" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>{ 1 − yif ( xi ; β 0 ,β )}++\n</code></pre></div><div class="language-"><pre><code>λ\n2\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>wjβ^2 j\n</code></pre></div><h6 id="-11"><a class="header-anchor" href="#-11" aria-hidden="true">#</a> </h6><h6 id="-10"><a class="header-anchor" href="#-10" aria-hidden="true">#</a> </h6><h6 id="-10"><a class="header-anchor" href="#-10" aria-hidden="true">#</a> </h6><h6 id="_3-51"><a class="header-anchor" href="#_3-51" aria-hidden="true">#</a> . (3.51)</h6><div class="language-"><pre><code>(a) Show that if we solve the problem (3.51) with wj = 1 / | β ̂ j |, then ( β ̃ 0 ,β ̃) =\n( β ̂ 0 ,β ̂).\n(b) For a given weight sequence{ wj } pj =1with wj ∈(0 , ∞) for all j = 1 ,...,p ,\nshow how to solve the criterion (3.51) using a regular unweighted SVM\nsolver. What do you do if wj =∞for some subset of indices?\n(c) In light of the preceding parts, suggest an iterative algorithm for the\nproblem (3.50) using a regular SVM solver.\n</code></pre></div><div class="language-"><pre><code>Chapter 4\n</code></pre></div><h2 id="generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> Generalizations of the Lasso Penalty</h2><h3 id="_4-1-introduction"><a class="header-anchor" href="#_4-1-introduction" aria-hidden="true">#</a> 4.1 Introduction</h3><p>In the previous chapter, we considered some generalizations of the lasso ob- tained by varying the loss function. In this chapter, we turn to some useful variations of the basic lasso <em><code>_ 1 -penalty itself, which expand the scope of the basic model. They all inherit the two essential features of the standard lasso, namely the shrinkage and selection of variables, or groups of variables. Such generalized penalties arise in a wide variety of settings. For instance, in microarray studies, we often find groups of correlated features, such as genes that operate in the same biological pathway. Empirically, the lasso sometimes does not perform well with highly correlated variables. By combining a squared _</code></em> 2 -penalty with the <em><code>_ 1 -penalty, we obtain the _elastic net_ , another penalized method that deals better with such correlated groups, and tends to select the correlated features (or not) together. In other applications, features may be structurally grouped. Examples include the dummy variables that are used to code a multilevel categorical predictor, or sets of coefficients in a multiple regression problem. In such settings, it is natural to select or omit all the coefficients within a group together. The _group lasso_ and the _overlap group lasso_ achieve these effects by using sums of (un-squared) _</code></em> 2 penalties. Another kind of structural grouping arises from an underlying index set such as time; our parameters might each have an associated time stamp. We might then ask for time-neighboring coefficients to be the same or similar. The <em>fused lasso</em> is a method naturally tailored to such situations. Finally, a variety of nonparametric smoothing methods operate implic- itly with large groups of variables. For example, each term in an additive smoothing-spline model has an associated cubic-spline basis. The grouped lasso extends naturally to these situations as well; the COSSO and the SPAM families are examples of such nonparametric models. In summary, all these variants deal with different kinds of groupings of the features in natural ways, and it is the goal of this chapter to explore them in some more detail.</p><div class="language-"><pre><code>55\n</code></pre></div><h6 id="_56-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_56-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 56 GENERALIZATIONS OF THE LASSO PENALTY</h6><h3 id="_4-2-the-elastic-net"><a class="header-anchor" href="#_4-2-the-elastic-net" aria-hidden="true">#</a> 4.2 The Elastic Net</h3><p>The lasso does not handle highly correlated variables very well; the coefficient paths tend to be erratic and can sometimes show wild behavior. Consider a simple but extreme example, where the coefficient for a variable <em>Xj</em> with a particular value for <em>λ</em> is <em>β</em> ̂ <em>j &gt;</em> 0. If we augment our data with an <em>identical</em> copy <em>Xj</em> ′= <em>Xj</em> , then they can share this coefficient in infinitely many ways—</p><p>any <em>β</em> ̃ <em>j</em> + <em>β</em> ̃ <em>j</em> ′ = <em>β</em> ˆ <em>j</em> with both pieces positive—and the loss and <em>`</em> 1 penalty are indifferent. So the coefficients for this pair are not defined. A quadratic penalty, on the other hand, will divide <em>β</em> ˆ <em>j</em> exactly equally between these two twins (see Exercise 4.1). In practice, we are unlikely to have an identical</p><div class="language-"><pre><code>0 1 2 3 4\n</code></pre></div><div class="language-"><pre><code>−1.5\n</code></pre></div><div class="language-"><pre><code>−0.5 0.0 0.5 1.0 1.5\n</code></pre></div><div class="language-"><pre><code>0 2 2 5 5\n</code></pre></div><div class="language-"><pre><code>0 1 2 3 4\n</code></pre></div><div class="language-"><pre><code>−1.5\n</code></pre></div><div class="language-"><pre><code>−0.5 0.0 0.5 1.0 1.5\n</code></pre></div><div class="language-"><pre><code>0 3 6 6 6\n</code></pre></div><div class="language-"><pre><code>‖βˆ‖ 1 ‖βˆ‖ 1\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>ˆβj\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>ˆβj\n</code></pre></div><div class="language-"><pre><code>α= 1. 0 α= 0. 3\n</code></pre></div><p><strong>Figure 4.1</strong> <em>Six variables, highly correlated in groups of three. The lasso estimates (α</em> = 1 <em>), as shown in the left panel, exhibit somewhat erratic behavior as the regu- larization parameterλis varied. In the right panel, the elastic net with (α</em> = 0_._ 3 <em>) includes al l the variables, and the correlated groups are pul led together.</em></p><p>pair of variables, but often we do have groups of very correlated variables. In microarray studies, groups of genes in the same biological pathway tend to be expressed (or not) together, and hence measures of their expression tend to be strongly correlated. The left panel of Figure 4.1 shows the lasso coefficient path for such a situation. There are two sets of three variables, with pairwise correlations around 0_._ 97 in each group. With a sample size of <em>N</em> = 100, the data were simulated as follows:</p><div class="language-"><pre><code>Z 1 , Z 2 ∼ N (0 , 1) independent,\nY = 3· Z 1 − 1. 5 Z 2 + 2 ε, with ε ∼ N (0 , 1),\nXj = Z 1 + ξj/ 5 , with ξj ∼ N (0 , 1) for j = 1 , 2 , 3, and\nXj = Z 2 + ξj/ 5 , with ξj ∼ N (0 , 1) for j = 4 , 5 , 6.\n</code></pre></div><h6 id="_4-1"><a class="header-anchor" href="#_4-1" aria-hidden="true">#</a> (4.1)</h6><p>As shown in the left panel of Figure 4.1, the lasso coefficients do not reflect the relative importance of the individual variables.</p><h6 id="the-elastic-net-57"><a class="header-anchor" href="#the-elastic-net-57" aria-hidden="true">#</a> THE ELASTIC NET 57</h6><p>The <em>elastic net</em> makes a compromise between the ridge and the lasso penal- ties (Zou and Hastie 2005); it solves the convex program</p><div class="language-"><pre><code>minimize\n( β 0 ,β )∈R×R p\n</code></pre></div><h6 id="-62"><a class="header-anchor" href="#-62" aria-hidden="true">#</a> {</h6><h6 id="_1-30"><a class="header-anchor" href="#_1-30" aria-hidden="true">#</a> 1</h6><h6 id="_2-16"><a class="header-anchor" href="#_2-16" aria-hidden="true">#</a> 2</h6><h6 id="∑-n-44"><a class="header-anchor" href="#∑-n-44" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi − β 0 − xTiβ )^2 + λ\n</code></pre></div><h6 id="-63"><a class="header-anchor" href="#-63" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>1\n2 (1− α )‖ β ‖\n</code></pre></div><div class="language-"><pre><code>2\n2 + α ‖ β ‖^1\n</code></pre></div><h6 id="-64"><a class="header-anchor" href="#-64" aria-hidden="true">#</a> ]</h6><h6 id="-65"><a class="header-anchor" href="#-65" aria-hidden="true">#</a> }</h6><h6 id="_4-2"><a class="header-anchor" href="#_4-2" aria-hidden="true">#</a> , (4.2)</h6><p>where <em>α</em> ∈[0 <em>,</em> 1] is a parameter that can be varied. By construction, the penalty applied to an individual coefficient (disregarding the regularization weight <em>λ &gt;</em> 0) is given by</p><div class="language-"><pre><code>1\n2\n</code></pre></div><div class="language-"><pre><code>(1− α ) β^2 j + α | βj |. (4.3)\n</code></pre></div><p>When <em>α</em> = 1, it reduces to the <em><code>_ 1 -norm or lasso penalty, and with _α_ = 0, it reduces to the squared _</code></em> 2 -norm, corresponding to the ridge penalty.^1 Returning to Figure 4.1, the right-hand panel shows the elastic-net coeffi- cient path with <em>α</em> = 0_._ 3. We see that in contrast to the lasso paths in the left panel, the coefficients are selected approximately together in their groups, and also approximately share their values equally. Of course, this example is ide- alized, and in practice the group structure will not be so cleanly evident. But by adding some component of the ridge penalty to the <em><code>_ 1 -penalty, the elastic net automatically controls for strong within-group correlations. Moreover, for any _α &lt;_ 1 and _λ &gt;_ 0, the elastic-net problem (4.2) is _strictly convex:_ a unique solution exists irrespective of the correlations or duplications in the _Xj_. Figure 4.2 compares the constraint region for the elastic net (left image) to that of the lasso (right image) when there are three variables. We see that the elastic-net ball shares attributes of the _</code></em> 2 ball and the <em>`</em> 1 ball: the sharp corners and edges encourage selection, and the curved contours encourage sharing of coefficients. See Exercise 4.2 for further exploration of these properties. The elastic net has an additional tuning parameter <em>α</em> that has to be de- termined. In practice, it can be viewed as a higher-level parameter, and can be set on subjective grounds. Alternatively, one can include a (coarse) grid of values of <em>α</em> in a cross-validation scheme. The elastic-net problem (4.2) is convex in the pair ( <em>β</em> 0 <em>,β</em> )∈R×R <em>p</em> , and a variety of different algorithms can be used to solve it. Coordinate descent is particularly effective, and the updates are a simple extension of those for the lasso in Chapter 2. We have included an unpenalized intercept in the model, which can be dispensed with at the onset; we simply center the covariates <em>xij</em> , and then the optimal intercept is <em>β</em> ̂ 0 = ̄ <em>y</em> = <em>N</em>^1</p><h6 id="∑-n-45"><a class="header-anchor" href="#∑-n-45" aria-hidden="true">#</a> ∑ N</h6><p><em>j</em> =1 <em>yj</em>. Having solved for the optimal <em>β</em> ̂ 0 , it remains to compute the optimal vector <em>β</em> ̂= ( <em>β</em> ̂ 1 <em>,...,β</em> ̂ <em>p</em> ). It can be verified (Exercise 4.3) that the coordinate descent update for the <em>jth</em></p><p>(^1) The 1 2 in the quadratic part of the elastic-net penalty (4.3) leads to a more intuitive soft-thresholding operator in the optimization.</p><h6 id="_58-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_58-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 58 GENERALIZATIONS OF THE LASSO PENALTY</h6><div class="language-"><pre><code>β 2\n</code></pre></div><div class="language-"><pre><code>β 1\n</code></pre></div><div class="language-"><pre><code>β 3\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>β 1\nβ 2\n</code></pre></div><div class="language-"><pre><code>β 3\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><p><strong>Figure 4.2</strong> <em>The elastic-net bal l withα</em> = 0_._ 7 <em>(left panel) in</em> R^3 <em>, compared to the `</em> 1 <em>ball (right panel). The curved contours encourage strongly correlated variables to share coefficients (see Exercise 4.2 for details).</em></p><p>coefficient takes the form</p><div class="language-"><pre><code>β ̂ j = S λα\n</code></pre></div><h6 id="∑-n-46"><a class="header-anchor" href="#∑-n-46" aria-hidden="true">#</a> (∑ N</h6><div class="language-"><pre><code>i =1 rijxij\n</code></pre></div><h6 id="-66"><a class="header-anchor" href="#-66" aria-hidden="true">#</a> )</h6><h6 id="∑-n-47"><a class="header-anchor" href="#∑-n-47" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1 x\n2\nij + λ (1− α )\n</code></pre></div><h6 id="_4-4"><a class="header-anchor" href="#_4-4" aria-hidden="true">#</a> , (4.4)</h6><p>whereS <em>μ</em></p><h6 id="-67"><a class="header-anchor" href="#-67" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>z\n</code></pre></div><h6 id="-68"><a class="header-anchor" href="#-68" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>: = sign( z ) ( z − μ )+is the soft-thresholding operator, and\n</code></pre></div><p><em>rij</em> : = <em>yi</em> − <em>β</em> ̂ 0 −</p><h6 id="∑-15"><a class="header-anchor" href="#∑-15" aria-hidden="true">#</a> ∑</h6><p><em>k</em> 6 = <em>jxik β</em> ̂ <em>k</em> is the partial residual. We cycle over the up- dates (4.4) until convergence. Friedman et al. (2015) give more details, and provide an efficient implementation of the elastic net penalty for a variety of loss functions.</p><h3 id="_4-3-the-group-lasso"><a class="header-anchor" href="#_4-3-the-group-lasso" aria-hidden="true">#</a> 4.3 The Group Lasso</h3><p>There are many regression problems in which the covariates have a natural group structure, and it is desirable to have all coefficients within a group become nonzero (or zero) simultaneously. The various forms of group lasso penalty are designed for such situations. A leading example is when we have qualitative factors among our predictors. We typically code their levels using a set of dummy variables or contrasts, and would want to include or exclude this group of variables together. We first define the group lasso and then develop this and other motivating examples. Consider a linear regression model involving <em>J</em> groups of covariates, where for <em>j</em> = 1 <em>,...,J</em> , the vector <em>Zj</em> ∈R <em>pj</em> represents the covariates in group <em>j</em>. Our goal is to predict a real-valued response <em>Y</em> ∈Rbased on the collection of covariates ( <em>Z</em> 1 <em>,...,ZJ</em> ). A linear model for the regression functionE( <em>Y</em> | <em>Z</em> )</p><h6 id="the-group-lasso-59"><a class="header-anchor" href="#the-group-lasso-59" aria-hidden="true">#</a> THE GROUP LASSO 59</h6><p>takes the form <em>θ</em> 0 +</p><h6 id="∑-j"><a class="header-anchor" href="#∑-j" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1 Z\n</code></pre></div><div class="language-"><pre><code>T\njθj , where θj ∈R\npj represents a group of pj\n</code></pre></div><p>regression coefficients.^2 Given a collection of <em>N</em> samples{( <em>yi,zi</em> 1 <em>,zi,</em> 2 <em>,...,zi,J</em> )} <em>Ni</em> =1, the group lasso solves the convex problem</p><div class="language-"><pre><code>minimize\nθ 0 ∈R ,θj ∈R pj\n</code></pre></div><h6 id="-13"><a class="header-anchor" href="#-13" aria-hidden="true">#</a> </h6><h6 id="-11"><a class="header-anchor" href="#-11" aria-hidden="true">#</a> </h6><h6 id="-11"><a class="header-anchor" href="#-11" aria-hidden="true">#</a> </h6><h6 id="_1-31"><a class="header-anchor" href="#_1-31" aria-hidden="true">#</a> 1</h6><h6 id="_2-18"><a class="header-anchor" href="#_2-18" aria-hidden="true">#</a> 2</h6><h6 id="∑-n-48"><a class="header-anchor" href="#∑-n-48" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><h6 id="-69"><a class="header-anchor" href="#-69" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>yi − θ 0 −\n</code></pre></div><h6 id="∑-j-1"><a class="header-anchor" href="#∑-j-1" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>zijTθj\n</code></pre></div><h6 id="_2-20"><a class="header-anchor" href="#_2-20" aria-hidden="true">#</a> ) 2</h6><div class="language-"><pre><code>+ λ\n</code></pre></div><h6 id="∑-j-2"><a class="header-anchor" href="#∑-j-2" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>‖ θj ‖ 2\n</code></pre></div><h6 id="-12"><a class="header-anchor" href="#-12" aria-hidden="true">#</a> </h6><h6 id="-11"><a class="header-anchor" href="#-11" aria-hidden="true">#</a> </h6><h6 id="-11"><a class="header-anchor" href="#-11" aria-hidden="true">#</a> </h6><h6 id="_4-5"><a class="header-anchor" href="#_4-5" aria-hidden="true">#</a> , (4.5)</h6><p>where‖ <em>θj</em> ‖ 2 is the Euclidean norm of the vector <em>θj</em>. This is a group generalization of the lasso, with the properties:</p><ul><li>depending on <em>λ</em> ≥0, either the entire vector <em>θ</em> ˆ <em>j</em> will be zero, or all its elements will be nonzero;^3</li><li>when <em>pj</em> = 1, then we have‖ <em>θj</em> ‖ 2 =| <em>θj</em> |, so if all the groups are singletons, the optimization problem (4.5) reduces to the ordinary lasso.</li></ul><p>Figure 4.3 compares the constraint region for the group lasso (left image) to that of the lasso (right image) when there are three variables. We see that the group lasso ball shares attributes of both the <em><code>_ 2 and _</code></em> 1 balls.</p><div class="language-"><pre><code>β 1\n</code></pre></div><div class="language-"><pre><code>β 3\n</code></pre></div><div class="language-"><pre><code>β 2\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>β 1\nβ 2\n</code></pre></div><div class="language-"><pre><code>β 3\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><p><strong>Figure 4.3</strong> <em>The group lasso bal l (left panel) in</em> R^3 <em>, compared to the`</em> 1 <em>ball (right panel). In this case, there are two groups with coefficientsθ</em> 1 = ( <em>β</em> 1 <em>,β</em> 2 )∈R^2 <em>and θ</em> 2 = <em>β</em> 3 ∈R^1_._</p><p>In the formulation (4.5), all groups are equally penalized, a choice which leads larger groups to be more likely to be selected. In their original pro- posal, Yuan and Lin (2006) recommended weighting the penalties for each group according to their size, by a factor</p><h6 id="√-5"><a class="header-anchor" href="#√-5" aria-hidden="true">#</a> √</h6><p><em>pj</em>. In their case, the group ma- trices <strong>Z</strong> <em>j</em> were orthonormal; for general matrices one can argue for a factor</p><p>(^2) To avoid confusion, we use <em>Zj</em> and <em>θj</em> to represent groups of variables and their coeffi- cients, rather than the <em>Xj</em> and <em>βj</em> we have used for scalars. (^3) Nonzero for generic problems, although special structure could result in some coefficients in a group being zero, just as they can for linear or ridge regression.</p><h6 id="_60-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_60-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 60 GENERALIZATIONS OF THE LASSO PENALTY</h6><p>‖ <strong>Z</strong> <em>j</em> ‖ <em>F</em> (Exercise 4.5). These choices are somewhat subjective, and are easily accommodated; for simplicity, we omit this modification in our presentation. We now turn to some examples to illustrate applications of the group lasso (4.5).</p><p><em>Example 4.1. Regression with multilevel factors.</em> When a predictor variable in a linear regression is a multilevel factor, we typically include a separate coefficient for each level of the factor. Take the simple case of one continuous predictor <em>X</em> and a three-level factor <em>G</em> with levels <em>g</em> 1 , <em>g</em> 2 , and <em>g</em> 3. Our linear model for the mean is</p><div class="language-"><pre><code>E( Y | X,G ) = Xβ +\n</code></pre></div><h6 id="∑-3-1"><a class="header-anchor" href="#∑-3-1" aria-hidden="true">#</a> ∑^3</h6><div class="language-"><pre><code>k =1\n</code></pre></div><div class="language-"><pre><code>θk I k [ G ] , (4.6)\n</code></pre></div><p>whereI <em>k</em> [ <em>G</em> ] is a 0-1 valued indicator function for the event{ <em>G</em> = <em>gk</em> }. The model (4.6) corresponds to a linear regression in <em>X</em> with different intercepts <em>θk</em> depending on the level of <em>G</em>. By introducing a vector <em>Z</em> = ( <em>Z</em> 1 <em>,Z</em> 2 <em>,Z</em> 3 ) of three <em>dummy variables</em> with <em>Zk</em> =I <em>k</em> [ <em>G</em> ], we can write this model as a standard linear regression</p><div class="language-"><pre><code>E( Y | X,G ) =E( Y | X,Z ) = Xβ + ZTθ, (4.7)\n</code></pre></div><p>where <em>θ</em> = ( <em>θ</em> 1 <em>,θ</em> 2 <em>,θ</em> 3 ). In this case <em>Z</em> is a group variable that represents the single factor <em>G</em>. If the variable <em>G</em> —as coded by the vector <em>Z</em> —has no predictive power, then the full vector <em>θ</em> = ( <em>θ</em> 1 <em>,θ</em> 2 <em>,θ</em> 3 ) should be zero. On the other hand, when <em>G</em> is useful for prediction, then at least generically, we expect that all coefficients of <em>θ</em> are likely to be nonzero. More generally we can have a number of such single and group variables, and so have models of the form</p><div class="language-"><pre><code>E( Y | X,G 1 ,...,GJ ) = β 0 + XTβ +\n</code></pre></div><h6 id="∑-j-3"><a class="header-anchor" href="#∑-j-3" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>ZjTθj. (4.8)\n</code></pre></div><p>When selecting variables for such a model we would typically want to include or exclude groups at a time, rather than individual coefficients, and the group lasso is designed to enforce such behavior. With unpenalized linear regression with factors, one has to worry about aliasing; in the example here, the dummy variables in a set add to one, which is aliased with the intercept term. One would then use contrasts to code factors that enforce, for example, that the coefficients in a group sum to zero. With the group lasso this is not a concern, because of the <em>`</em> 2 penalties. We use the symmetric full representation as above, because the penalty term ensures that the coefficients in a group sum to zero (see Exercise 4.4). ♦</p><p>Variables can be grouped for other reasons. For example, in gene-expression arrays, we might have a set of highly correlated genes from the same biological pathway. Selecting the group amounts to selecting a pathway. Figure 4.4 shows the coefficient path for a group-lasso fit to some genomic data for splice-site</p><h6 id="the-group-lasso-61"><a class="header-anchor" href="#the-group-lasso-61" aria-hidden="true">#</a> THE GROUP LASSO 61</h6><p>detection (Meier, van de Geer and B ̈uhlmann 2008, Section 5). The data arise from human DNA, and each observation consists of seven bases with values { <em>A,G,C,T</em> }^7. Some of the observations are at exon-intron boundaries (splice sites), and others not, coded in a binary response; see Burge and Karlin (1977) for further details about these data. The regression problem is to predict the binary response <em>Y</em> using the seven four-level factors <em>Gj</em> as predictors, and we use a training sample of 5610 observations in each class.</p><div class="language-"><pre><code>Group Lasso\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>0.16 0.14 0.12 0.1 0.08 0.06 0.04 0.02 0\n</code></pre></div><div class="language-"><pre><code>−0.1\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.1\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>t\n</code></pre></div><div class="language-"><pre><code>a\n</code></pre></div><div class="language-"><pre><code>a\n</code></pre></div><div class="language-"><pre><code>a\n</code></pre></div><div class="language-"><pre><code>c\n</code></pre></div><div class="language-"><pre><code>c\n</code></pre></div><div class="language-"><pre><code>Pos7\n</code></pre></div><div class="language-"><pre><code>Pos2\n</code></pre></div><div class="language-"><pre><code>Pos5\n</code></pre></div><div class="language-"><pre><code>Pos4\n</code></pre></div><div class="language-"><pre><code>Pos3\n</code></pre></div><div class="language-"><pre><code>Pos6\n</code></pre></div><div class="language-"><pre><code>λ\n</code></pre></div><p><strong>Figure 4.4</strong> <em>Coefficient profiles from the group lasso, fit to splice-site detection data. The coefficients come in groups of four, corresponding to the nucleotidesA,G,C,T. The vertical lines indicate when a group enters. On the right-hand side we label some of the variables; for example, “Pos6” and the level “c”. The coefficients in a group have the same color, and they always average zero.</em></p><p><em>Example 4.2. Multivariate regression.</em> Sometimes we are interested in predict- ing a multivariate response <em>Y</em> ∈R <em>K</em> on the basis of a vector <em>X</em> ∈R <em>p</em> of predic- tors (also known as <em>multitask learning</em> ). Given <em>N</em> observations{( <em>yi,xi</em> )} <em>Ni</em> =1, we let <strong>Y</strong> ∈R <em>N</em> × <em>K</em> and <strong>X</strong> ∈R <em>N</em> × <em>p</em> be matrices with <em>yi</em> and <em>xi</em> , respectively, as their <em>ith</em> row. If we assume a linear model for the full collection of data, then it can be written in the form</p><div class="language-"><pre><code>Y = XΘ + E (4.9)\n</code></pre></div><p>where <strong>Θ</strong> ∈R <em>p</em> × <em>K</em> is a matrix of coefficients, and <strong>E</strong> ∈R <em>N</em> × <em>K</em> a matrix of errors. One way to understand the model (4.9) is as a coupled collection of <em>K</em> standard regression problems inR <em>p</em> , each sharing the same covariates, in which</p><h6 id="_62-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_62-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 62 GENERALIZATIONS OF THE LASSO PENALTY</h6><p>the <em>kth</em> column <em>θk</em> ∈R <em>p</em> of <strong>Θ</strong> is the coefficient vector for the <em>kth</em> problem. Thus, in principle, we could fit a separate regression coefficient vector <em>θk</em> for each of the <em>K</em> different problems, using the lasso in the case of a sparse linear model. In many applications, the different components of the response vector <em>Y</em> ∈R <em>K</em> are strongly related, so that one would expect that the underlying regression vectors would also be related. For instance, in collaborative filtering applications, the different components of <em>Y</em> might represent a given user’s preference scores for different categories of objects, such as books, movies, music, and so on, all of which are closely related. For this reason, it is natural— and often leads to better prediction performance—to solve the <em>K</em> regression problems jointly, imposing some type of group structure on the coefficients. In another example, each response might be the daily return of an equity in a particular market sector; hence we have multiple equities, and all being predicted by the same market signals. As one example, in the setting of sparsity, we might posit that there is an unknown subset <em>S</em> ⊂ { 1 <em>,</em> 2 <em>,...,p</em> }of the covariates that are relevant for prediction, and this same subset is <em>preserved</em> across all <em>K</em> components of the response variable. In this case, it would be natural to consider a group lasso penalty, in which the <em>p</em> groups are defined by the rows{ <em>θ</em> ′ <em>j</em> ∈R <em>K, j</em> = 1 <em>,...,p</em> } of the full coefficient matrix <strong>Θ</strong> ∈R <em>p</em> × <em>K</em>. Using this penalty, we then solve the regularized least-squares problem</p><div class="language-"><pre><code>minimize\nΘ ∈R p × K\n</code></pre></div><h6 id="-14"><a class="header-anchor" href="#-14" aria-hidden="true">#</a> </h6><h6 id="-12"><a class="header-anchor" href="#-12" aria-hidden="true">#</a> </h6><h6 id="-12"><a class="header-anchor" href="#-12" aria-hidden="true">#</a> </h6><h6 id="_1-32"><a class="header-anchor" href="#_1-32" aria-hidden="true">#</a> 1</h6><h6 id="_2-22"><a class="header-anchor" href="#_2-22" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ Y − XΘ ‖^2 F+ λ\n</code></pre></div><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>‖ θ ′ j ‖ 2\n</code></pre></div><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id="-13"><a class="header-anchor" href="#-13" aria-hidden="true">#</a> </h6><h6 id="-12"><a class="header-anchor" href="#-12" aria-hidden="true">#</a> </h6><h6 id="-12"><a class="header-anchor" href="#-12" aria-hidden="true">#</a> </h6><h6 id="_4-10"><a class="header-anchor" href="#_4-10" aria-hidden="true">#</a> , (4.10)</h6><p>where‖·‖Fdenotes the Frobenius norm.^4 This problem is a special case of the general group lasso (4.5), in which <em>J</em> = <em>p</em> , and <em>pj</em> = <em>K</em> for all groups <em>j</em> .♦</p><h4 id="_4-3-1-computation-for-the-group-lasso"><a class="header-anchor" href="#_4-3-1-computation-for-the-group-lasso" aria-hidden="true">#</a> 4.3.1 Computation for the Group Lasso</h4><p>Turning to computational issues associated with the group lasso, let us rewrite the relevant optimization problem (4.5) in a more compact matrix-vector no- tation:</p><div class="language-"><pre><code>minimize\n( θ 1 ,...,θJ )\n</code></pre></div><h6 id="-15"><a class="header-anchor" href="#-15" aria-hidden="true">#</a> </h6><h6 id="-13"><a class="header-anchor" href="#-13" aria-hidden="true">#</a> </h6><h6 id="-13"><a class="header-anchor" href="#-13" aria-hidden="true">#</a> </h6><h6 id="_1-33"><a class="header-anchor" href="#_1-33" aria-hidden="true">#</a> 1</h6><h6 id="_2-23"><a class="header-anchor" href="#_2-23" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ y −\n</code></pre></div><h6 id="∑-j-4"><a class="header-anchor" href="#∑-j-4" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>Z jθj ‖^22 + λ\n</code></pre></div><h6 id="∑-j-5"><a class="header-anchor" href="#∑-j-5" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>‖ θj ‖ 2\n</code></pre></div><h6 id="-14"><a class="header-anchor" href="#-14" aria-hidden="true">#</a> </h6><h6 id="-13"><a class="header-anchor" href="#-13" aria-hidden="true">#</a> </h6><h6 id="-13"><a class="header-anchor" href="#-13" aria-hidden="true">#</a> </h6><h6 id="_4-11"><a class="header-anchor" href="#_4-11" aria-hidden="true">#</a> . (4.11)</h6><p>For simplicity we ignore the intercept <em>θ</em> 0 , since in practice we can center all the variables and the response, and it goes away. For this problem, the zero subgradient equations (see Section 5.2.2 take the form</p><div class="language-"><pre><code>− Z Tj ( y −\n</code></pre></div><h6 id="∑-j-6"><a class="header-anchor" href="#∑-j-6" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>` =1\n</code></pre></div><div class="language-"><pre><code>Z ` ̂ θ` ) + λ ̂ sj = 0 , for j = 1 , ··· J , (4.12)\n</code></pre></div><p>(^4) The Frobenius norm of a matrix is simply the <em>`</em> 2 -norm applied to its entries.</p><h6 id="the-group-lasso-63"><a class="header-anchor" href="#the-group-lasso-63" aria-hidden="true">#</a> THE GROUP LASSO 63</h6><p>wherê <em>sj</em> ∈R <em>pj</em> is an element of the subdifferential of the norm‖·‖ 2 evaluated at <em>θ</em> ̂ <em>j</em>. As verified in Exercise 5.5 on page 135, whenever <em>θ</em> ̂ <em>j</em> 6 = 0, then we</p><p>necessarily havê <em>sj</em> =̂ <em>θj/</em> ‖̂ <em>θj</em> ‖ 2 , whereas when <em>θ</em> ̂ <em>j</em> = 0, then̂ <em>sj</em> is any vector with‖̂ <em>sj</em> ‖ 2 ≤1. One method for solving the zero subgradient equations is by</p><p>holding fixed all block vectors{̂ <em>θk, k</em> 6 = <em>j</em> }, and then solving for̂ <em>θj</em>. Doing so amounts to performing block coordinate descent on the group lasso objective function. Since the problem is convex, and the penalty is block separable, it is guaranteed to converge to an optimal solution (Tseng 1993). With all {̂ <em>θk, k</em> 6 = <em>j</em> }fixed, we write</p><div class="language-"><pre><code>− Z Tj ( r j − Z jθ ̂ j ) + λ ̂ sj = 0 , (4.13)\n</code></pre></div><p>where <strong><em>r</em></strong> <em>j</em> = <strong>y</strong> −</p><h6 id="∑-16"><a class="header-anchor" href="#∑-16" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>k 6 = j Z kθ ̂ k is the j\nthpartial residual. From the conditions\n</code></pre></div><p>satisfied by the subgradient̂ <em>sj</em> , we must havê <em>θj</em> = 0 if‖ <strong>Z</strong> <em>Tj</em> <strong><em>r</em></strong> <em>j</em> ‖ 2 <em>&lt; λ</em> , and</p><p>otherwise the minimizer̂ <em>θj</em> must satisfy</p><div class="language-"><pre><code>̂ θj =\n</code></pre></div><h6 id="-70"><a class="header-anchor" href="#-70" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Z Tj Z j +\n</code></pre></div><div class="language-"><pre><code>λ\n‖̂ θj ‖ 2\n</code></pre></div><h6 id="i"><a class="header-anchor" href="#i" aria-hidden="true">#</a> I</h6><h6 id="−-1-1"><a class="header-anchor" href="#−-1-1" aria-hidden="true">#</a> )− 1</h6><div class="language-"><pre><code>Z Tj r j. (4.14)\n</code></pre></div><p>This update is similar to the solution of a ridge regression problem, except that the underlying penalty parameter depends on‖ <em>θ</em> ̂ <em>j</em> ‖ 2. Unfortunately, Equa- tion (4.14) does not have a closed-form solution for <em>θ</em> ̂ <em>j</em> unless <strong>Z</strong> <em>j</em> is orthonor- mal. In this special case, we have the simple update</p><div class="language-"><pre><code>̂ θj =\n</code></pre></div><h6 id="-71"><a class="header-anchor" href="#-71" aria-hidden="true">#</a> (</h6><h6 id="_1-−-1"><a class="header-anchor" href="#_1-−-1" aria-hidden="true">#</a> 1 −</h6><div class="language-"><pre><code>λ\n‖ Z Tj r j ‖ 2\n</code></pre></div><h6 id="-72"><a class="header-anchor" href="#-72" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>+\n</code></pre></div><div class="language-"><pre><code>Z Tj r j, (4.15)\n</code></pre></div><p>where ( <em>t</em> )+: = max{ 0 <em>,t</em> }is the positive part function. See Exercise 4.6 for further details. Although the original authors (Yuan and Lin 2006) and many others since have made the orthonormality assumption, it has implications that are not always reasonable (Simon and Tibshirani 2012). Exercise 4.8 explores the im- pact of this assumption on the dummy coding used here for factors. In the general (nonorthonormal case) one has to solve (4.14) using iterative methods, and it reduces to a very simple one-dimensional search (Exercise 4.7). An alternative approach is to apply the composite gradient methods of Section 5.3.3 to this problem. Doing so leads to an algorithm that is also iterative within each block; at each iteration the block-optimization problem is approximated by an easier problem, for which an update such as (4.15) is possible. In detail, the algorithm would iterate until convergence the updates</p><div class="language-"><pre><code>ω ← θ ̂ j + ν · Z Tj ( r j − Z jθ ̂ j ) , and (4.16a)\n</code></pre></div><div class="language-"><pre><code>̂ θj ←\n</code></pre></div><h6 id="-73"><a class="header-anchor" href="#-73" aria-hidden="true">#</a> (</h6><h6 id="_1-−-2"><a class="header-anchor" href="#_1-−-2" aria-hidden="true">#</a> 1 −</h6><div class="language-"><pre><code>νλ\n‖ ω ‖ 2\n</code></pre></div><h6 id="-74"><a class="header-anchor" href="#-74" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>+\n</code></pre></div><div class="language-"><pre><code>ω, (4.16b)\n</code></pre></div><p>where <em>ν</em> is a step-size parameter. See Exercise 4.9 for details of this derivation.</p><h6 id="_64-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_64-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 64 GENERALIZATIONS OF THE LASSO PENALTY</h6><h4 id="_4-3-2-sparse-group-lasso"><a class="header-anchor" href="#_4-3-2-sparse-group-lasso" aria-hidden="true">#</a> 4.3.2 Sparse Group Lasso</h4><p>When a group is included in a group-lasso fit, all the coefficients in that group are nonzero. This is a consequence of the <em><code>_ 2 norm. Sometimes we would like sparsity both with respect to which groups are selected, and which coefficients are nonzero within a group. For example, although a biological pathway may be implicated in the progression of a particular type of cancer, not all genes in the pathway need be active. The _sparse group lasso_ is designed to achieve such within-group sparsity. In order to achieve within-group sparsity, we augment the basic group lasso (4.11) with an additional _</code></em> 1 -penalty, leading to the convex program</p><div class="language-"><pre><code>minimize\n{ θj ∈R pj } Jj =1\n</code></pre></div><h6 id="-16"><a class="header-anchor" href="#-16" aria-hidden="true">#</a> </h6><h6 id="-14"><a class="header-anchor" href="#-14" aria-hidden="true">#</a> </h6><h6 id="-14"><a class="header-anchor" href="#-14" aria-hidden="true">#</a> </h6><h6 id="_1-34"><a class="header-anchor" href="#_1-34" aria-hidden="true">#</a> 1</h6><h6 id="_2-27"><a class="header-anchor" href="#_2-27" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ y −\n</code></pre></div><h6 id="∑-j-7"><a class="header-anchor" href="#∑-j-7" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>Z jθj ‖^22 + λ\n</code></pre></div><h6 id="∑-j-8"><a class="header-anchor" href="#∑-j-8" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><h6 id="-75"><a class="header-anchor" href="#-75" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>(1− α )‖ θj ‖ 2 + α ‖ θj ‖ 1\n</code></pre></div><h6 id="-76"><a class="header-anchor" href="#-76" aria-hidden="true">#</a> ]</h6><h6 id="-15"><a class="header-anchor" href="#-15" aria-hidden="true">#</a> </h6><h6 id="-14"><a class="header-anchor" href="#-14" aria-hidden="true">#</a> </h6><h6 id="-14"><a class="header-anchor" href="#-14" aria-hidden="true">#</a> </h6><h6 id="_4-17"><a class="header-anchor" href="#_4-17" aria-hidden="true">#</a> , (4.17)</h6><p>with <em>α</em> ∈[0 <em>,</em> 1]. Much like the elastic net of Section 4.2, the parameter <em>α</em> creates a bridge between the group lasso ( <em>α</em> = 0) and the lasso ( <em>α</em> = 1). Figure 4.5 contrasts the group lasso constraint region with that of the sparse group lasso for the case of three variables. Note that in the two horizontal axes, the constraint region resembles that of the elastic net.</p><div class="language-"><pre><code>β 1\n</code></pre></div><div class="language-"><pre><code>β 3\n</code></pre></div><div class="language-"><pre><code>β 2\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>β 3\n</code></pre></div><div class="language-"><pre><code>β 2\n</code></pre></div><div class="language-"><pre><code>β 1\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><p><strong>Figure 4.5</strong> <em>The group lasso bal l (left panel) in</em> R^3 <em>, compared to the sparse group- lasso bal l withα</em> = 0_._ 5 <em>(right panel). Depicted are two groups with coefficientsθ</em> 1 = ( <em>β</em> 1 <em>,β</em> 2 )∈R^2 <em>andθ</em> 2 = <em>β</em> 3 ∈R^1_._</p><p>Since the optimization problem (4.17) is convex, its optima are specified by zero subgradient equations, similar to (4.13) for the group lasso. More precisely, any optimal solution must satisfy the condition</p><div class="language-"><pre><code>− Z Tj ( y −\n</code></pre></div><h6 id="∑-j-9"><a class="header-anchor" href="#∑-j-9" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>` =1\n</code></pre></div><div class="language-"><pre><code>Z `θ ̂ ` ) + λ (1− α )·̂ sj + λα ̂ tj = 0 , for j = 1 , ··· ,J , (4.18)\n</code></pre></div><p>wherê <em>sj</em> ∈R <em>pj</em> belongs to the subdifferential of the Euclidean norm at̂ <em>θj</em> ,</p><h6 id="the-group-lasso-65"><a class="header-anchor" href="#the-group-lasso-65" aria-hidden="true">#</a> THE GROUP LASSO 65</h6><p>and̂ <em>tj</em> ∈R <em>pj</em> belongs to the subdifferential of the <em>`</em> 1 -norm at̂ <em>θj</em> ; in particular, we have eacĥ <em>tjk</em> ∈sign( <em>θjk</em> ) as with the usual lasso. We once again solve these equations via block-wise coordinate descent, although the solution is a bit more complex than before. As in Equation (4.13), with <strong><em>r</em></strong> <em>j</em> the partial residual in the <em>jth</em> coordinate, it can be seen that <em>θ</em> ̂ <em>j</em> = 0 if and only if the equation</p><div class="language-"><pre><code>Z Tj r j = λ (1− α )̂ sj + λα ̂ tj (4.19)\n</code></pre></div><p>has a solution with‖̂ <em>sj</em> ‖ 2 ≤1 and̂ <em>tjk</em> ∈[− 1 <em>,</em> 1] for <em>k</em> = 1 <em>,...,pj</em>. Fortunately, this condition is easily checked, and we find that (Exercise 4.12)</p><div class="language-"><pre><code>θ ̂ j = 0 if and only if ‖S λα\n</code></pre></div><h6 id="-77"><a class="header-anchor" href="#-77" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Z Tj r j\n</code></pre></div><h6 id="-78"><a class="header-anchor" href="#-78" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>‖ 2 ≤ λ (1− α ) , (4.20)\n</code></pre></div><p>where S <em>λα</em></p><h6 id="-79"><a class="header-anchor" href="#-79" aria-hidden="true">#</a> (</h6><h6 id="·"><a class="header-anchor" href="#·" aria-hidden="true">#</a> ·</h6><h6 id="-80"><a class="header-anchor" href="#-80" aria-hidden="true">#</a> )</h6><p>is the soft-thresholding operator applied here component- wise to its vector argument <strong>Z</strong> <em>Tj</em> <strong><em>r</em></strong> <em>j</em>. Notice the similarity with the conditions for the group lasso (4.13), except here we use the soft-thresholded gradient S <em>λα</em></p><h6 id="-81"><a class="header-anchor" href="#-81" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Z Tj r j\n</code></pre></div><h6 id="-82"><a class="header-anchor" href="#-82" aria-hidden="true">#</a> )</h6><p>. Likewise, if <strong>Z</strong> <em>Tj</em> <strong>Z</strong> <em>j</em> = <strong>I</strong> , then as shown in Exercise 4.13, we have</p><div class="language-"><pre><code>θ ̂ j =\n</code></pre></div><h6 id="-83"><a class="header-anchor" href="#-83" aria-hidden="true">#</a> (</h6><h6 id="_1-−-3"><a class="header-anchor" href="#_1-−-3" aria-hidden="true">#</a> 1 −</h6><div class="language-"><pre><code>λ (1− α )\n‖S λα\n</code></pre></div><h6 id="-84"><a class="header-anchor" href="#-84" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Z Tj r j\n</code></pre></div><h6 id="-85"><a class="header-anchor" href="#-85" aria-hidden="true">#</a> )</h6><h6 id="‖-2"><a class="header-anchor" href="#‖-2" aria-hidden="true">#</a> ‖ 2</h6><h6 id="-86"><a class="header-anchor" href="#-86" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>+\n</code></pre></div><div class="language-"><pre><code>S λα\n</code></pre></div><h6 id="-87"><a class="header-anchor" href="#-87" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Z Tj r j\n</code></pre></div><h6 id="-88"><a class="header-anchor" href="#-88" aria-hidden="true">#</a> )</h6><h6 id="_4-21"><a class="header-anchor" href="#_4-21" aria-hidden="true">#</a> . (4.21)</h6><p>In the general case when the <strong>Z</strong> <em>j</em> are not orthonormal and we have checked</p><p>that <em>θ</em> ̂ <em>j</em> 6 = 0, findinĝ <em>θj</em> amounts to solving the subproblem</p><div class="language-"><pre><code>minimize\nθj ∈R pj\n</code></pre></div><h6 id="-89"><a class="header-anchor" href="#-89" aria-hidden="true">#</a> {</h6><h6 id="_1-35"><a class="header-anchor" href="#_1-35" aria-hidden="true">#</a> 1</h6><h6 id="_2-29"><a class="header-anchor" href="#_2-29" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ r j − Z jθj ‖^22 + λ (1− α )‖ θj ‖ 2 + λα ‖ θj ‖ 1\n</code></pre></div><h6 id="-90"><a class="header-anchor" href="#-90" aria-hidden="true">#</a> }</h6><h6 id="_4-22"><a class="header-anchor" href="#_4-22" aria-hidden="true">#</a> . (4.22)</h6><p>Here we can again use generalized gradient descent (Section (5.3.3)) to produce a simple iterative algorithm to solve each block, as in Equation (4.16a). The algorithm would iterate until convergence the sequence</p><div class="language-"><pre><code>ω ← θ ̂ j + ν · Z Tj ( r j − Z jθ ̂ j ) , and (4.23a)\n</code></pre></div><div class="language-"><pre><code>θj ←\n</code></pre></div><h6 id="-91"><a class="header-anchor" href="#-91" aria-hidden="true">#</a> (</h6><h6 id="_1-−-4"><a class="header-anchor" href="#_1-−-4" aria-hidden="true">#</a> 1 −</h6><div class="language-"><pre><code>νλ (1− α )\n‖S λα\n</code></pre></div><h6 id="-92"><a class="header-anchor" href="#-92" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>ω\n</code></pre></div><h6 id="-93"><a class="header-anchor" href="#-93" aria-hidden="true">#</a> )</h6><h6 id="‖-2-1"><a class="header-anchor" href="#‖-2-1" aria-hidden="true">#</a> ‖ 2</h6><h6 id="-94"><a class="header-anchor" href="#-94" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>+\n</code></pre></div><div class="language-"><pre><code>S λα\n</code></pre></div><h6 id="-95"><a class="header-anchor" href="#-95" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>ω\n</code></pre></div><h6 id="-96"><a class="header-anchor" href="#-96" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>, (4.23b)\n</code></pre></div><p>where <em>ν</em> is the step size. See Exercise 4.10 for the details.</p><h4 id="_4-3-3-the-overlap-group-lasso"><a class="header-anchor" href="#_4-3-3-the-overlap-group-lasso" aria-hidden="true">#</a> 4.3.3 The Overlap Group Lasso</h4><p>Sometimes variables can belong to more than one group: for example, genes can belong to more than one biological pathway. The <em>overlap group lasso</em> is a modification that allows variables to contribute to more than one group. To gain some intuition, consider the case of <em>p</em> = 5 variables partitioned into two groups, say of the form</p><div class="language-"><pre><code>Z 1 = ( X 1 ,X 2 ,X 3 ) , and Z 2 = ( X 3 ,X 4 ,X 5 ). (4.24)\n</code></pre></div><h6 id="_66-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_66-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 66 GENERALIZATIONS OF THE LASSO PENALTY</h6><p>Here <em>X</em> 3 belongs to both groups. The overlap group lasso simply replicates a variable in whatever group it appears, and then fits the ordinary group lasso as before. In this particular example, the variable <em>X</em> 3 is replicated, and we fit coefficient vectors <em>θ</em> 1 = ( <em>θ</em> 11 <em>,θ</em> 12 <em>,θ</em> 13 ) and <em>θ</em> 2 = ( <em>θ</em> 21 <em>,θ</em> 22 <em>,θ</em> 23 ) using the group lasso (4.5), using a group penalty‖ <em>θ</em> 1 ‖ 2 +‖ <em>θ</em> 2 ‖ 2. In terms of the original variables, the coefficient <em>β</em> ̂ 3 of <em>X</em> 3 is given by the sum <em>β</em> ̂ 3 =̂ <em>θ</em> 13 +̂ <em>θ</em> 21. As a consequence, the coefficient <em>β</em> ̂ 3 can be nonzero if either (or both) of the coefficients <em>θ</em> ̂ 13 or <em>θ</em> ̂ 21 are nonzero. Hence, all else being equal, the variable <em>X</em> 3 has a better chance of being included in the model than the other variables, by virtue of belonging to two groups. Rather than replicate variables, it is tempting to simply replicate the coef- ficients in the group-lasso penalty. For instance, for the given grouping above, with <em>X</em> = ( <em>X</em> 1 <em>,...,X</em> 5 ), and <em>β</em> = ( <em>β</em> 1 <em>,...,β</em> 5 ), suppose that we define</p><div class="language-"><pre><code>θ 1 = ( β 1 ,β 2 ,β 3 ) , and θ 2 = ( β 3 ,β 4 ,β 5 ) , (4.25)\n</code></pre></div><p>and then apply the group-lasso penalty‖ <em>θ</em> 1 ‖ 2 +‖ <em>θ</em> 2 ‖ 2 as before. However, this approach has a major drawback. Whenever̂ <em>θ</em> 1 = 0 in any optimal solution, then we must necessarily have <em>β</em> ̂ 3 = 0 in <em>both groups</em>. Consequently, in this particular example, the only possible sets of nonzero coefficients are{ 1 <em>,</em> 2 }, { 4 <em>,</em> 5 }, and{ 1 <em>,</em> 2 <em>,</em> 3 <em>,</em> 4 <em>,</em> 5 }; the original groups{ 1 <em>,</em> 2 <em>,</em> 3 }and{ 3 <em>,</em> 4 <em>,</em> 5 }are not con- sidered as possibilities, since if either group appears, then both groups must appear.^5 As a second practical point, the penalty in this approach is not sep- arable, and hence coordinate-descent algorithms may fail to converge to an optimal solution (see Section 5.4 for more details). Jacob, Obozinski and Vert (2009) recognized this problem, and hence pro- posed the replicated variable approach (4.24) or <em>overlap group lasso</em>. For our motivating example, the possible sets of nonzero coefficients for the overlap group lasso are{ 1 <em>,</em> 2 <em>,</em> 3 },{ 3 <em>,</em> 4 <em>,</em> 5 }, and{ 1 <em>,</em> 2 <em>,</em> 3 <em>,</em> 4 <em>,</em> 5 }. In general, the sets of pos- sible nonzero coefficients always correspond to groups, or the unions of groups. They also defined an implicit penalty on the original variables that yields the replicated variable approach as its solution, which we now describe. Denote by <em>νj</em> ∈R <em>p</em> a vector which is zero everywhere except in those positions corresponding to the members of group <em>j</em> , and letV <em>j</em> ⊆R <em>p</em> be the subspace of such possible vectors. In terms of the original variables, <em>X</em> = ( <em>X</em> 1 <em>,</em> ··· <em>,Xp</em> ), the coefficient vector is given by the sum <em>β</em> =</p><h6 id="∑-j-10"><a class="header-anchor" href="#∑-j-10" aria-hidden="true">#</a> ∑ J</h6><p><em>j</em> =1 <em>νj</em> , and hence the overlap group lasso solves the problem</p><div class="language-"><pre><code>minimize\nνj ∈V jj =1 ,...,J\n</code></pre></div><h6 id="-17"><a class="header-anchor" href="#-17" aria-hidden="true">#</a> </h6><h6 id="-15"><a class="header-anchor" href="#-15" aria-hidden="true">#</a> </h6><h6 id="-15"><a class="header-anchor" href="#-15" aria-hidden="true">#</a> </h6><h6 id="_1-36"><a class="header-anchor" href="#_1-36" aria-hidden="true">#</a> 1</h6><h6 id="_2-30"><a class="header-anchor" href="#_2-30" aria-hidden="true">#</a> 2</h6><h6 id="∥"><a class="header-anchor" href="#∥" aria-hidden="true">#</a> ∥</h6><div class="language-"><pre><code>∥ y − X (\n</code></pre></div><h6 id="∑-j-11"><a class="header-anchor" href="#∑-j-11" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>νj\n</code></pre></div><h6 id="∥∥-2"><a class="header-anchor" href="#∥∥-2" aria-hidden="true">#</a> )∥∥ 2</h6><div class="language-"><pre><code>2 + λ\n</code></pre></div><h6 id="∑-j-12"><a class="header-anchor" href="#∑-j-12" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>‖ νj ‖ 2\n</code></pre></div><h6 id="-16"><a class="header-anchor" href="#-16" aria-hidden="true">#</a> </h6><h6 id="-15"><a class="header-anchor" href="#-15" aria-hidden="true">#</a> </h6><h6 id="-15"><a class="header-anchor" href="#-15" aria-hidden="true">#</a> </h6><h6 id="_4-26"><a class="header-anchor" href="#_4-26" aria-hidden="true">#</a> . (4.26)</h6><p>This optimization problem can be re-cast in the terms of the original <em>β</em> vari-</p><p>(^5) More generally, the replicated-variable approach always yields solutions in which the sets of zero coefficients are unions of groups, so that the sets of nonzeros must be the intersections of complements of groups.</p><h6 id="the-group-lasso-67"><a class="header-anchor" href="#the-group-lasso-67" aria-hidden="true">#</a> THE GROUP LASSO 67</h6><p>ables by defining a suitable penalty function. With</p><div class="language-"><pre><code>ΩV( β ) : = inf\nνj ∈V j\nβ =\n</code></pre></div><h6 id="∑-j-13"><a class="header-anchor" href="#∑-j-13" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1 νj\n</code></pre></div><h6 id="∑-j-14"><a class="header-anchor" href="#∑-j-14" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>‖ νj ‖ 2 , (4.27)\n</code></pre></div><p>it can then be shown (Jacob et al. 2009) that solving problem (4.26) is equiv- alent to solving</p><div class="language-"><pre><code>minimize\nβ ∈R p\n</code></pre></div><h6 id="-97"><a class="header-anchor" href="#-97" aria-hidden="true">#</a> {</h6><h6 id="_1-37"><a class="header-anchor" href="#_1-37" aria-hidden="true">#</a> 1</h6><h6 id="_2-31"><a class="header-anchor" href="#_2-31" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ y − X β ‖^22 + λ ΩV( β )\n</code></pre></div><h6 id="-98"><a class="header-anchor" href="#-98" aria-hidden="true">#</a> }</h6><h6 id="_4-28"><a class="header-anchor" href="#_4-28" aria-hidden="true">#</a> . (4.28)</h6><p>This equivalence is intuitively obvious, and underscores the mechanism un- derlying this penalty; the contributions to the coefficient for a variable are distributed among the groups to which it belongs in a norm-efficient manner. Figure 4.6 contrasts the group lasso constraint region with that of the overlap group lasso when there are three variables. There are two rings corre- sponding to the two groups, with <em>X</em> 2 in both groups.</p><div class="language-"><pre><code>β 1\n</code></pre></div><div class="language-"><pre><code>β 3\n</code></pre></div><div class="language-"><pre><code>β 2\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>β 1\n</code></pre></div><div class="language-"><pre><code>β 3\n</code></pre></div><div class="language-"><pre><code>β 2\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><p><strong>Figure 4.6</strong> <em>The group-lasso ball (left panel) in</em> R^3 <em>, compared to the overlap-group- lasso bal l (right panel). Depicted are two groups in both. In the left panel the groups are</em> { <em>X</em> 1 <em>,X</em> 2 } <em>andX</em> 3 <em>; in the right panel the groups are</em> { <em>X</em> 1 <em>,X</em> 2 } <em>and</em> { <em>X</em> 2 <em>,X</em> 3 }<em>. There are two rings corresponding to the two groups in the right panel. Whenβ</em> 2 <em>is close to zero, the penalty on the other two variables is much like the lasso. Whenβ</em> 2 <em>is far from zero, the penalty on the other two variables “softens” and resembles the `</em> 2 <em>penalty.</em></p><p><em>Example 4.3. Interactions and hierarchy.</em> The overlap-group lasso can also be used to enforce <em>hierarchy</em> when selecting interactions in linear models. What this means is that interactions are allowed in the model only in the presence of both of their main effects. Suppose <em>Z</em> 1 represents the <em>p</em> 1 dummy variables for the <em>p</em> 1 levels of factor <em>G</em> 1 ; likewise <em>Z</em> 2 the <em>p</em> 2 dummy variables for <em>G</em> 2. A linear model with <em>Z</em> 1 and <em>Z</em> 2 is a <em>main-effects</em> model. Now let <em>Z</em> 1:2= <em>Z</em> 1_? Z_ 2 ,</p><h6 id="_68-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_68-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 68 GENERALIZATIONS OF THE LASSO PENALTY</h6><p>a <em>p</em> 1 × <em>p</em> 2 vector of dummy variables (the vector of all pairwise products). Lim and Hastie (2014) consider the following formulation for a pair of such categorical variables^6</p><div class="language-"><pre><code>minimize\nμ,α,α ̃\n</code></pre></div><h6 id="-18"><a class="header-anchor" href="#-18" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="_1-38"><a class="header-anchor" href="#_1-38" aria-hidden="true">#</a> 1</h6><h6 id="_2-32"><a class="header-anchor" href="#_2-32" aria-hidden="true">#</a> 2</h6><h6 id="∥-∥-∥-∥-∥-∥"><a class="header-anchor" href="#∥-∥-∥-∥-∥-∥" aria-hidden="true">#</a> ∥ ∥ ∥ ∥ ∥ ∥</h6><div class="language-"><pre><code>y − μ 1 − Z 1 α 1 − Z 2 α 2 −[ Z 1 Z 2 Z 1:2]\n</code></pre></div><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>α ̃ 1\nα ̃ 2\nα 1:2\n</code></pre></div><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id="∥-∥-∥-∥-∥-∥-1"><a class="header-anchor" href="#∥-∥-∥-∥-∥-∥-1" aria-hidden="true">#</a> ∥ ∥ ∥ ∥ ∥ ∥</h6><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>+ λ\n</code></pre></div><h6 id="-99"><a class="header-anchor" href="#-99" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>‖ α 1 ‖ 2 +‖ α 2 ‖ 2 +\n</code></pre></div><h6 id="√-6"><a class="header-anchor" href="#√-6" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>p 2 ‖ α ̃ 1 ‖^22 + p 1 ‖ α ̃ 2 ‖^22 +‖ α 1:2‖^22\n</code></pre></div><h6 id="-100"><a class="header-anchor" href="#-100" aria-hidden="true">#</a> )}</h6><h6 id="_4-29"><a class="header-anchor" href="#_4-29" aria-hidden="true">#</a> (4.29)</h6><p>subject to the constraints</p><div class="language-"><pre><code>∑ p^1\n</code></pre></div><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>αi 1 = 0 ,\n</code></pre></div><div class="language-"><pre><code>∑ p^2\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>αj 2 = 0 ,\n</code></pre></div><div class="language-"><pre><code>∑ p^1\n</code></pre></div><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>α ̃ i 1 = 0 ,\n</code></pre></div><div class="language-"><pre><code>∑ p^2\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>α ̃ j 2 = 0 , (4.30)\n</code></pre></div><div class="language-"><pre><code>∑ p^1\n</code></pre></div><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>αij 1:2= 0 for fixed j,\n</code></pre></div><div class="language-"><pre><code>∑ p^2\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>αij 1:2= 0 for fixed i. (4.31)\n</code></pre></div><p>The summation constraints are standard in hierarchical ANOVA formulations. Notice that the main effect matrices <strong>Z</strong> 1 and <strong>Z</strong> 2 each have two different coeffi- cient vectors <em>αj</em> and ̃ <em>αj</em> , creating an overlap in the penalties, and their ultimate coefficient is the sum <em>θj</em> = <em>αj</em> + ̃ <em>αj</em>. The</p><h6 id="√-7"><a class="header-anchor" href="#√-7" aria-hidden="true">#</a> √</h6><p><em>p</em> 2 ‖ <em>α</em> ̃ 1 ‖^22 + <em>p</em> 1 ‖ <em>α</em> ̃ 2 ‖^22 +‖ <em>α</em> 1:2‖^22 term results in estimates that satisfy strong hierarchy, because either̂ <em>α</em> ̃ 1 =̂ <em>α</em> ̃ 2 = <em>α</em> ̂1:2= 0 or <em>al l</em> are nonzero, i.e., interactions are always present with both main effects. They show that the solution to the above constrained problem (4.29)–(4.31) is equivalent to the solution to the simpler unconstrained prob- lem</p><div class="language-"><pre><code>minimize\nμ,β\n</code></pre></div><h6 id="-101"><a class="header-anchor" href="#-101" aria-hidden="true">#</a> {</h6><h6 id="_1-39"><a class="header-anchor" href="#_1-39" aria-hidden="true">#</a> 1</h6><h6 id="_2-33"><a class="header-anchor" href="#_2-33" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ y − μ 1 − Z 1 β 1 − Z 2 β 2 − Z 1:2 β 1:2‖^22\n</code></pre></div><div class="language-"><pre><code>+ λ (‖ β 1 ‖ 2 +‖ β 2 ‖ 2 +‖ β 1:2‖ 2 )} (4.32)\n</code></pre></div><p>(Exercise 4.14). In other words, a linear model in <em>Z</em> 1:2is the full interaction model (i.e., interactions with main effects implicitly included). A group lasso in <em>Z</em> 1 , <em>Z</em> 2 , and <em>Z</em> 1:2will hence result in a hierarchical model; whenever <em>Z</em> 1:2is in the model, the pair of main effects is implicitly included. In this case the variables do not strictly overlap, but their subspaces do. A different approach to the estimation of hierarchical interactions is the <em>hierNet</em> proposal of Bien, Taylor and Tibshirani (2013). ♦</p><p>(^6) This extends naturally to more than two pairs, as well as other loss functions, e.g., logistic regression, as well as interactions between factors and quantitative variables.</p><h6 id="sparse-additive-models-and-the-group-lasso-69"><a class="header-anchor" href="#sparse-additive-models-and-the-group-lasso-69" aria-hidden="true">#</a> SPARSE ADDITIVE MODELS AND THE GROUP LASSO 69</h6><h3 id="_4-4-sparse-additive-models-and-the-group-lasso"><a class="header-anchor" href="#_4-4-sparse-additive-models-and-the-group-lasso" aria-hidden="true">#</a> 4.4 Sparse Additive Models and the Group Lasso</h3><p>Suppose we have a zero-mean response variable <em>Y</em> ∈R, and a vector of predic- tors <em>X</em> ∈R <em>J</em> , and that we are interested in estimating the regression function <em>f</em> ( <em>x</em> ) =E( <em>Y</em> | <em>X</em> = <em>x</em> ). It is well-known that nonparametric regression suffers from the curse of dimensionality, so that approximations are essential. Addi- tive models are one such approximation, and effectively reduce the estimation problem to that of many one-dimensional problems. When <em>J</em> is very large, this may not be sufficient; the class of sparse additive models limits these approxi- mations further, by encouraging many of the components to be zero. Methods for estimating sparse additive models are closely related to the group lasso.</p><h4 id="_4-4-1-additive-models-and-backfitting"><a class="header-anchor" href="#_4-4-1-additive-models-and-backfitting" aria-hidden="true">#</a> 4.4.1 Additive Models and Backfitting</h4><p>We begin by introducing some background on the class of additive models, which are based on approximating the regression function by sums of the form</p><div class="language-"><pre><code>f ( x ) = f ( x 1 ,...,xJ )≈\n</code></pre></div><h6 id="∑-j-15"><a class="header-anchor" href="#∑-j-15" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>fj ( xj ) , (4.33)\n</code></pre></div><div class="language-"><pre><code>fj ∈F j, j = 1 ,...,J,\n</code></pre></div><p>where theF <em>j</em> are a fixed set of univariate function classes. Typically, eachF <em>j</em> is assumed to be a subset of <em>L</em>^2 (P <em>j</em> ) whereP <em>j</em> is the distribution of covariate <em>Xj</em> , and equipped with the usual squared <em>L</em>^2 (P <em>j</em> ) norm‖ <em>fj</em> ‖^22 : =E[ <em>fj</em>^2 ( <em>Xj</em> )]. In the population setting, the best additive approximation to the regression functionE( <em>Y</em> | <em>X</em> = <em>x</em> ), as measured in the <em>L</em>^2 (P) sense, solves the problem</p><div class="language-"><pre><code>minimize\nfj ∈F j, j =1 ,...,J\n</code></pre></div><h6 id="e"><a class="header-anchor" href="#e" aria-hidden="true">#</a> E</h6><h6 id="-102"><a class="header-anchor" href="#-102" aria-hidden="true">#</a> [(</h6><h6 id="y-−"><a class="header-anchor" href="#y-−" aria-hidden="true">#</a> Y −</h6><h6 id="∑-j-16"><a class="header-anchor" href="#∑-j-16" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>fj ( Xj )\n</code></pre></div><h6 id="_2-34"><a class="header-anchor" href="#_2-34" aria-hidden="true">#</a> ) 2 ]</h6><h6 id="_4-34"><a class="header-anchor" href="#_4-34" aria-hidden="true">#</a> . (4.34)</h6><p>The optimal solution ( <em>f</em> ̃ 1 <em>,...,f</em> ̃ <em>J</em> ) is characterized by the <em>backfitting equations</em> , namely</p><div class="language-"><pre><code>f ̃ j ( xj ) =E\n</code></pre></div><h6 id="-103"><a class="header-anchor" href="#-103" aria-hidden="true">#</a> [</h6><h6 id="y-−-1"><a class="header-anchor" href="#y-−-1" aria-hidden="true">#</a> Y −</h6><h6 id="∑-17"><a class="header-anchor" href="#∑-17" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>k 6 = j\n</code></pre></div><div class="language-"><pre><code>f ̃ k ( Xk )| Xj = xj\n</code></pre></div><h6 id="-104"><a class="header-anchor" href="#-104" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>, for j = 1 ,...,J. (4.35)\n</code></pre></div><p>More compactly, this update can be written in the form <em>f</em> ̃ <em>j</em> =P <em>j</em> ( <em>Rj</em> ), whereP <em>j</em> is the conditional-expectation operator in the <em>jth</em> coordinate, and the quantity <em>Rj</em> : = <em>Y</em> −</p><h6 id="∑-18"><a class="header-anchor" href="#∑-18" aria-hidden="true">#</a> ∑</h6><p><em>k</em> 6 = <em>jf</em> ̃ <em>k</em> ( <em>Xk</em> ) is the <em>j th</em> partial residual. Given data{( <em>xi,yi</em> )} <em>Ni</em> =1, a natural approach is to replace the population operatorP <em>j</em> with empirical versions, such as scatterplot smoothersS <em>j</em> , and then solve a data-based version version of the updates (4.35) by coordinate descent or <em>backfitting</em> (Hastie and Tibshirani 1990). Hence we repeatedly cycle over the coordinates <em>j</em> = 1 <em>,...,J</em> , and update each function estimate <em>f</em> ̂ <em>j</em> using</p><h6 id="_70-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_70-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 70 GENERALIZATIONS OF THE LASSO PENALTY</h6><p>the smooth of the partial residuals</p><div class="language-"><pre><code>f ̂ j ←S j ( y −\n</code></pre></div><h6 id="∑-19"><a class="header-anchor" href="#∑-19" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>k 6 = j\n</code></pre></div><div class="language-"><pre><code>̂ f k ) , j = 1 ,...,J, (4.36)\n</code></pre></div><p>until the fitted functions <em>f</em> ̂ <em>j</em> stabilize. In (4.36)̂ <strong>f</strong> <em>k</em> is the fitted function <em>f</em> ˆ <em>k</em> eval- uated at the <em>N</em> sample values ( <em>x</em> 1 <em>k,...,xNk</em> ). The operatorS <em>j</em> represents an algorithm that takes a response vector <strong><em>r</em></strong> , <em>smooths</em> it against the vector <strong>x</strong> <em>j</em> , and</p><p>returns the function <em>f</em> ̂ <em>j</em>. AlthoughS <em>j</em> will have its own tuning parameters and <em>bel ls and whistles</em> , for the moment we regard it as a black-box that estimates a conditional expectation using data.</p><h4 id="_4-4-2-sparse-additive-models-and-backfitting"><a class="header-anchor" href="#_4-4-2-sparse-additive-models-and-backfitting" aria-hidden="true">#</a> 4.4.2 Sparse Additive Models and Backfitting</h4><p>An extension of the basic additive model is the notion of a <em>sparse additive model</em> , in which we assume that there is a subset <em>S</em> ⊂ { 1 <em>,</em> 2 <em>,...,J</em> }such that the regression function <em>f</em> ( <em>x</em> ) =E( <em>Y</em> | <em>X</em> = <em>x</em> ) satisfies an approximation of the form <em>f</em> ( <em>x</em> )≈</p><h6 id="∑-20"><a class="header-anchor" href="#∑-20" aria-hidden="true">#</a> ∑</h6><p><em>j</em> ∈ <em>Sfj</em> ( <em>xj</em> ). Ravikumar, Liu, Lafferty and Wasserman (2009) proposed a natural extension of the backfitting equations, motivated by a sparse analog of the population level problem (4.34). For a given sparsity level <em>k</em> ∈ { 1 <em>,...,J</em> }, the best <em>k</em> -sparse approximation to the regression function is given by</p><div class="language-"><pre><code>minimize\n| S |= k\nfj ∈F j,j =1 ,...,J\n</code></pre></div><h6 id="e-1"><a class="header-anchor" href="#e-1" aria-hidden="true">#</a> E</h6><h6 id="-105"><a class="header-anchor" href="#-105" aria-hidden="true">#</a> (</h6><h6 id="y-−-2"><a class="header-anchor" href="#y-−-2" aria-hidden="true">#</a> Y −</h6><h6 id="∑-21"><a class="header-anchor" href="#∑-21" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>j ∈ S\n</code></pre></div><div class="language-"><pre><code>fj ( Xj )\n</code></pre></div><h6 id="_2-35"><a class="header-anchor" href="#_2-35" aria-hidden="true">#</a> ) 2</h6><h6 id="_4-37"><a class="header-anchor" href="#_4-37" aria-hidden="true">#</a> . (4.37)</h6><p>Unfortunately, this criterion is nonconvex and computationally intractable, due to combinatorial number—namely</p><h6 id="j"><a class="header-anchor" href="#j" aria-hidden="true">#</a> ( J</h6><div class="language-"><pre><code>k\n</code></pre></div><h6 id="-106"><a class="header-anchor" href="#-106" aria-hidden="true">#</a> )</h6><p>—of possible subsets of size <em>k</em>. Suppose that instead we measure the sparsity of an additive approxima- tion <em>f</em> =</p><h6 id="∑-j-17"><a class="header-anchor" href="#∑-j-17" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1 fj via the sum\n</code></pre></div><h6 id="∑-j-18"><a class="header-anchor" href="#∑-j-18" aria-hidden="true">#</a> ∑ J</h6><p>√ <em>j</em> =1‖ <em>fj</em> ‖^2 , where we recall that‖ <em>fj</em> ‖^2 = E[ <em>fj</em>^2 ( <em>Xj</em> )] is the <em>L</em>^2 (P <em>j</em> ) norm applied to component <em>j</em>. For a given regu-</p><p>larization parameter <em>λ</em> ≥0, this relaxed notion defines an alternative type of best sparse approximation, namely one that minimizes the penalized criterion</p><div class="language-"><pre><code>minimize\nfj ∈F j, j =1 ,...,J\n</code></pre></div><h6 id="-19"><a class="header-anchor" href="#-19" aria-hidden="true">#</a> </h6><h6 id="-16"><a class="header-anchor" href="#-16" aria-hidden="true">#</a> </h6><h6 id="-16"><a class="header-anchor" href="#-16" aria-hidden="true">#</a> </h6><h6 id="e-2"><a class="header-anchor" href="#e-2" aria-hidden="true">#</a> E</h6><h6 id="-107"><a class="header-anchor" href="#-107" aria-hidden="true">#</a> (</h6><h6 id="y-−-3"><a class="header-anchor" href="#y-−-3" aria-hidden="true">#</a> Y −</h6><h6 id="∑-j-19"><a class="header-anchor" href="#∑-j-19" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>fj ( Xj )\n</code></pre></div><h6 id="_2-36"><a class="header-anchor" href="#_2-36" aria-hidden="true">#</a> ) 2</h6><div class="language-"><pre><code>+ λ\n</code></pre></div><h6 id="∑-j-20"><a class="header-anchor" href="#∑-j-20" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>‖ fj ‖ 2\n</code></pre></div><h6 id="-17"><a class="header-anchor" href="#-17" aria-hidden="true">#</a> </h6><h6 id="-16"><a class="header-anchor" href="#-16" aria-hidden="true">#</a> </h6><h6 id="-16"><a class="header-anchor" href="#-16" aria-hidden="true">#</a> </h6><h6 id="_4-38"><a class="header-anchor" href="#_4-38" aria-hidden="true">#</a> . (4.38)</h6><p>Since this objective is a convex functional of ( <em>f</em> 1 <em>,...,fJ</em> ), Lagrangian duality ensures that it has an equivalent representation involving an explicit constraint on the norm</p><h6 id="∑-j-21"><a class="header-anchor" href="#∑-j-21" aria-hidden="true">#</a> ∑ J</h6><p><em>j</em> =1‖ <em>fj</em> ‖^2. See Exercise 4.15. Ravikumar et al. (2009) show that any optimal solution ( <em>f</em> ̃ 1 <em>,...,f</em> ̃ <em>J</em> ) to the penalized problem (4.38) is characterized by the <em>sparse backfitting equations</em></p><div class="language-"><pre><code>f ̃ j =\n</code></pre></div><h6 id="-108"><a class="header-anchor" href="#-108" aria-hidden="true">#</a> (</h6><h6 id="_1-−-5"><a class="header-anchor" href="#_1-−-5" aria-hidden="true">#</a> 1 −</h6><div class="language-"><pre><code>λ\n‖P j ( Rj )‖ 2\n</code></pre></div><h6 id="-109"><a class="header-anchor" href="#-109" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>+\n</code></pre></div><div class="language-"><pre><code>P j ( Rj ) , (4.39)\n</code></pre></div><h6 id="sparse-additive-models-and-the-group-lasso-71"><a class="header-anchor" href="#sparse-additive-models-and-the-group-lasso-71" aria-hidden="true">#</a> SPARSE ADDITIVE MODELS AND THE GROUP LASSO 71</h6><p>where the residual <em>Rj</em> and the conditional expectation operatorP <em>j</em> were de- fined in the text after the ordinary backfitting equations (4.35). In parallel with our earlier development, given data{( <em>xi,yi</em> )} <em>N</em> 1 , these population-level updates suggest the natural data-driven analog, in which we replace the population operatorP <em>j</em> with the scatterplot smootherS <em>j</em> , and then perform the updates</p><div class="language-"><pre><code>f ̃ j =S j ( y −\n</code></pre></div><h6 id="∑-22"><a class="header-anchor" href="#∑-22" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>k 6 = j\n</code></pre></div><div class="language-"><pre><code>̂ f k ) , and f ̂ j =\n</code></pre></div><h6 id="-110"><a class="header-anchor" href="#-110" aria-hidden="true">#</a> (</h6><h6 id="_1-−-6"><a class="header-anchor" href="#_1-−-6" aria-hidden="true">#</a> 1 −</h6><div class="language-"><pre><code>λ\n‖ ̃ f ‖ 2\n</code></pre></div><h6 id="-111"><a class="header-anchor" href="#-111" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>+\n</code></pre></div><div class="language-"><pre><code>f ̃ j, (4.40)\n</code></pre></div><p>for <em>j</em> = 1 <em>,...,J</em> , again iterating until convergence. Figure 4.7 illustrates the performance of the SPAM updates (4.40) on some air-pollution data. We use smoothing-splines, with a fixed degree of freedom <em>df</em> = 5 for each coordi- nate (Hastie and Tibshirani 1990).</p><div class="language-"><pre><code>0 50 150 250\n</code></pre></div><div class="language-"><pre><code>−1.0 −0.5 0.0\n</code></pre></div><div class="language-"><pre><code>0.5\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>rad\n</code></pre></div><div class="language-"><pre><code>f(rad)\n</code></pre></div><div class="language-"><pre><code>60 70 80 90\n</code></pre></div><div class="language-"><pre><code>−1.0 −0.5 0.0\n</code></pre></div><div class="language-"><pre><code>0.5\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>temp\n</code></pre></div><div class="language-"><pre><code>f(temp)\n</code></pre></div><div class="language-"><pre><code>5 10 15 20\n</code></pre></div><div class="language-"><pre><code>−1.0 −0.5 0.0\n</code></pre></div><div class="language-"><pre><code>0.5\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>wind\n</code></pre></div><div class="language-"><pre><code>f(wind)\n</code></pre></div><div class="language-"><pre><code>log(ozone) ~ s(rad) + s(temp) + s(wind)\n</code></pre></div><p><strong>Figure 4.7</strong> <em>A sequence of three SPAM models fit to some air-pol lution data. The response is the log of ozone concentration, and there are three predictors: radiation, temperature, and wind speed. Smoothing splines were used in the additive model fits, each withdf</em> = 5 <em>The three curves in each plot correspond toλ</em> = 0 <em>(black curves), λ</em> = 2 <em>(orange curves), andλ</em> = 4 <em>(red curves). We see that while the shrinkage leaves the functions of</em> temp <em>relatively untouched, it has a more dramatic effect on</em> rad <em>and</em> wind_._</p><p>We can make a more direct connection with the grouped lasso if the smoothing method for variable <em>Xj</em> is a projection on to a set of basis functions. Consider</p><div class="language-"><pre><code>fj (·) =\n</code></pre></div><div class="language-"><pre><code>∑ pj\n</code></pre></div><div class="language-"><pre><code>` =1\n</code></pre></div><div class="language-"><pre><code>ψj` (·) βj`, (4.41)\n</code></pre></div><p>where the{ <em>ψj<code>_ } _p_ 1 _j_ are a family of basis functions in _Xj_ , such as cubic splines with a collection of knots along the range of _Xj_. Let **Ψ** _j_ be the _N_ × _pj_ matrix of evaluations of the _ψj</code></em> , and assume that <strong>Ψ</strong> <em>Tj</em> <strong>Ψ</strong> <em>j</em> = <strong>I</strong> <em>pj</em>. Then for</p><h6 id="_72-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_72-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 72 GENERALIZATIONS OF THE LASSO PENALTY</h6><p>any coefficient vector <em>θj</em> = ( <em>βj</em> 1 <em>,...,βjpj</em> ) <em>T</em> and corresponding fitted vector <strong>f</strong> <em>j</em> = <strong>Ψ</strong> <em>jθj</em> , we have‖ <strong>f</strong> <em>j</em> ‖ 2 =‖ <em>θj</em> ‖ 2. In this case it is easy to show that the updates (4.40) are equivalent to those for a group lasso with predictor matrix <strong>Ψ</strong> : =</p><h6 id="-112"><a class="header-anchor" href="#-112" aria-hidden="true">#</a> [</h6><h6 id="ψ-1-ψ-2-···-ψ-j"><a class="header-anchor" href="#ψ-1-ψ-2-···-ψ-j" aria-hidden="true">#</a> Ψ 1 Ψ 2 ··· Ψ J</h6><h6 id="-113"><a class="header-anchor" href="#-113" aria-hidden="true">#</a> ]</h6><p>and a corresponding block vector of coefficients <em>θ</em> : =</p><h6 id="-114"><a class="header-anchor" href="#-114" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>θ 1 θ 2 ··· θJ\n</code></pre></div><h6 id="-115"><a class="header-anchor" href="#-115" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>(see Exercise 4.16 for more details).\n</code></pre></div><h4 id="_4-4-3-approaches-using-optimization-and-the-group-lasso"><a class="header-anchor" href="#_4-4-3-approaches-using-optimization-and-the-group-lasso" aria-hidden="true">#</a> 4.4.3 Approaches Using Optimization and the Group Lasso</h4><p>Although the population-level sparse backfitting equations (4.39) do solve an optimization problem, in general, the empirical versions (4.40) do not, but rather are motivated by analogy to the population version. We now discuss the <em>Component Selection and Smoothing Operator</em> or COSSO for short, which does solve a data-defined optimization problem. The COSSO method (Lin and Zhang 2003) is a predecessor to the SPAM method, and operates in the world of reproducing kernel Hilbert spaces, with a special case being the smoothing spline model. We begin by recalling the traditional form of an additive smoothing-spline model, obtained from the optimization of a penalized objective function:</p><div class="language-"><pre><code>minimize\nfj ∈H j, j =1 ,...,J\n</code></pre></div><h6 id="-20"><a class="header-anchor" href="#-20" aria-hidden="true">#</a> </h6><h6 id="-17"><a class="header-anchor" href="#-17" aria-hidden="true">#</a> </h6><h6 id="-17"><a class="header-anchor" href="#-17" aria-hidden="true">#</a> </h6><h6 id="_1-40"><a class="header-anchor" href="#_1-40" aria-hidden="true">#</a> 1</h6><h6 id="n-16"><a class="header-anchor" href="#n-16" aria-hidden="true">#</a> N</h6><h6 id="∑-n-49"><a class="header-anchor" href="#∑-n-49" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi −\n</code></pre></div><h6 id="∑-j-22"><a class="header-anchor" href="#∑-j-22" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>fj ( xij ))^2 + λ\n</code></pre></div><h6 id="∑-j-23"><a class="header-anchor" href="#∑-j-23" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><h6 id="_1-41"><a class="header-anchor" href="#_1-41" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>γj\n</code></pre></div><div class="language-"><pre><code>‖ fj ‖^2 H j\n</code></pre></div><h6 id="-18"><a class="header-anchor" href="#-18" aria-hidden="true">#</a> </h6><h6 id="-17"><a class="header-anchor" href="#-17" aria-hidden="true">#</a> </h6><h6 id="-17"><a class="header-anchor" href="#-17" aria-hidden="true">#</a> </h6><h6 id="_4-42"><a class="header-anchor" href="#_4-42" aria-hidden="true">#</a> . (4.42)</h6><p>Here‖ <em>fj</em> ‖H <em>j</em> is an appropriate Hilbert-space norm for the <em>jth</em> coordinate. Typically, the Hilbert spaceH <em>j</em> is chosen to enforce some type of smoothness, in which context the parameter <em>λ</em> ≥0 corresponds to overall smoothness, and the parameters <em>γj</em> ≥0 are coordinate specific modifiers. For example, a roughness norm for a cubic smoothing spline on [0 <em>,</em> 1] is</p><div class="language-"><pre><code>‖ g ‖^2 H: =\n</code></pre></div><h6 id="∫-1"><a class="header-anchor" href="#∫-1" aria-hidden="true">#</a> (∫ 1</h6><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>g ( t ) dt\n</code></pre></div><h6 id="_2-37"><a class="header-anchor" href="#_2-37" aria-hidden="true">#</a> ) 2</h6><h6 id="-116"><a class="header-anchor" href="#-116" aria-hidden="true">#</a> +</h6><h6 id="∫-1-1"><a class="header-anchor" href="#∫-1-1" aria-hidden="true">#</a> (∫ 1</h6><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>g ′( t ) dt\n</code></pre></div><h6 id="_2-38"><a class="header-anchor" href="#_2-38" aria-hidden="true">#</a> ) 2</h6><h6 id="-117"><a class="header-anchor" href="#-117" aria-hidden="true">#</a> +</h6><h6 id="∫-1-2"><a class="header-anchor" href="#∫-1-2" aria-hidden="true">#</a> ∫ 1</h6><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>g ′′( t )^2 dt. (4.43)\n</code></pre></div><p>When this particular Hilbert norm is used in the objective function (4.42), each component <em>f</em> ̂ <em>j</em> of the optimal solution is a cubic spline with knots at the unique sample values of <em>Xj</em>. The solution can be computed by the backfitting updates (4.36), where eachS <em>j</em> is a type of cubic spline smoother with penalty <em>λ/γj</em>. Instead of the classical formulation (4.42), the COSSO method is based on the objective function</p><div class="language-"><pre><code>minimize\nfj ∈H j, j =1 ,...,J\n</code></pre></div><h6 id="-21"><a class="header-anchor" href="#-21" aria-hidden="true">#</a> </h6><h6 id="-18"><a class="header-anchor" href="#-18" aria-hidden="true">#</a> </h6><h6 id="-18"><a class="header-anchor" href="#-18" aria-hidden="true">#</a> </h6><h6 id="_1-42"><a class="header-anchor" href="#_1-42" aria-hidden="true">#</a> 1</h6><h6 id="n-17"><a class="header-anchor" href="#n-17" aria-hidden="true">#</a> N</h6><h6 id="∑-n-50"><a class="header-anchor" href="#∑-n-50" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi −\n</code></pre></div><h6 id="∑-j-24"><a class="header-anchor" href="#∑-j-24" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>fj ( xij ))^2 + τ\n</code></pre></div><h6 id="∑-j-25"><a class="header-anchor" href="#∑-j-25" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>‖ fj ‖H j\n</code></pre></div><h6 id="-19"><a class="header-anchor" href="#-19" aria-hidden="true">#</a> </h6><h6 id="-18"><a class="header-anchor" href="#-18" aria-hidden="true">#</a> </h6><h6 id="-18"><a class="header-anchor" href="#-18" aria-hidden="true">#</a> </h6><h6 id="_4-44"><a class="header-anchor" href="#_4-44" aria-hidden="true">#</a> . (4.44)</h6><p>As before, the penalties are norms rather than squared norms, and as such result in coordinate selection for sufficiently large <em>τ</em>. Note that, unlike the</p><h6 id="sparse-additive-models-and-the-group-lasso-73"><a class="header-anchor" href="#sparse-additive-models-and-the-group-lasso-73" aria-hidden="true">#</a> SPARSE ADDITIVE MODELS AND THE GROUP LASSO 73</h6><p>usual penalty for a cubic smoothing spline, the norm in (4.43) includes a linear component; this ensures that the entire function is zero when the term is selected out of the model, rather than just its nonlinear component. Despite the similarity with the additive spline problem (4.38), the structure of the penalty‖ <em>fj</em> ‖H <em>j</em> means that the solution is not quite as simple as the sparse backfitting equations (4.40). Equipped with the norm (4.43), the spaceH <em>j</em> of cubic splines is a particular instance of a reproducing-kernel Hilbert space (RKHS) on the unit interval [0 <em>,</em> 1]. Any such space is characterized by a symmetric positive definite kernel functionR <em>j</em> : [0 <em>,</em> 1]×[0 <em>,</em> 1]→Rwith the so-called reproducing property. In particular, we are guaranteed for each <em>x</em> ∈[0 <em>,</em> 1], the functionR <em>j</em> (· <em>,x</em> ) is a member ofH <em>j</em> , and moreover that〈R(· <em>,x</em> ) <em>, f</em> 〉H <em>j</em> = <em>f</em> ( <em>x</em> ) for all <em>f</em> ∈H <em>j</em>. Here 〈· <em>,</em> ·〉H <em>j</em> denotes the inner product on the Hilbert spaceH <em>j</em>. Using the reproducing property, it can be shown (Exercise 4.17) that the <em>jth</em> coordinate function <em>f</em> ̂ <em>j</em> in any optimal COSSO solution can be written in</p><p>the form <em>f</em> ̂ <em>j</em> (·) =</p><h6 id="∑-n-51"><a class="header-anchor" href="#∑-n-51" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\nθ ̂ ij R j (· , xij ), for a suitably chosen weight vector̂ θ j ∈\n</code></pre></div><p>R <em>N</em>. Moreover, it can be shown that <em>f</em> ̂ <em>j</em> has Hilbert norm‖ <em>f</em> ̂ <em>j</em> ‖^2 H <em>j</em> =̂ <strong><em>θ</em></strong></p><p><em>T j</em> <strong>R</strong> <em>j</em> ̂ <strong><em>θ</em></strong> <em>j</em> , where <strong>R</strong> <em>j</em> ∈R <em>N</em> × <em>N</em> is a <em>Gram matrix</em> defined by the kernel—in particular, with entries ( <strong>R</strong> <em>j</em> ) <em>ii</em> ′=R <em>j</em> ( <em>xij,xi</em> ′ <em>j</em> ). Consequently, the COSSO problem (4.44) can be rewritten as a more general version of the group lasso: in particular, it is equivalent to the optimization problem</p><div class="language-"><pre><code>minimize\nθ j ∈R N, j =1 ,...,J\n</code></pre></div><h6 id="-22"><a class="header-anchor" href="#-22" aria-hidden="true">#</a> </h6><h6 id="-19"><a class="header-anchor" href="#-19" aria-hidden="true">#</a> </h6><h6 id="-19"><a class="header-anchor" href="#-19" aria-hidden="true">#</a> </h6><h6 id="_1-43"><a class="header-anchor" href="#_1-43" aria-hidden="true">#</a> 1</h6><h6 id="n-18"><a class="header-anchor" href="#n-18" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>‖ y −\n</code></pre></div><h6 id="∑-j-26"><a class="header-anchor" href="#∑-j-26" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>R j θ j ‖^22 + τ\n</code></pre></div><h6 id="∑-j-27"><a class="header-anchor" href="#∑-j-27" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><h6 id="√-8"><a class="header-anchor" href="#√-8" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>θ Tj R j θ j\n</code></pre></div><h6 id="-20"><a class="header-anchor" href="#-20" aria-hidden="true">#</a> </h6><h6 id="-19"><a class="header-anchor" href="#-19" aria-hidden="true">#</a> </h6><h6 id="-19"><a class="header-anchor" href="#-19" aria-hidden="true">#</a> </h6><h6 id="_4-45"><a class="header-anchor" href="#_4-45" aria-hidden="true">#</a> , (4.45)</h6><p>as verified in Exercise 4.17. We are now back in a parametric setting, and the solution is a more general version of the group lasso (4.14). It can be shown that any optimal solution (̂ <strong><em>θ</em></strong> 1 <em>,...,</em> ̂ <strong><em>θ</em></strong> <em>J</em> ) is specified by the fixed point equations</p><div class="language-"><pre><code>̂ θ j =\n</code></pre></div><h6 id="-23"><a class="header-anchor" href="#-23" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>0 if\n</code></pre></div><h6 id="√-9"><a class="header-anchor" href="#√-9" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>r Tj R j r j&lt; τ\n\n R j +√ τ\n̂ θ Tj R j ̂ θ j\n</code></pre></div><h6 id="i-1"><a class="header-anchor" href="#i-1" aria-hidden="true">#</a> I</h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>− 1\nr j otherwise,\n</code></pre></div><h6 id="_4-46"><a class="header-anchor" href="#_4-46" aria-hidden="true">#</a> (4.46)</h6><p>where <strong><em>r</em></strong> <em>j</em> : = <strong>y</strong> −</p><h6 id="∑-23"><a class="header-anchor" href="#∑-23" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>k 6 = j R k ̂ θ k corresponds to the j\nth partial residual. Although\n</code></pre></div><p>̂ <strong><em>θ</em></strong> <em>j</em> appears in both sides of the Equation (4.46), it can be solved with a one-</p><p>time SVD of <strong>R</strong> <em>j</em> and a simple one-dimensional search; see Exercise 4.7 for the details. Lin and Zhang (2003) propose an alternative approach, based on intro- ducing a vector <em>γ</em> ∈R <em>J</em> of auxiliary variables, and then considering the joint</p><h6 id="_74-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_74-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 74 GENERALIZATIONS OF THE LASSO PENALTY</h6><p>optimization problem</p><div class="language-"><pre><code>minimize\nγ ≥ 0\nfj ∈H j, j =1 ,...J\n</code></pre></div><h6 id="-24"><a class="header-anchor" href="#-24" aria-hidden="true">#</a> </h6><h6 id="-20"><a class="header-anchor" href="#-20" aria-hidden="true">#</a> </h6><h6 id="-20"><a class="header-anchor" href="#-20" aria-hidden="true">#</a> </h6><h6 id="_1-44"><a class="header-anchor" href="#_1-44" aria-hidden="true">#</a> 1</h6><h6 id="n-19"><a class="header-anchor" href="#n-19" aria-hidden="true">#</a> N</h6><h6 id="∑-n-52"><a class="header-anchor" href="#∑-n-52" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi −\n</code></pre></div><h6 id="∑-j-28"><a class="header-anchor" href="#∑-j-28" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>fj ( xij ))^2 +\n</code></pre></div><h6 id="∑-j-29"><a class="header-anchor" href="#∑-j-29" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><h6 id="_1-45"><a class="header-anchor" href="#_1-45" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>γj\n</code></pre></div><div class="language-"><pre><code>‖ fj ‖^2 H j + λ\n</code></pre></div><h6 id="∑-j-30"><a class="header-anchor" href="#∑-j-30" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>γj\n</code></pre></div><h6 id="-21"><a class="header-anchor" href="#-21" aria-hidden="true">#</a> </h6><h6 id="-20"><a class="header-anchor" href="#-20" aria-hidden="true">#</a> </h6><h6 id="-20"><a class="header-anchor" href="#-20" aria-hidden="true">#</a> </h6><h6 id="_4-47"><a class="header-anchor" href="#_4-47" aria-hidden="true">#</a> (4.47)</h6><p>As shown in Exercise 4.18, if we set <em>λ</em> = <em>τ</em>^4 <em>/</em> 4 in the lifted problem (4.47), then thê <strong><em>θ</em></strong> component of any optimal solution coincides with an optimal solution of the original COSSO (4.44). The reformulation (4.47) is useful, because it naturally leads to a conve- nient algorithm that alternates between two steps:</p><ul><li>For <em>γj</em> fixed, the problem is a version of our earlier objective (4.42), and results in an additive-spline fit.</li><li>With the fitted additive spline fixed, updating the vector of coefficients <em>γ</em> = ( <em>γ</em> 1 <em>,...,γJ</em> ) amounts to a nonnegative lasso problem. More precisely, for each <em>j</em> = 1 <em>,...,J</em> , define the vector <strong>g</strong> <em>j</em> = <strong>R</strong> <em>j</em> <strong><em>θ</em></strong> <em>j/γj</em> ∈R <em>N</em> , where <strong>f</strong> <em>j</em> = <strong>R</strong> <em>j</em> <strong><em>θ</em></strong> <em>j</em> is the fitted vector for the <em>jth</em> function using the current value of <em>γj</em>. Then we update the vector <em>γ</em> = ( <em>γ</em> 1 <em>,...,γJ</em> ) by solving</li></ul><div class="language-"><pre><code>min\nγ ≥ 0\n</code></pre></div><h6 id="-118"><a class="header-anchor" href="#-118" aria-hidden="true">#</a> {</h6><h6 id="_1-46"><a class="header-anchor" href="#_1-46" aria-hidden="true">#</a> 1</h6><h6 id="n-20"><a class="header-anchor" href="#n-20" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>‖ y − G γ ‖^22 + λ ‖ γ ‖ 1\n</code></pre></div><h6 id="-119"><a class="header-anchor" href="#-119" aria-hidden="true">#</a> }</h6><h6 id="_4-48"><a class="header-anchor" href="#_4-48" aria-hidden="true">#</a> , (4.48)</h6><p>where <strong>G</strong> is the <em>N</em> × <em>J</em> matrix with columns{ <strong>g</strong> <em>j,j</em> = 1 <em>,...,J</em> }. These updates are a slightly different form than that given in Lin and Zhang (2003); full details are mapped out in Exercise 4.19. When applied with the cubic smoothing-spline norm (4.43), the COSSO is aimed at setting component functions <em>fj</em> to zero. There are many extensions to this basic idea. For instance, given a univariate function <em>g</em> , we might instead represent each univariate function in the form <em>g</em> ( <em>t</em> ) = <em>α</em> 0 + <em>α</em> 1 <em>t</em> + <em>h</em> ( <em>t</em> ), and focus the penalty on departures from linearity using the norm</p><div class="language-"><pre><code>‖ h ‖^2 H: =\n</code></pre></div><h6 id="∫-1-3"><a class="header-anchor" href="#∫-1-3" aria-hidden="true">#</a> ∫ 1</h6><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>h ′′( t )^2 dt, (4.49)\n</code></pre></div><p>In this setting, a variant of COSSO can select between nonlinear and linear forms for each component function. We discuss penalties for additive models further in Section 4.4.4, in par- ticular the benefits of using more than one penalty in this context.</p><h4 id="_4-4-4-multiple-penalization-for-sparse-additive-models"><a class="header-anchor" href="#_4-4-4-multiple-penalization-for-sparse-additive-models" aria-hidden="true">#</a> 4.4.4 Multiple Penalization for Sparse Additive Models</h4><p>As we have seen thus far, there are multiple ways of enforcing sparsity for a nonparametric problem. Some methods, such as the SPAM back-fitting pro-</p><h6 id="sparse-additive-models-and-the-group-lasso-75"><a class="header-anchor" href="#sparse-additive-models-and-the-group-lasso-75" aria-hidden="true">#</a> SPARSE ADDITIVE MODELS AND THE GROUP LASSO 75</h6><p>cedure, are based on a combination of the <em>`</em> 1 -norm with the empirical <em>L</em>^2 - norm—namely, the quantity</p><div class="language-"><pre><code>‖ f ‖ N, 1 : =\n</code></pre></div><h6 id="∑-j-31"><a class="header-anchor" href="#∑-j-31" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>‖ fj ‖ N, (4.50)\n</code></pre></div><p>where‖ <em>fj</em> ‖^2 <em>N</em> : = <em>N</em>^1</p><h6 id="∑-n-53"><a class="header-anchor" href="#∑-n-53" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1 f\n2\nj ( xij ) is the squared empirical L\n</code></pre></div><p>(^2) -norm for the univariate function <em>fj</em>.^7 Other methods, such as the COSSO method, enforce sparsity using a combination of the <em>`</em> 1 -norm with the Hilbert norm ‖ <em>f</em> ‖H <em>,</em> 1 : =</p><h6 id="∑-j-32"><a class="header-anchor" href="#∑-j-32" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>‖ fj ‖H j. (4.51)\n</code></pre></div><p>Which of these two different regularizers is to be preferred for enforcing spar- sity in the nonparametric setting? Instead of focusing on only one regularizer, one might consider the more general family of estimators</p><div class="language-"><pre><code>min\nfj ∈H j\nj =1 ,...,J\n</code></pre></div><h6 id="-25"><a class="header-anchor" href="#-25" aria-hidden="true">#</a> </h6><h6 id="-21"><a class="header-anchor" href="#-21" aria-hidden="true">#</a> </h6><h6 id="-21"><a class="header-anchor" href="#-21" aria-hidden="true">#</a> </h6><h6 id="_1-47"><a class="header-anchor" href="#_1-47" aria-hidden="true">#</a> 1</h6><h6 id="n-21"><a class="header-anchor" href="#n-21" aria-hidden="true">#</a> N</h6><h6 id="∑-n-54"><a class="header-anchor" href="#∑-n-54" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi −\n</code></pre></div><h6 id="∑-j-33"><a class="header-anchor" href="#∑-j-33" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>fj ( xij ))^2 + λ H\n</code></pre></div><h6 id="∑-j-34"><a class="header-anchor" href="#∑-j-34" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>‖ fj ‖H j + λN\n</code></pre></div><h6 id="∑-j-35"><a class="header-anchor" href="#∑-j-35" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>‖ fj ‖ N\n</code></pre></div><h6 id="-22"><a class="header-anchor" href="#-22" aria-hidden="true">#</a> </h6><h6 id="-21"><a class="header-anchor" href="#-21" aria-hidden="true">#</a> </h6><h6 id="-21"><a class="header-anchor" href="#-21" aria-hidden="true">#</a> </h6><h6 id="-120"><a class="header-anchor" href="#-120" aria-hidden="true">#</a> ,</h6><h6 id="_4-52"><a class="header-anchor" href="#_4-52" aria-hidden="true">#</a> (4.52)</h6><p>parametrized by the pair of nonnegative regularization weights ( <em>λ</em> H <em>,λN</em> ). If we set <em>λN</em> = 0, then the optimization problem (4.52) reduces to the COSSO esti- mator, whereas for <em>λ</em> H= 0, we obtain a method closely related to the SPAM estimator. For any nonnegative ( <em>λ</em> H <em>,λN</em> ), the optimization problem (4.52) is convex. When the underlying univariate Hilbert spacesH <em>j</em> are described by a reproducing kernel, then the problem (4.52) can be re-formulated as a second-order cone program, and is closely related to the group lasso. When- ever the Hilbert spaceH <em>j</em> is defined by a reproducing kernelR <em>j</em> , then the</p><p><em>jth</em> coordinate function <em>f</em> ̂ <em>j</em> in any optimal solution again takes the form</p><p><em>f</em> ̂ <em>j</em> (·) =∑ <em>N i</em> =1 <em>θ</em> ̂ <em>ij</em> R <em>j</em> (· <em>,xij</em> ) for a vector of weights <em>θ</em> ̂ <em>j</em> ∈R <em>N</em>. This fact allows us to reduce the solution of the infinite-dimensional problem (4.52) to the simpler problem</p><div class="language-"><pre><code>min\nθ j ∈R N\nj =1 ,...,J\n</code></pre></div><h6 id="-26"><a class="header-anchor" href="#-26" aria-hidden="true">#</a> </h6><h6 id="-22"><a class="header-anchor" href="#-22" aria-hidden="true">#</a> </h6><h6 id="-22"><a class="header-anchor" href="#-22" aria-hidden="true">#</a> </h6><h6 id="_1-48"><a class="header-anchor" href="#_1-48" aria-hidden="true">#</a> 1</h6><h6 id="n-22"><a class="header-anchor" href="#n-22" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>‖ y −\n</code></pre></div><h6 id="∑-j-36"><a class="header-anchor" href="#∑-j-36" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>R j θ j ‖^22 + λ H\n</code></pre></div><h6 id="∑-j-37"><a class="header-anchor" href="#∑-j-37" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><h6 id="√-10"><a class="header-anchor" href="#√-10" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>θ Tj R j θ j + λN\n</code></pre></div><h6 id="∑-j-38"><a class="header-anchor" href="#∑-j-38" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><h6 id="√-11"><a class="header-anchor" href="#√-11" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>θ Tj R^2 j θ j\n</code></pre></div><h6 id="-23"><a class="header-anchor" href="#-23" aria-hidden="true">#</a> </h6><h6 id="-22"><a class="header-anchor" href="#-22" aria-hidden="true">#</a> </h6><h6 id="-22"><a class="header-anchor" href="#-22" aria-hidden="true">#</a> </h6><h6 id="-121"><a class="header-anchor" href="#-121" aria-hidden="true">#</a> .</h6><h6 id="_4-53"><a class="header-anchor" href="#_4-53" aria-hidden="true">#</a> (4.53)</h6><p>As before, for each coordinate <em>j</em> ∈ { 1 <em>,...,J</em> }, the matrix <strong>R</strong> <em>j</em> ∈R <em>N</em> × <em>N</em> is the kernel Gram matrix, with entries [ <strong>R</strong> <em>j</em> ] <em>ii</em> ′=R <em>j</em> ( <em>xij,xi</em> ′ <em>j</em> ). See Exercise 4.20 for more details on this reduction.</p><p>(^7) ‖ <em>fj</em> ‖ <em>N,</em> 1 is the same as the‖ <strong>f</strong> <em>j</em> ‖ 2 used in Section 4.4.2; here we are using a more gener- alizable notation.</p><h6 id="_76-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_76-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 76 GENERALIZATIONS OF THE LASSO PENALTY</h6><div class="language-"><pre><code>0 200 400 600 800 1000\n</code></pre></div><div class="language-"><pre><code>−2\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>Genome order\n</code></pre></div><div class="language-"><pre><code>log2 ratio\n</code></pre></div><p><strong>Figure 4.8</strong> <em>Fused lasso applied to CGH data. Each spike represents the copy number of a gene in a tumor sample, relative to that of a control (on the log base-2 scale). The piecewise-constant green curve is the fused lasso estimate.</em></p><p>The optimization problem (4.53) is an instance of a second-order cone program, and can be solved efficiently by a variant of the methods previously described. But why it is useful to impose two forms of regularization? As shown by Raskutti, Wainwright and Yu (2012), the combination of these two regularizers yields an estimator that is minimax-optimal, in that its conver- gence rate—as a function of sample size, problem dimension, and sparsity—is the fastest possible.</p><h3 id="_4-5-the-fused-lasso"><a class="header-anchor" href="#_4-5-the-fused-lasso" aria-hidden="true">#</a> 4.5 The Fused Lasso</h3><p>Consider the gray spikes in Figure 4.8, the results of a <em>comparative genomic hybridization (CGH)</em> experiment. Each of these represents the (log base 2) rel- ative copy number of a gene in a cancer sample relative to a control sample; these copy numbers are plotted against the chromosome order of the gene. These data are very noisy, so that some kind of smoothing is essential. Bio- logical considerations dictate that it is typically segments of a chromosome— rather than individual genes—that are replicated, Consequently, we might ex- pect that the underlying vector of true copy numbers to be piecewise-constant over contiguous regions of a chromosome. The <em>fused lasso signal approximator</em> exploits such structure within a signal, and is the solution of the following optimization problem</p><div class="language-"><pre><code>minimize\nθ ∈R N\n</code></pre></div><h6 id="-122"><a class="header-anchor" href="#-122" aria-hidden="true">#</a> {</h6><h6 id="_1-49"><a class="header-anchor" href="#_1-49" aria-hidden="true">#</a> 1</h6><h6 id="_2-39"><a class="header-anchor" href="#_2-39" aria-hidden="true">#</a> 2</h6><h6 id="∑-n-55"><a class="header-anchor" href="#∑-n-55" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi − θi )^2 + λ 1\n</code></pre></div><h6 id="∑-n-56"><a class="header-anchor" href="#∑-n-56" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>| θi |+ λ 2\n</code></pre></div><h6 id="∑-n-57"><a class="header-anchor" href="#∑-n-57" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =2\n</code></pre></div><div class="language-"><pre><code>| θi − θi − 1 |\n</code></pre></div><h6 id="-123"><a class="header-anchor" href="#-123" aria-hidden="true">#</a> }</h6><h6 id="_4-54"><a class="header-anchor" href="#_4-54" aria-hidden="true">#</a> . (4.54)</h6><h6 id="the-fused-lasso-77"><a class="header-anchor" href="#the-fused-lasso-77" aria-hidden="true">#</a> THE FUSED LASSO 77</h6><p>The first penalty is the familiar <em>`</em> 1 -norm, and serves to shrink the <em>θi</em> toward zero. Since the observation index <em>i</em> orders the data (in this case along the chromosome), the second penalty encourages neighboring coefficients <em>θi</em> to be similar, and will cause some to be identical (also known as <em>total-variation de- noising</em> ). Notice that (4.54) does not include a constant term <em>θ</em> 0 ; the coefficient <em>θi</em> represents the response <em>yi</em> directly, and for these kinds of problems zero is a natural origin. (See Exercise 4.21 for further exploration of this intercept issue.) The green curve in Figure 4.8 is fit to these data using the fused lasso. There are more general forms of the fused lasso; we mention two here.</p><ul><li>We can generalize the notion of neighbors from a linear ordering to more general neighborhoods, for examples adjacent pixels in an image. This leads to a penalty of the form <em>λ</em> 2</li></ul><h6 id="∑-24"><a class="header-anchor" href="#∑-24" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i ∼ i ′\n</code></pre></div><div class="language-"><pre><code>| θi − θi ′| , (4.55)\n</code></pre></div><div class="language-"><pre><code>where we sum over all neighboring pairs i ∼ i ′.\n</code></pre></div><ul><li>In (4.54) every observation is associated with a coefficient. More generally we can solve</li></ul><div class="language-"><pre><code>minimize\n( β 0 ,β )∈R×R p\n</code></pre></div><h6 id="-27"><a class="header-anchor" href="#-27" aria-hidden="true">#</a> </h6><h6 id="-23"><a class="header-anchor" href="#-23" aria-hidden="true">#</a> </h6><h6 id="-23"><a class="header-anchor" href="#-23" aria-hidden="true">#</a> </h6><h6 id="_1-50"><a class="header-anchor" href="#_1-50" aria-hidden="true">#</a> 1</h6><h6 id="_2-40"><a class="header-anchor" href="#_2-40" aria-hidden="true">#</a> 2</h6><h6 id="∑-n-58"><a class="header-anchor" href="#∑-n-58" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi − β 0 −\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>xijβj )^2\n</code></pre></div><div class="language-"><pre><code>+ λ 1\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>| βj |+ λ 2\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =2\n</code></pre></div><div class="language-"><pre><code>| βj − βj − 1 |\n</code></pre></div><h6 id="-24"><a class="header-anchor" href="#-24" aria-hidden="true">#</a> </h6><h6 id="-23"><a class="header-anchor" href="#-23" aria-hidden="true">#</a> </h6><h6 id="-23"><a class="header-anchor" href="#-23" aria-hidden="true">#</a> </h6><h6 id="_4-56"><a class="header-anchor" href="#_4-56" aria-hidden="true">#</a> . (4.56)</h6><div class="language-"><pre><code>Here the covariates xij and their coefficients βj are indexed along some\nsequence j for which neighborhood clumping makes sense; (4.54) is clearly\na special case.\n</code></pre></div><h4 id="_4-5-1-fitting-the-fused-lasso"><a class="header-anchor" href="#_4-5-1-fitting-the-fused-lasso" aria-hidden="true">#</a> 4.5.1 Fitting the Fused Lasso</h4><p>Problem (4.54) and its relatives are all convex optimization problems, and so all have well-defined solutions. As in other problems of this kind, here we seek efficient <em>path algorithms</em> for finding solutions for a range of values for the tuning parameters. Although coordinate descent is one of our favorite algorithms for lasso-like problems, it need not work for the fused lasso (4.54), because the difference penalty is not a separable function of the coordinates. Consequently, coordinate descent can become “stuck” at a nonoptimal point as illustrated in Figure 5.8 on page 111. This separability condition is discussed in more detail in Section 5.4. We begin by considering the structure of the optimal solution̂ <strong><em>θ</em></strong> ( <em>λ</em> 1 <em>,λ</em> 2 ) of the fused lasso problem (4.54) as a function of the two regularization param- eters <em>λ</em> 1 and <em>λ</em> 2. The following result due to Friedman et al. (2007) provides some useful insight into the behavior of this optimum:</p><h6 id="_78-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_78-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 78 GENERALIZATIONS OF THE LASSO PENALTY</h6><p><em>Lemma 4.1.</em> For any <em>λ</em> ′ 1 <em>&gt; λ</em> 1 , we have</p><div class="language-"><pre><code>θ ̂ i ( λ ′ 1 ,λ 2 ) =S λ ′\n1 − λ^1\n</code></pre></div><h6 id="̂"><a class="header-anchor" href="#̂" aria-hidden="true">#</a> (̂</h6><div class="language-"><pre><code>θi ( λ 1 ,λ 2 )\n</code></pre></div><h6 id="-124"><a class="header-anchor" href="#-124" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>for each i = 1 ,...,N , (4.57)\n</code></pre></div><p>whereSis the soft-thresholding operatorS <em>λ</em></p><h6 id="-125"><a class="header-anchor" href="#-125" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>z\n</code></pre></div><h6 id="-126"><a class="header-anchor" href="#-126" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>: = sign( z )(| z |− λ )+.\n</code></pre></div><p>One important special case of Lemma 4.1 is the equality</p><div class="language-"><pre><code>̂ θi ( λ 1 ,λ 2 ) =S λ 1\n</code></pre></div><h6 id="-127"><a class="header-anchor" href="#-127" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>̂ θi (0 ,λ 2 )\n</code></pre></div><h6 id="-128"><a class="header-anchor" href="#-128" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>for each i = 1 ,...,N. (4.58)\n</code></pre></div><p>Consequently, if we solve the fused lasso with <em>λ</em> 1 = 0, all other solutions can be obtained immediately by soft thresholding. This useful reduction also applies to the more general versions of the fused lasso (4.55). On the basis of Lemma 4.1, it suffices to focus our attention on solving the problem^8</p><div class="language-"><pre><code>minimize\nθ ∈R N\n</code></pre></div><h6 id="-129"><a class="header-anchor" href="#-129" aria-hidden="true">#</a> {</h6><h6 id="_1-51"><a class="header-anchor" href="#_1-51" aria-hidden="true">#</a> 1</h6><h6 id="_2-41"><a class="header-anchor" href="#_2-41" aria-hidden="true">#</a> 2</h6><h6 id="∑-n-59"><a class="header-anchor" href="#∑-n-59" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi − θi )^2 + λ\n</code></pre></div><h6 id="∑-n-60"><a class="header-anchor" href="#∑-n-60" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =2\n</code></pre></div><div class="language-"><pre><code>| θi − θi − 1 |\n</code></pre></div><h6 id="-130"><a class="header-anchor" href="#-130" aria-hidden="true">#</a> }</h6><h6 id="_4-59"><a class="header-anchor" href="#_4-59" aria-hidden="true">#</a> . (4.59)</h6><p>We consider several approaches to solving (4.59).</p><h5 id="_4-5-1-1-reparametrization"><a class="header-anchor" href="#_4-5-1-1-reparametrization" aria-hidden="true">#</a> 4.5.1.1 Reparametrization</h5><p>One simple approach is to reparametrize problem (4.59) so that the penalty is additive. In detail, suppose that we consider a linear transformation of the form <strong><em>γ</em></strong> = <strong>M</strong> <strong><em>θ</em></strong> for an invertible matrix <strong>M</strong> ∈R <em>N</em> × <em>N</em> such that</p><div class="language-"><pre><code>γ 1 = θ 1 , and γi = θi − θi − 1 for i = 2 ,...,N. (4.60)\n</code></pre></div><p>In these transformed coordinates, the problem (4.59) is equivalent to the or- dinary lasso problem</p><div class="language-"><pre><code>minimize\nγ ∈R N\n</code></pre></div><h6 id="-131"><a class="header-anchor" href="#-131" aria-hidden="true">#</a> {</h6><h6 id="_1-52"><a class="header-anchor" href="#_1-52" aria-hidden="true">#</a> 1</h6><h6 id="_2-42"><a class="header-anchor" href="#_2-42" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ y − X γ ‖^2 + λ ‖ γ ‖ 1\n</code></pre></div><h6 id="-132"><a class="header-anchor" href="#-132" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>, with X = M −^1. (4.61)\n</code></pre></div><p>In principle, the reparametrize problem (4.61) can be solved using any effi- cient algorithm for the lasso, including coordinate descent, projected gradient descent or the LARS procedure. However, <strong>X</strong> is a lower-triangular matrix with all nonzero entries equal to 1, and hence has large correlations among the “variables.” Neither coordinate-descent nor LARS performs well under these circumstances (see Exercise 4.22). So despite the fact that reparametrization appears to solve the problem, it is not recommended, and more efficient algo- rithms exist, as we now discuss.</p><p>(^8) Here we have adopted the notation <em>λ</em> (as opposed to <em>λ</em> 2 ) for the regularization param- eter, since we now have only one penalty.</p><h6 id="the-fused-lasso-79"><a class="header-anchor" href="#the-fused-lasso-79" aria-hidden="true">#</a> THE FUSED LASSO 79</h6><h5 id="_4-5-1-2-a-path-algorithm"><a class="header-anchor" href="#_4-5-1-2-a-path-algorithm" aria-hidden="true">#</a> 4.5.1.2 A Path Algorithm</h5><p>The one-dimensional fused lasso (4.59) has an interesting property, namely that as the regularization parameter <em>λ</em> increases, pieces of the optimal solution can only be joined together, not split apart. More precisely, letting <strong><em>θ</em></strong> ̂( <em>λ</em> ) denote the optimal solution to the convex program (4.59) as a function of <em>λ</em> , we have:</p><p><em>Lemma 4.2. Monotone fusion.</em> Suppose that for some value of <em>λ</em> and some index <em>i</em> ∈{ 1 <em>,...,N</em> − 1 }, the optimal solution satisfieŝ <em>θi</em> ( <em>λ</em> ) =̂ <em>θi</em> +1( <em>λ</em> ). Then</p><p>for all <em>λ</em> ′ <em>&gt; λ</em> , we also have <em>θ</em> ̂ <em>i</em> ( <em>λ</em> ′) =̂ <em>θi</em> +1( <em>λ</em> ′).</p><p>Friedman et al. (2007) observed that this fact greatly simplifies the con- struction of the fused lasso solution path. One starts with <em>λ</em> = 0, for which there are no fused groups, and then computes the smallest value of <em>λ</em> that causes a fused group to form. The parameter estimates for this group are then fused together (i.e., constrained to be equal) for the remainder of the path. Along the way, a simple formula is available for the estimate within each fused group, so that the resulting procedure is quite fast, requiringO( <em>N</em> ) operations. However, we note that the monotone-fusion property in Lemma 4.2 is special to the one-dimensional fused lasso (4.59). For example, it does not hold for the general fused lasso (4.56) with a model matrix <strong>X</strong> , nor for the two-dimensional fused lasso (4.55). See Friedman et al. (2007) and Hoefling (2010) for more details on this approach.</p><h5 id="_4-5-1-3-a-dual-path-algorithm"><a class="header-anchor" href="#_4-5-1-3-a-dual-path-algorithm" aria-hidden="true">#</a> 4.5.1.3 A Dual Path Algorithm</h5><p>Tibshirani 2 and Taylor (2011) take a different approach, and develop path algorithms for the <em>convex duals</em> of fused lasso problems. Here we illustrate their approach on the problem (4.59), but note that their methodology applies to the general problem (4.56) as well. We begin by observing that problem (4.59) can be written in an equivalent <em>lifted</em> form</p><div class="language-"><pre><code>minimize\n( θ , z )∈R N ×R N −^1\n</code></pre></div><h6 id="-133"><a class="header-anchor" href="#-133" aria-hidden="true">#</a> {</h6><h6 id="_1-53"><a class="header-anchor" href="#_1-53" aria-hidden="true">#</a> 1</h6><h6 id="_2-43"><a class="header-anchor" href="#_2-43" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ y − θ ‖^22 + λ ‖ z ‖ 2\n</code></pre></div><h6 id="-134"><a class="header-anchor" href="#-134" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>subject to D θ = z , (4.62)\n</code></pre></div><p>where we have introduced a vector <strong>z</strong> ∈R <em>N</em> −^1 of auxiliary variables, and <strong>D</strong> is a ( <em>N</em> −1)× <em>N</em> matrix of first differences. Now consider the Lagrangian associated with the lifted problem, namely</p><div class="language-"><pre><code>L ( θ , z ; u ) : =\n</code></pre></div><h6 id="_1-54"><a class="header-anchor" href="#_1-54" aria-hidden="true">#</a> 1</h6><h6 id="_2-44"><a class="header-anchor" href="#_2-44" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ y − θ ‖^22 + λ ‖ z ‖+ u T ( D θ − z ) , (4.63)\n</code></pre></div><p>where <strong>u</strong> ∈R <em>N</em> −^1 is a vector of Lagrange multipliers. A straightforward com- putation shows that the Lagrangian dual functionQtakes form</p><div class="language-"><pre><code>Q( u ) : = inf\n( θ , z )∈R N ×R N −^1\n</code></pre></div><div class="language-"><pre><code>L ( θ , z ; u ) =\n</code></pre></div><h6 id="-135"><a class="header-anchor" href="#-135" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>−^12 ‖ y − D T u ‖^2 if‖ u ‖∞≤ λ,\n−∞ otherwise.\n(4.64)\n</code></pre></div><h6 id="_80-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_80-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 80 GENERALIZATIONS OF THE LASSO PENALTY</h6><p>The Lagrangian dual problem is to maximizeQ( <strong>u</strong> ), and given an optimal solution <strong>u</strong> ̂=̂ <strong>u</strong> ( <em>λ</em> ), we can recover an optimal solution <strong><em>θ</em></strong> ̂=̂ <strong><em>θ</em></strong> ( <em>λ</em> ) to the original problem by settinĝ <strong><em>θ</em></strong> = <strong>y</strong> − <strong>D</strong> <em>T</em> ̂ <strong>u</strong>. See Exercise 4.23 for the details of these duality calculations.</p><div class="language-"><pre><code>0.0 0.5 1.0 1.5\n</code></pre></div><div class="language-"><pre><code>−1.5\n</code></pre></div><div class="language-"><pre><code>−1.0\n</code></pre></div><div class="language-"><pre><code>−0.5\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.5\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>1.5\n</code></pre></div><div class="language-"><pre><code>λ\n</code></pre></div><div class="language-"><pre><code>Dual Coordinates\n</code></pre></div><div class="language-"><pre><code>0.0 0.5 1.0 1.5\n</code></pre></div><div class="language-"><pre><code>−1.0 −0.5\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.5\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>1.5\n</code></pre></div><div class="language-"><pre><code>λ\n</code></pre></div><div class="language-"><pre><code>Primal Coordinates\n</code></pre></div><p><strong>Figure 4.9</strong> <em>The dual path algorithm in action on a smal l example. The left panel shows the progress of</em> ̂ <strong>u</strong> ( <em>λ</em> ) <em>, while the right panel shows</em> ̂ <strong><em>θ</em></strong> ( <em>λ</em> )<em>. We see that in the dual coordinates, as a parameter hits the boundary, an</em> unfusing <em>occurs in the primal coordinates.</em></p><p>When the regularization parameter <em>λ</em> is sufficiently large, the dual max- imization, or equivalently the problem of minimizing−Q( <strong>u</strong> ), reduces to an unrestricted linear regression problem, with optimal solution</p><div class="language-"><pre><code>u ∗: = ( DD T )−^1 Dy. (4.65)\n</code></pre></div><p>The restrictions kick in when <em>λ</em> decreases to the critical level ‖ <strong>u</strong> ∗‖∞. Tibshirani 2 and Taylor (2011) show that as we decrease <em>λ</em> , once elements <em>u</em> ̂ <em>j</em> ( <em>λ</em> ) of the optimal solution hit the bound <em>λ</em> , then they are guaranteed to never leave the bound. This property leads to a very straightforward path algorithm, similar in spirit to LARS in Section 5.6; see Figure 4.9 for an illus- tration of the dual path algorithm in action. Exercise 4.23 explores some of the details.</p><h5 id="_4-5-1-4-dynamic-programming-for-the-fused-lasso"><a class="header-anchor" href="#_4-5-1-4-dynamic-programming-for-the-fused-lasso" aria-hidden="true">#</a> 4.5.1.4 Dynamic Programming for the Fused Lasso</h5><p>Dynamic programming is a computational method for solving difficult prob- lems by breaking them down into simpler subproblems. In the case of the one-dimensional fused lasso, the linear ordering of the variables means that</p><h6 id="the-fused-lasso-81"><a class="header-anchor" href="#the-fused-lasso-81" aria-hidden="true">#</a> THE FUSED LASSO 81</h6><p>fixing any variable breaks down the problem into two separate subproblems to the left and right of the fixed variable. In the “forward pass,” we move from left to right, fixing one variable and solving for the variable to its left, as a function of this fixed variable. When we reach the right end, a backward pass then gives the complete solution. Johnson (2013) proposed this dynamic programming approach to the fused lasso. In more detail, we begin by separating off terms in (4.59) that depend on <em>θ</em> 1 , and rewrite the objective function (4.59) in the form</p><div class="language-"><pre><code>f ( θ ) =\n</code></pre></div><h6 id="_1-55"><a class="header-anchor" href="#_1-55" aria-hidden="true">#</a> 1</h6><h6 id="_2-45"><a class="header-anchor" href="#_2-45" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>( y 1 − θ 1 )^2 + λ | θ 2 − θ 1 |\n︸ ︷︷ ︸\ng ( θ 1 ,θ 2 )\n</code></pre></div><h6 id="-136"><a class="header-anchor" href="#-136" aria-hidden="true">#</a> +</h6><h6 id="-137"><a class="header-anchor" href="#-137" aria-hidden="true">#</a> {</h6><h6 id="_1-56"><a class="header-anchor" href="#_1-56" aria-hidden="true">#</a> 1</h6><h6 id="_2-46"><a class="header-anchor" href="#_2-46" aria-hidden="true">#</a> 2</h6><h6 id="∑-n-61"><a class="header-anchor" href="#∑-n-61" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =2\n</code></pre></div><div class="language-"><pre><code>( yi − θi )^2 + λ\n</code></pre></div><h6 id="∑-n-62"><a class="header-anchor" href="#∑-n-62" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =3\n</code></pre></div><div class="language-"><pre><code>| θi − θi − 1 |\n</code></pre></div><h6 id="-138"><a class="header-anchor" href="#-138" aria-hidden="true">#</a> }</h6><h6 id="_4-66"><a class="header-anchor" href="#_4-66" aria-hidden="true">#</a> (4.66)</h6><p>This decomposition shows the subproblem to be solved in the first step of the forward pass: we compute <em>θ</em> ̂ 1 ( <em>θ</em> 2 ) : = arg min <em>θ</em> 1 ∈R <em>g</em> ( <em>θ</em> 1 <em>,θ</em> 2 ), We have thus eliminated the first variable, and can now focus on the reduced objective function <em>f</em> 2 :R <em>N</em> −^1 →Rgiven by</p><div class="language-"><pre><code>f 2 ( θ 2 ,...θN ) = f (̂ θ 1 ( θ 2 ) ,θ 2 ,...θN ). (4.67)\n</code></pre></div><p>We can then iterate the procedure, maximizing over <em>θ</em> 2 to obtain <em>θ</em> ̂ 2 ( <em>θ</em> 3 ), and so on until we obtain <em>θ</em> ̂ <em>N</em>. Then we back-substitute to obtain <em>θ</em> ̂ <em>N</em> − 1 =̂ <em>θN</em> − 1 ( <em>θ</em> ̂ <em>N</em> ),</p><p>and so on for the sequenceŝ <em>θN</em> − 2 <em>,...,θ</em> ̂ 2 <em>,</em> ̂ <em>θ</em> 1. If each parameter <em>θi</em> can take only one of <em>K</em> distinct values, then each of the minimizerŝ <em>θj</em> ( <em>θj</em> +1) can be easily computed and stored as a <em>K</em> × <em>K</em> matrix. In the continuous case, the functions to be minimized are piecewise linear and quadratic, and care must be taken to compute and store the relevant information in an efficient manner; see Johnson (2013) for the details. The resulting algorithm is the fastest that we are aware of, requiring justO( <em>N</em> ) operations, and considerably faster than the path algorithm described above. Interestingly, if we change the <em><code>_ 1 difference penalty to an _</code></em> 0 , this approach can still be applied, despite the fact that the problem is no longer convex. Exercise 4.24 asks the reader to implement the discrete case.</p><h4 id="_4-5-2-trend-filtering"><a class="header-anchor" href="#_4-5-2-trend-filtering" aria-hidden="true">#</a> 4.5.2 Trend Filtering</h4><p>The first-order absolute difference penalty in the fused lasso can be generalized to use a higher-order difference, leading to the problem</p><div class="language-"><pre><code>minimize\nθ ∈R N\n</code></pre></div><h6 id="-139"><a class="header-anchor" href="#-139" aria-hidden="true">#</a> {</h6><h6 id="_1-57"><a class="header-anchor" href="#_1-57" aria-hidden="true">#</a> 1</h6><h6 id="_2-47"><a class="header-anchor" href="#_2-47" aria-hidden="true">#</a> 2</h6><h6 id="∑-n-63"><a class="header-anchor" href="#∑-n-63" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi − θi )^2 + λ ·‖ D ( k +1) θ ‖ 1\n</code></pre></div><h6 id="-140"><a class="header-anchor" href="#-140" aria-hidden="true">#</a> }</h6><h6 id="_4-68"><a class="header-anchor" href="#_4-68" aria-hidden="true">#</a> . (4.68)</h6><p>This is known as <em>trend filtering</em>. Here <strong>D</strong> ( <em>k</em> +1) is a matrix of dimension ( <em>N</em> − <em>k</em> −1)× <em>N</em> that computes discrete differences of order <em>k</em> + 1. The fused</p><h6 id="_82-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_82-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 82 GENERALIZATIONS OF THE LASSO PENALTY</h6><p>lasso uses first-order differences ( <em>k</em> = 0), while higher-order differences en- courage higher-order smoothness. In general, trend filtering of order <em>k</em> results in solutions that are piecewise polynomials of degree <em>k</em>. Linear trend filter- ing ( <em>k</em> = 1) is especially attractive, leading to piecewise-linear solutions. The</p><div class="language-"><pre><code>60 70 80 90\n</code></pre></div><div class="language-"><pre><code>20\n</code></pre></div><div class="language-"><pre><code>40\n</code></pre></div><div class="language-"><pre><code>60\n</code></pre></div><div class="language-"><pre><code>80\n</code></pre></div><div class="language-"><pre><code>100\n</code></pre></div><div class="language-"><pre><code>Temperature\n</code></pre></div><div class="language-"><pre><code>Ozone\n</code></pre></div><div class="language-"><pre><code>Smoothing Spline\nTrend Filter\n</code></pre></div><p><strong>Figure 4.10</strong> <em>A piecewise-linear function fit to some air-pol lution data using trend filtering. For comparison, a smoothing spline with the same degrees of freedom is included.</em></p><p>knots in the solution need not be specified but fall out of the convex optimiza- tion procedure. Kim, Koh, Boyd and Gorinevsky (2009) propose an efficient interior point algorithm for this problem. Tibshirani 2 (2014) proves that the trend filtering estimate adapts to the local level of smoothness much better than smoothing splines, and displays a surprising similarity to locally-adaptive regression splines. Further, he shows that the estimate converges to the true underlying function at the minimax rate for functions whose <em>kth</em> derivative is of bounded variation (a property not shared by linear estimators such as smooth- ing splines). Furthermore, Tibshirani 2 and Taylor (2011) show that a solution with <em>m</em> knots has estimated degrees of freedom given by df = <em>m</em> + <em>k</em> + 1.^9 Figure 4.10 shows a piecewise-linear function fit by trend filtering to some air-pollution data. As a comparison, we include the fit of a smoothing spline, with the same effective df = 4. While the fits are similar, it appears that trend filtering has found natural changepoints in the data. In (4.68) it is assumed that the observations occur at evenly spaced posi- tions. The penalty can be modified (Tibshirani 2 2014) to accommodate arbi- trary (ordered) positions <em>xi</em> as follows:</p><div class="language-"><pre><code>minimize\nθ ∈R N\n</code></pre></div><h6 id="-141"><a class="header-anchor" href="#-141" aria-hidden="true">#</a> {</h6><h6 id="_1-58"><a class="header-anchor" href="#_1-58" aria-hidden="true">#</a> 1</h6><h6 id="_2-48"><a class="header-anchor" href="#_2-48" aria-hidden="true">#</a> 2</h6><h6 id="∑-n-64"><a class="header-anchor" href="#∑-n-64" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi − θi )^2 + λ ·\n</code></pre></div><h6 id="n-∑−-2"><a class="header-anchor" href="#n-∑−-2" aria-hidden="true">#</a> N ∑− 2</h6><div class="language-"><pre><code>i =1\n</code></pre></div><h6 id="∣"><a class="header-anchor" href="#∣" aria-hidden="true">#</a> ∣</h6><h6 id="∣-1"><a class="header-anchor" href="#∣-1" aria-hidden="true">#</a> ∣</h6><h6 id="∣-2"><a class="header-anchor" href="#∣-2" aria-hidden="true">#</a> ∣</h6><h6 id="∣-3"><a class="header-anchor" href="#∣-3" aria-hidden="true">#</a> ∣</h6><div class="language-"><pre><code>θi +2− θi +1\nxi +2− xi +1\n</code></pre></div><h6 id="−-10"><a class="header-anchor" href="#−-10" aria-hidden="true">#</a> −</h6><div class="language-"><pre><code>θi +1− θi\nxi +1− xi\n</code></pre></div><h6 id="∣-4"><a class="header-anchor" href="#∣-4" aria-hidden="true">#</a> ∣</h6><h6 id="∣-5"><a class="header-anchor" href="#∣-5" aria-hidden="true">#</a> ∣</h6><h6 id="∣-6"><a class="header-anchor" href="#∣-6" aria-hidden="true">#</a> ∣</h6><h6 id="∣-7"><a class="header-anchor" href="#∣-7" aria-hidden="true">#</a> ∣</h6><h6 id="-142"><a class="header-anchor" href="#-142" aria-hidden="true">#</a> }</h6><h6 id="_4-69"><a class="header-anchor" href="#_4-69" aria-hidden="true">#</a> (4.69)</h6><p>(^9) This is an unbiased estimate of the degrees of freedom; see Section 2.5.</p><h6 id="the-fused-lasso-83"><a class="header-anchor" href="#the-fused-lasso-83" aria-hidden="true">#</a> THE FUSED LASSO 83</h6><div class="language-"><pre><code>1850 1900 1950 2000\n</code></pre></div><div class="language-"><pre><code>−0.6\n</code></pre></div><div class="language-"><pre><code>−0.4\n</code></pre></div><div class="language-"><pre><code>−0.2\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>Year\n</code></pre></div><div class="language-"><pre><code>Temperature Anomalies\n</code></pre></div><div class="language-"><pre><code>Near Isotonic\nIsotonic\n</code></pre></div><p><strong>Figure 4.11</strong> <em>Near isotonic fit to global-warming data, showing annual temperature anomalies. The value ofλwas chosen by cross-validation, and the fit appears to support the evidence of nonmonotone behavior seen in the data.</em></p><p>It compares the empirical slopes for adjacent pairs, and encourages them to be the same. This is the penalty that was used in Figure 4.10, since the Temperaturevalues are not uniformly spaced.</p><h4 id="_4-5-3-nearly-isotonic-regression"><a class="header-anchor" href="#_4-5-3-nearly-isotonic-regression" aria-hidden="true">#</a> 4.5.3 Nearly Isotonic Regression</h4><p>Tibshirani 2 , Hoefling and Tibshirani (2011) suggest a simple modification of the one-dimensional fused lasso that encourages the solution to be monotone. It is based on a relaxation of isotonic regression. In the classical form of iso- tonic regression, we estimate <strong><em>θ</em></strong> ∈R <em>N</em> by solving the constrained minimization problem</p><div class="language-"><pre><code>minimize\nθ ∈R N\n</code></pre></div><h6 id="n-23"><a class="header-anchor" href="#n-23" aria-hidden="true">#</a> { N</h6><h6 id="∑-25"><a class="header-anchor" href="#∑-25" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi − θi )^2\n</code></pre></div><h6 id="-143"><a class="header-anchor" href="#-143" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>subject to θ 1 ≤ θ 2 ≤ ... ≤ θN. (4.70)\n</code></pre></div><p>The resulting solution gives the best monotone (nondecreasing) fit to the data. Monotone nonincreasing solutions can be obtaining by first flipping the signs of the data. There is a unique solution to problem (4.70), and it can be obtained using the <em>pool adjacent violators algorithm</em> (Barlow, Bartholomew, Bremner and Brunk 1972), or PAVA for short. Nearly isotonic regression is a natural relaxation, in which we introduce a regularization parameter <em>λ</em> ≥0, and instead solve the penalized problem</p><div class="language-"><pre><code>minimize\nθ ∈R N\n</code></pre></div><h6 id="-144"><a class="header-anchor" href="#-144" aria-hidden="true">#</a> {</h6><h6 id="_1-59"><a class="header-anchor" href="#_1-59" aria-hidden="true">#</a> 1</h6><h6 id="_2-49"><a class="header-anchor" href="#_2-49" aria-hidden="true">#</a> 2</h6><h6 id="∑-n-65"><a class="header-anchor" href="#∑-n-65" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi − θi )^2 + λ\n</code></pre></div><h6 id="n-∑−-1"><a class="header-anchor" href="#n-∑−-1" aria-hidden="true">#</a> N ∑− 1</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( θi − θi +1)+\n</code></pre></div><h6 id="-145"><a class="header-anchor" href="#-145" aria-hidden="true">#</a> }</h6><h6 id="_4-71"><a class="header-anchor" href="#_4-71" aria-hidden="true">#</a> . (4.71)</h6><p>The penalty term penalizes adjacent pairs that violate the monotonicity prop- erty, that is, having <em>θi&gt; θi</em> +1. When <em>λ</em> = 0, the solution interpolates the data,</p><h6 id="_84-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_84-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 84 GENERALIZATIONS OF THE LASSO PENALTY</h6><p>and letting <em>λ</em> → ∞, we recover the solution to the classical isotonic regres- sion problem (4.70). Intermediate value of <em>λ</em> yield nonmonotone solutions that trade off monotonicity with goodness-of-fit; this trade-off allows one to assess the validity of the monotonicity assumption for the given data sequence. Fig- ure 4.11 illustrates the method on data on annual temperature anomalies from 1856 to 1999, relative to the 1961–1990 mean. The solution to the nearly iso- tonic problem (4.71) can be obtained from a simple modification of the path algorithm discussed previously, a procedure that is analogous to the PAVA algorithm for problem (4.70); see Tibshirani 2 et al. (2011) for details.</p><h3 id="_4-6-nonconvex-penalties"><a class="header-anchor" href="#_4-6-nonconvex-penalties" aria-hidden="true">#</a> 4.6 Nonconvex Penalties</h3><div class="language-"><pre><code>β 1\n</code></pre></div><div class="language-"><pre><code>β 3\n</code></pre></div><div class="language-"><pre><code>β 2\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>β 1\nβ 2\n</code></pre></div><div class="language-"><pre><code>β 3\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>β 3\n</code></pre></div><div class="language-"><pre><code>β 1\nβ 2\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><p><strong>Figure 4.12</strong> <em>The`qunit balls in</em> R^3 <em>forq</em> = 2 <em>(left),q</em> = 1 <em>(middle), andq</em> = 0_._ 8 <em>(right). Forq &lt;</em> 1 <em>the constraint regions are nonconvex. Smal lerqwil l correspond to fewer nonzero coefficients, and less shrinkage. The nonconvexity leads to combi- natorial ly hard optimization problems.</em></p><p>By moving from an <em><code>_ 2 penalty to _</code></em> 1 , we have seen that for the same <em>effective df</em> the lasso selects a subset of variables to have nonzero coefficients, and shrinks their coefficients less. When <em>p</em> is large and the number of relevant variables is small, this may not be enough; in order to reduce the set of chosen variables sufficiently, lasso may end up over-shrinking the retained variables. For this reason there has been interest in nonconvex penalties. The natural choice might be the <em><code>q_ penalty, for 0≤ _q_ ≤1, with the limiting _</code></em> 0 corresponding to best-subset selection. Figure 4.12 compares the <em><code>q_ unit balls for _q_ ∈ { 2 _,_ 1 _,_ 0_._ 8 }. The spiky nonconvex nature of the ball on the right implies edges and coordinate axes will be favored in selection under such constraints. Unfortunately, along with nonconvexity comes combinatorial computational complexity; even the simplest case of _</code></em> 0 can be solved exactly only for <em>p</em> ≈40 or less. For this and related statistical reasons alternative nonconvex penalties have been proposed. These include the SCAD (Fan and Li 2001, <em>smoothly clipped absolute deviation</em> ) and MC+ (Zhang 2010, <em>mini- max concave</em> ) penalties. Figure 4.13 shows four members of the MC+ penalty family inR^1 , indexed by the nonconvexity parameter <em>γ</em> ∈(1 <em>,</em> ∞); this family</p><h6 id="nonconvex-penalties-85"><a class="header-anchor" href="#nonconvex-penalties-85" aria-hidden="true">#</a> NONCONVEX PENALTIES 85</h6><div class="language-"><pre><code>−4 −2 0 2 4\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>3\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>MC+ Penalty\n</code></pre></div><div class="language-"><pre><code>0 1 2 3 4\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>3\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>MC+ Threshold Function\n</code></pre></div><div class="language-"><pre><code>β β\n</code></pre></div><div class="language-"><pre><code>γ=∞lasso\nγ= 3\nγ= 1. 7\nγ= 1+subset\n</code></pre></div><p><strong>Figure 4.13</strong> <em>Left: The MC+ family of nonconvex sparsity penalties, indexed by a sparsity parameterγ</em> ∈(1 <em>,</em> ∞)<em>. Right: piecewise-linear and continuous threshold functions associated with MC+ (only the north-east quadrant is shown), making this penalty family suitable for coordinate descent algorithms.</em></p><p>bridges the gap between lasso ( <em>γ</em> =∞) and best-subset ( <em>γ</em> = 1+). The penalty functions are piecewise quadratic (see Exercise 4.25), and importantly the cor- responding threshold functions are piecewise linear and continuous. In detail, for squared-error loss we pose the (nonconvex) optimization problem</p><div class="language-"><pre><code>minimize\nβ ∈R p\n</code></pre></div><h6 id="-28"><a class="header-anchor" href="#-28" aria-hidden="true">#</a> </h6><h6 id="-24"><a class="header-anchor" href="#-24" aria-hidden="true">#</a> </h6><h6 id="-24"><a class="header-anchor" href="#-24" aria-hidden="true">#</a> </h6><h6 id="_1-60"><a class="header-anchor" href="#_1-60" aria-hidden="true">#</a> 1</h6><h6 id="_2-50"><a class="header-anchor" href="#_2-50" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ y − X β ‖^22 +\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>Pλ,γ ( βj )\n</code></pre></div><h6 id="-25"><a class="header-anchor" href="#-25" aria-hidden="true">#</a> </h6><h6 id="-24"><a class="header-anchor" href="#-24" aria-hidden="true">#</a> </h6><h6 id="-24"><a class="header-anchor" href="#-24" aria-hidden="true">#</a> </h6><h6 id="_4-72"><a class="header-anchor" href="#_4-72" aria-hidden="true">#</a> , (4.72)</h6><p>with the MC+ penalty on each coordinate defined by</p><div class="language-"><pre><code>Pλ,γ ( θ ) : =\n</code></pre></div><div class="language-"><pre><code>∫| θ |\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><h6 id="-146"><a class="header-anchor" href="#-146" aria-hidden="true">#</a> (</h6><h6 id="_1-−-7"><a class="header-anchor" href="#_1-−-7" aria-hidden="true">#</a> 1 −</h6><div class="language-"><pre><code>x\nλγ\n</code></pre></div><h6 id="-147"><a class="header-anchor" href="#-147" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>+\n</code></pre></div><div class="language-"><pre><code>dx. (4.73)\n</code></pre></div><p>With coordinate descent in mind, we consider solving a one-dimensional ver- sion of (4.72) (in standardized form)</p><div class="language-"><pre><code>minimize\nβ ∈R^1\n</code></pre></div><h6 id="-148"><a class="header-anchor" href="#-148" aria-hidden="true">#</a> {</h6><h6 id="_1-61"><a class="header-anchor" href="#_1-61" aria-hidden="true">#</a> 1</h6><h6 id="_2-51"><a class="header-anchor" href="#_2-51" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>( β − β ̃)^2 + λ\n</code></pre></div><div class="language-"><pre><code>∫| β |\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><h6 id="-149"><a class="header-anchor" href="#-149" aria-hidden="true">#</a> (</h6><h6 id="_1-−-8"><a class="header-anchor" href="#_1-−-8" aria-hidden="true">#</a> 1 −</h6><div class="language-"><pre><code>x\nλγ\n</code></pre></div><h6 id="-150"><a class="header-anchor" href="#-150" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>+\n</code></pre></div><div class="language-"><pre><code>dx\n</code></pre></div><h6 id="-151"><a class="header-anchor" href="#-151" aria-hidden="true">#</a> }</h6><h6 id="_4-74"><a class="header-anchor" href="#_4-74" aria-hidden="true">#</a> . (4.74)</h6><p>The solution is unique^10 for <em>γ &gt;</em> 1 and is given by</p><div class="language-"><pre><code>S λ,γ ( β ̃) =\n</code></pre></div><h6 id="-29"><a class="header-anchor" href="#-29" aria-hidden="true">#</a> </h6><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>0 if| β ̃|≤ λ\nsign( β ̃)\n</code></pre></div><h6 id="-152"><a class="header-anchor" href="#-152" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>| β ̃|− λ\n1 − γ^1\n</code></pre></div><h6 id="-153"><a class="header-anchor" href="#-153" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>if λ &lt; | β ̃|≤ λγ\nβ ̃ if| β ̃| &gt; λγ\n</code></pre></div><h6 id="_4-75"><a class="header-anchor" href="#_4-75" aria-hidden="true">#</a> (4.75)</h6><p>(^10) Despite the nonconvexity, there is a unique solution inR (^1) ; this is not necessarily the case for the <em>p</em> -dimensional problem (4.72).</p><h6 id="_86-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_86-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 86 GENERALIZATIONS OF THE LASSO PENALTY</h6><p>The right panel in Figure 4.13 shows examples of (4.75). Large values of <em>β</em> ̃are left alone, small values are set to zero, and intermediate values are shrunk. As <em>γ</em> gets smaller, the intermediate zone gets narrower, until eventually it becomes the hard-thresholding function of best subset (orange curve in figure). By contrast, the threshold functions for the <em>`q</em> family ( <em>q &lt;</em> 1) are discontinuous in <em>β</em> ̃. Mazumder, Friedman and Hastie (2011) exploit the continuity ofS <em>λ,γ</em> (in both <em>λ</em> and <em>γ</em> ) in a coordinate-descent scheme for fitting solution paths for the entire MC+ family. Starting with the lasso solution, their Rpackage sparsenet(Mazumder, Hastie and Friedman 2012) moves down a sequence in <em>γ</em> toward sparser models, and for each fits a regularization path in <em>λ</em>. Al- though it cannot claim to solve the nonconvex problem (4.72), this approach is both very fast and appears to find good solutions. Zou (2006) proposed the <em>adaptive lasso</em> as a means for fitting models sparser than lasso. Using a pilot estimate <em>β</em> ̃, the adaptive lasso solves</p><div class="language-"><pre><code>minimize\nβ ∈R p\n</code></pre></div><h6 id="-30"><a class="header-anchor" href="#-30" aria-hidden="true">#</a> </h6><h6 id="-25"><a class="header-anchor" href="#-25" aria-hidden="true">#</a> </h6><h6 id="-25"><a class="header-anchor" href="#-25" aria-hidden="true">#</a> </h6><h6 id="_1-62"><a class="header-anchor" href="#_1-62" aria-hidden="true">#</a> 1</h6><h6 id="_2-52"><a class="header-anchor" href="#_2-52" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ y − X β ‖^22 + λ\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>wj | βj |\n</code></pre></div><h6 id="-26"><a class="header-anchor" href="#-26" aria-hidden="true">#</a> </h6><h6 id="-25"><a class="header-anchor" href="#-25" aria-hidden="true">#</a> </h6><h6 id="-25"><a class="header-anchor" href="#-25" aria-hidden="true">#</a> </h6><h6 id="_4-76"><a class="header-anchor" href="#_4-76" aria-hidden="true">#</a> , (4.76)</h6><p>where <em>wj</em> = 1 <em>/</em> | <em>β</em> ̃ <em>j</em> | <em>ν</em>. The adaptive lasso penalty can be seen as an approxima- tion to the <em>`q</em> penalties with <em>q</em> = 1− <em>ν</em>. One advantage of the adaptive lasso is that given the pilot estimates, the criterion (4.76) is convex in <em>β</em>. Further- more, if the pilot estimates are</p><h6 id="√-12"><a class="header-anchor" href="#√-12" aria-hidden="true">#</a> √</h6><p><em>N</em> consistent, Zou (2006) showed that the method recovers the true model under more general conditions than does the lasso. If <em>p &lt; N</em> one can use the least-squares solutions as the pilot estimates. When <em>p</em> ≥ <em>N</em> , the least-squares estimates are not defined, but the univariate regression coefficients can be used for the pilot estimates and result in good recovery properties under certain conditions (Huang, Ma and Zhang 2008). Exercise 4.26 explores the close connections between the adaptive lasso and the nonnegative garrote of Section 2.8. We end this section by mentioning a practical alternative to nonconvex op- timization for sparse model-path building. Forward-stepwise methods (Hastie et al. 2009, Chapter 3) are very efficient, and are hard to beat in terms of find- ing good, sparse subsets of variables. Forward stepwise is a greedy algorithm— at each step fixing the identity of the terms already in the model, and finding the best variable to include among those remaining. The theoretical properties of forward-stepwise model paths are less well understood, partly because of the algorithmic definition of the procedure, as opposed to being a solution to an optimization problem.</p><h3 id="bibliographic-notes-1"><a class="header-anchor" href="#bibliographic-notes-1" aria-hidden="true">#</a> Bibliographic Notes</h3><p>The elastic net was proposed by Zou and Hastie (2005), who also distinguished between the naive version, similar to the one presented here, and a debiased</p><h6 id="bibliographic-notes-87"><a class="header-anchor" href="#bibliographic-notes-87" aria-hidden="true">#</a> BIBLIOGRAPHIC NOTES 87</h6><p>version that attempts to undo the biasing effect of the ridge shrinkage. Fried- man et al. (2015) build a system of coordinate-descent algorithms for fitting elastic-net penalized generalized linear models, implemented in theRpackage glmnet. Yuan and Lin (2006) introduced the group lasso, and their paper has stimulated much research. Meier et al. (2008) extended the group lasso to logis- tic regression problems, whereas Zhao, Rocha and Yu (2009) describe a more general family of structured penalties, including the group lasso as a special case. A line of theoretical work has sought to understand when the group lasso estimator has lower statistical error than the ordinary lasso. Huang and Zhang (2010) and Lounici, Pontil, Tsybakov and van de Geer (2009) establish error bounds for the group lasso, which show how it outperforms the ordinary lasso in certain settings. Negahban, Ravikumar, Wainwright and Yu (2012) provide a general framework for analysis of <em>M</em> -estimators, including the group lasso as a particular case as well as more general structured penalties. Obozinski, Wainwright and Jordan (2011) characterize multivariate regression problems for which the group lasso does (or does not) yield better variable selection performance than the ordinary lasso. The overlap group lasso was introduced by Jacob et al. (2009), and the sparse group lasso by Puig, Wiesel and Hero (2009) and Simon, Friedman, Hastie and Tibshirani (2013). Various algorithms have been developed for solving the group and overlap group lassos, as well as a variety of structured generalizations; see Bach, Jenatton, Mairal and Obozinski (2012) for a good review. Additive models were proposed by Stone (1985) as a means of side-stepping the curse of dimensionality in nonparametric regression; see Hastie and Tib- shirani (1990) for further background on (generalized) additive models. The COSSO model was developed by Lin and Zhang (2003) in the context of reproducing kernel Hilbert spaces, and ANOVA spline decompositions. The books by Wahba (1990) and Gu (2002) provide further background on splines and RKHSs. Ravikumar et al. (2009) followed up with the SPAM model, which is somewhat simpler and more general, and established certain forms of high-dimensional consistency for their estimator. Meier, van de Geer and B ̈uhlmann (2009) studied a related family of estimators, based on explicit penalization with the empirical <em>L</em>^2 -norm, corresponding to the doubly penal- ized estimator with <em>λ</em> H= 0. Koltchinski and Yuan (2008, 2010) analyzed the COSSO estimator, as well as the doubly penalized estimator (4.52). Raskutti et al. (2009, 2012) derived minimax bounds for sparse additive models, and also show that the doubly penalized estimator (4.52) can achieve these bounds for various RKHS families, including splines as a special case. The fused lasso was introduced by Tibshirani, Saunders, Rosset, Zhu and Knight (2005). Various algorithms have been proposed for versions of the fused lasso, including the methods of Hoefling (2010), Johnson (2013), and Tibshirani 2 and Taylor (2011). The MC+ threshold function was first described in Gao and Bruce (1997) in the context of wavelet shrinkage. There has been a lot of activity in non-</p><h6 id="_88-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_88-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 88 GENERALIZATIONS OF THE LASSO PENALTY</h6><p>convex penalties for sparse modeling. Zou and Li (2008) develop local linear approximation algorithms for tackling the nonconvex optimization problems. These and other approaches are discussed in Mazumder et al. (2011).</p><h3 id="exercises-2"><a class="header-anchor" href="#exercises-2" aria-hidden="true">#</a> Exercises</h3><p>Ex. 4.1 Suppose we have two identical variables <em>X</em> 1 = <em>X</em> 2 , and a response <em>Y</em> , and we perform a ridge regression (see (2.7) in Section 2.2) with penalty <em>λ &gt;</em> 0. Characterize the coefficient estimates <em>β</em> ̂ <em>j</em> ( <em>λ</em> ).</p><p>Ex. 4.2 Consider a slightly noisy version of the identical twins example in the beginning of Section 4.2, where the two variables are strongly positively correlated. Draw a schematic of the contours of the loss function and the penalty function, and demonstrate why the elastic net encourages coefficient sharing more than does the lasso.</p><p>Ex. 4.3 Consider the elastic-net problem (4.2).</p><div class="language-"><pre><code>(a) Show how to simplify the calculation of β ̂ 0 by centering each of the pre-\ndictors, leading to β ̂ 0 = ̄ y (for all values of λ ). How does one convert back\nto the estimate of β ̂ 0 for uncentered predictors?\n(b) Verify the soft-thesholding expression (4.4) for the update of β ̂ j by coor-\ndinate descent.\n</code></pre></div><p>Ex. 4.4 Consider the solution to the group lasso problem (4.5) when some of the variables are factors. Show that when there is an intercept in the model, the optimal coefficients for each factor sum to zero.</p><p>Ex. 4.5 This exercise investigates the penalty modifier for the group lasso. Consider the <em>entry</em> criterion‖ <strong>Z</strong> <em>Tj</em> <strong><em>r</em></strong> <em>j</em> ‖ 2 <em>&lt; λ</em> for the group lasso (Section 4.3.1). Suppose <strong><em>r</em></strong> <em>j</em> is i.i.d noise with mean <strong>0</strong> and covariance <em>σ</em>^2 <strong>I</strong> —a null situation. Show that E‖ <strong>Z</strong> <em>Tj</em> <strong><em>r</em></strong> <em>j</em> ‖^22 = <em>σ</em>^2 ‖ <strong>Z</strong> ‖^2 <em>F.</em> (4.77)</p><p>Hence argue that to make comparisons fair among the penalty terms in the group lasso, one should replace <em>λ</em></p><h6 id="∑-j-39"><a class="header-anchor" href="#∑-j-39" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1‖ θj ‖^2 in Equation (4.5) with\n</code></pre></div><div class="language-"><pre><code>λ\n</code></pre></div><h6 id="∑-j-40"><a class="header-anchor" href="#∑-j-40" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>τj ‖ θj ‖ 2 , (4.78)\n</code></pre></div><p>where <em>τj</em> =‖ <strong>Z</strong> <em>j</em> ‖ <em>F</em>. Show that when <strong>Z</strong> <em>j</em> is orthonormal, this results in <em>τj</em> = √ <em>pj</em>.</p><p>Ex. 4.6 Show that under the orthonormality condition <strong>Z</strong> <em>Tj</em> <strong>Z</strong> <em>j</em> = <strong>I</strong> , the up- date (4.15) solves the fixed point Equation (4.13).</p><h6 id="exercises-89"><a class="header-anchor" href="#exercises-89" aria-hidden="true">#</a> EXERCISES 89</h6><p>Ex. 4.7 Consider the block-wise solution vector (4.14) for the group lasso. If ‖ <em>θ</em> ˆ <em>j</em> ‖is known, we can write the solution in closed form. Let <strong>Z</strong> <em>j</em> = <strong>UDV</strong> <em>T</em> be the singular value decomposition of <strong>Z</strong> <em>j</em>. Let <em>r</em> ∗= <strong>U</strong> <em>T</em> <strong><em>r</em></strong> <em>j</em> ∈R <em>pj</em>. Show that <em>φ</em> =‖ <em>θ</em> ˆ <em>j</em> ‖solves ∑ <em>pj</em></p><div class="language-"><pre><code>` =1\n</code></pre></div><div class="language-"><pre><code>r ∗ `^2 d^2 `\n( d^2 `φ + λ )^2\n</code></pre></div><h6 id="_1-4-79"><a class="header-anchor" href="#_1-4-79" aria-hidden="true">#</a> = 1 , (4.79)</h6><p>where <em>d<code>_ is the _</code>th</em> diagonal element of <strong>D</strong>. Show how to use a golden search strategy to solve for <em>φ</em>. Write an R function to implement this algorithm, along with the golden search.</p><p>Ex. 4.8 Discuss the impact of the normalization <strong>Z</strong> <em>Tj</em> <strong>Z</strong> <em>j</em> = <strong>I</strong> in the context of a matrix of dummy variables representing a factor with <em>pj</em> levels. Does the use of contrasts rather than dummy variables alleviate the situation?</p><p>Ex. 4.9 Using the approach outlined in Section 5.3.3, derive the generalized gradient update (4.16a) for the group lasso. Write a R function to implement this algorithm (for a single group). Include an option to implement Nesterov acceleration.</p><p>Ex. 4.10 Using the approach outlined in Section 5.3.3, derive the generalized gradient update (4.23) for the sparse group lasso.</p><p>Ex. 4.11 Run a series of examples of increasing dimension to compare the performance of your algorithms in Exercises 4.7 and 4.9. Make sure they are producing the same solutions. Compare their computational speed—for in- stance, the commandsystem.time()can be used in R.</p><p>Ex. 4.12 Consider the condition (4.19) for <em>θ</em> ˆ <em>j</em> to be zero for the sparse group lasso. Define</p><div class="language-"><pre><code>J ( t ) =\n</code></pre></div><h6 id="_1-63"><a class="header-anchor" href="#_1-63" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>λ (1− α )\n</code></pre></div><div class="language-"><pre><code>‖ Z Tj r j − λα · t ‖ 2 (4.80)\n</code></pre></div><div class="language-"><pre><code>= ‖ s ‖ 2.\n</code></pre></div><p>Now solve min <em>t</em> : <em>tk</em> ∈[− 1 <em>,</em> 1]</p><div class="language-"><pre><code>J ( t ) , (4.81)\n</code></pre></div><p>and show that this leads to the condition <em>θ</em> ˆ <em>j</em> = 0 if and only if‖ˆ <em>gj</em> ‖ 2 ≤ <em>λ</em> (1− <em>α</em> ) with ˆ <em>gj</em> =S <em>λα</em> ( <strong>Z</strong> <em>Tj</em> <strong><em>r</em></strong> <em>j</em> ).</p><p>Ex. 4.13 Show that if <strong>Z</strong> <em>Tj</em> <strong>Z</strong> <em>j</em> = <strong>I</strong> , then (4.21) solves (4.12).</p><h6 id="_90-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_90-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 90 GENERALIZATIONS OF THE LASSO PENALTY</h6><p>Ex. 4.14 Consider the hierarchical interaction formulation in Example 4.3, and the optimization problem (4.29)–(4.31).</p><div class="language-"><pre><code>(a) Give an argument why the multipliers p 1 and p 2 make sense in the third\npenalty.\n(b) Suppose we augment the third matrix in (4.29) with a vector of ones\n[ 1 Z 1 Z 2 Z 1:2], and augment the parameter vector with ̃ μ. We now replace\nthe third group penalty term with\n√\np 1 p 2 μ ̃^2 + p 2 ‖ α ̃ 1 ‖^22 + p 1 ‖ α ̃ 2 ‖^22 +‖ α 1:2‖^22.\n</code></pre></div><div class="language-"><pre><code>Show that for any λ &gt; 0,̂ μ ̃= 0.\n(c) Show that the solution to (4.29)–(4.31) is equivalent to the solution to\n(4.32) for any λ &gt; 0. Show how to map the solution to the latter to the\nsolution to the former.\n</code></pre></div><p>Ex. 4.15 Consider a criterion for sparse additive models:</p><div class="language-"><pre><code>minimize\nβ ∈R J, { fj ∈F j } J 1\n</code></pre></div><h6 id="e-y-−"><a class="header-anchor" href="#e-y-−" aria-hidden="true">#</a> E( Y −</h6><h6 id="∑-j-41"><a class="header-anchor" href="#∑-j-41" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>βjfj ( Xj ))^2\n</code></pre></div><div class="language-"><pre><code>subject to ‖ fj ‖ 2 = 1∀ j (4.82)\n∑ J\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>| βj |≤ t.\n</code></pre></div><p>Although evocative, this criterion is not convex, but rather biconvex in <em>β</em> and { <em>fj</em> } <em>J</em> 1. Show that one can absorb the <em>βj</em> into <em>fj</em> and that solving (4.82) is equivalent to solving the convex (4.38):</p><div class="language-"><pre><code>minimize\nfj ∈F j, j =1 ,...,J\n</code></pre></div><h6 id="-31"><a class="header-anchor" href="#-31" aria-hidden="true">#</a> </h6><h6 id="-26"><a class="header-anchor" href="#-26" aria-hidden="true">#</a> </h6><h6 id="-26"><a class="header-anchor" href="#-26" aria-hidden="true">#</a> </h6><h6 id="e-3"><a class="header-anchor" href="#e-3" aria-hidden="true">#</a> E</h6><h6 id="-154"><a class="header-anchor" href="#-154" aria-hidden="true">#</a> [(</h6><h6 id="y-−-4"><a class="header-anchor" href="#y-−-4" aria-hidden="true">#</a> Y −</h6><h6 id="∑-j-42"><a class="header-anchor" href="#∑-j-42" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>fj ( Xj )\n</code></pre></div><h6 id="_2-53"><a class="header-anchor" href="#_2-53" aria-hidden="true">#</a> ) 2 ]</h6><div class="language-"><pre><code>+ λ\n</code></pre></div><h6 id="∑-j-43"><a class="header-anchor" href="#∑-j-43" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>‖ fj ‖ 2\n</code></pre></div><h6 id="-27"><a class="header-anchor" href="#-27" aria-hidden="true">#</a> </h6><h6 id="-26"><a class="header-anchor" href="#-26" aria-hidden="true">#</a> </h6><h6 id="-26"><a class="header-anchor" href="#-26" aria-hidden="true">#</a> </h6><h6 id="-155"><a class="header-anchor" href="#-155" aria-hidden="true">#</a> .</h6><p>(Ravikumar et al. 2009)</p><p>Ex. 4.16 The SPAM backfitting equations (4.40) are in terms of function updates, where <em>f</em> ˆ <em>j</em> is a fitted function returned by a smoothing operatorS <em>j</em> , and the <em>N</em> -vector form <strong>f</strong> <em>j</em> is <em>fj</em> evaluated at the <em>N</em> samples values of <em>Xj</em>. Suppose that the smoothing operatorS <em>j</em> fits a linear expansion of the form</p><div class="language-"><pre><code>fj (·) =\n</code></pre></div><div class="language-"><pre><code>∑ pj\n</code></pre></div><div class="language-"><pre><code>` =1\n</code></pre></div><div class="language-"><pre><code>βj`ψj` (·) , (4.83)\n</code></pre></div><p>where <em>θj</em> =</p><h6 id="-156"><a class="header-anchor" href="#-156" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>βj 1 βj 2 ··· βjpj\n</code></pre></div><h6 id="-157"><a class="header-anchor" href="#-157" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>is the coefficient vector.\n</code></pre></div><h6 id="exercises-91"><a class="header-anchor" href="#exercises-91" aria-hidden="true">#</a> EXERCISES 91</h6><div class="language-"><pre><code>(a) Suppose that the basis matrices are orthonormal: Ψ Tj Ψ j = I pj. Show that\nthe SPAM backfitting equations are equivalent to the ordinary group-lasso\nestimating equations in terms of the parameters θj.\n(b) What changes if Ψ j is not orthonormal?\n</code></pre></div><p>Ex. 4.17 In this exercise, we show that any optimal solution to the COSSO problem (4.44) is a member ofH 0 , the linear span of the kernel functions {R(· <em>, xi</em> ) <em>,i</em> = 1 <em>,...,N</em> }. We use the fact that any function <em>f</em> ∈ Hhas a de- composition of the form <em>g</em> + <em>h</em> , where <em>g</em> ∈ H 0 and <em>h</em> is orthogonal toH 0 , meaning that〈 <em>h, f</em> 0 〉Hfor all <em>f</em> 0 ∈H 0.</p><div class="language-"><pre><code>(a) For a function of the form f = g + h as above, show that the term\n1\nN\n</code></pre></div><h6 id="∑-n-66"><a class="header-anchor" href="#∑-n-66" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1( y − f ( xi ))\n</code></pre></div><p>(^2) depends only on <em>g</em>. ( <em>Hint:</em> The kernel reproducing prop- erty could be useful here.) (b) Show that the penalty term is only increased by including a component <em>h</em> 6 = 0. Conclude that any optimal solution <em>f</em> ̂must belong toH 0. Ex. 4.18 Verify that the solutions for <em>fj</em> in (4.47) with <em>λ</em> = <em>τ</em>^4 <em>/</em> 4 coincide with the solutions in (4.44). Ex. 4.19 Consider the additive model criterion (4.42), and assume associated with each function <em>fj</em> is a reproducing kernelR <em>j</em> , leading to a data criterion minimize <strong><em>θ</em></strong> <em>j</em> ∈R <em>N j</em> =1 <em>,...J</em></p><h6 id="-32"><a class="header-anchor" href="#-32" aria-hidden="true">#</a> </h6><h6 id="-27"><a class="header-anchor" href="#-27" aria-hidden="true">#</a> </h6><h6 id="-27"><a class="header-anchor" href="#-27" aria-hidden="true">#</a> </h6><h6 id="∥-1"><a class="header-anchor" href="#∥-1" aria-hidden="true">#</a> ∥</h6><div class="language-"><pre><code>∥ y −\n</code></pre></div><h6 id="∑-j-44"><a class="header-anchor" href="#∑-j-44" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>R j θ j\n</code></pre></div><h6 id="∥-2"><a class="header-anchor" href="#∥-2" aria-hidden="true">#</a> ∥</h6><div class="language-"><pre><code>∥^2 + λ\n</code></pre></div><h6 id="∑-j-45"><a class="header-anchor" href="#∑-j-45" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\n</code></pre></div><h6 id="_1-64"><a class="header-anchor" href="#_1-64" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>γj\n</code></pre></div><div class="language-"><pre><code>θ Tj R j θ j\n</code></pre></div><h6 id="-28"><a class="header-anchor" href="#-28" aria-hidden="true">#</a> </h6><h6 id="-27"><a class="header-anchor" href="#-27" aria-hidden="true">#</a> </h6><h6 id="-27"><a class="header-anchor" href="#-27" aria-hidden="true">#</a> </h6><h6 id="_4-84"><a class="header-anchor" href="#_4-84" aria-hidden="true">#</a> (4.84)</h6><p>(The 1 <em>/N</em> has been absorbed into <em>λ</em> ).</p><div class="language-"><pre><code>(a) Define R ̃ j = γj R j and ̃ θ j = θ j/γj. In this new parametrization, show\nthat the estimating equations for θ ̃ j are\n</code></pre></div><div class="language-"><pre><code>− R ̃ j ( y − f +) + λ R ̃ j θ ̃ j = 0 , j = 1 ,...,J, (4.85)\n</code></pre></div><div class="language-"><pre><code>where f +=\n</code></pre></div><h6 id="∑-j-46"><a class="header-anchor" href="#∑-j-46" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1 f j , and f j =\nR ̃ j θ ̃ j.\n(b) Show that these can be rewritten as\ñ θ j = ( R ̃ j + λ I )−^1 r j, and (4.86a)\ñ f j = R ̃ j ( R ̃ j + λ I )−^1 r j, (4.86b)\n</code></pre></div><div class="language-"><pre><code>where r j = y − f ++ f j.\n(c) Define R ̃+=\n</code></pre></div><h6 id="∑-j-47"><a class="header-anchor" href="#∑-j-47" aria-hidden="true">#</a> ∑ J</h6><div class="language-"><pre><code>j =1\nR ̃ j =∑ J\nj =1 γj R j. Show that\n</code></pre></div><div class="language-"><pre><code>f += R ̃+( R ̃++ λ I )−^1 y = R ̃+ c (4.87a)\nc = ( R ̃++ λ I )−^1 y. (4.87b)\n</code></pre></div><div class="language-"><pre><code>Compare with the previous item.\n</code></pre></div><h6 id="_92-generalizations-of-the-lasso-penalty"><a class="header-anchor" href="#_92-generalizations-of-the-lasso-penalty" aria-hidden="true">#</a> 92 GENERALIZATIONS OF THE LASSO PENALTY</h6><div class="language-"><pre><code>(d) Show that ̃ θ j = c ∀ j. So even though there are J N -dimensional param-\neters ̃ θ j in this representation, their estimates are all the same.\n</code></pre></div><p>This shows that given <em>γj</em> , <strong>f</strong> <em>j</em> = <em>γj</em> <strong>R</strong> <em>j</em> <strong>c</strong> = <em>γj</em> <strong>g</strong> <em>j</em> , and justifies the second step (4.46) in the alternating algorithm for fitting the COSSO model (see Section 4.4).</p><p>Ex. 4.20 Show that any optimal solution to the doubly regularized estima- tor (4.52) takes the form <em>f</em> ̂ <em>j</em> (·) =</p><h6 id="∑-n-67"><a class="header-anchor" href="#∑-n-67" aria-hidden="true">#</a> ∑ N</h6><p><em>i</em> =1 <em>θ</em> ̂ <em>ij</em> R(· <em>,xij</em> ), where the optimal weights ( <em>θ</em> ̂ <em>j,j</em> = 1 <em>,...,J</em> ) are obtained by solving the convex program (4.53).</p><p>Ex. 4.21 Consider the fused lasso problem (4.56). Characterize <em>β</em> ̂ 0. Show that if we center the predictors and the response by subtracting their sample means, we can omit the term <em>β</em> 0 and the estimates <em>β</em> ̂ <em>j</em> are unaffected. Now consider a version of the fused-lasso signal approximator (4.54) with a constant term <em>θ</em> 0 included:</p><div class="language-"><pre><code>minimize\nθ 0 , θ\n</code></pre></div><h6 id="∑-n-68"><a class="header-anchor" href="#∑-n-68" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi − θ 0 − θi )^2 + λ 1\n</code></pre></div><h6 id="∑-n-69"><a class="header-anchor" href="#∑-n-69" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>| θi |+ λ 2\n</code></pre></div><h6 id="∑-n-70"><a class="header-anchor" href="#∑-n-70" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =2\n</code></pre></div><div class="language-"><pre><code>| θi − θi − 1 |. (4.88)\n</code></pre></div><p>Characterize <em>θ</em> ˆ 0 , and show that median(ˆ <em>θi</em> ) = 0.</p><p>Ex. 4.22 Consider the matrix <strong>M</strong> corresponding to the linear transforma- tion (4.60).</p><div class="language-"><pre><code>(a) Show that its inverse M −^1 is lower triangular with all ones on and below\nthe diagonal.\n(b) Explore the pairwise correlations between the columns of such a matrix\nfor the CGH data of Figure 4.8.\n(c) Usingglmnetwith maxdf=200, andtype=&quot;naive&quot;, fit model (4.61), and\nshow that the fitted values correspond to the parameters of interest. Com-\npare the performance oflarsfor the same task. Using a soft-thresholding\npost-processor, try to match Figure 4.8.\n</code></pre></div><p>Ex. 4.23 Derive the dual optimization problem (4.64) in Section 4.5.1.3. Sup- pose the <em>kth</em> element ofˆ <strong>u</strong> ( <em>λ</em> ) has reached the bound at <em>λ</em> = <em>λk</em> , and let the set <em>B</em> hold their indices, and <strong>s</strong> a vector of their signs. Show that the solution to (4.64) at <em>λk</em> also solves</p><div class="language-"><pre><code>minimize\nu − B\n</code></pre></div><h6 id="_1-65"><a class="header-anchor" href="#_1-65" aria-hidden="true">#</a> 1</h6><h6 id="_2-54"><a class="header-anchor" href="#_2-54" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ y − λk D TB s − D T − B u − B ‖^2 , (4.89)\n</code></pre></div><p>with solutionˆ <strong>u</strong> <em>B</em> ( <em>λ</em> ) = <em>λ</em> <strong>s</strong> andˆ <strong>u</strong> − <em>B</em> ( <em>λ</em> ) = ( <strong>D</strong> − <em>B</em> <strong>D</strong> <em>T</em> − <em>B</em> )−^1 <strong>D</strong> − <em>B</em> ( <strong>y</strong> − <em>λ</em> <strong>D</strong> <em>TB</em> <strong>s</strong> ) and <em>λ</em> = <em>λk</em>. By definition each of the elements ofˆ <strong>u</strong> − <em>B</em> ( <em>λ</em> ) has absolute value less than <em>λk</em>. Show that the solution is piecewise-linear in <em>λ &lt; λk</em> , and remains the solution until the next element of <strong>u</strong> ˆ− <em>B</em> ( <em>λ</em> ) hits the boundary. Show that one can determine exactly for which element and value of <em>λ</em> this will be.</p><h6 id="exercises-93"><a class="header-anchor" href="#exercises-93" aria-hidden="true">#</a> EXERCISES 93</h6><p>Ex. 4.24 Here we use dynamic programming to fit the fused lasso.</p><div class="language-"><pre><code>(a) Implement the dynamic programming approach to the fused lasso, in the\nsimple case where each βi can take one of K distinct values.\n(b) Do the same as in (a), replacing the ` 1 difference penalty with an ` 0\ndifference penalty. Compare the two procedures on the CGH data.\n</code></pre></div><p>Ex. 4.25 Derive the threshold function (4.75) for the uni-dimensional MC+ criterion (4.74) in Section 4.6.</p><p>Ex. 4.26 Show that with <em>ν</em> = 1 in (4.76), the adaptive-lasso solutions are similar to those of the nonnegative garrote (2.19). In particular, if we constrain the adaptive lasso solutions to have the same sign as the pilot estimates, then they are the same as the solutions to the garrote with a suitably chosen regularization parameter.</p><div class="language-"><pre><code>Chapter 5\n</code></pre></div><h2 id="optimization-methods"><a class="header-anchor" href="#optimization-methods" aria-hidden="true">#</a> Optimization Methods</h2><h3 id="_5-1-introduction"><a class="header-anchor" href="#_5-1-introduction" aria-hidden="true">#</a> 5.1 Introduction</h3><p>In this chapter, we present an overview of some basic optimization concepts and algorithms for convex problems, with an emphasis on aspects of particular relevance to regularized estimators such as the lasso. At the algorithmic level, we focus primarily on <em>first-order methods</em> , since they are especially useful for large-scale optimization problems. We begin with an overview of some basic optimality theory for convex programs, and then move on to consider various types of iterative algorithms. Although we limit our focus mainly to convex problems, we do touch upon algorithms for biconvex problems later in the chapter.</p><h3 id="_5-2-convex-optimality-conditions"><a class="header-anchor" href="#_5-2-convex-optimality-conditions" aria-hidden="true">#</a> 5.2 Convex Optimality Conditions</h3><p>An important class of optimization problems involves convex cost functions and convex constraints. A setC ⊆R <em>p</em> is <em>convex</em> if for all <em>β,β</em> ′∈ Cand all scalars <em>s</em> ∈[0 <em>,</em> 1], all vectors of the form <em>β</em> ( <em>s</em> ) = <em>sβ</em> + (1− <em>s</em> ) <em>β</em> ′also belong to C. A function <em>f</em> :R <em>p</em> →Ris convex means that for any two vectors <em>β,β</em> ′in the domain of <em>f</em> and any scalar <em>s</em> ∈(0 <em>,</em> 1), we have</p><div class="language-"><pre><code>f ( β ( s )) = f ( sβ + (1− s ) β ′)≤ sf ( β ) + (1− s ) f ( β ′). (5.1)\n</code></pre></div><p>In geometric terms, this inequality implies that the chord joining the <em>f</em> ( <em>β</em> ) and <em>f</em> ( <em>β</em> ′) lies above the graph of <em>f</em> , as illustrated in Figure 5.1(a). This inequality guarantees that a convex function cannot have any local minima that are not also globally minimal, as illustrated Figure 5.1(b).</p><h4 id="_5-2-1-optimality-for-differentiable-problems"><a class="header-anchor" href="#_5-2-1-optimality-for-differentiable-problems" aria-hidden="true">#</a> 5.2.1 Optimality for Differentiable Problems</h4><p>Consider the constrained optimization problem</p><div class="language-"><pre><code>minimize\nβ ∈R p\nf ( β ) such that β ∈C , (5.2)\n</code></pre></div><p>where <em>f</em> :R <em>p</em> →Ris a convex objective function to be minimized, andC ⊂R <em>p</em> is a convex constraint set. When the cost function <em>f</em> is differentiable, then a</p><div class="language-"><pre><code>95\n</code></pre></div><h6 id="_96-optimization-methods"><a class="header-anchor" href="#_96-optimization-methods" aria-hidden="true">#</a> 96 OPTIMIZATION METHODS</h6><div class="language-"><pre><code>f ( β )\n</code></pre></div><div class="language-"><pre><code>f ( β ′)\n</code></pre></div><div class="language-"><pre><code>f ( sβ + (1− s ) β ′)\n</code></pre></div><div class="language-"><pre><code>sf ( β ) + (1− s ) f ( β ′)\n</code></pre></div><div class="language-"><pre><code>βββ ( s ) ′ ββ\nβ ( s ) ′\n</code></pre></div><div class="language-"><pre><code>(a) (b)\n</code></pre></div><p><strong>Figure 5.1</strong> <em>(a) For a convex function, the linesf</em> ( <em>β</em> ) + (1− <em>s</em> ) <em>f</em> ( <em>β</em> ′) <em>always lies above the function valuef</em> ( <em>sβ</em> + (1− <em>s</em> ) <em>β</em> ′)<em>. (b) A nonconvex function that violates the inequality</em> (5.1)<em>. Without convexity, there may be local minima that are not global ly minima, as shown by the pointβ</em> ′<em>.</em></p><p>necessary and sufficient condition for a vector <em>β</em> ∗∈Cto be a global optimum is that</p><div class="language-"><pre><code>〈∇ f ( β ∗) , β − β ∗〉≥ 0 (5.3)\n</code></pre></div><p>for all <em>β</em> ∈ C. The sufficiency of this condition is easy to see; for any <em>β</em> ∈ C, we have</p><div class="language-"><pre><code>f ( β )\n</code></pre></div><div class="language-"><pre><code>( i )\n≥ f ( β ∗) +〈∇ f ( β ∗) , β − β ∗〉\n</code></pre></div><div class="language-"><pre><code>( ii )\n≥ f ( β ∗) , (5.4)\n</code></pre></div><p>where inequality (i) follows from the convexity of <em>f</em> ,^1 and inequality (ii) follows from the optimality condition (5.3). As a special case, whenC=R <em>p</em> so that the problem (5.2) is actually unconstrained, then the first-order condition (5.3) reduces to the classical zero-gradient condition∇ <em>f</em> ( <em>β</em> ∗) = 0. Frequently, it is the case that the constraint setC can be described in terms of the sublevel sets of some convex constraint functions. For any convex function <em>g</em> :R <em>p</em> →R, it follows from the definition (5.1) that the sublevel set { <em>β</em> ∈R <em>p</em> | <em>g</em> ( <em>β</em> )≤ 0 }is a convex set. On this basis, the convex optimization problem</p><div class="language-"><pre><code>minimize\nβ ∈R p\n</code></pre></div><div class="language-"><pre><code>f ( β ) such that gj ( β )≤0 for j = 1 ,...,m , (5.5)\n</code></pre></div><p>(^1) Inequality (i) is an equivalent definition of convexity for a differentiable function <em>f</em> ; the first-order Taylor approximation centered at any point <em>β</em> ̃∈ Cgives a tangent lower bound to <em>f</em>.</p><h6 id="convex-optimality-conditions-97"><a class="header-anchor" href="#convex-optimality-conditions-97" aria-hidden="true">#</a> CONVEX OPTIMALITY CONDITIONS 97</h6><p>where <em>gj,j</em> = 1 <em>,...,m</em> are convex functions that express constraints to be satisfied, is an instance of the general program (5.2). We let <em>f</em> ∗denote the optimal value of the optimization problem (5.5). An important function associated with the problem (5.5) is the Lagrangian <em>L</em> :R <em>p</em> ×R <em>m</em> +→R, defined by</p><div class="language-"><pre><code>L ( β ; λ ) = f ( β ) +\n</code></pre></div><div class="language-"><pre><code>∑ m\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>λjgj ( β ). (5.6)\n</code></pre></div><p>The nonnegative weights <em>λ</em> ≥0 are known as <em>Lagrange multipliers</em> ; the pur- pose of the multiplier <em>λj</em> is to impose a penalty whenever the constraint <em>gj</em> ( <em>β</em> )≤0 is violated. Indeed, if we allow the multipliers to be chosen op- timally, then we recover the original program (5.5), since</p><div class="language-"><pre><code>sup\nλ ≥ 0\n</code></pre></div><div class="language-"><pre><code>L ( β ; λ ) =\n</code></pre></div><h6 id="-158"><a class="header-anchor" href="#-158" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>f ( β ) if gj ( β )≤0 for all j = 1 ,...,m , and\n+∞ otherwise,\n</code></pre></div><h6 id="_5-7"><a class="header-anchor" href="#_5-7" aria-hidden="true">#</a> (5.7)</h6><p>and thus <em>f</em> ∗= inf <em>β</em> ∈R <em>p</em> sup <em>λ</em> ≥ 0 <em>L</em> ( <em>β</em> ; <em>λ</em> ). See Exercise 5.2 for further details on this equivalence. For convex programs, the Lagrangian allows for the constrained prob- lem (5.5) to be solved by reduction to an equivalent unconstrained problem. More specifically, under some technical conditions on <em>f</em> and{ <em>gj</em> }, the theory of Lagrange duality guarantees that there exists an optimal vector <em>λ</em> ∗≥ 0 of Lagrange multipliers such that <em>f</em> ∗= min <em>β</em> ∈R <em>pL</em> ( <em>β</em> ; <em>λ</em> ∗). As a result, any optimum <em>β</em> ∗of the problem (5.5), in addition to satisfying the feasibility con- straints <em>gj</em> ( <em>β</em> ∗)≤0, must also be a zero-gradient point of the Lagrangian, and hence satisfy the equation</p><div class="language-"><pre><code>0 =∇ βL ( β ∗; λ ∗) = ∇ f ( β ∗) +\n</code></pre></div><div class="language-"><pre><code>∑ m\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>λ ∗ j ∇ gj ( β ∗). (5.8)\n</code></pre></div><p>When there is only a single constraint function <em>g</em> , this condition reduces to ∇ <em>f</em> ( <em>β</em> ∗) =− <em>λ</em> ∗∇ <em>g</em> ( <em>β</em> ∗), and has an intuitive geometric interpretation, as shown in Figure 5.2. In particular, at the optimal solution <em>β</em> ∗, the normal vector ∇ <em>f</em> ( <em>β</em> ∗) to the contour line of <em>f</em> points in the opposite direction to the normal vector to the constraint curve <em>g</em> ( <em>β</em> ) = 0. Equivalently, the normal vector to the contour <em>f</em> lies at right angles to the tangent vector of the constraint. Consequently, if we start at the optimum <em>β</em> ∗and travel along the tangent at <em>g</em> ( <em>β</em> ) = 0, we cannot decrease the value of <em>f</em> ( <em>β</em> ) up to first order. In general, the <em>Karush–Kuhn–Tucker</em> conditions relate the optimal La- grange multiplier vector <em>λ</em> ∗≥0, also known as the dual vector, to the optimal primal vector <em>β</em> ∗∈R <em>p</em> :</p><div class="language-"><pre><code>(a) Primal feasibility:gj ( β ∗)≤0 for all j = 1 ,...,m.\n(b) Complementary slackness:λ ∗ jgj ( β ∗) = 0 for all j = 1 ,...,m.\n</code></pre></div><h6 id="_98-optimization-methods"><a class="header-anchor" href="#_98-optimization-methods" aria-hidden="true">#</a> 98 OPTIMIZATION METHODS</h6><div class="language-"><pre><code>f ( β )= c 1\n</code></pre></div><div class="language-"><pre><code>f ( β )= c 2 &lt;c 1 g ( β )=0\n</code></pre></div><div class="language-"><pre><code>β ∗\n</code></pre></div><p><strong>Figure 5.2</strong> <em>Il lustration of the method of Lagrange multipliers. We are minimizing a functionfsubject to a single constraintg</em> ( <em>β</em> )≤ 0_. At an optimal solutionβ_ ∗ <em>, the normal vector</em> ∇ <em>f</em> ( <em>β</em> ∗) <em>to the level sets of the cost functionfpoints in the opposite direction to the normal vector</em> ∇ <em>g</em> ( <em>β</em> ∗) <em>of the constraint boundaryg</em> ( <em>β</em> ) = 0_. Conse- quently, up to first order, the value off_ ( <em>β</em> ∗) <em>cannot be decreased by moving along the contourg</em> ( <em>β</em> ) = 0_._</p><p>(c) <em>Lagrangian condition:</em> The pair ( <em>β</em> ∗ <em>,λ</em> ∗) satisfies condition (5.8). These KKT conditions are necessary and sufficient for <em>β</em> ∗to be a global optimum whenever the optimization problem satisfies a regularity condition known as <em>strong duality</em>. (See Exercise 5.4 for more details.) The comple- mentary slackness condition asserts that the multiplier <em>λ</em> ∗ <em>j</em> must be zero if the constraint <em>gj</em> ( <em>β</em> )≤0 is inactive at the optimum—that is, if <em>gj</em> ( <em>β</em> ∗) <em>&lt;</em> 0. Consequently, under complementary slackness, the Lagrangian gradient con- dition (5.8) guarantees that the normal vector−∇ <em>f</em> ( <em>β</em> ∗) lies in the positive linear span of the gradient vectors{∇ <em>gj</em> ( <em>β</em> ∗)| <em>λ</em> ∗ <em>j&gt;</em> 0 }.</p><h4 id="_5-2-2-nondifferentiable-functions-and-subgradients"><a class="header-anchor" href="#_5-2-2-nondifferentiable-functions-and-subgradients" aria-hidden="true">#</a> 5.2.2 Nondifferentiable Functions and Subgradients</h4><p>In practice, many optimization problems arising in statistics involve convex but nondifferentiable cost functions. For instance, the <em>`</em> 1 -norm <em>g</em> ( <em>β</em> ) =</p><p>∑ <em>p j</em> =1| <em>βj</em> |is a convex function, but it fails to be differentiable at any point where at least one coordinate <em>βj</em> is equal to zero. For such problems, the optimality conditions that we have developed—in particular, the first- order condition (5.3) and the Lagrangian condition (5.8)—are not directly applicable, since they involve gradients of the cost and constraint functions. Nonetheless, for convex functions, there is a natural generalization of the no- tion of gradient that allows for a more general optimality theory. A basic property of differentiable convex functions is that the first-order tangent approximation always provides a lower bound. The notion of subgra-</p><h6 id="convex-optimality-conditions-99"><a class="header-anchor" href="#convex-optimality-conditions-99" aria-hidden="true">#</a> CONVEX OPTIMALITY CONDITIONS 99</h6><div class="language-"><pre><code>β 1 β 2\n</code></pre></div><div class="language-"><pre><code>f(β)\n</code></pre></div><div class="language-"><pre><code>f(β 2 )+zc(β−β 2 )\nf(β 2 )+zb(β−β 2 )\n</code></pre></div><div class="language-"><pre><code>f(β 1 )+za(β−β 1 )\n</code></pre></div><p><strong>Figure 5.3</strong> <em>A convex functionf</em> :R→R <em>, along with some examples of subgradients atβ</em> 1 <em>andβ</em> 2_._</p><p>dient is based on a natural generalization of this idea. In particular, given a convex function <em>f</em> :R <em>p</em> →R, a vector <em>z</em> ∈R <em>p</em> is said to be a <em>subgradient</em> of <em>f</em> at <em>β</em> if</p><p><em>f</em> ( <em>β</em> ′)≥ <em>f</em> ( <em>β</em> ) +〈 <em>z, β</em> ′− <em>β</em> 〉 for all <em>β</em> ′∈R <em>p</em>. (5.9) In geometric terms, the subgradient vector <em>z</em> is the normal to a (nonverti- cal) hyperplane that supports the epigraph of <em>f</em>. The set of all subgradients of <em>f</em> at <em>β</em> is called the <em>subdifferential</em> , denoted by <em>∂f</em> ( <em>β</em> ). Whenever <em>f</em> is dif- ferentiable at <em>β</em> , then the sub-differential reduces to a single vector—namely <em>∂f</em> ( <em>β</em> ) ={∇ <em>f</em> ( <em>β</em> )}. At points of nondifferentiability, the subdifferential is a convex set containing all possible subgradients. For example, for the absolute value function <em>f</em> ( <em>β</em> ) =| <em>β</em> |, we have</p><div class="language-"><pre><code>∂f ( β ) =\n</code></pre></div><h6 id="-33"><a class="header-anchor" href="#-33" aria-hidden="true">#</a> </h6><h6 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> </h6><h6 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>{+1} if β &gt; 0\n{− 1 } if β &lt; 0\n[− 1 , +1] if β = 0.\n</code></pre></div><h6 id="_5-10"><a class="header-anchor" href="#_5-10" aria-hidden="true">#</a> (5.10)</h6><p>We frequently write <em>z</em> ∈sign( <em>β</em> ) to mean that <em>z</em> belongs to sub-differential of the absolute value function at <em>β</em>. Figure 5.3 shows a function <em>f</em> :R→R, and some examples of subgradients at the two points <em>β</em> 1 and <em>β</em> 2. At the point <em>β</em> 1 , the function is differentiable and hence there is only one subgradient—namely, <em>f</em> ′( <em>β</em> 1 ). At the point <em>β</em> 2 , it is not differentiable, and there are multiple subgradients; each one specifies a tangent plane that provides a lower bound on <em>f</em>. How is this useful? Recall the convex optimization problem (5.5), and assume that one or more of the functions{ <em>f,gj</em> }are convex but nondiffer- entiable. In this case, the zero-gradient Lagrangian condition (5.8) no longer makes sense. Nonetheless, again under mild conditions on the functions, the generalized KKT theory can still be applied using the modified condition</p><div class="language-"><pre><code>0 ∈ ∂f ( β ∗) +\n</code></pre></div><div class="language-"><pre><code>∑ m\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>λ ∗ j∂gj ( β ∗) , (5.11)\n</code></pre></div><h6 id="_100-optimization-methods"><a class="header-anchor" href="#_100-optimization-methods" aria-hidden="true">#</a> 100 OPTIMIZATION METHODS</h6><p>in which we replace the gradients in the KKT condition (5.8) with subdif- ferentials. Since the subdifferential is a set, Equation (5.11) means that the all-zeros vector belongs to the sum of the subdifferentials.^2</p><p><em>Example 5.1. Lasso and subgradients.</em> As an example, suppose that we want to solve a minimization problem of the form (5.5) with a convex and differentiable cost function <em>f</em> , and a single constraint specified by <em>g</em> ( <em>β</em> ) =</p><p>∑ <em>p j</em> =1| <em>βj</em> |− <em>R</em> for some positive constant <em>R</em>. Thus, the constraint <em>g</em> ( <em>β</em> )≤0 is equivalent to requiring that <em>β</em> belongs to an <em>`</em> 1 -ball of radius <em>R</em>. Recalling the form of the subdifferential (5.10) for the absolute value function, condition (5.11) becomes</p><div class="language-"><pre><code>∇ f ( β ∗) + λ ∗ z ∗= 0 , (5.12)\n</code></pre></div><p>where the subgradient vector satisfies <em>zj</em> ∗∈sign( <em>βj</em> ∗) for each <em>j</em> = 1 <em>,...,p</em>.</p><p>When the cost function <em>f</em> is the squared error <em>f</em> ( <em>β</em> ) = 21 <em>N</em> ‖ <strong>y</strong> − <strong>X</strong> <em>β</em> ‖^22 , this condition is equivalent to Equation (2.6) from Chapter 2. ♦</p><p><em>Example 5.2. Nuclear norm and subgradients.</em> The <em>nuclear norm</em> is a convex function on the space of matrices. Given a matrix <strong>Θ</strong> ∈R <em>m</em> × <em>n</em> (where we as- sume <em>m</em> ≤ <em>n</em> ), it can always be decomposed in the form <strong>Θ</strong> =</p><p>∑ <em>m j</em> =1 <em>σjujv T j</em>. where{ <em>uj</em> } <em>mj</em> =1and{ <em>vj</em> } <em>mj</em> =1are the (left and right) singular vectors, chosen to be orthonormal inR <em>m</em> andR <em>n</em> , respectively, and the nonnegative numbers <em>σj</em> ≥0 are the singular values. This is known as the <em>singular-value decompo- sition (SVD)</em> of <strong>Θ</strong>. The <em>nuclear norm</em> is the sum of the singular values—that is,‖ <strong>Θ</strong> ‖<em>?</em> =</p><p>∑ <em>m j</em> =1 <em>σj</em> ( <strong>Θ</strong> ). Note that it is a natural generalization of the vector <em><code>_ 1 -norm, since for any (square) diagonal matrix, the nuclear norm reduces to the _</code></em> 1 -norm of its diagonal entries. As we discuss in Chapter 7, the nuclear norm is useful for various types of matrix approximation and decomposition. The subdifferential <em>∂</em> ‖ <strong>Θ</strong> ‖<em>?</em> of the nuclear norm at <strong>Θ</strong> consists of all matri- ces of the form <strong>Z</strong> =</p><div class="language-"><pre><code>∑ m\nj =1 zjujv\n</code></pre></div><p><em>T j</em> , where each for <em>j</em> = 1 <em>,...,m</em> , the scalar <em>zj</em> ∈sign( <em>σj</em> ( <strong>Θ</strong> )). We leave it as an exercise for the reader to verify this claim using the definition (5.9). ♦</p><h3 id="_5-3-gradient-descent"><a class="header-anchor" href="#_5-3-gradient-descent" aria-hidden="true">#</a> 5.3 Gradient Descent</h3><p>Thus far, we have seen various types of optimality conditions for different types of convex programs. We now turn to various classes of iterative algorithms for solving optimization problems. In this section, we focus on first-order al- gorithms, meaning methods that exploit only gradient (or subgradient) in- formation, as opposed to information from higher-order gradients. First-order methods are particularly attractive for large-scale problems that arise in much of modern statistics.</p><p>(^2) Here we define the sum of two subsets <em>A</em> and <em>B</em> ofR <em>p</em> as <em>A</em> + <em>B</em> : ={ <em>α</em> + <em>β</em> | <em>α</em> ∈ <em>A,β</em> ∈ <em>B</em> }.</p><h6 id="gradient-descent-101"><a class="header-anchor" href="#gradient-descent-101" aria-hidden="true">#</a> GRADIENT DESCENT 101</h6><h4 id="_5-3-1-unconstrained-gradient-descent"><a class="header-anchor" href="#_5-3-1-unconstrained-gradient-descent" aria-hidden="true">#</a> 5.3.1 Unconstrained Gradient Descent</h4><p>We begin with the simplest case —namely, unconstrained minimization of a convex differentiable function <em>f</em> :R <em>p</em> →R. In this case, assuming that the global minimum is achieved, then a necessary and sufficient condition for optimality of <em>β</em> ∗∈R <em>p</em> is provided by the zero-gradient condition∇ <em>f</em> ( <em>β</em> ∗) = 0. Gradient descent is an iterative algorithm for solving this fixed point equation: it generates a sequence of iterates{ <em>βt</em> }∞ <em>t</em> =0via the update</p><div class="language-"><pre><code>βt +1= βt − st ∇ f ( βt ) , for t = 0 , 1 , 2 ,..., (5.13)\n</code></pre></div><p>where <em>st&gt;</em> 0 is a stepsize parameter. This update has a natural geomet- ric interpretation: by computing the gradient, we determine the direction of steepest descent−∇ <em>f</em> ( <em>βt</em> ), and then walk in this direction for a certain amount determined by the stepsize <em>st</em>. More generally, the class of <em>descent methods</em> is based on choosing a direc- tion ∆ <em>t</em> ∈R <em>p</em> such that〈∇ <em>f</em> ( <em>βt</em> ) <em>,</em> ∆ <em>t</em> 〉 <em>&lt;</em> 0, and then performing the update</p><div class="language-"><pre><code>βt +1= βt + st ∆ t for t = 0 , 1 , 2 ,.... (5.14)\n</code></pre></div><p>In geometric terms, the inner product condition〈∇ <em>f</em> ( <em>βt</em> ) <em>,</em> ∆ <em>t</em> 〉 <em>&lt;</em> 0 means that the chosen direction ∆ <em>t</em> forms an angle of less than 90◦with the di- rection of steepest descent. The gradient descent update (5.13) is a special case with ∆ <em>t</em> =−∇ <em>f</em> ( <em>βt</em> ). Other interesting choices include <em>diagonal ly-scaled gradient descent:</em> given a diagonal matrix <strong>D</strong> <em>t</em>  <strong>0</strong> , it uses the descent direc- tion ∆ <em>t</em> =−( <strong>D</strong> <em>t</em> )−^1 ∇ <em>f</em> ( <em>βt</em> ). This type of diagonal scaling is helpful when the function varies more rapidly along some coordinates than others. More gen- erally, <em>Newton’s method</em> is applicable to functions that are twice continuously differentiable, and is based on the descent direction</p><div class="language-"><pre><code>∆ t =−\n</code></pre></div><h6 id="-159"><a class="header-anchor" href="#-159" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>∇^2 f ( βt )\n</code></pre></div><h6 id="−-1-2"><a class="header-anchor" href="#−-1-2" aria-hidden="true">#</a> )− 1</h6><div class="language-"><pre><code>∇ f ( βt ) , (5.15)\n</code></pre></div><p>where∇^2 <em>f</em> ( <em>βt</em> ) is the Hessian of <em>f</em> , assumed to be invertible. Newton’s method is a second-order method, since it involves first and second derivatives. In particular, a Newton step (with stepsize one) amounts to exactly minimizing the second-order Taylor approximation to <em>f</em> at <em>βt</em>. Under some regularity conditions, it enjoys a quadratic rate of convergence; however, computation of the Newton direction (5.15) is more expensive than first-order methods. An important issue for all iterative algorithms, among them the gradient descent update (5.13), is how to choose the stepsize <em>st</em>. For certain problems with special structure, it can be shown that a constant stepsize (meaning <em>st</em> = <em>s</em> for all iterations <em>t</em> = 0 <em>,</em> 1 <em>,...</em> ) will guarantee convergence; see Exercise 5.1 for an illustration. In general, it is <em>not sufficient</em> to simply choose a stepsize for which <em>f</em> ( <em>βt</em> +1) <em>&lt; f</em> ( <em>βt</em> ); without some care, this choice may cause the algorithm to converge to a nonstationary point. Fortunately, there are various kinds of stepsize selection rules that are relatively simple, and have associated convergence guarantees:</p><h6 id="_102-optimization-methods"><a class="header-anchor" href="#_102-optimization-methods" aria-hidden="true">#</a> 102 OPTIMIZATION METHODS</h6><div class="language-"><pre><code>s = 0 s 1 s 0 = 1\n</code></pre></div><div class="language-"><pre><code>f ( β ) f ( β + s ∆)\n</code></pre></div><div class="language-"><pre><code>f ( β ) + αs f ( β ) + αs^1 h∇ f ( β ) , ∆i\n0 h∇ f ( β ) , ∆i\n</code></pre></div><p><strong>Figure 5.4</strong> <em>Armijo rule or backtracking line search. Starting with step- size s</em> 0 = 1 <em>, we repeatedly reduce s by a fraction γ until the condition f</em> ( <em>β</em> + <em>s</em> ∆)≤ <em>f</em> ( <em>β</em> ) + <em>αs</em> 〈∇ <em>f</em> ( <em>β</em> ) <em>,</em> ∆〉 <em>is satisfied. This is achieved here ats</em> 1_._</p><ul><li><em>Limited minimization rule:</em> choose the stepsize <em>st</em> = arg min <em>s</em> ∈[0 <em>,</em> 1]</li></ul><div class="language-"><pre><code>f ( βt + s ∆ t ).\n</code></pre></div><div class="language-"><pre><code>Although this choice is very intuitive, it does require solving a one-\ndimensional optimization problem at each step.\n</code></pre></div><ul><li><em>Armijo or backtracking rule:</em> Given parameters <em>α</em> ∈(0 <em>,</em> 1) and <em>γ</em> ∈(0 <em>,</em> 1) and an initial stepsize <em>s</em> = 1, perform the reduction <em>s</em> ← <em>γs</em> until the descent condition</li></ul><div class="language-"><pre><code>f\n</code></pre></div><h6 id="-160"><a class="header-anchor" href="#-160" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>βt + s ∆ t\n</code></pre></div><h6 id="-161"><a class="header-anchor" href="#-161" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>≤ f ( βt ) + αs 〈∇ f ( βt ) , ∆ t 〉 (5.16)\n</code></pre></div><div class="language-"><pre><code>is met. In practice, the choices α = 0. 5 and γ = 0. 8 are reasonable. The\ncondition (5.16) can be interpreted as saying that we will accept a fraction α\nof the decrease in f ( β ) that is predicted by linear extrapolation (Figure 5.4).\n</code></pre></div><p>For convex functions, both of these stepsize choices, when combined with suitable choices of the descent directions{∆ <em>t</em> }∞ <em>t</em> =0, yield algorithms that are guaranteed to converge to a global minimum of the convex function <em>f</em>. See the bibliographic section on page 131 for further discussion.</p><h4 id="_5-3-2-projected-gradient-methods"><a class="header-anchor" href="#_5-3-2-projected-gradient-methods" aria-hidden="true">#</a> 5.3.2 Projected Gradient Methods</h4><p>We now turn to gradient methods for problems that involve additional side constraints. In order to provide some geometric intuition for these methods, it is useful to observe that the gradient step (5.13) has the alternative repre- sentation</p><div class="language-"><pre><code>βt +1= arg min\nβ ∈R p\n</code></pre></div><h6 id="-162"><a class="header-anchor" href="#-162" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>f ( βt ) +〈∇ f ( βt ) , β − βt 〉+\n</code></pre></div><h6 id="_1-66"><a class="header-anchor" href="#_1-66" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>2 st\n</code></pre></div><div class="language-"><pre><code>‖ β − βt ‖^22\n</code></pre></div><h6 id="-163"><a class="header-anchor" href="#-163" aria-hidden="true">#</a> }</h6><h6 id="_5-17"><a class="header-anchor" href="#_5-17" aria-hidden="true">#</a> . (5.17)</h6><h6 id="gradient-descent-103"><a class="header-anchor" href="#gradient-descent-103" aria-hidden="true">#</a> GRADIENT DESCENT 103</h6><p>Thus, it can be viewed as minimizing the linearization of <em>f</em> around the cur- rent iterate, combined with a smoothing penalty that penalizes according to Euclidean distance. This view of gradient descent—an algorithm tailored specifically for uncon- strained minimization—leads naturally to the method of projected gradient descent, suitable for minimization subject to a constraint <em>β</em> ∈C:</p><div class="language-"><pre><code>βt +1= arg min\nβ ∈C\n</code></pre></div><h6 id="-164"><a class="header-anchor" href="#-164" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>f ( βt ) +〈∇ f ( βt ) , β − βt 〉+\n</code></pre></div><h6 id="_1-67"><a class="header-anchor" href="#_1-67" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>2 st\n</code></pre></div><div class="language-"><pre><code>‖ β − βt ‖^22\n</code></pre></div><h6 id="-165"><a class="header-anchor" href="#-165" aria-hidden="true">#</a> }</h6><h6 id="_5-18"><a class="header-anchor" href="#_5-18" aria-hidden="true">#</a> . (5.18)</h6><p>Equivalently, as illustrated in Figure 5.5, this method corresponds to taking a gradient step <em>βt</em> − <em>s</em> ∇ <em>f</em> ( <em>βt</em> ), and then projecting the result back onto the convex constraint setC. It is an efficient algorithm as long as this projection can be computed relatively easily. For instance, given an <em>`</em> 1 -ball constraint C={ <em>β</em> ∈R <em>p</em> | ‖ <em>β</em> ‖ 1 ≤ <em>R</em> }, this projection can be computed easily by a variant of soft thresholding, as we discuss in more detail later.</p><div class="language-"><pre><code>βt\nβt +1\n</code></pre></div><div class="language-"><pre><code>βt − st ∇ f ( βt )\n</code></pre></div><p><strong>Figure 5.5</strong> <em>Geometry of projected gradient descent. Starting from the current iterate βt, it moves in the negative gradient direction toβt</em> − <em>st</em> ∇ <em>f</em> ( <em>βt</em> ) <em>, and then performs a Euclidean projection of the result back onto the convex constraint set</em> C <em>in order to obtain the next iterateβt</em> +1_._</p><h4 id="_5-3-3-proximal-gradient-methods"><a class="header-anchor" href="#_5-3-3-proximal-gradient-methods" aria-hidden="true">#</a> 5.3.3 Proximal Gradient Methods</h4><p>Here we discuss a general class of methods that include projected gradient descent as a special case. As discussed previously, many objective functions <em>f</em> can be decomposed as a sum <em>f</em> = <em>g</em> + <em>h</em> , where <em>g</em> is convex and differentiable, and <em>h</em> is convex but nondifferentiable. Suppose that we would like to minimize such an objective function by a gradient-type algorithm. How do we deal with the nondifferentiability of the component <em>h</em>? In order to see how this difficulty can be finessed, recall that an ordinary gradient step can be viewed as minimizing the combination of a local linear approximation to <em>f</em> combined with a quadratic smoothness term—in particu- lar, see Equation (5.17). This perspective suggests the following strategy: form</p><h6 id="_104-optimization-methods"><a class="header-anchor" href="#_104-optimization-methods" aria-hidden="true">#</a> 104 OPTIMIZATION METHODS</h6><p>a local approximation to <em>f</em> by linearizing the differentiable component <em>g</em> , but leaving the nondifferentiable component fixed. This leads to the <em>generalized gradient update</em> , defined by</p><div class="language-"><pre><code>βt +1= arg min\nβ ∈R p\n</code></pre></div><h6 id="-166"><a class="header-anchor" href="#-166" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>g ( βt ) +〈∇ g ( βt ) , β − βt 〉+\n</code></pre></div><h6 id="_1-68"><a class="header-anchor" href="#_1-68" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>2 st\n</code></pre></div><div class="language-"><pre><code>‖ β − βt ‖^22 + h ( β )\n</code></pre></div><h6 id="-167"><a class="header-anchor" href="#-167" aria-hidden="true">#</a> }</h6><h6 id="_5-19"><a class="header-anchor" href="#_5-19" aria-hidden="true">#</a> , (5.19)</h6><p>where we have approximated the differentiable part <em>g</em> , but retained an exact form of the nondifferentiable component <em>h</em>. The update (5.19) is closely related to the projected gradient descent up- date (5.18); in fact, it can be viewed as a Lagrangian analog. In order to make this connection explicit, we define the <em>proximal map</em> of a convex function <em>h</em> , a type of generalized projection operator:</p><div class="language-"><pre><code>prox h ( z ) : = arg min\nθ ∈R p\n</code></pre></div><h6 id="-168"><a class="header-anchor" href="#-168" aria-hidden="true">#</a> {</h6><h6 id="_1-69"><a class="header-anchor" href="#_1-69" aria-hidden="true">#</a> 1</h6><h6 id="_2-55"><a class="header-anchor" href="#_2-55" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ z − θ ‖^22 + h ( θ )\n</code></pre></div><h6 id="-169"><a class="header-anchor" href="#-169" aria-hidden="true">#</a> }</h6><h6 id="_5-20"><a class="header-anchor" href="#_5-20" aria-hidden="true">#</a> . (5.20)</h6><div class="language-"><pre><code>From this definition we immediately have the following relations:\n(a) prox sh ( z ) = arg min θ ∈R p\n</code></pre></div><h6 id="_1-70"><a class="header-anchor" href="#_1-70" aria-hidden="true">#</a> { 1</h6><div class="language-"><pre><code>2 s ‖ z − θ ‖\n</code></pre></div><div class="language-"><pre><code>2\n2 + h ( θ )\n</code></pre></div><h6 id="-170"><a class="header-anchor" href="#-170" aria-hidden="true">#</a> }</h6><h6 id="-171"><a class="header-anchor" href="#-171" aria-hidden="true">#</a> .</h6><div class="language-"><pre><code>(b) When\n</code></pre></div><div class="language-"><pre><code>h ( θ ) = I C( θ ) =\n</code></pre></div><h6 id="-172"><a class="header-anchor" href="#-172" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>0 if θ ∈C, and\n+∞ otherwise\n</code></pre></div><div class="language-"><pre><code>we have prox h ( z ) = arg min θ ∈C‖ z − θ ‖^22 , corresponding to the usual Eu-\nclidean projection onto the setC.\n(c) If h ( θ ) = λ ‖ θ ‖ 1 , then prox h ( z ) = S λ ( z ), the element-wise soft-\nthresholded version of z. See Example 5.3 below.\n</code></pre></div><p>As we show in Exercise 5.7, it follows that the update (5.19) has the equivalent representation</p><div class="language-"><pre><code>βt +1= prox sth\n</code></pre></div><h6 id="-173"><a class="header-anchor" href="#-173" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>βt − st ∇ g ( βt )\n</code></pre></div><h6 id="-174"><a class="header-anchor" href="#-174" aria-hidden="true">#</a> )</h6><h6 id="_5-21"><a class="header-anchor" href="#_5-21" aria-hidden="true">#</a> . (5.21)</h6><p>Similarly, it is easy to see that the proximal-gradient update</p><div class="language-"><pre><code>βt +1= prox I C\n</code></pre></div><h6 id="-175"><a class="header-anchor" href="#-175" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>βt − st ∇ g ( βt )\n</code></pre></div><h6 id="-176"><a class="header-anchor" href="#-176" aria-hidden="true">#</a> )</h6><h6 id="_5-22"><a class="header-anchor" href="#_5-22" aria-hidden="true">#</a> (5.22)</h6><p>is exactly the projected gradient step (5.18). The updates (5.21) will be computationally efficient as long as the proximal map is relatively easy to compute. For many problems that arise in statistics— among them the <em><code>_ 1 -norm, group-lasso _</code></em> 2 norm, and nuclear norms—the proxi- mal map (5.20) can be computed quite cheaply. Typically the update (5.21) is better suited to statistical problems that impose regularization via a penalty, as opposed to a constraint of the form <em>h</em> ( <em>θ</em> )≤ <em>R</em>.</p><p><em>Example 5.3. Proximal gradient descent for <code>_ 1 _-penalty._ Suppose that the nondifferentiable component is a (scaled) _</code></em> 1 penalty, say <em>h</em> ( <em>θ</em> ) = <em>λ</em> ‖ <em>θ</em> ‖ 1. With this choice of <em>h</em> , proximal gradient descent with stepsize <em>st</em> at iteration <em>t</em> con- sists of two very simple steps:</p><h6 id="gradient-descent-105"><a class="header-anchor" href="#gradient-descent-105" aria-hidden="true">#</a> GRADIENT DESCENT 105</h6><ol><li>First, take a gradient step <em>z</em> = <em>βt</em> − <em>st</em> ∇ <em>g</em> ( <em>βt</em> ).</li><li>Second, perform elementwise soft-thresholding <em>βt</em> +1=S <em>stλ</em> ( <em>z</em> ).</li></ol><div class="language-"><pre><code>In detail, the proximal map (5.21) is given by\n</code></pre></div><div class="language-"><pre><code>prox sh ( z ) = arg min\nθ ∈R p\n</code></pre></div><h6 id="-177"><a class="header-anchor" href="#-177" aria-hidden="true">#</a> {</h6><h6 id="_1-71"><a class="header-anchor" href="#_1-71" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>2 s\n</code></pre></div><div class="language-"><pre><code>‖ z − θ ‖^22 + λ ‖ θ ‖ 1\n</code></pre></div><h6 id="-178"><a class="header-anchor" href="#-178" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>= arg min\nθ ∈R p\n</code></pre></div><h6 id="-179"><a class="header-anchor" href="#-179" aria-hidden="true">#</a> {</h6><h6 id="_1-72"><a class="header-anchor" href="#_1-72" aria-hidden="true">#</a> 1</h6><h6 id="_2-56"><a class="header-anchor" href="#_2-56" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ z − θ ‖^22 + λs ‖ θ ‖ 1\n</code></pre></div><h6 id="-180"><a class="header-anchor" href="#-180" aria-hidden="true">#</a> }</h6><h6 id="-181"><a class="header-anchor" href="#-181" aria-hidden="true">#</a> .</h6><h6 id="_5-23"><a class="header-anchor" href="#_5-23" aria-hidden="true">#</a> (5.23)</h6><div class="language-"><pre><code>This optimization problem has an explicit closed-form solution; in particular,\nsince the objective function decouples across coordinates as\n</code></pre></div><div class="language-"><pre><code>1\n2\n‖ z − θ ‖^22 + λs ‖ θ ‖ 1 =\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><h6 id="-182"><a class="header-anchor" href="#-182" aria-hidden="true">#</a> {</h6><h6 id="_1-73"><a class="header-anchor" href="#_1-73" aria-hidden="true">#</a> 1</h6><h6 id="_2-57"><a class="header-anchor" href="#_2-57" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>( zj − θj )^2 + λs | θj |\n</code></pre></div><h6 id="-183"><a class="header-anchor" href="#-183" aria-hidden="true">#</a> }</h6><h6 id="_5-24"><a class="header-anchor" href="#_5-24" aria-hidden="true">#</a> , (5.24)</h6><div class="language-"><pre><code>we can solve the p -dimensional problem by solving each of the univariate\nproblems separately. We leave it as an exercise for the reader to verify the\nsolution is obtained by applying the soft thresholding operator S τ :R p →R p\nwith coordinates\n</code></pre></div><div class="language-"><pre><code>[S τ ( z )] j = sign( zj )(| zj |− τ )+ , (5.25)\n</code></pre></div><div class="language-"><pre><code>with the threshold choice τ = sλ. (Here we use ( x )+ as a shorthand for\nmax{ x, 0 }.) ♦\nExample 5.4. Proximal gradient descent for nuclear norm penalty. As a sec-\nond illustration, suppose that h is λ times the nuclear norm. As previously\nintroduced in Example 5.2, the nuclear norm is a real-valued function on the\nspace of m × n matrices, given by‖ Θ ‖? =\n</code></pre></div><div class="language-"><pre><code>∑ m\nj =1 σj ( Θ ), where{ σj ( Θ )}are\nthe singular values of Θ. With this choice of h , the generalized projection\noperator (5.20) takes the form\n</code></pre></div><div class="language-"><pre><code>prox sh ( Z ) = arg min\nΘ ∈R m × n\n</code></pre></div><h6 id="-184"><a class="header-anchor" href="#-184" aria-hidden="true">#</a> {</h6><h6 id="_1-74"><a class="header-anchor" href="#_1-74" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>2 s\n</code></pre></div><div class="language-"><pre><code>‖ Z − Θ ‖^2 F+ λ ‖ Θ ‖?\n</code></pre></div><h6 id="-185"><a class="header-anchor" href="#-185" aria-hidden="true">#</a> }</h6><h6 id="_5-26"><a class="header-anchor" href="#_5-26" aria-hidden="true">#</a> . (5.26)</h6><div class="language-"><pre><code>Here the Frobenius norm‖ Z − Θ ‖^2 F=\n</code></pre></div><div class="language-"><pre><code>∑ m\nj =1\n</code></pre></div><div class="language-"><pre><code>∑ n\nk =1( Zjk −Θ jk )\n</code></pre></div><p>(^2) is simply the usual Euclidean norm applied to the entries of the matrices. Although this proximal map (5.26) is no longer separable, it still has a relatively simple so- lution. Indeed, as we explore in Exercise 5.8, the update Π <em>s,h</em> ( <strong>Z</strong> ) is obtained by computing the singular value decomposition of <strong>Z</strong> , and then soft-thresholding its singular values. ♦ Nesterov (2007) provides sufficient conditions for the convergence of the updates (5.21) when applied to a composite objective function <em>f</em> = <em>g</em> + <em>h</em>. Suppose that the component <em>g</em> is continuously differentiable with a Lipschitz gradient, meaning that there is some constant <em>L</em> such that ‖∇ <em>g</em> ( <em>β</em> )−∇ <em>g</em> ( <em>β</em> ′)‖ 2 ≤ <em>L</em> ‖ <em>β</em> − <em>β</em> ′‖ 2 for all <em>β,β</em> ′∈R <em>p</em>. (5.27)</p><h6 id="_106-optimization-methods"><a class="header-anchor" href="#_106-optimization-methods" aria-hidden="true">#</a> 106 OPTIMIZATION METHODS</h6><p>Under this condition and with a constant stepsize <em>st</em> = <em>s</em> ∈(0 <em>,</em> 1 <em>/L</em> ], it can be shown that there is a constant <em>C</em> , independent of the iteration number, such that the updates (5.21) satisfy</p><div class="language-"><pre><code>f ( βt )− f ( β ∗)≤\n</code></pre></div><h6 id="c"><a class="header-anchor" href="#c" aria-hidden="true">#</a> C</h6><div class="language-"><pre><code>t + 1\n</code></pre></div><div class="language-"><pre><code>‖ βt − β ∗‖ 2 for all t = 1 , 2 ,... , (5.28)\n</code></pre></div><p>where <em>β</em> ∗is an optimal solution. In words, the difference between the value <em>f</em> ( <em>βt</em> ) of the <em>tth</em> iterate and the optimal value <em>f</em> ( <em>β</em> ∗) decreases at the rate O(1 <em>/t</em> ). This rate is known as <em>sublinear convergence</em> , and is guaranteed for any fixed stepsize in the interval (0 <em>,</em> 1 <em>/L</em> ]. Such a choice requires an upper bound on the Lipschitz constant <em>L</em> , which may or may not be available. In practice, the Armijo rule also yields the same rate (5.28). (See Figure 5.6.)</p><div class="language-"><pre><code>0 10 20 30 40 50\n−8\n</code></pre></div><div class="language-"><pre><code>−7\n</code></pre></div><div class="language-"><pre><code>−6\n</code></pre></div><div class="language-"><pre><code>−5\n</code></pre></div><div class="language-"><pre><code>−4\n</code></pre></div><div class="language-"><pre><code>−3\n</code></pre></div><div class="language-"><pre><code>−2\n</code></pre></div><div class="language-"><pre><code>−1\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>Iteration\n</code></pre></div><div class="language-"><pre><code>Log |f(\n</code></pre></div><div class="language-"><pre><code>tβ) − f(\n</code></pre></div><div class="language-"><pre><code>*β\n)|\n</code></pre></div><div class="language-"><pre><code>Linear versus sublinear convergence\n</code></pre></div><div class="language-"><pre><code>Sublinear\nLinear\n</code></pre></div><p><strong>Figure 5.6</strong> <em>Plot of</em> log| <em>f</em> ( <em>βt</em> )− <em>f</em> ( <em>β</em> ∗)| <em>versus the iteration numbert, comparing a sublinear convergence rate</em> (5.28) <em>with a linear or geometric convergence rate</em> (5.30)<em>. For an algorithm with geometric convergence, the error decay on this logarithmic scale is linear with a negative slope.</em></p><p>Significantly faster rates are possible if the objective function has addi- tional structure. For instance, suppose that in addition to having a Lipschitz continuous gradient (5.27), the differentiable component <em>g</em> is <em>strongly convex</em> , meaning that there exists some <em>γ &gt;</em> 0 such that</p><div class="language-"><pre><code>g ( β + ∆)− g ( β )−〈∇ g ( β ) , ∆〉≥ γ^2 ‖∆‖^22 for all β, ∆∈R p. (5.29)\n</code></pre></div><p>This condition guarantees that <em>g</em> has at least as much curvature as the quadratic function <em>β</em> 7→ <em>γ</em>^2 ‖ <em>β</em> ‖^22 in all directions. Under conditions (5.27) and (5.29), it can be shown that with a constant stepsize <em>s</em> ∈(0 <em>,</em> 1 <em>/L</em> ], the</p><h6 id="gradient-descent-107"><a class="header-anchor" href="#gradient-descent-107" aria-hidden="true">#</a> GRADIENT DESCENT 107</h6><p>updates (5.21) will achieve a <em>linear or geometric rate</em> of convergence, meaning that there exists a positive constant <em>C</em> and contraction factor <em>κ</em> ∈(0 <em>,</em> 1) such that</p><div class="language-"><pre><code>f ( βt )− f ( β ∗)≤ C κt ‖ β^0 − β ∗‖ 2 for all t = 1 , 2 ,... , (5.30)\n</code></pre></div><p>Thus, under the additional strong convexity condition, the error <em>f</em> ( <em>βt</em> )− <em>f</em> ( <em>β</em> ∗) is guaranteed to contract at a geometric rate specified by <em>κ</em> ∈(0 <em>,</em> 1). See Figure 5.6 for an illustration of the difference between this linear rate and the earlier sublinear rate (5.28).</p><p><em>Example 5.5. Proximal gradient for lasso.</em> For the lasso, we have</p><div class="language-"><pre><code>g ( β ) =\n</code></pre></div><h6 id="_1-75"><a class="header-anchor" href="#_1-75" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-12"><a class="header-anchor" href="#_2-n-12" aria-hidden="true">#</a> 2 N</h6><div class="language-"><pre><code>‖ y − X β ‖^22 and h ( β ) = λ ‖ β ‖ 1 ,\n</code></pre></div><p>so that the proximal gradient update (5.21) takes the form</p><div class="language-"><pre><code>βt +1=S stλ\n</code></pre></div><h6 id="-186"><a class="header-anchor" href="#-186" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>βt + st\n</code></pre></div><h6 id="_1-76"><a class="header-anchor" href="#_1-76" aria-hidden="true">#</a> 1</h6><h6 id="n-24"><a class="header-anchor" href="#n-24" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>X T ( y − X βt )\n</code></pre></div><h6 id="-187"><a class="header-anchor" href="#-187" aria-hidden="true">#</a> )</h6><h6 id="_5-31"><a class="header-anchor" href="#_5-31" aria-hidden="true">#</a> . (5.31)</h6><p>Note that this has a very similar form to the coordinate descent update (see Section 5.4), especially if we take the stepsize <em>s</em> = 1 and assume that the predictors are standardized. Then both procedures operate on the same quan- tities, one in a cyclical manner and the other (proximal gradients) in a simul- taneous manner on all coordinates. It is not clear which is a more effective approach. The coordinate descent procedure can exploit sparsity of the coeffi- cient vector and doesn’t need to worry about step-size optimization, while the proximal gradient may gain efficiency by moving all parameters at the same time. It may also have speed advantages in problems where the multiplication of a vector by both <strong>X</strong> and <strong>X</strong> <em>T</em> can be done quickly, for example by a fast Fourier transform. The Lipschitz constant <em>L</em> here is the maximum eigenvalue of <strong>X</strong> <em>T</em> <strong>X</strong> <em>/N</em> ; one can use a fixed stepsize in (0 <em>,</em> 1 <em>/L</em> ] or a form of backtracking step selection. We compare these in a numerical example in Section 5.5. ♦</p><h4 id="_5-3-4-accelerated-gradient-methods"><a class="header-anchor" href="#_5-3-4-accelerated-gradient-methods" aria-hidden="true">#</a> 5.3.4 Accelerated Gradient Methods</h4><p>In this section, we discuss a class of accelerated gradient methods due to Nesterov (2007). Suppose that we have a convex differentiable function <em>f</em> , and recall the standard gradient step (5.13). For certain objective functions, this update may exhibit an undesirable type of “zig-zagging” behavior from step to step, which could conceivably slow down convergence. With the motivation of alleviating this drawback, Nesterov (2007) proposed the class of accelerated gradient methods that use weighted combinations of the current and previous gradient directions. In more detail, the accelerated gradient method involves a pair of se- quences{ <em>βt</em> }∞ <em>t</em> =0and{ <em>θt</em> }∞ <em>t</em> =0, and some initialization <em>β</em>^0 = <em>θ</em>^0. For iterations</p><h6 id="_108-optimization-methods"><a class="header-anchor" href="#_108-optimization-methods" aria-hidden="true">#</a> 108 OPTIMIZATION METHODS</h6><p><em>t</em> = 0 <em>,</em> 1 <em>,</em> 2 <em>,...</em> , the pair is then updated according to the recursions</p><div class="language-"><pre><code>βt +1= θt − st ∇ f ( θt ) , and (5.32a)\n</code></pre></div><div class="language-"><pre><code>θt +1= βt +1+\nt\nt + 3\n</code></pre></div><div class="language-"><pre><code>( βt +1− βt ). (5.32b)\n</code></pre></div><p>For non-smooth functions <em>f</em> that have the “smooth plus non-smooth” de- composition <em>g</em> + <em>h</em> , Nesterov’s acceleration scheme can be combined with the proximal gradient update: in particular, we replace the ordinary gradi- ent step (5.32a) with the update</p><div class="language-"><pre><code>βt +1= prox sth\n</code></pre></div><h6 id="-188"><a class="header-anchor" href="#-188" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>θt − st ∇ g ( θt )\n</code></pre></div><h6 id="-189"><a class="header-anchor" href="#-189" aria-hidden="true">#</a> )</h6><h6 id="_5-33"><a class="header-anchor" href="#_5-33" aria-hidden="true">#</a> . (5.33)</h6><p>In either case, the stepsize <em>st</em> is either fixed to some value, or chosen according to some type of backtracking line search.</p><p><em>Example 5.6. Proximal gradient descent with momentum.</em> Let us consider the combination of proximal gradient steps with the acceleration scheme in ap- plication to the <em>`</em> 1 -regularized lasso program. Recalling the form (5.31) of the composite gradient update, we see that the accelerated scheme consists of the updates</p><div class="language-"><pre><code>βt +1=S stλ\n</code></pre></div><h6 id="-190"><a class="header-anchor" href="#-190" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>θt + stN^1 X T ( y − X θt )\n</code></pre></div><h6 id="-191"><a class="header-anchor" href="#-191" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>θt +1= βt +1+\n</code></pre></div><div class="language-"><pre><code>t\nt + 3\n</code></pre></div><h6 id="-192"><a class="header-anchor" href="#-192" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>βt +1− βt\n</code></pre></div><h6 id="-193"><a class="header-anchor" href="#-193" aria-hidden="true">#</a> )</h6><p><em>.</em> (5.34a)</p><p>This algorithm for the lasso is essentially equivalent, modulo some minor dif- ferences in the acceleration weights, to the Fast Iterative Soft-thresholding Algorithm (FISTA) of Beck and Teboulle (2009). To investigate how well this works, we generated data from a regression problem with <em>N</em> = 1000 observations and <em>p</em> = 500 features. The features <em>xij</em> are standard Gaussian having pairwise correlation 0.5. Twenty of the 500 co- efficients <em>βj</em> were nonzero, each distributed as standard Gaussian variates and chosen at random locations between 1 and <em>p</em>. Figure 5.7 shows the performance of the generalized gradient and Nesterov’s method, for two different values of the regularization parameter <em>λ</em>. We tried the algorithms using a fixed value of the step-size <em>st</em> (equal to the reciprocal of the largest eigenvalue of <em>N</em>^1 <strong>X</strong> <em>T</em> <strong>X</strong> ). We also tried the approximate backtracking line search for <em>st</em> ∈[0 <em>,</em> 0_._ 5]. We see that Nesterov’s momentum method yields substantial speedups, over the generalized gradient, and backtracking is faster than the fixed stepsize choice. In the latter comparison this does not even take into account the cost of computing the largest eigenvalue of <em>N</em>^1 <strong>X</strong> <em>T</em> <strong>X</strong> : backtracking can speed up the computation by allowing a larger stepsize to be used when it is appropri- ate. Note that we are simply counting the number of iterations, rather than measuring the total elapsed time; however the Nesterov momentum steps are only slightly more costly than the generalized gradient steps. We also note that the relative error and hence the iterates <em>f</em> ( <em>βt</em> ) are not strictly monotone decreasing for Nesterov’s momentum method. ♦</p><h6 id="coordinate-descent-109"><a class="header-anchor" href="#coordinate-descent-109" aria-hidden="true">#</a> COORDINATE DESCENT 109</h6><div class="language-"><pre><code>5 10 15 20\n</code></pre></div><div class="language-"><pre><code>1e−08\n</code></pre></div><div class="language-"><pre><code>1e−06\n</code></pre></div><div class="language-"><pre><code>1e−04\n</code></pre></div><div class="language-"><pre><code>1e−02\n</code></pre></div><div class="language-"><pre><code>Number of Steps k\n</code></pre></div><div class="language-"><pre><code>Relative Error\n</code></pre></div><div class="language-"><pre><code>Proximal gradient\nwith backtrack\nNesterov momentum\nwith backtrack\n</code></pre></div><div class="language-"><pre><code>0 10 20 30 40 50\n</code></pre></div><div class="language-"><pre><code>1e−07\n</code></pre></div><div class="language-"><pre><code>1e−05\n</code></pre></div><div class="language-"><pre><code>1e−03\n</code></pre></div><div class="language-"><pre><code>1e−01\n</code></pre></div><div class="language-"><pre><code>Number of Steps k\n</code></pre></div><div class="language-"><pre><code>Relative Error\n</code></pre></div><div class="language-"><pre><code>Sparse— λ =0. 7 Dense— λ =0. 05\n</code></pre></div><p><strong>Figure 5.7</strong> <em>Performance of the generalized gradient and Nesterov momentum meth- ods for a sample lasso problem described in the text. The vertical axis shows the error measure</em> [ <em>f</em> ( <em>βt</em> )− <em>f</em> ( <em>β</em> ∗)] <em>/f</em> ( <em>β</em> ∗) <em>, whereβ</em> ∗ <em>is the minimizer, andβtis the solution aftertsteps. On the left, the solutionβ</em> ∗ <em>is sparse with just</em> 20 <em>of the</em> 500 <em>coefficients not equal to zero; on the right,</em> 237 <em>of the coefficients are nonzero.</em></p><p>In computational terms, the momentum updates (5.32) and (5.33) only in- volve slightly more work than an ordinary gradient update. Nonetheless, Nes- terov (2007) proves that the change yields a significant improvement in conver- gence rates: in particular, whenever <em>g</em> satisfies the Lipschitz condition (5.27), then there is a constant <em>C &gt;</em> 0 such that the iterates satisfy</p><div class="language-"><pre><code>f ( βt )− f ( β ∗)≤\n</code></pre></div><h6 id="c-1"><a class="header-anchor" href="#c-1" aria-hidden="true">#</a> C</h6><div class="language-"><pre><code>( t + 1)^2\n‖ β^0 − β ∗‖ 2. (5.35)\n</code></pre></div><p>Consequently, the error <em>f</em> ( <em>βt</em> )− <em>f</em> ( <em>β</em> ∗) decreases at the rateO(1 <em>/t</em>^2 ), as opposed to the slowerO(1 <em>/t</em> ) rate of a nonaccelerated method (see Equation (5.28)). When <em>g</em> is strongly convex (5.29), the accelerated gradient method again en- joys a geometric rate of convergence (5.30), although with a smaller con- traction factor <em>κ</em>. More precisely, the nonaccelerated method converges with a contraction factor determined by the condition number of <em>g</em> , whereas the accelerated variant converges according to the <em>square root</em> of this condition number.</p><h3 id="_5-4-coordinate-descent"><a class="header-anchor" href="#_5-4-coordinate-descent" aria-hidden="true">#</a> 5.4 Coordinate Descent</h3><p>Certain classes of problems, among them the lasso and variants, have an ad- ditional separability property that lends itself naturally to a coordinate mini- mization algorithm. <em>Coordinate descent</em> is an iterative algorithm that updates</p><h6 id="_110-optimization-methods"><a class="header-anchor" href="#_110-optimization-methods" aria-hidden="true">#</a> 110 OPTIMIZATION METHODS</h6><p>from <em>βt</em> to <em>βt</em> +1by choosing a single coordinate to update, and then performing a univariate minimization over this coordinate. More precisely, if coordinate <em>k</em> is chosen at iteration <em>t</em> , then the update is given by</p><div class="language-"><pre><code>βkt +1= arg min\nβk\n</code></pre></div><div class="language-"><pre><code>f\n</code></pre></div><h6 id="-194"><a class="header-anchor" href="#-194" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>βt 1 ,β 2 t,...,βkt − 1 ,βk,βkt +1 ,...,βpt\n</code></pre></div><h6 id="-195"><a class="header-anchor" href="#-195" aria-hidden="true">#</a> )</h6><h6 id="_5-36"><a class="header-anchor" href="#_5-36" aria-hidden="true">#</a> , (5.36)</h6><p>and <em>βjt</em> +1= <em>βjt</em> for <em>j</em> 6 = <em>k</em>. A typical choice would be to cycle through the coordinates in some fixed order. This approach can also be generalized to <em>block coordinate descent</em> , in which the variables are partitioned into non-overlapping blocks (as in the group lasso), and we perform minimization over a single block at each round.</p><h4 id="_5-4-1-separability-and-coordinate-descent"><a class="header-anchor" href="#_5-4-1-separability-and-coordinate-descent" aria-hidden="true">#</a> 5.4.1 Separability and Coordinate Descent</h4><p>When does this procedure converge to the global minimum of a convex func- tion? One sufficient (but somewhat restrictive) condition is that <em>f</em> be contin- uously differentiable and strictly convex in each coordinate. However, the use of various statistical regularizers leads to optimization problems that need not be differentiable. For such cases, more care is required when using coordinate minimization, because, as we discuss below, it can become “stuck” at non- optimal points. One form of problem structure that ensures good behavior of coordinate minimization is a type of separability condition. In particular, suppose that the cost function <em>f</em> has the additive decomposition</p><div class="language-"><pre><code>f ( β 1 ,...βp ) = g ( β 1 ,...βp ) +\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>hj ( βj ) , (5.37)\n</code></pre></div><p>where <em>g</em> :R <em>p</em> →Ris differentiable and convex, and the univariate functions <em>hj</em> :R→Rare convex (but not necessarily differentiable). An important example of this problem structure is the standard lasso program (2.5), with <em>g</em> ( <em>β</em> ) = 21 <em>N</em> ‖ <strong>y</strong> − <strong>X</strong> <em>β</em> ‖^22 and <em>hj</em> ( <em>βj</em> ) = <em>λ</em> ·| <em>βj</em> |. Tseng (1988, 2001) shows that for any convex cost function <em>f</em> with the separable structure (5.37), the co- ordinate descent Algorithm (5.36) is guaranteed to converge to the global minimizer. The key property underlying this result is the separability of the nondifferentiable component <em>h</em> ( <em>β</em> ) =</p><p>∑ <em>p j</em> =1 <em>hj</em> ( <em>βj</em> ), as a sum of functions of each individual parameter. This result implies that coordinate descent is a suitable algorithm for the lasso as well as certain other problems discussed in this book. In contrast, when the nondifferentiable component <em>h</em> is <em>not</em> sepa- rable, coordinate descent is no longer guaranteed to converge. Instead, it is possible to create problems for which it will become “stuck,” and fail to reach the global minimum.</p><p><em>Example 5.7. Failure of coordinate descent.</em> As an illustration, we con- sider an instance of a problem that violates (5.37)—the fused lasso, dis- cussed in Section 4.5. Here the nondifferentiable component takes the form <em>h</em> ( <em>β</em> ) =</p><div class="language-"><pre><code>∑ p\nj =1| βj − βj −^1 |. Figure 5.8 illustrates the difficulty. We created a\n</code></pre></div><h6 id="coordinate-descent-111"><a class="header-anchor" href="#coordinate-descent-111" aria-hidden="true">#</a> COORDINATE DESCENT 111</h6><p>fused lasso problem with 100 parameters, with the solutions for two of the parameters, <em>β</em> 63 = <em>β</em> 64 ≈ −1. The left and middle panels show slices of the function <em>f</em> varying <em>β</em> 63 and <em>β</em> 64 , with the other parameters set to the global minimizers. We see that the coordinate-wise descent algorithm has got stuck in a corner of the response surface, and is stationary under single-coordinate moves. In order to advance to the minimum, we have to move both <em>β</em> 63 and <em>β</em> 64 together.</p><div class="language-"><pre><code>−1.0 −0.5 0.0\n102.0\n</code></pre></div><div class="language-"><pre><code>102.5\n</code></pre></div><div class="language-"><pre><code>103.0\n</code></pre></div><div class="language-"><pre><code>103.5\n</code></pre></div><div class="language-"><pre><code>104.0\n</code></pre></div><div class="language-"><pre><code>−1.0 −0.5 0.0\n102.0\n</code></pre></div><div class="language-"><pre><code>102.5\n</code></pre></div><div class="language-"><pre><code>103.0\n</code></pre></div><div class="language-"><pre><code>103.5\n</code></pre></div><div class="language-"><pre><code>104.0\n</code></pre></div><div class="language-"><pre><code>−1.0 −0.5 0.0\n</code></pre></div><div class="language-"><pre><code>−1.0\n</code></pre></div><div class="language-"><pre><code>−0.5\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>β 63 β 63\n</code></pre></div><div class="language-"><pre><code>β^64\n</code></pre></div><div class="language-"><pre><code>β 64\n</code></pre></div><div class="language-"><pre><code>f(\nβ)\nf(\nβ)\n</code></pre></div><p><strong>Figure 5.8</strong> <em>Failure of coordinate-wise descent in a fused lasso problem with 100 parameters. The optimal values for two of the parameters,β</em> 63 <em>andβ</em> 64 <em>, are both</em> − 1_._ 05 <em>, as shown by the dot in the right panel. The left and middle panels show slices of the objective functionfas a function ofβ</em> 63 <em>andβ</em> 64 <em>, with the other pa- rameters set to the global minimizers. The coordinate-wise minimizer over bothβ</em> 63 <em>andβ</em> 64 <em>(separately) is -0.69, rather than</em> − 1_._ 05_. The right panel shows contours of the two-dimensional surface. The coordinate-descent algorithm is stuck at the point_ (− 0_._ 69 <em>,</em> − 0_._ 69)<em>. Despite being strictly convex, the surface has corners, in which the coordinate-wise procedure can get stuck. In order to travel to the minimum we have to move bothβ</em> 63 <em>andβ</em> 64 <em>together.</em></p><p>♦ Tseng (2001) gives a more general and intuitive condition for convergence of coordinate descent, one which depends on the behavior of the directional derivatives of the cost function <em>f</em>. For a given direction ∆∈R <em>p</em> , the lower directional derivative at <em>β</em> is given by</p><div class="language-"><pre><code>f ′( β ; ∆) : = lim inf\ns ↓ 0\n</code></pre></div><div class="language-"><pre><code>f ( β + s ∆)− f ( β )\ns\n</code></pre></div><h6 id="_5-38"><a class="header-anchor" href="#_5-38" aria-hidden="true">#</a> . (5.38)</h6><p>In rough terms, a coordinate descent algorithm only gains information about directions of the form <strong><em>e</em></strong> <em>j</em> =</p><h6 id="-196"><a class="header-anchor" href="#-196" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>0 , 0 , ... 0 , ej, 0 , ..., 0\n</code></pre></div><h6 id="-197"><a class="header-anchor" href="#-197" aria-hidden="true">#</a> )</h6><p>for some <em>ej</em> ∈R. Therefore, suppose that the coordinate descent algorithm reaches a point <em>β</em> for which</p><div class="language-"><pre><code>f ′( β ; ej )≥0 for all j = 1 ,...,p , and coordinate vectors ej. (5.39)\n</code></pre></div><h6 id="_112-optimization-methods"><a class="header-anchor" href="#_112-optimization-methods" aria-hidden="true">#</a> 112 OPTIMIZATION METHODS</h6><p>At any such point, there are no coordinate directions that will further reduce the function value. Therefore, we require that any <em>β</em> satisfying the condi- tion (5.39) also satisfies <em>f</em> ′( <em>β</em> ; ∆)≥0 for all directions ∆∈R <em>p</em>. Tseng (2001) calls this condition <em>regularity</em>. It rules out a situation like that of Figure 5.8, in which moves along all coordinate directions fail to decrease the criterion, but an oblique move does improve the criterion. As a side-note, it is worth observing that separability of the nondifferentiable component of the objective function implies regularity, but that there are nondifferentiable and nonsepa- rable functions that are still regular. An example is the function</p><div class="language-"><pre><code>h ( β 1 ,...,βp ) =| β | T P | β | =\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j,k =1\n</code></pre></div><div class="language-"><pre><code>| βj | Pjk | βk | , (5.40)\n</code></pre></div><p>where <strong>P</strong> is a symmetric positive definite matrix.</p><h4 id="_5-4-2-linear-regression-and-the-lasso"><a class="header-anchor" href="#_5-4-2-linear-regression-and-the-lasso" aria-hidden="true">#</a> 5.4.2 Linear Regression and the Lasso</h4><p>Recall the optimization problem (2.5) that underlies the lasso estimator. As discussed in Chapter 2, the optimality conditions for this problem are</p><h6 id="−-11"><a class="header-anchor" href="#−-11" aria-hidden="true">#</a> −</h6><h6 id="_1-77"><a class="header-anchor" href="#_1-77" aria-hidden="true">#</a> 1</h6><h6 id="n-25"><a class="header-anchor" href="#n-25" aria-hidden="true">#</a> N</h6><h6 id="∑-n-71"><a class="header-anchor" href="#∑-n-71" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( yi − β 0 −\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>k =1\n</code></pre></div><div class="language-"><pre><code>xikβk ) xij + λsj = 0 , (5.41)\n</code></pre></div><p>where <em>sj</em> ∈ sign( <em>βj</em> ) for <em>j</em> = 1 <em>,</em> 2 <em>,...,p</em>. The coordinate descent proce- dure simply solves these equations in a cyclical fashion, iterating over <em>j</em> = 1 <em>,</em> 2 <em>,...,p,</em> 1 <em>,</em> 2 <em>,...</em>. Since the intercept <em>β</em> 0 is typically not penalized, we can center both the response <em>yi</em> and the covariate vectors <em>xi</em> by their means, and then omit the in- tercept in the calculations of the other <em>βj</em>. (Of course, as in OLS, the intercept</p><p>is calculated at the end using the <em>β</em> ̂ 0 = ̄ <em>y</em> −</p><div class="language-"><pre><code>∑ p\nk =1 x ̄ k\nβ ̂ k .) To simplify matters,\n</code></pre></div><p>we define the <em>partial residualr</em> ( <em>ij</em> )= <em>yi</em> −</p><h6 id="∑-26"><a class="header-anchor" href="#∑-26" aria-hidden="true">#</a> ∑</h6><p><em>k</em> 6 = <em>jxikβ</em> ̂ <em>k</em> , which removes from the outcome the current fit from all but the <em>jth</em> predictor. Then the solution for <em>β</em> ̂ <em>j</em> satisfies</p><div class="language-"><pre><code>β ̂ j =\n</code></pre></div><div class="language-"><pre><code>S λ\n</code></pre></div><h6 id="-198"><a class="header-anchor" href="#-198" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>1\nN\n</code></pre></div><h6 id="∑-n-72"><a class="header-anchor" href="#∑-n-72" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1 r\n</code></pre></div><div class="language-"><pre><code>( j )\ni xij\n</code></pre></div><h6 id="-199"><a class="header-anchor" href="#-199" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>1\nN\n</code></pre></div><h6 id="∑-n-73"><a class="header-anchor" href="#∑-n-73" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1 x\n</code></pre></div><div class="language-"><pre><code>2\nij\n</code></pre></div><h6 id="_5-42"><a class="header-anchor" href="#_5-42" aria-hidden="true">#</a> , (5.42)</h6><p>whereas beforeS <em>λ</em> ( <em>θ</em> ) = sign( <em>θ</em> )(| <em>θ</em> |− <em>λ</em> )+is the soft-thresholding operator. If in addition to centering, the variables are standardized to have unit variance (typically a good idea, especially if the variables are in different units), then the update has the particularly succinct form</p><div class="language-"><pre><code>β ̂ j =S λ ( β ̃ j ) , (5.43)\n</code></pre></div><p>where <em>β</em> ̃ <em>j</em> is the simple linear regression coefficient of the partial residual on</p><h6 id="coordinate-descent-113"><a class="header-anchor" href="#coordinate-descent-113" aria-hidden="true">#</a> COORDINATE DESCENT 113</h6><p>variable <em>j</em>. If instead we have an elastic net penalty (1− <em>α</em> ) <em>β</em>^2 <em>j/</em> 2 + <em>α</em> | <em>βj</em> |, the update (5.42) becomes</p><div class="language-"><pre><code>β ̂ j =\n</code></pre></div><div class="language-"><pre><code>S αλ\n</code></pre></div><h6 id="-200"><a class="header-anchor" href="#-200" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>1\nN\n</code></pre></div><h6 id="∑-n-74"><a class="header-anchor" href="#∑-n-74" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1 r\n</code></pre></div><div class="language-"><pre><code>( j )\ni xij\n</code></pre></div><h6 id="-201"><a class="header-anchor" href="#-201" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>1\nN\n</code></pre></div><h6 id="∑-n-75"><a class="header-anchor" href="#∑-n-75" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1 x\n2\nij + (1− α ) λ\n</code></pre></div><h6 id="_5-44"><a class="header-anchor" href="#_5-44" aria-hidden="true">#</a> , (5.44)</h6><p>or in the standardized case</p><div class="language-"><pre><code>β ̂ j = S αλ (\n</code></pre></div><div class="language-"><pre><code>β ̃ j )\n1 + (1− α ) λ\n</code></pre></div><h6 id="_5-45"><a class="header-anchor" href="#_5-45" aria-hidden="true">#</a> . (5.45)</h6><p>There are a number of strategies for making these operations efficient. For ease of notation we assume that the predictors are standardized to have mean zero and variance one; for nonstandardized data, the steps are similar.</p><p><em>Partial residuals.</em> Note that we can write <em>r</em> ( <em>ij</em> )= <em>yi</em> −</p><h6 id="∑-27"><a class="header-anchor" href="#∑-27" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>k 6 = jxik\nβ ̂ k = ri +\n</code></pre></div><p><em>xijβ</em> ̂ <em>j</em> , where <em>ri</em> denotes the current residual for observation <em>i</em>. Since the vectors { <strong>x</strong> <em>j</em> } <em>pj</em> =1are standardized, we can write</p><div class="language-"><pre><code>1\nN\n</code></pre></div><h6 id="∑-n-76"><a class="header-anchor" href="#∑-n-76" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>xijri ( j )=\n</code></pre></div><h6 id="_1-78"><a class="header-anchor" href="#_1-78" aria-hidden="true">#</a> 1</h6><h6 id="n-26"><a class="header-anchor" href="#n-26" aria-hidden="true">#</a> N</h6><h6 id="∑-n-77"><a class="header-anchor" href="#∑-n-77" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>xijri + β ̂ j, (5.46)\n</code></pre></div><p>a representation that reveals the computational efficiency of coordinate de- scent. Many coefficients are zero and remain so after thresholding, and so nothing needs to be changed. The primary cost arises from computing the sum in Equation (5.46), which requiresO( <em>N</em> ) operations. On the other hand, if a coefficient does change after the thresholding, <em>ri</em> is changed inO( <em>N</em> ) and the step costsO(2 <em>N</em> ). A full cycle through all <em>p</em> variables costsO( <em>pN</em> ) oper- ations. Friedman et al. (2010 <em>b</em> ) refer to this as <em>naive updating</em> , since it works directly with the inner products of the data.</p><p><em>Covariance updating.</em> Naive updating is generally less efficient than <em>covari- ance updating</em> when <em>N</em>  <em>p</em> and <em>N</em> is large. Up to a factor 1 <em>/N</em> , we can write the first term on the right of expression (5.46)</p><div class="language-"><pre><code>∑ N\n</code></pre></div><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>xijri =〈 x j, y 〉−\n</code></pre></div><h6 id="∑-28"><a class="header-anchor" href="#∑-28" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>k || β ̂ k | &gt; 0\n</code></pre></div><div class="language-"><pre><code>〈 x j, x k 〉 β ̂ k. (5.47)\n</code></pre></div><p>In this approach, we compute inner products of each feature with <strong>y</strong> initially, and then each time a new feature <strong>x</strong> <em>k</em> enters the model for the first time, we compute and store its inner product with all the rest of the features, requiring O( <em>Np</em> ) operations. We also store the <em>p</em> gradient components (5.47). If one of the coefficients currently in the model changes, we can update each gradient in O( <em>p</em> ) operations. Hence with <em>k</em> nonzero terms in the model, a complete cycle costsO( <em>pk</em> ) operations if no new variables become nonzero, and costsO( <em>Np</em> ) for each new variable entered. Importantly, each step does not require making O( <em>N</em> ) calculations.</p><h6 id="_114-optimization-methods"><a class="header-anchor" href="#_114-optimization-methods" aria-hidden="true">#</a> 114 OPTIMIZATION METHODS</h6><p><em>Warm starts.</em> Typically one wants a sequence of lasso solutions, say for a decreasing sequence of values{ <em>λ`</em> } <em>L</em> 0. It is easy to see that the largest value that we need consider is</p><div class="language-"><pre><code>λ 0 = N^1 max\nj\n</code></pre></div><div class="language-"><pre><code>|〈 x j, y 〉| , (5.48)\n</code></pre></div><p>since any value larger would yield an empty model. One strategy, as employed by the R packageglmnet, is to create a sequence of values{ <em>λ<code>_ } _L</code></em> =0decreasing from <em>λ</em> 0 down to <em>λL</em> = <em>λ</em> 0 ≈0 on a log scale. The solution <em>β</em> ̂( <em>λ`</em> ) is typically a</p><p>very good warm start for the solution <em>β</em> ̂( <em>λ<code>_ +1). Likewise the number of nonzero elements tends to increase slowly with _</code></em> , starting at zero at <em>`</em> = 0. Doubling the number <em>L</em> = 100 to say 2 <em>L</em> does not double the compute time, since the warm starts are much better, and fewer iterations are needed each time.</p><p><em>Active-set convergence.</em> After a single iteration through the set of <em>p</em> variables at a new value <em>λ<code>_ , starting from the warm start _β_ ̂( _λ</code></em> − 1 ), we can define the active setAto index those variables with nonzero coefficients at present. The idea is to iterate the algorithm using only the variables inA. Upon convergence, we do a pass through all the omitted variables. If they all pass the simple exclusion test <em>N</em>^1 |〈 <strong>x</strong> <em>j,</em> <strong><em>r</em></strong> 〉| <em>&lt; λ`</em> , where <strong><em>r</em></strong> is the current residual, we have the solution for the entire set of <em>p</em> variables. Those that fail are included inAand the process is repeated. In practice we maintain an <em>ever-active</em> set— any variable that had a nonzero coefficient somewhere along the path until present is kept inA.</p><p><em>Strong-set convergence.</em> Similar to the above, we identify a subset of variables likely to be candidates for the active set. Let <strong><em>r</em></strong> be the residual at <em>β</em> ̂( <em>λ<code>_ − 1 ), and we wish to compute the solution at _λ</code></em>. Define the strong setSas</p><div class="language-"><pre><code>S={ j || N^1 〈 x j, r 〉| &gt; λ` −( λ` − 1 − λ` )}. (5.49)\n</code></pre></div><p>We now compute the solution restricting attention to <em>only</em> the variables in S. Apart from rare exceptions, the strong set will cover the optimal active set. Strong rules are extremely useful, especially when <em>p</em> is very large (in the 100Ks or millions). We discuss them in some detail in Section 5.10.</p><p><em>Sparsity.</em> The main computational operation in all the above is an inner- product of a pair of <em>N</em> -vectors, at least one of which is a column of the design matrix <strong>X</strong>. If <strong>X</strong> is sparse, we can compute these inner products efficiently. An example is document classification, where often the feature vector follows the so-called “bag-of-words” model. Each document is scored for the pres- ence/absence of each of the words in the entire dictionary under consideration (sometimes counts are used, or some transformation of counts). Since most words are absent, the feature vector for each document is mostly zero, and so the entire matrix is mostly zero. Such matrices can be stored efficiently in <em>sparse-column format</em> , where we store only the nonzero entries and the coor- dinates where they occur. Now when we compute inner products, we sum only over the nonzero entries.</p><h6 id="coordinate-descent-115"><a class="header-anchor" href="#coordinate-descent-115" aria-hidden="true">#</a> COORDINATE DESCENT 115</h6><p><em>Penalty strength.</em> The default formulation applies the same penalty param- eter <em>λ</em> to each term in the model. It is a simple matter to include a relative penalty strength <em>γj</em> ≥0 per variable, making the overall penalty</p><div class="language-"><pre><code>λ\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>γjPα ( βj ). (5.50)\n</code></pre></div><p>This allows for some <em>γj</em> to be zero, which means those variables are always in the model, unpenalized.</p><p><em>Parameter bounds.</em> Coordinate descent also makes it easy to set upper and lower bounds on each parameter:</p><div class="language-"><pre><code>Lj ≤ βj ≤ Uj, (5.51)\n</code></pre></div><p>where typically−∞ ≤ <em>Lj</em> ≤ 0 ≤ <em>Uj</em> ≤ ∞. For example, we sometimes want to constrain all coefficients to be nonnegative. One simply computes the coor- dinate update, and if the parameter violates the bound, it is set to the closest boundary.</p><h4 id="_5-4-3-logistic-regression-and-generalized-linear-models"><a class="header-anchor" href="#_5-4-3-logistic-regression-and-generalized-linear-models" aria-hidden="true">#</a> 5.4.3 Logistic Regression and Generalized Linear Models</h4><p>Here we move from squared-error loss to other members of the exponential family—the so-called <em>generalized linear models</em>. For simplicity, we focus on the most prominent (nonlinear) member of this class—namely, logistic regression. In logistic regression, the response is binary, and can be modeled as a class label <em>G</em> taking the values−1 or 1. The standard logistic model represents the class probabilities as a linear model in the log-odds</p><div class="language-"><pre><code>log\n</code></pre></div><div class="language-"><pre><code>Pr( G =− 1 | x )\nPr( G = 1| x )\n</code></pre></div><div class="language-"><pre><code>= β 0 + xTβ. (5.52)\n</code></pre></div><p>See Section 3.2 for more detail. We consider fitting this model by regularized maximum (binomial) likeli- hood. Introducing the shorthand notation <em>p</em> ( <em>xi</em> ; <em>β</em> 0 <em>,β</em> ) = Pr( <em>G</em> = 1| <em>xi</em> ) for the probability (5.52) of observation <em>i</em> , we maximize the penalized log-likelihood</p><div class="language-"><pre><code>1\nN\n</code></pre></div><h6 id="∑-n-78"><a class="header-anchor" href="#∑-n-78" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><h6 id="-202"><a class="header-anchor" href="#-202" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>I ( gi = 1) log p ( xi ; β 0 ,β ) + I ( gi =−1) log(1− p ( xi ; β 0 ,β ))\n</code></pre></div><h6 id="-203"><a class="header-anchor" href="#-203" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>− λPα ( β ).\n</code></pre></div><p>(5.53) Denoting <em>yi</em> = <em>I</em> ( <em>gi</em> =−1), the log-likelihood part of (5.53) can be written in the more explicit form</p><div class="language-"><pre><code>` ( β 0 ,β ) =\n</code></pre></div><h6 id="_1-79"><a class="header-anchor" href="#_1-79" aria-hidden="true">#</a> 1</h6><h6 id="n-27"><a class="header-anchor" href="#n-27" aria-hidden="true">#</a> N</h6><h6 id="∑-n-79"><a class="header-anchor" href="#∑-n-79" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><h6 id="-204"><a class="header-anchor" href="#-204" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>yi ·( β 0 + xTiβ )−log(1 + eβ^0 + x\n</code></pre></div><div class="language-"><pre><code>Tiβ\n)\n</code></pre></div><h6 id="-205"><a class="header-anchor" href="#-205" aria-hidden="true">#</a> ]</h6><h6 id="_5-54"><a class="header-anchor" href="#_5-54" aria-hidden="true">#</a> , (5.54)</h6><p>which corresponds to a concave function of the parameters. By way of</p><h6 id="_116-optimization-methods"><a class="header-anchor" href="#_116-optimization-methods" aria-hidden="true">#</a> 116 OPTIMIZATION METHODS</h6><p>background, the Newton algorithm for maximizing the (unpenalized) log- likelihood (5.54) amounts to iteratively reweighted least squares. Hence, if the current estimates of the parameters are ( <em>β</em> ̃ 0 <em>,β</em> ̃), we form a second- order Taylor expansion about current estimates. In terms of the shorthand <em>p</em> ̃( <em>xi</em> ) = <em>p</em> ( <em>xi</em> ; <em>β</em> ̃ 0 <em>,β</em> ̃), and <em>wi</em> = ̃ <em>p</em> ( <em>xi</em> )(1− <em>p</em> ̃( <em>xi</em> )), this Taylor expansion leads to the quadratic objective function</p><div class="language-"><pre><code>`Q ( β 0 ,β ) =−\n</code></pre></div><h6 id="_1-80"><a class="header-anchor" href="#_1-80" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-13"><a class="header-anchor" href="#_2-n-13" aria-hidden="true">#</a> 2 N</h6><h6 id="∑-n-80"><a class="header-anchor" href="#∑-n-80" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>wi ( zi − β 0 − xTiβ )^2 + C ( ̃ β 0 ,β ̃)^2. (5.55)\n</code></pre></div><p>where <em>zi</em> = <em>β</em> ̃ 0 + <em>xTiβ</em> ̃+ <em>p</em> ̃( <em>xyii</em> )(1− <em>p</em> ̃−( <em>xp</em> ̃( <em>ix</em> ) <em>i</em> )) is the current working response. The Newton update is obtained by minimizing <em><code>Q_ , which is a simple weighted- least-squares problem. In order to solve the regularized problem, one could apply coordinate descent directly to the criterion (5.53). A disadvantage of this approach is that the optimizing values along each coordinate are not explicitly available and require a line search. In our experience, it is better to apply coordinate descent to the quadratic approximation, resulting in a nested algorithm. For each value of _λ_ , we create an outer loop which computes the quadratic approximation _</code>Q</em> about the current parameters ( <em>β</em> ̃ 0 <em>,β</em> ̃). Then we use coordinate descent to solve the penalized weighted least-squares problem</p><div class="language-"><pre><code>minimize\n( β 0 ,β )∈R p +1\n</code></pre></div><div class="language-"><pre><code>{− `Q ( β 0 ,β ) + λPα ( β )}. (5.56)\n</code></pre></div><p>By analogy with Section 5.3.3, this is known as a generalized Newton algo- rithm, and the solution to the minimization problem (5.56)) defines a <em>proximal Newton map</em> (see the paper (Lee et al. 2014) for details). Overall, the proce- dure consists of a sequence of nested loops:</p><p>outer loop:Decrement <em>λ</em>.</p><p>middle loop:Update the quadratic approximation <em>`Q</em> using the current pa- rameters ( <em>β</em> ̃ 0 <em>,β</em> ̃).</p><p>inner loop:Run the coordinate descent algorithm on the penalized weighted-least-squares problem (5.56). When <em>p</em>  <em>N</em> , one cannot run <em>λ</em> all the way to zero, because the sat- urated logistic regression fit is undefined (parameters wander off to±∞in order to achieve probabilities of 0 or 1). Also, the Newton algorithm is not guaranteed to converge without step-size optimization (Lee, Lee, Abneel and Ng 2006). Theglmnetprogram does not implement any checks for divergence; this would slow it down, and when used as recommended, it does not seem to be necessary. We have a closed form expression for the starting solutions, and each subsequent solution is warm-started from the previous close-by solution, which generally makes the quadratic approximations very accurate. We have not encountered any divergence problems so far. Theglmnetpackage generalizes this procedure to other GLMs, such as</p><h6 id="a-simulation-study-117"><a class="header-anchor" href="#a-simulation-study-117" aria-hidden="true">#</a> A SIMULATION STUDY 117</h6><p>multiclass logistic regression, the Poisson log-linear model and Cox’s propor- tional hazards model for survival data. More details are given in Chapter 3. The speed of this procedure is studied in Section 5.5.</p><h3 id="_5-5-a-simulation-study"><a class="header-anchor" href="#_5-5-a-simulation-study" aria-hidden="true">#</a> 5.5 A Simulation Study</h3><p>Both the coordinate descent algorithm and Nesterov’s composite gradient method are simple and computationally efficient approaches for solving the lasso. How do they compare in terms of computational cost per iteration? If (at a given iteration) the current iterate <em>βt</em> has <em>k</em> nonzero coefficients, each pass of coordinate descent over all <em>p</em> predictors (using naive updating) takes O( <em>pN</em> + <em>kN</em> ) operations. On the other hand, the generalized gradient up- date (5.31) requiresO( <em>kN</em> ) operations to compute the matrix-vector product <strong>X</strong> <em>β</em> , and thenO( <em>pN</em> ) to compute the product <strong>X</strong> <em>T</em> ( <strong>y</strong> − <strong>X</strong> <em>β</em> ), again a total of O( <em>pN</em> + <em>kN</em> ) operations. In order to examine more closely the relative efficiency of coordinate de- scent, proximal gradient descent, and Nesterov’s momentum method, we car- ried out a small simulation study.^3 We generated an <em>N</em> × <em>p</em> predictor matrix <strong>X</strong> with standard Gaussian entries and pairwise correlation 0 or 0.5 between the features. Coefficients <em>βj</em> were defined by| <em>βj</em> |= exp[−_._ 5( <em>u</em> ( <em>j</em> −1))^2 ] with <em>u</em> =</p><h6 id="√-13"><a class="header-anchor" href="#√-13" aria-hidden="true">#</a> √</h6><p><em>π/</em> 20 and alternating signs +1 <em>,</em> − 1 <em>,</em> +1 <em>,...</em>. Then the outcome <em>yi</em> was generated as</p><div class="language-"><pre><code>yi =\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>xijβj + σεi (5.57)\n</code></pre></div><p>with <em>σ</em> chosen so that the signal to noise ratio Sd[E( <em>yi</em> )] <em>/σ</em> equals 3. Table 5.1 shows the average (standard error) of CPU times for coordinate descent, gener- alized gradient and Nesterov’s momentum methods, for a scenario with <em>N &gt; p</em> and another with <em>N &lt; p</em>. Shown is the total time over a path of 20 values of the regularization parameter <em>λ</em>. Warm starts were used in each case, with</p><p><strong>Table 5.1</strong> <em>Lasso for linear regression: Average (standard error) of CPU times over ten realizations, for coordinate descent, generalized gradient, and Nesterov’s momen- tum methods. In each case, time shown is the total time over a path of 20λvalues.</em></p><div class="language-"><pre><code>N = 10000 , p = 100 N = 200 , p = 10000\nCorrelation 0 0.5 0 0.5\nCoordinate descent 0.110 (0.001) 0.127 (0.002) 0.298 (0.003) 0.513 (0.014)\nProximal gradient 0.218 (0.008) 0.671 (0.007) 1.207 (0.026) 2.912 (0.167)\nNesterov 0.251 (0.007) 0.604 (0.011) 1.555 (0.049) 2.914 (0.119)\n</code></pre></div><p>convergence defined as the maximum change in the parameter vector being less than 10−^4. An approximate backtracking line search was used for the</p><p>(^3) We thank Jerome Friedman for the programs used in this section.</p><h6 id="_118-optimization-methods"><a class="header-anchor" href="#_118-optimization-methods" aria-hidden="true">#</a> 118 OPTIMIZATION METHODS</h6><p><strong>Table 5.2</strong> <em>Lasso for logistic regression: average (standard error) of CPU times over ten realizations, for coordinate descent, generalized gradient, and Nesterov’s momen- tum methods. In each case, time shown is the total time over a path of 20λvalues.</em></p><div class="language-"><pre><code>N = 10000 , p = 100 N = 200 , p = 10000\nCorrelation 0 0.5 0 0.5\nCoordinate descent 0.309 (0.086) 0.306 (0.086) 0.646 (0.006) 0.882 (0.026)\nProximal gradient 2.023 (0.018) 6.955 (0.090) 2.434 (0.095) 4.350 (0.133)\nNesterov 1.482 (0.020) 2.867 (0.045) 2.910 (0.106) 8.292 (0.480)\n</code></pre></div><p>latter two methods. We see that coordinate descent is 2–6 times faster than the other methods, with a greater speedup in the <em>p &gt; N</em> case. Interestingly, momentum does not provide a consistent speedup over proximal gradient de- scent, as the aforementioned theory would suggest. Our investigation into this suggests that the warm starts are the reason: by starting close to the solution, the “zig-zagging”, that is ameliorated by the momentum term, is not nearly as much of a problem as it is when starting far from the solution. Table 5.2 shows the corresponding results for logistic regression. The pre- dictors were generated as before, but now there are 15 nonzero <em>βj</em> with alter- nating signs, and| <em>βj</em> |= 15− <em>j</em> + 1. Then defining <em>pi</em> = 1 <em>/</em> (1 + exp(−</p><h6 id="∑-29"><a class="header-anchor" href="#∑-29" aria-hidden="true">#</a> ∑</h6><p><em>xijβj</em> )) we generate 0/1 <em>yi</em> with Prob( <em>yi</em> = 1) = <em>pi</em>. We see that coordinate descent is 5–10 times faster than the other methods, with a greater speedup in the <em>p &gt; N</em> case. Again, momentum does not provide a consistent speedup over proximal gradient descent. The reader should take comparisons like those above with a grain of salt, as the performance of a method will depend on the details of its implementation. Further suspicion should arise, since two of the authors of this text are co- authors of the method (coordinate descent) that performs best. For our part, we can only say that we have tried to be fair to all methods and have coded all methods as efficiently as we could. More importantly, we have made available all of the scripts and programs to generate these results on the book website, so that the reader can investigate the comparisons further.</p><h3 id="_5-6-least-angle-regression"><a class="header-anchor" href="#_5-6-least-angle-regression" aria-hidden="true">#</a> 5.6 Least Angle Regression</h3><p>Least angle regression, also known as the <em>homotopy</em> approach, is a procedure for solving the lasso with squared-error loss that delivers the entire solution path as a function of the regularization parameter <em>λ</em>. It is a fairly efficient algorithm, but does not scale up to large problems as well as some of the other methods in this chapter. However it has an interesting statistical motivation and can be viewed as a kind of “democratic” version of forward stepwise regression. Forward stepwise regression builds a model sequentially, adding one vari- able at a time. At each step, it identifies the best variable to include in the <em>active set</em> , and then updates the least-squares fit to include all the active vari-</p><h6 id="least-angle-regression-119"><a class="header-anchor" href="#least-angle-regression-119" aria-hidden="true">#</a> LEAST ANGLE REGRESSION 119</h6><div class="language-"><pre><code>ables. Least angle regression (LAR) uses a similar strategy, but only enters “as\nmuch” of a predictor as it deserves. At the first step it identifies the variable\nmost correlated with the response. Rather than fit this variable completely,\nthe LAR method moves the coefficient of this variable continuously toward\nits least-squares value (causing its correlation with the evolving residual to\ndecrease in absolute value). As soon as another variable catches up in terms\nof correlation with the residual, the process is paused. The second variable\nthen joins the active set, and their coefficients are moved together in a way\nthat keeps their correlations tied and decreasing. This process is continued\nuntil all the variables are in the model, and ends at the full least-squares fit.\nThe details are given in Algorithm 5.1. Although the LAR algorithm is stated\nin terms of correlations, since the input features are standardized, it is equiva-\nlent and easier to work with inner products. The number of terms K at step 3\nrequires some explanation. If p &gt; N −1, the LAR algorithm reaches a zero\nresidual solution after N −1 steps (the−1 is because there is an intercept in\nthe model, and we have centered the data to take care of this).\n</code></pre></div><div class="language-"><pre><code>Algorithm 5.1 Least Angle Regression.\n</code></pre></div><ol><li>Standardize the predictors to have mean zero and unit <em>`</em> 2 norm. Start with the residual <strong><em>r</em></strong> 0 = <strong>y</strong> − ̄ <strong>y</strong> , <em>β</em>^0 = ( <em>β</em> 1 <em>,β</em> 2 <em>,...,βp</em> ) = <strong>0</strong>.</li><li>Find the predictor <strong>x</strong> <em>j</em> most correlated with <strong><em>r</em></strong> 0 ; i.e., with largest value for |〈 <strong>x</strong> <em>j,</em> <strong><em>r</em></strong> 0 〉|. Call this value <em>λ</em> 0 , define the active setA={ <em>j</em> }, and <strong>X</strong> A, the matrix consisting of this single variable.</li><li>For <em>k</em> = 1 <em>,</em> 2 <em>,...,K</em> = min( <em>N</em> − 1 <em>,p</em> ) do:</li></ol><div class="language-"><pre><code>(a) Define the least-squares direction δ = λk^1 − 1 ( X T A X A)−^1 X T A r k − 1 , and de-\nfine the p -vector ∆ such that ∆A= δ , and the remaining elements are\nzero.\n(b) Move the coefficients β from βk −^1 in the direction ∆ toward their least-\nsquares solution on X A: β ( λ ) = βk −^1 + ( λk − 1 − λ )∆ for 0 &lt; λ ≤ λk − 1 ,\nkeeping track of the evolving residuals r ( λ ) = y − X β ( λ ) = r k − 1 −\n( λk − 1 − λ ) X ∆.\n(c) Keeping track of|〈 x `, r ( λ )〉|for ` / ∈ A, identify the largest value of λ\nat which a variable “catches up” with the active set; if the variable has\nindex j , that means|〈 x j, r ( λ )〉|= λ. This defines the next “knot” λk.\n(d) SetA=A∪{ j }, βk = β ( λk ) = βk −^1 + ( λk − 1 − λk )∆, and r k = y − X βk.\n</code></pre></div><ol start="4"><li>Return the sequence{ <em>λk, βk</em> } <em>K</em> 0.</li></ol><div class="language-"><pre><code>We make a few observations to clarify the steps in the algorithm. In step 3b,\nit is easy to check that|〈 x j, r ( λ )〉|= λ, ∀ j ∈ A—that is, the correlations\nremain tied along this path, and decrease to zero with λ. In fact β^0 = βk −^1 +\nλk − 1 ∆ is the least-squares coefficient vector corresponding to the subsetA.\nBy construction the coefficients in LAR change in a piecewise linear fash-\nion. Figure 5.9 [left panel] shows the LAR coefficient profile evolving as a\n</code></pre></div><h6 id="_120-optimization-methods"><a class="header-anchor" href="#_120-optimization-methods" aria-hidden="true">#</a> 120 OPTIMIZATION METHODS</h6><div class="language-"><pre><code>0 5 10 15\n</code></pre></div><div class="language-"><pre><code>−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>Least Angle Regression\n</code></pre></div><div class="language-"><pre><code>0 5 10 15 20\n</code></pre></div><div class="language-"><pre><code>−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>Lasso\n</code></pre></div><div class="language-"><pre><code>ℓ 1 ArcLength ℓ 1 ArcLength\n</code></pre></div><p><strong>Figure 5.9</strong> <em>Left panel shows the LAR coefficient profiles on the simulated data, as a function of theL</em>^1 <em>arc length. The right panel shows the lasso profile. They are identical until the red coefficient crosses zero at an arc length of about</em> 16_._</p><p>function of their <em><code>_ 1 arc length.^4 Note that we do not need to take small steps and recheck the correlations in step 3c. Variable _</code></em> “catching up” means that |〈 <strong>x</strong> <em><code>,_ **_r_** ( _λ_ )〉|= _λ_ , a pair of equations linear in _λ_. We solve for _λ_ for each _</code> /</em> ∈A and pick the largest (Exercise 5.9). The right panel of Figure 5.9 shows the lasso coefficient profiles on the same data. They are almost identical to those in the left panel, and differ for the first time when the pink coefficient passes back through zero. These observations lead to a simple modification in step 3c of the LAR algorithm that gives the entire lasso path, which is also piecewise-linear:</p><p>3(c)+ <em>lasso modification</em> : If a nonzero coefficient crosses zero before the next variable enters, drop it fromAand recompute the current joint least- squares direction.</p><p>Notice in the figure that the pink coefficient remains zero for a while, and then it becomes active again, but this time negative. We can give a heuristic argument for why these procedures are so similar. As observed, we have at any stage of the algorithm</p><div class="language-"><pre><code>x Tj ( y − X β ( λ )) = λ · sj, ∀ j ∈A , (5.58)\n</code></pre></div><p>where <em>sj</em> ∈{− 1 <em>,</em> 1 }indicates the sign of the inner-product, and <em>λ</em> is the com-</p><p>(^4) The <em>`</em> 1 arc-length of a differentiable curve{ <em>s</em> 7→ <em>β</em> ( <em>s</em> ) | <em>s</em> ∈ [0 <em>,S</em> ]}is given by TV( <em>β,S</em> ) =</p><h6 id="∫-s"><a class="header-anchor" href="#∫-s" aria-hidden="true">#</a> ∫ S</h6><p>0 ‖ <em>β</em> ̇( <em>s</em> )‖ 1 <em>ds</em> , where <em>β</em> ̇( <em>s</em> ) = <em>∂β</em> ( <em>s</em> ) <em>/∂s</em>. For the piecewise-linear LAR coeffi- cient profile, this amounts to summing the <em>`</em> 1 -norms of the changes in coefficients from step to step.</p><h6 id="alternating-direction-method-of-multipliers-121"><a class="header-anchor" href="#alternating-direction-method-of-multipliers-121" aria-hidden="true">#</a> ALTERNATING DIRECTION METHOD OF MULTIPLIERS 121</h6><p>mon value. Also by definition of the LAR active set,| <strong>x</strong> <em>Tk</em> ( <strong>y</strong> − <strong>X</strong> <em>β</em> ( <em>λ</em> ))|≤ <em>λ</em> ∀ <em>k</em> 6∈ A. Now consider the lasso criterion^5</p><div class="language-"><pre><code>R ( β ) =^12 ‖ y − X β ‖^22 + λ ‖ β ‖ 1. (5.59)\n</code></pre></div><p>LetBbe the active set of variables in the solution for a given value of <em>λ</em>. For these variables <em>R</em> ( <em>β</em> ) is differentiable, and the stationarity conditions give</p><div class="language-"><pre><code>x Tj ( y − X β ) = λ ·sign( βj ) , ∀ j ∈B. (5.60)\n</code></pre></div><p>Comparing (5.60) with (5.58), we see that they are identical only if the sign of <em>βj</em> matches the sign of the inner product. That is why the LAR algorithm and lasso start to differ when an active coefficient passes through zero; condi- tion (5.60) is violated for that variable, and it is removed from the active set Bin step 3(c)+. Exercise 5.9 shows that these equations imply a piecewise- linear coefficient profile as <em>λ</em> decreases, as was imposed in the LAR update. The stationarity conditions for the nonactive variables require that</p><div class="language-"><pre><code>| x Tk ( y − X β )|≤ λ, ∀ k 6∈B , (5.61)\n</code></pre></div><p>which again agrees with the LAR algorithm. The LAR algorithm exploits the fact that the coefficient paths for the lasso are piecewise linear. This property holds for a more general class of problems; see Rosset and Zhu (2007) for details.</p><h3 id="_5-7-alternating-direction-method-of-multipliers"><a class="header-anchor" href="#_5-7-alternating-direction-method-of-multipliers" aria-hidden="true">#</a> 5.7 Alternating Direction Method of Multipliers</h3><p>The <em>alternating direction method of multipliers</em> (ADMM) is a Lagrangian- based approach that has some attractive features for large-scale applications. It is based on a marriage of different ideas that developed over a long period of time. Here we provide a brief overview, referring the reader to Boyd et al. (2011) for a comprehensive discussion. Consider a problem of the form</p><div class="language-"><pre><code>minimize\nβ ∈R m,θ ∈R n\n</code></pre></div><div class="language-"><pre><code>f ( β ) + g ( θ ) subject to A β + B θ = c , (5.62)\n</code></pre></div><p>where <em>f</em> :R <em>m</em> →Rand <em>g</em> :R <em>n</em> →Rare convex functions, and <strong>A</strong> ∈R <em>d</em> × <em>m</em> and <strong>B</strong> ∈R <em>d</em> × <em>n</em> are (known) matrices of constraints, and <em>c</em> ∈R <em>d</em> is a constraint vector. To solve this problem we introduce a vector <em>μ</em> ∈R <em>d</em> of Lagrange multipliers associated with the constraint, and then consider the augmented Lagrangian</p><div class="language-"><pre><code>Lρ ( β,θ,μ ) : = f ( β ) + g ( θ ) +〈 μ, A β + B θ − c 〉+\nρ\n2\n</code></pre></div><div class="language-"><pre><code>‖ A β + B θ − c ‖^22 , (5.63)\n</code></pre></div><p>(^5) We have omitted the factor 1 <em>N</em> , to stay faithful to the original LAR procedure; all values of <em>λ</em> are hence larger by a factor of <em>N</em>.</p><h6 id="_122-optimization-methods"><a class="header-anchor" href="#_122-optimization-methods" aria-hidden="true">#</a> 122 OPTIMIZATION METHODS</h6><p>where <em>ρ &gt;</em> 0 is a small fixed parameter. The quadratic term involving <em>ρ</em> is an augmented Lagrangian that enforces the constraint in a smoother fashion. The ADMM algorithm is based on minimizing the augmented Lagrangian (5.63) successively over <em>β</em> and <em>θ</em> , and then applying a dual variable update to <em>μ</em>. Doing so yields the updates</p><div class="language-"><pre><code>βt +1= arg min\nβ ∈R m\n</code></pre></div><div class="language-"><pre><code>Lρ ( β,θt,μt ) (5.64a)\n</code></pre></div><div class="language-"><pre><code>θt +1= arg min\nθ ∈R n\n</code></pre></div><div class="language-"><pre><code>Lρ ( βt +1 ,θ,μt ) (5.64b)\n</code></pre></div><div class="language-"><pre><code>μt +1= μt + ρ\n</code></pre></div><h6 id="-206"><a class="header-anchor" href="#-206" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>A βt +1+ B θt +1− c\n</code></pre></div><h6 id="-207"><a class="header-anchor" href="#-207" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>, (5.64c)\n</code></pre></div><p>for iterations <em>t</em> = 0 <em>,</em> 1 <em>,</em> 2 <em>,...</em>. The update (5.64c) can be shown to be a dual ascent step for the Lagrange multiplier vector <em>μ</em>. Under relatively mild condi- tions, one can show that this procedure converges to an optimal solution to Problem (5.62). The ADMM framework has several advantages. First, convex problems with nondifferentiable constraints can be easily handled by the separation of parameters into <em>β</em> and <em>θ</em>. We illustrate this procedure via application to the lasso, as discussed in the example to follow. A second advantage of ADMM is its ability to break up a large problem into smaller pieces. For datasets with large number of observations we break up the data into blocks, and carry out the optimization over each block. As discussed in more detail in Exercise 5.12, constraints are included to ensure that the solution vectors delivered by the optimization over each data block agree with one another at convergence. In a similar way, the problem can be split up into feature blocks, and solved in a coordinated blockwise fashion.</p><p><em>Example 5.8. ADMM for the lasso.</em> The Lagrange form of the lasso can be expressed in equivalent form as</p><div class="language-"><pre><code>minimize\nβ ∈R p,θ ∈R p\n</code></pre></div><h6 id="-208"><a class="header-anchor" href="#-208" aria-hidden="true">#</a> {</h6><h6 id="_1-81"><a class="header-anchor" href="#_1-81" aria-hidden="true">#</a> 1</h6><h6 id="_2-58"><a class="header-anchor" href="#_2-58" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ y − X β ‖^22 + λ ‖ θ ‖ 1\n</code></pre></div><h6 id="-209"><a class="header-anchor" href="#-209" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>such that β − θ = 0. (5.65)\n</code></pre></div><p>When applied to this problem, the ADMM updates take the form</p><div class="language-"><pre><code>βt +1= ( X T X + ρ I )−^1 ( X T y + ρθt − μt )\nθt +1=S λ/ρ ( βt +1+ μt/ρ )\nμt +1= μt + ρ ( βt +1− θt +1).\n</code></pre></div><h6 id="_5-66"><a class="header-anchor" href="#_5-66" aria-hidden="true">#</a> (5.66)</h6><p>Thus, the algorithm involves a ridge regression update for <em>β</em> , a soft- thresholding step for <em>θ</em> and then a simple linear update for <em>μ</em>. The first step is the main work, and after an initial singular value decomposition of <strong>X</strong> , subsequent iterations can be done quickly. The initial SVD requiresO( <em>p</em>^3 ) operations, but can be done off-line, whereas subsequent iterations have cost O( <em>Np</em> ). Consequently, after the start-up phase, the cost per iteration is similar to coordinate descent or the composite gradient method. ♦</p><h6 id="minorization-maximization-algorithms-123"><a class="header-anchor" href="#minorization-maximization-algorithms-123" aria-hidden="true">#</a> MINORIZATION-MAXIMIZATION ALGORITHMS 123</h6><p><strong>5.8 Minorization-Maximization Algorithms</strong></p><p>In this section, we turn to a class of methods, known either as minorization- maximization or majorization-minimization (MM) algorithms, that are espe- cially useful for optimization of nonconvex functions. These belong to the class of auxiliary-variable methods, in that they are based on introducing extra vari- ables and using them to majorize (or upper bound) the objective function to be minimized. Although these methods apply more generally to constrained problems, here we describe them in application to a simple unconstrained problem of the form minimize <em>β</em> ∈R <em>pf</em> ( <em>β</em> ), where <em>f</em> :R <em>p</em> →Ris a (possibly) nonconvex function. A function Ψ :R <em>p</em> ×R <em>p</em> 7→R^1 majorizes the function <em>f</em> at a point <em>β</em> ∈R <em>p</em> if</p><div class="language-"><pre><code>f ( β )≤Ψ( β,θ ) for all θ ∈R p (5.67)\n</code></pre></div><p>with <em>equality</em> holding when <em>β</em> = <em>θ</em>. (Naturally, there is a corresponding defi- nition of minorization, with the inequality reversed in direction.) Figure 5.10 shows a schematic of a majorizing function.</p><div class="language-"><pre><code>θ β\n</code></pre></div><div class="language-"><pre><code>f ( β )\n</code></pre></div><div class="language-"><pre><code>Ψ( β,θ )\n</code></pre></div><p><strong>Figure 5.10</strong> <em>Il lustration of a majorizing function for use in an MM algorithm. The function</em> Ψ( <em>β,θ</em> ) <em>lies on or abovef</em> ( <em>β</em> ) <em>for al lβand is equal tof</em> ( <em>β</em> ) <em>whenβ</em> = <em>θ. The MM algorithm seeks to minimize the target functionfby solving a sequence of subproblems involving the majorizing function</em> Ψ <em>and the current iterate.</em></p><p>An MM algorithm for performing an unconstrained minimization of <em>f</em> in- volves initializing <em>β</em>^0 , and then updating via the recursion</p><div class="language-"><pre><code>βt +1= arg min\nβ ∈R p\n</code></pre></div><h6 id="ψ"><a class="header-anchor" href="#ψ" aria-hidden="true">#</a> Ψ</h6><h6 id="-210"><a class="header-anchor" href="#-210" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>β,βt\n</code></pre></div><h6 id="-211"><a class="header-anchor" href="#-211" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>for t = 0 , 1 , 2 ,.... (5.68)\n</code></pre></div><p>By the majorization property (5.67), this scheme generates a sequence for which the cost <em>f</em> ( <em>βt</em> ) is nonincreasing. In particular, we have</p><div class="language-"><pre><code>f ( βt ) = Ψ( βt,βt )\n</code></pre></div><div class="language-"><pre><code>( i )\n≥ Ψ( βt +1 ,βt )\n</code></pre></div><div class="language-"><pre><code>( ii )\n≥ f ( βt +1) , (5.69)\n</code></pre></div><h6 id="_124-optimization-methods"><a class="header-anchor" href="#_124-optimization-methods" aria-hidden="true">#</a> 124 OPTIMIZATION METHODS</h6><p>where inequality (i) uses the fact that <em>βt</em> +1is a minimizer of the function <em>β</em> 7→Ψ( <em>β,βt</em> ), and inequality (ii) uses the majorization property (5.67). If the original function <em>f</em> is strictly convex, it can be shown the MM algorithm converges to the global minimizer. There are different classes of majorizing functions that are useful for dif- ferent problems. In general, a good majorization function is one for which the update (5.68) is relatively easy to compute, at least relative to direct minimization of <em>f</em>. See Lange (2004) for more details.</p><p><em>Example 5.9. Proximal gradient as an MM algorithm.</em> Recall from Sec- tion 5.3.3 the proximal gradient algorithm that can be applied to cost functions that decompose as a sum <em>f</em> = <em>g</em> + <em>h</em> , where <em>g</em> is convex and differentiable, and <em>h</em> is convex and (potentially) nondifferentiable. By applying a second-order Taylor series expansion (with remainder) to <em>g</em> , we obtain</p><div class="language-"><pre><code>f ( β ) = g ( β ) + h ( β )\n</code></pre></div><div class="language-"><pre><code>= g ( θ ) +〈∇ g ( θ ) ,θ − β 〉+\n</code></pre></div><h6 id="_1-82"><a class="header-anchor" href="#_1-82" aria-hidden="true">#</a> 1</h6><h6 id="_2-59"><a class="header-anchor" href="#_2-59" aria-hidden="true">#</a> 2</h6><h6 id="〈"><a class="header-anchor" href="#〈" aria-hidden="true">#</a> 〈</h6><div class="language-"><pre><code>θ − β, ∇^2 g\n</code></pre></div><h6 id="-212"><a class="header-anchor" href="#-212" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>β ′\n</code></pre></div><h6 id="-213"><a class="header-anchor" href="#-213" aria-hidden="true">#</a> )(</h6><div class="language-"><pre><code>θ − β\n</code></pre></div><h6 id="〉"><a class="header-anchor" href="#〉" aria-hidden="true">#</a> )〉</h6><div class="language-"><pre><code>+ h ( β ) ,\n</code></pre></div><p>where <em>β</em> ′= <em>sβ</em> + (1− <em>s</em> ) <em>θ</em> for some <em>s</em> ∈[0 <em>,</em> 1]. It can be verified that Lipschitz condition (5.27) on the gradient∇ <em>g</em> implies a uniform upper bound on the Hessian, namely∇^2 <em>g</em> ( <em>β</em> ′) <em>L</em> <strong>I</strong> <em>p</em> × <em>p</em> , from which we obtain the inequality</p><div class="language-"><pre><code>f ( β )≤ g ( θ ) +〈∇ g ( θ ) , θ − β 〉+\n</code></pre></div><h6 id="l"><a class="header-anchor" href="#l" aria-hidden="true">#</a> L</h6><h6 id="_2-60"><a class="header-anchor" href="#_2-60" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ θ − β ‖^22 + h ( β )\n︸ ︷︷ ︸\nΨ( β,θ )\n</code></pre></div><h6 id="-214"><a class="header-anchor" href="#-214" aria-hidden="true">#</a> ,</h6><p>with equality holding when <em>θ</em> = <em>β</em>. Thus, we see that the proximal gradi- ent method can be viewed as an MM algorithm with a particular choice of majorizing function. ♦</p><p>Apart from a direct bound on the Hessian, there are other ways of deriving majorizing functions, For example, Jensen’s inequality can be used to derive the usual EM algorithm as an instance of an MM algorithm (Hunter and Lange 2004, Wu and Lange 2010). As we discuss in Chapter 8, MM algorithms turn out to be useful in procedures for sparse multivariate analysis.</p><h3 id="_5-9-biconvexity-and-alternating-minimization"><a class="header-anchor" href="#_5-9-biconvexity-and-alternating-minimization" aria-hidden="true">#</a> 5.9 Biconvexity and Alternating Minimization</h3><p>Recall the class of coordinate descent algorithms discussed in Section 5.4. Algorithms of this form are also useful for optimizing a class of (potentially) nonconvex functions known as biconvex functions. A function <em>f</em> :R <em>m</em> ×R <em>n</em> →R is <em>biconvex</em> if for each <em>β</em> ∈R <em>n</em> , the function <em>α</em> 7→ <em>f</em> ( <em>α,β</em> ) is convex, and for each <em>α</em> ∈R <em>m</em> , the function <em>β</em> 7→ <em>f</em> ( <em>α,β</em> ) is convex. Of course, any function that is jointly convex in the pair ( <em>α,β</em> ) is also biconvex. But a function can be biconvex without being jointly convex. For instance, consider the biconvex function <em>f</em> ( <em>α,β</em> ) = (1− <em>αβ</em> )^2 for| <em>α</em> |≤ 2 <em>,</em> | <em>β</em> |≤ 2_._ (5.70)</p><h6 id="biconvexity-and-alternating-minimization-125"><a class="header-anchor" href="#biconvexity-and-alternating-minimization-125" aria-hidden="true">#</a> BICONVEXITY AND ALTERNATING MINIMIZATION 125</h6><p>As illustrated in Figure 5.11, it is convex when sliced along lines parallel to the axes, as required by the definition of biconvexity, but other slices can lead to nonconvex functions.</p><div class="language-"><pre><code>−2 −1 0 1 2\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>5\n</code></pre></div><div class="language-"><pre><code>10\n</code></pre></div><div class="language-"><pre><code>15\n</code></pre></div><div class="language-"><pre><code>20\n</code></pre></div><div class="language-"><pre><code>25\n</code></pre></div><div class="language-"><pre><code>−2 −1 0 1 2\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>6\n</code></pre></div><div class="language-"><pre><code>8\n</code></pre></div><div class="language-"><pre><code>−2 −1 0 1 2\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>6\n</code></pre></div><div class="language-"><pre><code>8\n</code></pre></div><div class="language-"><pre><code>α\n</code></pre></div><div class="language-"><pre><code>α\n</code></pre></div><div class="language-"><pre><code>β\n</code></pre></div><div class="language-"><pre><code>β\n</code></pre></div><div class="language-"><pre><code>f(\n</code></pre></div><div class="language-"><pre><code>α,\n</code></pre></div><div class="language-"><pre><code>2)\n</code></pre></div><div class="language-"><pre><code>f(\n</code></pre></div><div class="language-"><pre><code>−\n1 ,β\n</code></pre></div><div class="language-"><pre><code>)\n</code></pre></div><div class="language-"><pre><code>f(\n</code></pre></div><div class="language-"><pre><code>t,t\n</code></pre></div><div class="language-"><pre><code>)\n</code></pre></div><div class="language-"><pre><code>t\n</code></pre></div><div class="language-"><pre><code>f(\nα,\nβ)\n</code></pre></div><p><strong>Figure 5.11</strong> <em>Example of a biconvex function. Shown in the top left is the function f</em> ( <em>α,β</em> ) = (1− <em>αβ</em> )^2 <em>over the range</em> − 2 ≤ <em>α,β</em> ≤ 2_. The top right and bottom left panels show slices of the function forβ_ = 2 <em>andα</em> =− 1_. The bottom right panel shows the function along the lineα_ = <em>β</em> = <em>t.</em></p><p>More generally, letA ⊆R <em>m</em> andB ⊆R <em>n</em> be two nonempty and convex sets, and letC ⊆A×B. For each fixed <em>α</em> ∈Aand <em>β</em> ∈B, the sets</p><div class="language-"><pre><code>C α : ={ β ∈B |( α,β )∈C} , and C β : ={ α ∈A |( α,β )∈C} (5.71)\n</code></pre></div><p>are called the <em>α</em> and <em>β</em> sections ofC. The setC ⊆ A×Bis called a <em>biconvex set</em> if the sectionC <em>α</em> is convex for each <em>α</em> ∈ A, and the sectionC <em>β</em> is convex for every <em>β</em> ∈ B. Given a biconvex setC, a function <em>f</em> :C →Ris a <em>biconvex function</em> if the function <em>α</em> 7→ <em>f</em> ( <em>α,β</em> ) is convex in <em>α</em> for each fixed <em>β</em> ∈B, and the function <em>β</em> 7→ <em>f</em> ( <em>α,β</em> ) is convex in <em>β</em> for each fixed <em>α</em> ∈A. Given these ingredients, a biconvex optimization problem has the form minimize( <em>α,β</em> )∈C <em>f</em> ( <em>α,β</em> ), where the setCis biconvex onA×B, and the objective function is biconvex onC. The most obvious method for solving a biconvex optimization problem is</p><h6 id="_126-optimization-methods"><a class="header-anchor" href="#_126-optimization-methods" aria-hidden="true">#</a> 126 OPTIMIZATION METHODS</h6><p>based on <em>Alternate Convex Search</em> (ACS), which is simply block coordinate descent applied to the <em>α</em> and <em>β</em> blocks:</p><p>(a) Initialize ( <em>α</em>^0 <em>,β</em>^0 ) at some point inC.</p><p>(b) For iterations <em>t</em> = 0 <em>,</em> 1 <em>,</em> 2 <em>,...</em> :</p><ol><li>Fix <em>β</em> = <em>βt</em> , and perform the update <em>αt</em> +1∈arg min <em>α</em> ∈C <em>βtf</em> ( <em>α,βt</em> ).</li><li>Fix <em>α</em> = <em>αt</em> +1, and perform the update <em>βt</em> +1∈arg min <em>β</em> ∈C <em>αt</em> +1 <em>f</em> ( <em>αt</em> +1 <em>,β</em> ).</li></ol><p>Given the biconvex structure, each of the two updates involve solving a convex optimization problem. The ACS procedure will be efficient as long as these convex sub-problems can be solved relatively quickly. By construction, the sequence of function values{ <em>f</em> ( <em>αt,βt</em> )}∞ <em>t</em> =0is nonin- creasing. Consequently, if <em>f</em> is bounded from below overC, then the function values converge to some limiting value. We note that this form of convergence is relatively weak, and only ensures that the function values converge. The so- lution sequence{( <em>αt,βt</em> }may not converge, and in some cases may diverge to infinity. Assuming convergence, to what does the solution sequence converge? Since a biconvex function <em>f</em> need not be convex in general, we cannot expect it to converge to the global minimum. All we can say in general is that if it converges, it converges to a partial optimum. More specifically, we say that ( <em>α</em> ∗ <em>,β</em> ∗)∈Cis a <em>partial optimum</em> if</p><div class="language-"><pre><code>f ( α ∗ ,β ∗)≤ f ( α ∗ ,β ) for all β ∈C α ∗, and\nf ( α ∗ ,β ∗)≤ f ( α,β ∗) for all α ∈C β ∗.\n</code></pre></div><p><em>Example 5.10. Alternating subspace algorithm.</em> One biconvex problem in which convergence of ACS can be fully characterized is the alternating subspace al- gorithm for computing the maximal singular vectors/value of a matrix. Given a matrix <strong>X</strong> ∈R <em>m</em> × <em>n</em> , consider the problem of finding the best rank-one ap- proximation in the Frobenius norm.^6 This approximation problem can be formulated in terms of minimizing the objective function</p><div class="language-"><pre><code>f ( α,β,s ) =‖ X − sαβT ‖^2 F (5.72)\n</code></pre></div><p>over vectors <em>α</em> ∈R <em>m</em> and <em>β</em> ∈R <em>n</em> , with‖ <em>α</em> ‖ 2 =‖ <em>β</em> ‖ 2 = 1, and a scalar <em>s &gt;</em> 0. The ACS procedure for this problem starts with any random unit- norm initialization for <em>β</em>^0 , and then for iterations <em>t</em> = 1 <em>,</em> 2 <em>,...</em> , it performs the updates</p><div class="language-"><pre><code>αt =\n</code></pre></div><div class="language-"><pre><code>X βt −^1\n‖ X βt −^1 ‖ 2\n</code></pre></div><div class="language-"><pre><code>, and βt =\n</code></pre></div><div class="language-"><pre><code>X Tαt\n‖ X Tαt ‖ 2\n</code></pre></div><h6 id="_5-73"><a class="header-anchor" href="#_5-73" aria-hidden="true">#</a> . (5.73)</h6><p>The scalar <em>s</em> can be computed as <em>s</em> =‖ <strong>X</strong> <em>βt</em> ‖ 2 at convergence. It can be shown (see Exercise 5.13 that as long as <em>β</em>^0 is not orthogonal to the largest right</p><p>(^6) The Frobenius norm of a matrix is the Euclidean norm applied to its vectorized version.</p><h6 id="screening-rules-127"><a class="header-anchor" href="#screening-rules-127" aria-hidden="true">#</a> SCREENING RULES 127</h6><p>singular vector, the iterates ( <em>αt,βt</em> ) converge to the left and right singular vectors of <strong>X</strong> corresponding to the largest singular value of <strong>X</strong>. The procedure is related to the <em>power method</em> for finding the largest eigen- vector of a symmetric positive semi-definite matrix. The <em>βt</em> iterates for the right singular vector have the form</p><div class="language-"><pre><code>βt +1=\n</code></pre></div><div class="language-"><pre><code>X T X βt\n‖ X T X βt ‖ 2\n</code></pre></div><h6 id="_5-74"><a class="header-anchor" href="#_5-74" aria-hidden="true">#</a> , (5.74)</h6><p>with similar updates for <em>αt</em> in terms of <strong>XX</strong> <em>T</em>. Consequently, the procedure simply “powers up” the operator <strong>X</strong> <em>T</em> <strong>X</strong> , with the normalization driving all but the largest eigenvalue to zero. See De Leeuw (1994) and Golub and Loan (1996,§7.3) for further details on the power method. ♦</p><p>In Chapter 7, we present Algorithm 7.2 on page 189 as another example of an ACS procedure.</p><h3 id="_5-10-screening-rules"><a class="header-anchor" href="#_5-10-screening-rules" aria-hidden="true">#</a> 5.10 Screening Rules</h3><p>As seen in Section 5.6, inner products play an important role in the lasso problem. For simplicity we assume all variables are mean centered (so we can ignore the intercept), and we consider solving the lasso problem^7</p><div class="language-"><pre><code>minimize\nβ ∈R p\n</code></pre></div><h6 id="_1-83"><a class="header-anchor" href="#_1-83" aria-hidden="true">#</a> 1</h6><h6 id="_2-61"><a class="header-anchor" href="#_2-61" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ y − X β ‖^22 + λ ‖ β ‖ 1 (5.75)\n</code></pre></div><p>with a decreasing sequence of values for <em>λ</em>. The first variable to enter the model has largest absolute inner-product <em>λ</em> max= max <em>j</em> |〈 <strong>x</strong> <em>j,</em> <strong>y</strong> 〉|, which also defines the entry value for <em>λ</em>. Also, at any stage, all variables <strong>x</strong> <em>j</em> in the active set have|〈 <strong>x</strong> <em>j,</em> <strong>y</strong> − <strong>y</strong> ˆ <em>λ</em> 〉|= <em>λ</em> , and all those out have smaller inner-products with the residuals. Hence one might expect a priori that predictors having small inner products with the response are not as likely to have a nonzero coefficient as compared to those with larger inner products. Based on this intuition, one might be able to eliminate predictors from the problem, and thereby reduce the computational load. For example, in some genomic applications we might have millions of variables (SNPs), and anticipate fitting models with only a handful of terms. In this section, we discuss screening rules that exploit this intuition, and have the potential to speed up the computation substantially while still delivering the exact numerical solution. We begin our discussion with the “dual polytope projection” (DPP) rule (Wang, Lin, Gong, Wonka and Ye 2013). Suppose we wish to compute a lasso solution at <em>λ &lt; λ</em> max. The DPP rule discards the <em>jth</em> variable if</p><div class="language-"><pre><code>| x Tj y | &lt; λ max−‖ x j ‖ 2 ‖ y ‖ 2\nλ max− λ\nλ\n</code></pre></div><h6 id="_5-76"><a class="header-anchor" href="#_5-76" aria-hidden="true">#</a> (5.76)</h6><p>(^7) In this section we have omitted the 1 <em>N</em> in the first part of the objective (to match the referenced formulas); this increases the scale of <em>λ</em> by a factor <em>N</em>.</p><h6 id="_128-optimization-methods"><a class="header-anchor" href="#_128-optimization-methods" aria-hidden="true">#</a> 128 OPTIMIZATION METHODS</h6><p>It may come as a surprise that such a rule can work, as it surprised us when we first saw it. We know that in a linear regression, a predictor can be insignificant on its own, but can become significant when included in the model with other predictors. It seems that the same phenomenon should occur with the lasso. In fact, there is no contradiction, and a similar rule applies at any stage of the regularization path (not just the start). Suppose we have the lasso solution <em>β</em> ˆ( <em>λ</em> ′) at <em>λ</em> ′, and we wish to screen variables for the solution at <em>λ &lt; λ</em> ′. Then if</p><div class="language-"><pre><code>∣\n∣ x Tj\n</code></pre></div><h6 id="-215"><a class="header-anchor" href="#-215" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>y − X β ˆ( λ ′)\n</code></pre></div><h6 id="∣-8"><a class="header-anchor" href="#∣-8" aria-hidden="true">#</a> )∣</h6><div class="language-"><pre><code>∣ &lt; λ ′−‖ x j ‖ 2 ‖ y ‖ 2 λ\n</code></pre></div><div class="language-"><pre><code>′− λ\nλ\n</code></pre></div><h6 id="_5-77"><a class="header-anchor" href="#_5-77" aria-hidden="true">#</a> , (5.77)</h6><p>variable <em>j</em> is not part of the active set at <em>λ</em>. We refer to this as the sequential DPP rule. Figure 5.12 shows the performance of this rule on a simulated example with 5000 predictors (details in caption). The global DPP applies rule (5.76) for all values of <em>λ</em> , and we can see it quickly runs out of steam. By the time <em>λ</em> is small enough to admit 8 predictors into the model, all 5000 predictors survive the screen. But the sequential DPP rule is much more aggressive, and even with 250 predictors in the model, only 1200 need to be considered. So the sequential screening rule (5.77) works much better if <em>λ</em> ′and <em>λ</em> are close together. We derive the lasso dual and the DPP rules in Appendix B on Page 132. In order to achieve even better performance, it is natural to consider screen- ing rules that are less conservative, and allow for occasional failures. Such rules can be incorporated as part of an overall strategy that still yields the exact so- lution upon termination. A variant of the global DPP rule (5.76) is the <em>global strong rule</em> , which discards predictor <em>j</em> whenever</p><div class="language-"><pre><code>| x Tj y | &lt; λ −( λ max− λ ) = 2 λ − λ max. (5.78)\n</code></pre></div><p>This tends to discard more predictors than the global DPP rule (compare blue with orange points in Figure 5.12.) Similarly the <em>sequential strong rule</em> discards the <em>jth</em> predictor from the optimization problem at <em>λ</em> if ∣ ∣ <strong>x</strong> <em>Tj</em> ( <strong>y</strong> − <strong>X</strong> <em>β</em> ˆ( <em>λ</em> ′))</p><h6 id="∣-9"><a class="header-anchor" href="#∣-9" aria-hidden="true">#</a> ∣</h6><div class="language-"><pre><code>∣ &lt; 2 λ − λ ′. (5.79)\n</code></pre></div><p>Intuitively, the active set will include predictors that can achieve inner-product <em>λ</em> with the residuals. So we include all those that achieve inner product close to <em>λ</em> using the <em>current residuals atλ</em> ′ <em>&gt; λ</em> , where close is defined by the gap <em>λ</em> ′− <em>λ</em>. As with the sequential DPP rule, the sequential strong rule is based on solving the lasso over a grid of decreasing <em>λ</em> values. Figure 5.12 includes the global and sequential strong rules. In both cases they dominate the DPP coun- terparts. Neither of the strong rules make any errors in this example, where an error means that it discards some predictor with a nonzero coefficient in the</p><h6 id="screening-rules-129"><a class="header-anchor" href="#screening-rules-129" aria-hidden="true">#</a> SCREENING RULES 129</h6><div class="language-"><pre><code>0 50 100 150 200 250\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>1000\n</code></pre></div><div class="language-"><pre><code>2000\n</code></pre></div><div class="language-"><pre><code>3000\n</code></pre></div><div class="language-"><pre><code>4000\n</code></pre></div><div class="language-"><pre><code>5000\n</code></pre></div><div class="language-"><pre><code>Number of Predictors in Model\n</code></pre></div><div class="language-"><pre><code>Number of Predictors after Filtering\n</code></pre></div><div class="language-"><pre><code>global DPP\nglobal STRONG\nsequential DPP\nsequential STRONG\n</code></pre></div><div class="language-"><pre><code>Percent Variance Explained\n0 0.15 0.3 0.49 0.67 0.75 0.82 0.89 0.96 0.97 0.99 1 1 1\n</code></pre></div><p><strong>Figure 5.12</strong> <em>Lasso regression: Results of different rules applied to a simulated dataset. There areN</em> = 200 <em>observations andp</em> = 5000 <em>uncorrelated Gaussian pre- dictors; one-quarter of the true coefficients are nonzero. Shown are the number of predictors left after screening at each stage, plotted against the number of predictors in the model for a given value ofλ. The value ofλis decreasing as we move from left to right. In the plots, we are fitting along a path of 100 decreasingλvalues equal ly spaced on the log-scale, A broken line with unit slope is added for reference. The proportion of variance explained by the model is shown along the top of the plot. There were no violations for either of the strong rules.</em></p><h6 id="_130-optimization-methods"><a class="header-anchor" href="#_130-optimization-methods" aria-hidden="true">#</a> 130 OPTIMIZATION METHODS</h6><p>actual solution. The sequential strong rule (5.79) has remarkable performance, discarding almost all of the redundant predictors. We now give further motivation for the strong rules (Tibshirani, Bien, Friedman, Hastie, Simon, Taylor and Tibshirani 2 2012). Suppose that predic- tor <em>j</em> is not in the model at <em>λ</em> = <em>λ</em> max. The KKT conditions for the lasso then ensure that| <strong>x</strong> <em>Tj</em> <strong>y</strong> | <em>&lt; λ</em> max, so that the global rule (5.78) can be interpreted as</p><p>dictating that as we move from <em>λ</em> maxto <em>λ</em> , the inner product| <strong>x</strong> <em>Tj</em> ( <strong>y</strong> − <strong>X</strong> <em>β</em> ˆ( <em>λ</em> ))| can increase by at most <em>λ</em> max− <em>λ</em>. Consequently, if the inner product is below the strong bound <em>λ</em> −( <em>λ</em> max− <em>λ</em> ), it cannot reach the necessary level <em>λ</em> for inclusion in the model, where we have again used the KKT conditions in our reasoning. If we define <em>cj</em> ( <em>λ</em> ) : = <strong>x</strong> <em>Tj</em> ( <strong>y</strong> − <strong>X</strong> <em>β</em> ˆ( <em>λ</em> )), then for either the global or sequential strong rules to hold, it would be sufficient to have</p><div class="language-"><pre><code>∣\n∣\n∣\n</code></pre></div><div class="language-"><pre><code>dcj ( λ )\ndλ\n</code></pre></div><h6 id="∣-10"><a class="header-anchor" href="#∣-10" aria-hidden="true">#</a> ∣</h6><h6 id="∣-11"><a class="header-anchor" href="#∣-11" aria-hidden="true">#</a> ∣</h6><h6 id="∣≤-1-5-80"><a class="header-anchor" href="#∣≤-1-5-80" aria-hidden="true">#</a> ∣≤^1 , (5.80)</h6><p>assuming that this derivative exists.^8 Now the KKT conditions at <em>λ</em> are</p><div class="language-"><pre><code>cj ( λ ) = λsj ( λ ) , for j = 1 , 2 ,...,p , (5.81)\n</code></pre></div><p>where <em>sj</em> ( <em>λ</em> ) = sign( <em>β</em> ˆ <em>j</em> ( <em>λ</em> )) if <em>β</em> ˆ <em>j</em> ( <em>λ</em> ) 6 = 0 and <em>sj</em> ( <em>λ</em> )∈[− 1 <em>,</em> 1] if <em>β</em> ˆ <em>j</em> ( <em>λ</em> ) = 0. By the chain rule</p><div class="language-"><pre><code>dcj ( λ )\ndλ\n</code></pre></div><div class="language-"><pre><code>= sj ( λ ) + λ ·\ndsj ( λ )\ndλ\n</code></pre></div><h6 id="-216"><a class="header-anchor" href="#-216" aria-hidden="true">#</a> .</h6><p>If we ignore the second term, then we have| <em>dcdλj</em> ( <em>λ</em> )|≤1. Now the second term equals zero when a variable has a nonzero coefficient in an interval of <em>λ</em> values, for then <em>sj</em> ( <em>λ</em> ) is constant (equaling±1). In addition, the slope condition (5.80) always holds if ( <strong>X</strong> <em>T</em> <strong>X</strong> )−^1 is diagonally dominant (Tibshirani et al. 2012), a condition meaning that the predictors are nearly uncorrelated. In general, however, the slope condition can fail over short stretches of <em>λ</em> , and in these instances, the strong rules can fail (i.e., discard predictors in error). However these failures are rare, and are virtually nonexistent when <em>p</em>  <em>N</em>. In summary, we have found empirically that the strong rules, and especially the sequential strong rule (5.79) seem to be very good heuristics for discarding variables. This is the case in the lasso, lasso-penalized logistic regression, and the elastic net. One can use the sequential strong rule to save computation time, without sacrificing the exact solution, as follows. We compute the solution along a fine grid of decreasing <em>λ</em> values. For each value of <em>λ</em> , the screening rule is applied, yielding a subset of the predictors. Then the problem is solved using only this subset. The KKT conditions (5.81) for all predictors are then checked. If they are satisfied, we are done. Otherwise the predictors that violate the conditions</p><p>(^8) The arguments here are only heuristic, because <em>dcj</em> ( <em>λ</em> ) <em>/dλ</em> and <em>dsj</em> ( <em>λ</em> ) <em>/dλ</em> discussed below it do not exist at <em>β</em> ˆ <em>j</em> ( <em>λ</em> ) = 0.</p><h6 id="bibliographic-notes-131"><a class="header-anchor" href="#bibliographic-notes-131" aria-hidden="true">#</a> BIBLIOGRAPHIC NOTES 131</h6><p>are added to the active set and the problem is solved again. In principle this must be iterated until no violations occur. This approach is effective computationally because violations of the strong rule turn out to be rare, especially when <em>p</em>  <em>N</em>. Tibshirani et al. (2012) implement these rules for the coordinate descent approach inglmnetand the generalized gradient and Nesterov first-order methods. They report speedup factors in the range from 2 to 80, depending on the setting. Finally, suppose instead that we are interested in a more general convex problem of the form</p><div class="language-"><pre><code>minimize\nβ\n</code></pre></div><h6 id="-217"><a class="header-anchor" href="#-217" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>f ( β ) + λ\n</code></pre></div><div class="language-"><pre><code>∑ r\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>cj ‖ βj ‖ pj\n</code></pre></div><h6 id="-218"><a class="header-anchor" href="#-218" aria-hidden="true">#</a> }</h6><h6 id="_5-82"><a class="header-anchor" href="#_5-82" aria-hidden="true">#</a> . (5.82)</h6><p>Here <em>f</em> is a convex and differentiable function, and <em>β</em> = ( <em>β</em> 1 <em>,β</em> 2 <em>,...βr</em> ) with each <em>βj</em> being a scalar or a vector. Also <em>λ</em> ≥ 0, and <em>cj</em> ≥0, <em>pj</em> ≥1 for each <em>j</em> = 1 <em>,...r</em>. Then given <em>λ</em> ′ <em>&gt; λ</em> , the sequential strong rule for discarding predictor <em>j</em> takes the form ∥ ∥∇ <em>jf</em></p><h6 id="-219"><a class="header-anchor" href="#-219" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>β ˆ( λ ′)\n</code></pre></div><h6 id="∥-3"><a class="header-anchor" href="#∥-3" aria-hidden="true">#</a> )∥</h6><h6 id="∥-4"><a class="header-anchor" href="#∥-4" aria-hidden="true">#</a> ∥</h6><div class="language-"><pre><code>qj&lt; cj (2 λ − λ\n</code></pre></div><h6 id="′-5-83"><a class="header-anchor" href="#′-5-83" aria-hidden="true">#</a> ′) , (5.83)</h6><p>where∇ <em>jf</em> ( <em>β</em> ˆ) = ( <em>∂f</em> ( <em>β</em> ˆ) <em>/∂βj</em> 1 <em>,...∂f</em> ( <em>β</em> ˆ) <em>/∂βjm</em> ) where 1 <em>/pj</em> + 1 <em>/qj</em> = 1 (i.e., ‖·‖ <em>pj</em> and‖·‖ <em>qj</em> are dual norms). The rule (5.83) can be applied to a wide variety of problems, including logistic regression and other generalized linear models, the group lasso and the graphical lasso.</p><h3 id="bibliographic-notes-2"><a class="header-anchor" href="#bibliographic-notes-2" aria-hidden="true">#</a> Bibliographic Notes</h3><p>The behavior of descent algorithms, including convergence proofs for methods based on appropriate stepsize selection rules, such as limited minimization or the Armijo rule, is a classical subject in optimization; see Chapters 1 and 2 of Bertsekas (1999) for more details. Further background on Lagrangian methods and duality can be found in Bertsekas (1999), as well as Boyd and Vandenberghe (2004). Rockafellar (1996) provides a more advanced treat- ment of convex duality and convex analysis. Nesterov (2007) derives and an- alyzes the generalized gradient method (5.21) for composite objectives; see also Nesterov’s book (2004) for related analysis of projected gradient meth- ods. Minorization-maximization procedures, also known as auxiliary function methods, are discussed in Lange (2004) and Hunter and Lange (2004). Gorski, Pfeuffer and Klamroth (2007) provide an overview of biconvex functions, and alternating algorithms for optimizing them. El Ghaoui, Vial- lon and Rabbani (2010) introduced the use of screening rules such as (5.76); inspired by this work, we derived a very similar formula, and which led to our development of the strong rules in Section 5.10. However, the more recent DPP rules of Wang, Lin, Gong, Wonka and Ye (2013) dominate these earlier safe rules, and provide a simple sequential formula. Fu (1998) was an early proponent of coordinate descent for the lasso.</p><h6 id="_132-optimization-methods"><a class="header-anchor" href="#_132-optimization-methods" aria-hidden="true">#</a> 132 OPTIMIZATION METHODS</h6><p><strong>Appendix A: The Lasso Dual</strong></p><p>In this appendix, we derive a useful dual of the lasso primal problem (2.5), which we write in a slightly more convenient form</p><div class="language-"><pre><code>Lasso Primal: minimize\nβ ∈R p\n</code></pre></div><h6 id="_1-84"><a class="header-anchor" href="#_1-84" aria-hidden="true">#</a> 1</h6><h6 id="_2-62"><a class="header-anchor" href="#_2-62" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ y − X β ‖^22 + λ ‖ β ‖ 1. (5.84)\n</code></pre></div><p>Introducing the residual vector <strong><em>r</em></strong> = <strong>y</strong> − <strong>X</strong> <em>β</em> , we can rewrite the primal Equa- tion (5.84) as</p><div class="language-"><pre><code>minimize\nβ ∈R p\n</code></pre></div><h6 id="_1-85"><a class="header-anchor" href="#_1-85" aria-hidden="true">#</a> 1</h6><h6 id="_2-63"><a class="header-anchor" href="#_2-63" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ r ‖^22 + λ ‖ β ‖ 1 subject to r = y − X β. (5.85)\n</code></pre></div><p>Letting <strong><em>θ</em></strong> ∈R <em>N</em> denote a Lagrange multiplier vector, the Lagrangian of this problem can be written as</p><div class="language-"><pre><code>L ( β, r , θ ) : =\n</code></pre></div><h6 id="_1-86"><a class="header-anchor" href="#_1-86" aria-hidden="true">#</a> 1</h6><h6 id="_2-64"><a class="header-anchor" href="#_2-64" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ r ‖^22 + λ ‖ β ‖ 1 − θ T ( r − y + X β ). (5.86)\n</code></pre></div><p>The dual objective is derived by minimizing this expression (5.86) with respect to <em>β</em> and <strong><em>r</em></strong>. Isolating those terms involving <em>β</em> , we find</p><div class="language-"><pre><code>min\nβ ∈R p\n− θ T X β + λ ‖ β ‖ 1 =\n</code></pre></div><h6 id="-220"><a class="header-anchor" href="#-220" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>0 if‖ X T θ ‖∞≤ λ\n−∞ otherwise\n</code></pre></div><h6 id="_5-87"><a class="header-anchor" href="#_5-87" aria-hidden="true">#</a> (5.87)</h6><p>where‖ <strong>X</strong> <em>T</em> <strong><em>θ</em></strong> ‖∞= max <em>j</em> | <strong>x</strong> <em>Tj</em> <strong><em>θ</em></strong> |. Next we isolate terms involving <strong><em>r</em></strong> and find</p><div class="language-"><pre><code>min\nr\n</code></pre></div><h6 id="_1-87"><a class="header-anchor" href="#_1-87" aria-hidden="true">#</a> 1</h6><h6 id="_2-65"><a class="header-anchor" href="#_2-65" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ r ‖^22 − θ T r =−\n</code></pre></div><h6 id="_1-88"><a class="header-anchor" href="#_1-88" aria-hidden="true">#</a> 1</h6><h6 id="_2-66"><a class="header-anchor" href="#_2-66" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>θ T θ , (5.88)\n</code></pre></div><p>with <strong><em>r</em></strong> = <strong><em>θ</em></strong>. Substituting relations (5.87) and (5.88) into the Lagrangian rep- resentation (5.86), we obtain</p><div class="language-"><pre><code>Lasso Dual: maximize\nθ\n</code></pre></div><h6 id="_1-89"><a class="header-anchor" href="#_1-89" aria-hidden="true">#</a> 1</h6><h6 id="_2-67"><a class="header-anchor" href="#_2-67" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>{‖ y ‖^22 −‖ y − θ ‖^22 }subject to‖ X T θ ‖∞≤ λ. (5.89)\n</code></pre></div><p>Overall, this form of the lasso dual amounts to projecting <strong>y</strong> onto the feasi- ble setF <em>λ</em> ={ <strong><em>θ</em></strong> ∈R <em>N</em> | ‖ <strong>X</strong> <em>T</em> <strong><em>θ</em></strong> ‖∞≤ <em>λ</em> }.F <em>λ</em> is the intersection of the 2 <em>p</em> half- spaces defined by{| <strong>x</strong> <em>Tj</em> <strong><em>θ</em></strong> |≤ <em>λ</em> } <em>pj</em> =1, a convex-polytope inR <em>N</em>. In the language of Section 5.3.3, the solution is given by the proximal map <strong><em>θ</em></strong> ∗= <strong>prox</strong> <em>I</em> (F <em>λ</em> )( <strong>y</strong> ). Figure 5.13 provides an illustration of this geometric interpretation.</p><p><strong>Appendix B: Derivation of the DPP Rule</strong></p><p>Here we derive the sequential DPP screening rule (5.77); our proof follows that in Wang, Lin, Gong, Wonka and Ye (2013). We first modify the lasso dual via a change of variables <strong><em>φ</em></strong> = <strong><em>θ</em></strong> <em>/λ</em> , leading to</p><div class="language-"><pre><code>maximize\nθ\n</code></pre></div><h6 id="_1-90"><a class="header-anchor" href="#_1-90" aria-hidden="true">#</a> 1</h6><h6 id="_2-68"><a class="header-anchor" href="#_2-68" aria-hidden="true">#</a> 2</h6><h6 id="-221"><a class="header-anchor" href="#-221" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>‖ y ‖^22 − λ^2 ‖ y /λ − φ ‖^22\n</code></pre></div><h6 id="-222"><a class="header-anchor" href="#-222" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>subject to‖ X T φ ‖∞≤ 1. (5.90)\n</code></pre></div><h6 id="appendix-133"><a class="header-anchor" href="#appendix-133" aria-hidden="true">#</a> APPENDIX 133</h6><div class="language-"><pre><code>|xT 1 θ|=λ\n</code></pre></div><div class="language-"><pre><code>|xT 2 θ|=λ\n</code></pre></div><div class="language-"><pre><code>|xT 5 θ|=λ\n</code></pre></div><div class="language-"><pre><code>|xT 4 θ|=λ\n</code></pre></div><div class="language-"><pre><code>|xT 3 θ|=λ\n</code></pre></div><div class="language-"><pre><code>θ∗\n</code></pre></div><div class="language-"><pre><code>θ 0 =y\n</code></pre></div><div class="language-"><pre><code>G(θ) =G(θ∗)\n</code></pre></div><div class="language-"><pre><code>Dual feasible set|xTjθ|≤λ∀j\n</code></pre></div><p><strong>Figure 5.13</strong> <em>The Lagrange dual of the lasso, withG</em> ( <strong><em>θ</em></strong> ) =^12 (‖ <strong>y</strong> ‖^22 −‖ <strong>y</strong> − <strong><em>θ</em></strong> ‖^22 )<em>. The blue shaded region is the feasible set</em> F <em>λ. The unconstrained dual solution is</em> <strong><em>θ</em></strong> 0 = <strong>y</strong> <em>, the null residual. The dual solution</em> <strong><em>θ</em></strong> ∗= <strong>prox</strong> <em>I</em> (F <em>λ</em> )( <strong>y</strong> ) <em>, the projection of</em> <strong>y</strong> <em>onto the convex set</em> F <em>λ.</em></p><p><em>Theorem 5.1.</em> Suppose we are given a solution <strong><em>φ</em></strong> ˆ( <em>λ</em> ′) to the lasso dual (5.90) for a specific <em>λ</em> max≥ <em>λ</em> ′ <em>&gt;</em> 0. Let <em>λ</em> be a nonnegative value different from <em>λ</em> ′. If the following holds:</p><div class="language-"><pre><code>| x Tj φ ˆ( λ ′)| &lt; 1 −‖ x j ‖ 2 ‖ y ‖ 2\n</code></pre></div><h6 id="∣-12"><a class="header-anchor" href="#∣-12" aria-hidden="true">#</a> ∣</h6><h6 id="∣-13"><a class="header-anchor" href="#∣-13" aria-hidden="true">#</a> ∣</h6><h6 id="∣-14"><a class="header-anchor" href="#∣-14" aria-hidden="true">#</a> ∣</h6><h6 id="∣-15"><a class="header-anchor" href="#∣-15" aria-hidden="true">#</a> ∣</h6><h6 id="_1-91"><a class="header-anchor" href="#_1-91" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>λ ′\n</code></pre></div><h6 id="−-12"><a class="header-anchor" href="#−-12" aria-hidden="true">#</a> −</h6><h6 id="_1-92"><a class="header-anchor" href="#_1-92" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>λ\n</code></pre></div><h6 id="∣-16"><a class="header-anchor" href="#∣-16" aria-hidden="true">#</a> ∣</h6><h6 id="∣-17"><a class="header-anchor" href="#∣-17" aria-hidden="true">#</a> ∣</h6><h6 id="∣-18"><a class="header-anchor" href="#∣-18" aria-hidden="true">#</a> ∣</h6><h6 id="∣-5-91"><a class="header-anchor" href="#∣-5-91" aria-hidden="true">#</a> ∣ , (5.91)</h6><p>then <em>β</em> ˆ <em>j</em> ( <em>λ</em> ) = 0.</p><div class="language-"><pre><code>Since φ ˆ( λ ′) = ( y − X β ˆ( λ ′)) /λ ′, simple algebra leads to (5.77).\n</code></pre></div><p><em>Proof:</em> We know from the stationarity conditions for the lasso that</p><div class="language-"><pre><code>| x Tj φ ˆ( λ )| &lt; 1 =⇒ β ̂ j ( λ ) = 0. (5.92)\n</code></pre></div><p>From the dual (5.90), <strong><em>φ</em></strong> ˆ( <em>λ</em> ) is the projection of <strong>y</strong> <em>/λ</em> into the feasible setF <em>λ</em>. By the projection theorem (Bertsekas 2003) for closed convex sets, <strong><em>φ</em></strong> ˆ( <em>λ</em> ) is continuous and nonexpansive, which implies</p><div class="language-"><pre><code>‖ φ ˆ( λ )− φ ˆ( λ ′)‖ 2 ≤\n</code></pre></div><h6 id="∥-5"><a class="header-anchor" href="#∥-5" aria-hidden="true">#</a> ∥</h6><h6 id="∥-6"><a class="header-anchor" href="#∥-6" aria-hidden="true">#</a> ∥</h6><h6 id="∥-7"><a class="header-anchor" href="#∥-7" aria-hidden="true">#</a> ∥</h6><div class="language-"><pre><code>y\nλ\n</code></pre></div><h6 id="−-13"><a class="header-anchor" href="#−-13" aria-hidden="true">#</a> −</h6><div class="language-"><pre><code>y\nλ ′\n</code></pre></div><h6 id="∥-8"><a class="header-anchor" href="#∥-8" aria-hidden="true">#</a> ∥</h6><h6 id="∥-9"><a class="header-anchor" href="#∥-9" aria-hidden="true">#</a> ∥</h6><h6 id="∥-10"><a class="header-anchor" href="#∥-10" aria-hidden="true">#</a> ∥</h6><div class="language-"><pre><code>2\n</code></pre></div><h6 id="_5-93"><a class="header-anchor" href="#_5-93" aria-hidden="true">#</a> (5.93)</h6><div class="language-"><pre><code>=‖ y ‖ 2\n</code></pre></div><h6 id="∣-19"><a class="header-anchor" href="#∣-19" aria-hidden="true">#</a> ∣</h6><h6 id="∣-20"><a class="header-anchor" href="#∣-20" aria-hidden="true">#</a> ∣</h6><h6 id="∣-21"><a class="header-anchor" href="#∣-21" aria-hidden="true">#</a> ∣</h6><h6 id="∣-22"><a class="header-anchor" href="#∣-22" aria-hidden="true">#</a> ∣</h6><h6 id="_1-93"><a class="header-anchor" href="#_1-93" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>λ\n</code></pre></div><h6 id="−-14"><a class="header-anchor" href="#−-14" aria-hidden="true">#</a> −</h6><h6 id="_1-94"><a class="header-anchor" href="#_1-94" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>λ ′\n</code></pre></div><h6 id="∣-23"><a class="header-anchor" href="#∣-23" aria-hidden="true">#</a> ∣</h6><h6 id="∣-24"><a class="header-anchor" href="#∣-24" aria-hidden="true">#</a> ∣</h6><h6 id="∣-25"><a class="header-anchor" href="#∣-25" aria-hidden="true">#</a> ∣</h6><h6 id="∣-26"><a class="header-anchor" href="#∣-26" aria-hidden="true">#</a> ∣.</h6><h6 id="_134-optimization-methods"><a class="header-anchor" href="#_134-optimization-methods" aria-hidden="true">#</a> 134 OPTIMIZATION METHODS</h6><p>Then</p><div class="language-"><pre><code>| x jT φ ˆ( λ )|≤| x Tj φ ˆ( λ )− x Tj φ ˆ( λ ′)|+| x Tj φ ˆ( λ ′)| (5.94)\n</code></pre></div><div class="language-"><pre><code>&lt; ‖ x j ‖ 2 ‖ φ ˆ( λ )− φ ˆ( λ ′)‖ 2 + 1−‖ x j ‖ 2 ‖ y ‖ 2\n</code></pre></div><h6 id="∣-27"><a class="header-anchor" href="#∣-27" aria-hidden="true">#</a> ∣</h6><h6 id="∣-28"><a class="header-anchor" href="#∣-28" aria-hidden="true">#</a> ∣</h6><h6 id="∣-29"><a class="header-anchor" href="#∣-29" aria-hidden="true">#</a> ∣</h6><h6 id="∣-30"><a class="header-anchor" href="#∣-30" aria-hidden="true">#</a> ∣</h6><h6 id="_1-95"><a class="header-anchor" href="#_1-95" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>λ ′\n</code></pre></div><h6 id="−-15"><a class="header-anchor" href="#−-15" aria-hidden="true">#</a> −</h6><h6 id="_1-96"><a class="header-anchor" href="#_1-96" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>λ\n</code></pre></div><h6 id="∣-31"><a class="header-anchor" href="#∣-31" aria-hidden="true">#</a> ∣</h6><h6 id="∣-32"><a class="header-anchor" href="#∣-32" aria-hidden="true">#</a> ∣</h6><h6 id="∣-33"><a class="header-anchor" href="#∣-33" aria-hidden="true">#</a> ∣</h6><h6 id="∣-34"><a class="header-anchor" href="#∣-34" aria-hidden="true">#</a> ∣</h6><div class="language-"><pre><code>≤‖ x j ‖ 2 ‖ y ‖ 2\n</code></pre></div><h6 id="∣-35"><a class="header-anchor" href="#∣-35" aria-hidden="true">#</a> ∣</h6><h6 id="∣-36"><a class="header-anchor" href="#∣-36" aria-hidden="true">#</a> ∣</h6><h6 id="∣-37"><a class="header-anchor" href="#∣-37" aria-hidden="true">#</a> ∣</h6><h6 id="∣-38"><a class="header-anchor" href="#∣-38" aria-hidden="true">#</a> ∣</h6><h6 id="_1-97"><a class="header-anchor" href="#_1-97" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>λ ′\n</code></pre></div><h6 id="−-16"><a class="header-anchor" href="#−-16" aria-hidden="true">#</a> −</h6><h6 id="_1-98"><a class="header-anchor" href="#_1-98" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>λ\n</code></pre></div><h6 id="∣-39"><a class="header-anchor" href="#∣-39" aria-hidden="true">#</a> ∣</h6><h6 id="∣-40"><a class="header-anchor" href="#∣-40" aria-hidden="true">#</a> ∣</h6><h6 id="∣-41"><a class="header-anchor" href="#∣-41" aria-hidden="true">#</a> ∣</h6><div class="language-"><pre><code>∣+ 1−‖ x j ‖^2 ‖ y ‖^2\n</code></pre></div><h6 id="∣-42"><a class="header-anchor" href="#∣-42" aria-hidden="true">#</a> ∣</h6><h6 id="∣-43"><a class="header-anchor" href="#∣-43" aria-hidden="true">#</a> ∣</h6><h6 id="∣-44"><a class="header-anchor" href="#∣-44" aria-hidden="true">#</a> ∣</h6><h6 id="∣-45"><a class="header-anchor" href="#∣-45" aria-hidden="true">#</a> ∣</h6><h6 id="_1-99"><a class="header-anchor" href="#_1-99" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>λ ′\n</code></pre></div><h6 id="−-17"><a class="header-anchor" href="#−-17" aria-hidden="true">#</a> −</h6><h6 id="_1-100"><a class="header-anchor" href="#_1-100" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>λ\n</code></pre></div><h6 id="∣-46"><a class="header-anchor" href="#∣-46" aria-hidden="true">#</a> ∣</h6><h6 id="∣-47"><a class="header-anchor" href="#∣-47" aria-hidden="true">#</a> ∣</h6><h6 id="∣-48"><a class="header-anchor" href="#∣-48" aria-hidden="true">#</a> ∣</h6><h6 id="∣-1-1"><a class="header-anchor" href="#∣-1-1" aria-hidden="true">#</a> ∣ = 1.</h6><h3 id="exercises-3"><a class="header-anchor" href="#exercises-3" aria-hidden="true">#</a> Exercises</h3><p>Ex. 5.1 Consider the unconstrained minimization of the quadratic function <em>f</em> ( <em>β</em> ) =^12 <em>βT</em> <strong>Q</strong> <em>β</em> −〈 <em>β, b</em> 〉, where <strong>Q</strong> 0 is a symmetric positive definite matrix, and <em>b</em> ∈R <em>p</em>.</p><div class="language-"><pre><code>(a) Show that the optimal solution β ∗exists and is unique, and specify its\nform in terms of ( Q ,b ).\n(b) Write out the gradient descent updates with constant stepsize s for this\nproblem.\n(c) Show that there exists some constant c &gt; 0, depending only on Q , such\nthat gradient descent converges for any fixed stepsize s ∈(0 ,c ).\n</code></pre></div><p>Ex. 5.2 Consider the constrained program minimize <em>f</em> ( <em>β</em> ) subject to <em>gj</em> ( <em>β</em> )≤ 0 for <em>j</em> = 1 <em>,...,m</em> , and let <em>f</em> ∗be its optimal value. Define the Lagrangian function</p><div class="language-"><pre><code>L ( β ; λ ) = f ( β ) +\n</code></pre></div><div class="language-"><pre><code>∑ m\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>λjgj ( β ). (5.95)\n</code></pre></div><div class="language-"><pre><code>(a) Show that\n</code></pre></div><div class="language-"><pre><code>sup\nλ ≥ 0\n</code></pre></div><div class="language-"><pre><code>L ( β ; λ ) =\n</code></pre></div><h6 id="-223"><a class="header-anchor" href="#-223" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>f ( β ) if gj ( β )≤0 for j = 1 ,...,m\n+∞ otherwise.\n</code></pre></div><div class="language-"><pre><code>(b) Use part (a) to show that f ∗= inf β sup λ ≥ 0 L ( β ; λ ).\n(c) How is f ∗related to the quantity sup λ ≥ 0 inf βL ( β,λ )?\n</code></pre></div><p>Ex. 5.3 Let <em>f</em> :R <em>p</em> →Rbe a convex and differentiable function, and consider a subspace constraint of the formC={ <em>β</em> ∈R <em>p</em> | <em>Mβ</em> = <em>c</em> }, where <em>M</em> ∈R <em>m</em> × <em>p</em> is a fixed matrix, and <em>c</em> ∈R <em>m</em> is a fixed vector.</p><div class="language-"><pre><code>(a) Suppose that β ∗∈ Csatisfies the first-order optimality condition (5.4).\nShow that there must exist a vector λ ∗∈R m such that\n</code></pre></div><div class="language-"><pre><code>∇ f ( β ∗) + MTλ ∗= 0 (5.96)\n</code></pre></div><h6 id="exercises-135"><a class="header-anchor" href="#exercises-135" aria-hidden="true">#</a> EXERCISES 135</h6><div class="language-"><pre><code>(b) Conversely, suppose that condition (5.96) holds for some λ ∗∈R m. Show\nthat the first-order optimality condition (5.4) must be satisfied.\n</code></pre></div><p>Ex. 5.4 Consider the Lagrangian <em>L</em> ( <em>β,λ</em> ) = <em>f</em> ( <em>β</em> ) +</p><p>∑ <em>m j</em> =1 <em>λjgj</em> ( <em>β</em> ) associated with the constrained problem (5.5), and assume that the optimal value <em>f</em> ∗is finite. Suppose that there exist vectors <em>β</em> ∗∈R <em>p</em> and <em>λ</em> ∗∈R <em>m</em> +such that</p><div class="language-"><pre><code>L ( β ∗ ,λ )\n</code></pre></div><div class="language-"><pre><code>( i )\n≤ L ( β ∗ ,λ ∗)\n</code></pre></div><div class="language-"><pre><code>( ii )\n≤ L ( β,λ ∗) (5.97)\n</code></pre></div><p>for all <em>β</em> ∈R <em>p</em> and <em>λ</em> ∈R <em>m</em> +. Show that <em>β</em> ∗is optimal for the constrained program.</p><p>Ex. 5.5 <em>Subgradient of Euclidean norm.</em> Consider the Euclidean or <em>`</em> 2 norm</p><p>‖ <em>β</em> ‖ 2 =</p><div class="language-"><pre><code>√∑ p\nj =1 β\n2\nj , which is used in the group lasso. Show that:\n(a) For any β 6 = 0, the norm g ( β ) : =‖ β ‖ 2 is differentiable with∇ g ( β ) =\nβ\n‖ β ‖ 2.\n(b) For β = 0, any vector̂ s ∈R p with‖̂ s ‖ 2 ≤1 is an element of the subdif-\nferential of g at 0.\n</code></pre></div><p>Ex. 5.6 Show that the function</p><div class="language-"><pre><code>h ( β 1 ,...,βp ) =| β | T P | β |\n</code></pre></div><p>in Equation (5.40) satisfies the regularity conditions below Equation (5.39) on page 111. (As a consequence, coordinate descent will still work even though this function is not separable).</p><p>Ex. 5.7 Show that the proximal-gradient update step (5.21) is equal to the step (5.19)</p><p>Ex. 5.8 Show that when <em>h</em> is given by the nuclear norm, the composite gradient update (5.26) can be obtained by the following procedure:</p><div class="language-"><pre><code>(a) Compute the singular value decomposition of the input matrix Z , that\nis Z = UDV T where D = diag{ σj ( Z )}is a diagonal matrix of the singular\nvalues.\n(b) Apply the soft-thresholding operator (5.25) to compute the “shrunken”\nsingular values\n</code></pre></div><div class="language-"><pre><code>γj : =S sλ ( σj ( Z )) , for j = 1 ,...,p.\n</code></pre></div><div class="language-"><pre><code>(c) Return the matrix Z ̂= U diag{ γ 1 ,...,γp } V T.\n</code></pre></div><h6 id="_136-optimization-methods"><a class="header-anchor" href="#_136-optimization-methods" aria-hidden="true">#</a> 136 OPTIMIZATION METHODS</h6><p>Ex. 5.9 Consider a regression problem with all variables and response having mean zero and standard deviation one in the dataset. Suppose also that each variable has identical absolute correlation with the response—that is</p><div class="language-"><pre><code>1\nN\n|〈 x j, y 〉|= λ, for all j = 1 ,...,p.\n</code></pre></div><p>Let <em>β</em> ̂be the least-squares coefficient vector of <strong>y</strong> on <strong>X</strong> , assumed to be unique for this exercise. Let <strong>u</strong> ( <em>α</em> ) = <em>α</em> <strong>X</strong> <em>β</em> ˆfor <em>α</em> ∈[0 <em>,</em> 1] be the vector that moves a fraction <em>α</em> toward the least-squares fit <strong>u</strong>. Let <em>RSS</em> =‖ <em>y</em> − <strong>X</strong> <em>β</em> ̂‖^22 , the residual sum-of-squares from the full least-squares fit.</p><div class="language-"><pre><code>(a) Show that\n</code></pre></div><div class="language-"><pre><code>1\nN\n</code></pre></div><div class="language-"><pre><code>|〈 x j, y − u ( α )〉|= (1− α ) λ for j = 1 ,...,p ,\n</code></pre></div><div class="language-"><pre><code>and hence the correlations of each x j with the residuals remain equal in\nmagnitude as we progress toward u.\n(b) Show that these correlations are all equal to\n</code></pre></div><div class="language-"><pre><code>λ ( α ) =\n</code></pre></div><div class="language-"><pre><code>(1− α )\n√\n(1− α )^2 + α (2 N − α )· RSS\n</code></pre></div><div class="language-"><pre><code>· λ,\n</code></pre></div><div class="language-"><pre><code>and hence they decrease monotonically to zero.\n(c) Use these results to show that the LAR algorithm in Section 5.6 keeps\nthe correlations tied and monotonically decreasing.\n</code></pre></div><p>Ex. 5.10 Consider step 3c of the LAR Algorithm 5.1. Define <em>c<code>_ =〈 **x** _</code>,</em> <strong><em>r</em></strong> <em>k</em> − 1 〉 and <em>a<code>_ =〈 **x** _</code>,</em> <strong>X</strong> A <em>δ</em> 〉 <em>, ` /</em> ∈A. Define</p><div class="language-"><pre><code>α` = min+\n</code></pre></div><h6 id="-224"><a class="header-anchor" href="#-224" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>λk − 1 − c`\n1 − a`\n</code></pre></div><h6 id="-225"><a class="header-anchor" href="#-225" aria-hidden="true">#</a> ,</h6><div class="language-"><pre><code>λk − 1 + c`\n1 + a`\n</code></pre></div><h6 id="-226"><a class="header-anchor" href="#-226" aria-hidden="true">#</a> }</h6><h6 id="-227"><a class="header-anchor" href="#-227" aria-hidden="true">#</a> ,</h6><p>where min+only considers positive entries. Show that the variable to enter at step <em>k</em> has index <em>j</em> = arg min <em><code>/_ ∈A _α</code></em> , with value <em>λk</em> = <em>λk</em> − 1 − <em>αj</em>.</p><p>Ex. 5.11 <em>Strong rules</em></p><div class="language-"><pre><code>(a) Show that if the slope condition (5.80) holds, then the global and sequen-\ntial strong rules (5.78) and (5.79) are guaranteed to work.\n(b) In the case of orthogonal design X T X = I , show that the slope condition\n(5.80) always holds.\n(c) Design a simulation study to investigate the accuracy of the DPP and\nstrong rules for the lasso, in the cases ( N,p ) = (100 , 20), ( N,p ) = (100 , 100),\nand ( N,p ) = (100 , 1000).\n</code></pre></div><h6 id="exercises-137"><a class="header-anchor" href="#exercises-137" aria-hidden="true">#</a> EXERCISES 137</h6><p>Ex. 5.12 <em>ADMM for consensus optimization:</em> Suppose that we have a dataset { <em>xi,yi</em> } <em>Ni</em> =1, and that our goal is to minimize an objective function <em>L</em> ( <strong>X</strong> <em>β</em> − <strong>y</strong> ) that decomposes additively as a sum of <em>N</em> terms, one for each sample. A natural approach is to divide the dataset into <em>B</em> blocks, and denote by <em>Lb</em> ( <strong>X</strong> <em>bβb</em> − <strong>y</strong> <em>b</em> ) the objective function over the <em>bth</em> block of data, where <strong>X</strong> <em>b</em> and <strong>y</strong> <em>b</em> are the corresponding blocks of <strong>X</strong> and <strong>y</strong>. We thus arrive at the prob- lem</p><div class="language-"><pre><code>minimize\nβ ∈R p\n</code></pre></div><h6 id="b"><a class="header-anchor" href="#b" aria-hidden="true">#</a> { B</h6><h6 id="∑-30"><a class="header-anchor" href="#∑-30" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>b =1\n</code></pre></div><div class="language-"><pre><code>Lb ( X bβb − y b ) + r ( θ )\n</code></pre></div><h6 id="-228"><a class="header-anchor" href="#-228" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>such that βb = θ for all b = 1 ,...B.\n</code></pre></div><div class="language-"><pre><code>(5.98)\n</code></pre></div><div class="language-"><pre><code>(a) Show that the ADMM algorithm for this problem takes the form\n</code></pre></div><div class="language-"><pre><code>βtb +1←arg min\nβb\n</code></pre></div><h6 id="-229"><a class="header-anchor" href="#-229" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Lb ( X bβb − y b ) + ( ρ/ 2)‖ βb − θt + μtb ‖^22\n</code></pre></div><h6 id="-230"><a class="header-anchor" href="#-230" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>(5.99a)\n</code></pre></div><div class="language-"><pre><code>θt +1←arg min\nθ\n</code></pre></div><h6 id="-231"><a class="header-anchor" href="#-231" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>r ( z ) + ( Nρ/ 2)‖ θ − β ̄ t +1− μ ̄ t ‖^22\n</code></pre></div><h6 id="-232"><a class="header-anchor" href="#-232" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>(5.99b)\n</code></pre></div><div class="language-"><pre><code>μtb +1← μtb + ( βbt +1− θt +1) (5.99c)\n</code></pre></div><div class="language-"><pre><code>where the ̄ μk and β ̄ k +1denote averages over blocks. Interpret it as consensus\noptimization.\n(b) Now consider the lasso, which uses the regularizer r ( θ ) = λ ‖ θ ‖ 1. Show\nthat the algorithm has the form\n</code></pre></div><div class="language-"><pre><code>βtb +1←( X Tb X b + ρ I )−^1 ( X b y b + ρ ( θt − μtb )) (5.100a)\nθt +1←S λ/ ( ρN )( β ̄ t +1+ ̄ μt ) (5.100b)\nμtb +1← μtb + ( βtb +1− θt +1) (5.100c)\n</code></pre></div><div class="language-"><pre><code>(c) Implement the updates (5.100) in software and demonstrate it on a nu-\nmerical example.\n</code></pre></div><p>Ex. 5.13</p><div class="language-"><pre><code>(a) Derive the alternating convex minimization (ACS) for problem (5.72), and\nshow that it has the form of a power iteration (Equations (5.73) and (5.74)).\n(b) Show that it converges to the eigenvector corresponding to the largest\neigenvalue of X T X , provided that the starting vector v 0 is not orthogonal\nto this largest eigenvector.\n</code></pre></div><div class="language-"><pre><code>Chapter 6\n</code></pre></div><h2 id="statistical-inference"><a class="header-anchor" href="#statistical-inference" aria-hidden="true">#</a> Statistical Inference</h2><p>An attractive feature of <em>`</em> 1 -regularized procedures is their ability to combine variable selection with parameter fitting. We often select a model based on cross-validation—as an estimate for prediction or generalization error—and then do further validation on a held-out test set. It is sometimes of interest to determine the statistical strength of the in- cluded variables, as in “p-values” in traditional models. The adaptive nature of the estimation procedure makes this problem difficult—both conceptually and analytically. We describe some useful approaches to the inference problem in this chapter. We begin by discussing two “traditional” approaches—Bayesian methods and the bootstrap, and then present some newer approaches to this problem.</p><h3 id="_6-1-the-bayesian-lasso"><a class="header-anchor" href="#_6-1-the-bayesian-lasso" aria-hidden="true">#</a> 6.1 The Bayesian Lasso</h3><p>The Bayesian paradigm treats the parameters as random quantities, along with a prior distribution that characterizes our belief in what their values might be. Here we adopt the approach of Park and Casella (2008), involving a model of the form</p><div class="language-"><pre><code>y | β,λ,σ ∼ N ( X β,σ^2 I N × N ) (6.1a)\n</code></pre></div><div class="language-"><pre><code>β | λ,σ ∼\n</code></pre></div><div class="language-"><pre><code>∏ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>λ\n2 σ\n</code></pre></div><div class="language-"><pre><code>e −\nλσ | βj |\n, (6.1b)\n</code></pre></div><p>using the i.i.d. Laplacian prior (6.1b). Under this model, it is easy to show that the negative log posterior density for <em>β</em> | <strong>y</strong> <em>,λ,σ</em> is given by</p><div class="language-"><pre><code>1\n2 σ^2\n</code></pre></div><div class="language-"><pre><code>‖ y − X β ‖^22 +\n</code></pre></div><div class="language-"><pre><code>λ\nσ\n</code></pre></div><div class="language-"><pre><code>‖ β ‖ 1 , (6.2)\n</code></pre></div><p>where we have dropped an additive constant independent of <em>β</em>. Consequently, for any fixed values of <em>σ</em> and <em>λ</em> , the posterior mode coincides with the lasso estimate (with regularization parameter <em>σλ</em> ). Park and Casella (2008) include <em>σ</em>^2 in the prior specification (6.1b) for technical reasons. Here we have as- sumed there is no constant in the model, and that the columns of <strong>X</strong> are</p><div class="language-"><pre><code>139\n</code></pre></div><h6 id="_140-statistical-inference"><a class="header-anchor" href="#_140-statistical-inference" aria-hidden="true">#</a> 140 STATISTICAL INFERENCE</h6><div class="language-"><pre><code>Posterior Density\n</code></pre></div><div class="language-"><pre><code>−1.0 −0.5 0.0 0.5 1.0\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.5\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>1.5\n</code></pre></div><div class="language-"><pre><code>2.0\n</code></pre></div><div class="language-"><pre><code>2.5\n</code></pre></div><div class="language-"><pre><code>3.0\n</code></pre></div><div class="language-"><pre><code>3.5\n</code></pre></div><div class="language-"><pre><code>β 7\n</code></pre></div><p><strong>Figure 6.1</strong> <em>Prior and posterior distribution for the seventh variable in the diabetes example, withλheld fixed. The prior in the figure is a double exponential (Laplace) distribution with density proportional to</em> exp(−_._ 0065 | <em>β</em> 7 |)<em>. The prior rate.</em> 0065 <em>is a representative value just for il lustration.</em></p><p>mean-centered, as is <strong>y</strong>.^1 The posterior distribution provides more than point estimates: it provides an entire joint distribution. The red curve in Figure 6.1 is the Laplace prior used in the Bayesian lasso, applied to variable <em>β</em> 7 in the “diabetes data.” These data consist of observations on 442 patients, with the response of interest being a quantitative measure of disease progression one year after baseline. There are ten baseline variables—age, sex, body-mass index, average blood pressure, and six blood serum measurements—plus quadratic terms, giving a total of 64 features. The prior has a sharp peak at zero, which captures our belief that some parameters are zero. Given a probability distribution (likelihood) for the observed data given the parameters, we update our prior by conditioning on the observed data, yielding the posterior distribution of the parameters. The histogram in Figure 6.1 characterizes the posterior distribution for <em>β</em> 7 for the diabetes data. The prior distribution has a variance parameter that characterizes the strength of our belief in zero as a special value. The posterior mode is slightly away from zero, although a 95% posterior credible interval comfortably covers zero. Exact Bayesian calculations are typically intractable, except for the simplest of models. Fortunately, modern computation allows us to use Markov chain Monte Carlo (MCMC) to efficiently sample realizations from the posterior distributions of the parameters of interest. Figure 6.2 [left panel] shows a summary of MCMC samples from the posterior distribution of <em>β</em> | <em>λ</em> ; the median of 10 <em>,</em> 000 posterior samples is shown at each of 100 values of <em>λ</em>. Here</p><p>(^1) This is not a real restriction on the model, and is equivalent to assuming an improper flat prior on <em>β</em> 0 , which is rarely of interest.</p><h6 id="the-bayesian-lasso-141"><a class="header-anchor" href="#the-bayesian-lasso-141" aria-hidden="true">#</a> THE BAYESIAN LASSO 141</h6><div class="language-"><pre><code>0.0 0.5 1.0 1.5 2.0\n</code></pre></div><div class="language-"><pre><code>−0.4\n</code></pre></div><div class="language-"><pre><code>−0.2\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>Posterior Medians\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>3\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>5\n</code></pre></div><div class="language-"><pre><code>6\n</code></pre></div><div class="language-"><pre><code>7\n</code></pre></div><div class="language-"><pre><code>8\n</code></pre></div><div class="language-"><pre><code>9\n</code></pre></div><div class="language-"><pre><code>10\n</code></pre></div><div class="language-"><pre><code>0.0 0.5 1.0 1.5 2.0\n</code></pre></div><div class="language-"><pre><code>−0.4\n</code></pre></div><div class="language-"><pre><code>−0.2\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>Lasso\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>3\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>5\n</code></pre></div><div class="language-"><pre><code>6\n</code></pre></div><div class="language-"><pre><code>7\n</code></pre></div><div class="language-"><pre><code>8\n</code></pre></div><div class="language-"><pre><code>9\n</code></pre></div><div class="language-"><pre><code>10\n</code></pre></div><div class="language-"><pre><code>‖β‖ 1 ‖β‖ 1\n</code></pre></div><p><strong>Figure 6.2</strong> <em>Bayesian lasso on the diabetes data. The left plot shows the posterior medians from MCMC runs (conditional onλ). The right plot shows the lasso pro- file. In the left plot, the vertical line is at the posterior median of</em> ‖ <em>β</em> ‖ 1 <em>(from an unconditional model), while for the right plot the vertical line was found by N-fold cross-validation.</em></p><p><em>σ</em>^2 is allowed to vary (with <em>π</em> ( <em>σ</em>^2 )∼ <em>σ</em>^12 ). This, and the fact that we have displayed medians, accounts for the slight discrepancies with the right plot (the lasso), which shows the posterior mode for fixed values of <em>σλ</em>. A complete Bayesian model will also specify a prior distribution for <em>λ</em> ; in this case, a diffuse Gamma distribution is conjugate and hence convenient for the MCMC sampling. This is where the Bayesian approach can be worth the considerable extra effort and leap of faith. The full posterior distribution includes <em>λ</em> as well as <em>β</em> , so that model selection is performed automatically. Furthermore, the posterior credible intervals for <em>β</em> take into account the posterior variability in <em>λ</em>. Figure 6.3 shows a summary of 10 <em>,</em> 000 MCMC samples from the posterior distribution for the diabetes data. While the posterior mode has nine nonzero coefficients, the posterior distributions suggest that only 5–8 of these are well separated from zero. Specifying the Bayesian model is technically challenging, and there are several choices to be made along the way. These include priors for <em>λ</em> and <em>σ</em>^2 , which themselves have hyperparameters that have to be set. Our examples were fit inRusing the functionblassoin the packagemonomvn(Gramacy 2011), and for the most part we used the default parameter settings. For</p><h6 id="_142-statistical-inference"><a class="header-anchor" href="#_142-statistical-inference" aria-hidden="true">#</a> 142 STATISTICAL INFERENCE</h6><div class="language-"><pre><code>|| ||| |||| || ||||||||||||| ||||||||| |||| | ||||||||| ||| ||||| || |||||||||||||||||||| |||| || |||||||||| ||||||| ||||||||||||||||||| |||| || ||||||| | |\n</code></pre></div><div class="language-"><pre><code>|| |||||| |||||||||| | ||||||||||||||||| ||||||| || ||| ||||||||||||| || ||\n</code></pre></div><div class="language-"><pre><code>||||||||||||||| | || |||||||||||| |||||||| |||||||||| ||| |||||||||||| ||\n</code></pre></div><div class="language-"><pre><code>||||| ||||||| ||||| | ||||||||||||||| |||||||||||||| ||||| |||||| ||\n</code></pre></div><div class="language-"><pre><code>| ||| | ||||| || ||| ||| ||| | |||||| |||||| | || | || ||| |||| || | ||| || || | |||| |||| | ||| |||| || ||| |||||| || || || || |||||| ||| |||||| || | ||||||| | || || ||||| || | |||| ||| ||| | || || ||| |||| || ||||| || | ||| ||| | ||\n</code></pre></div><div class="language-"><pre><code>||||||| |||| ||||||||| |||||||||||||||||| ||| ||||||| || || ||| | || ||| | ||||| ||| ||| || |||| ||||||||| |||| || |||||| ||| ||||| || || |||| |||| || ||||||| | |||| ||| | ||||| || ||||| ||| |||| | ||| || | || || ||||| | |||||||| |||||| ||||| ||| || ||| | | ||| | || ||||| |||| | ||| |||| || ||| ||| | |||||||| |||||| |||||| || ||| || | || |||| || || || | | ||| ||||||| ||| ||||| || |||||| ||| |||| ||||||\n</code></pre></div><div class="language-"><pre><code>|||| ||||||| || | || || | || | |||| | ||| ||||| || | || || |||| || ||| |||| ||| || | || |\n</code></pre></div><div class="language-"><pre><code>|| ||||||||| | |||||| || |||||||| |||| || | ||| || |||||| | ||| | |||| |||||| | | || || ||||| ||||| || ||||| || |||| ||||||| |||| | || || || |\n</code></pre></div><div class="language-"><pre><code>| |||||||||||| ||||| ||||||||| | ||||||| ||| ||| |||||||||||||| ||||| || |||||| |||| ||| | ||||||| | || | || |||||| | | || |||||||||\n</code></pre></div><div class="language-"><pre><code>|||| |||| ||||| ||| |||||||| ||| ||| | | ||||||| | ||||| || ||| | |||| |\n</code></pre></div><div class="language-"><pre><code>−0.6 −0.4 −0.2 0.0 0.2 0.4 0.6\n</code></pre></div><div class="language-"><pre><code>Bayesian Posterior Samples\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>Density\n</code></pre></div><div class="language-"><pre><code>0.0 0.5 1.0 1.5 2.0\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.5\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>1.5\n</code></pre></div><div class="language-"><pre><code>2.0\n</code></pre></div><div class="language-"><pre><code>2.5\n</code></pre></div><div class="language-"><pre><code>3.0\n</code></pre></div><div class="language-"><pre><code>β^1\n</code></pre></div><div class="language-"><pre><code>β^2\n</code></pre></div><div class="language-"><pre><code>β^3\n</code></pre></div><div class="language-"><pre><code>β^4\n</code></pre></div><div class="language-"><pre><code>β^5\n</code></pre></div><div class="language-"><pre><code>β^6\n</code></pre></div><div class="language-"><pre><code>β^7\n</code></pre></div><div class="language-"><pre><code>β^8\n</code></pre></div><div class="language-"><pre><code>β^9\n</code></pre></div><div class="language-"><pre><code>β^10\n</code></pre></div><div class="language-"><pre><code>‖β‖ 1\n</code></pre></div><div class="language-"><pre><code>Figure 6.3 Posterior distributions for theβjand ‖ β ‖ 1 for the diabetes data. Sum-\nmary of 10 , 000 MCMC samples, with the first 1000 “burn-in” samples discarded.\n</code></pre></div><div class="language-"><pre><code>this 442×10 problem it took 5 seconds on a 2.3 GHz Macbook Pro. However,\nBayesian computations do not scale well; experiments in the next section show\nthat the computational cost scales roughly asO( p^2 ).\n</code></pre></div><h3 id="_6-2-the-bootstrap"><a class="header-anchor" href="#_6-2-the-bootstrap" aria-hidden="true">#</a> 6.2 The Bootstrap</h3><div class="language-"><pre><code>The bootstrap is a popular nonparametric tool for assessing the statistical\nproperties of complex estimators (Efron 1979, Efron and Tibshirani 1993). To\nmotivate its use, suppose that we have obtained an estimate β ̂(ˆ λCV ) for a\nlasso problem according to the following procedure:\n</code></pre></div><ol><li>Fit a lasso path to ( <strong>X</strong> <em>,</em> <strong>y</strong> ) over a dense grid of values Λ ={ <em>λ<code>_ } _L</code></em> =1.</li><li>Divide the training samples into 10 groups at random.</li><li>With the <em>kth</em> group left out, fit a lasso path to the remaining 9/10ths, using the same grid Λ.</li><li>For each <em>λ</em> ∈Λ compute the mean-squared prediction error for the left-out group.</li><li>Average these errors to obtain a prediction error curve over the grid Λ.</li><li>Find the valuê <em>λCV</em> that minimizes this curve, and then return the coeffi- cient vector from our original fit in step (1) at that value of <em>λ</em>.</li></ol><h6 id="the-bootstrap-143"><a class="header-anchor" href="#the-bootstrap-143" aria-hidden="true">#</a> THE BOOTSTRAP 143</h6><div class="language-"><pre><code>||| || || |||||| || || || ||| || ||| ||| | ||| | ||| | ||| ||||| || | |||| |||| | ||||| | ||| |||| ||| ||| || || | || || |||| ||||||| || ||| | |||| ||| || ||| |||| || |||| || || |||||| ||||||| || || || ||||| | || ||| | || || | ||| ||||\n</code></pre></div><div class="language-"><pre><code>| |||||||||||||||||||||| ||||\n</code></pre></div><div class="language-"><pre><code>| ||||| |\n</code></pre></div><div class="language-"><pre><code>|| |\n</code></pre></div><div class="language-"><pre><code>||| | || || ||||| || | || | || | ||| |||| | |||| || |||| |||| || |||| |||| ||| ||| |||| | | ||| ||\n</code></pre></div><div class="language-"><pre><code>| |||| ||||| ||||| |||| |||||| ||||||||| |||| ||| || |||| || | ||| |||| |||| ||||||||||||| || ||| |||| || |||||||| ||| | | || |||| || ||||||||| || ||||||| |||||||||||| || ||||| |||| |||| ||||| ||| | ||||| || ||||| |||||| |||||| | ||||||| |||||||| |||||| |||| | | || |||| ||| || |||||||||| | || ||| |||| |||||||| |||||||| ||| || ||||||| ||||| ||||||||||| ||| ||||||||||||||||| || | |||||||||| ||||| |||||||| ||| |||| ||| |\n</code></pre></div><div class="language-"><pre><code>|| ||| || | ||| ||| |||| | ||| ||| | || | || |||| |||| || || | |||| |\n</code></pre></div><div class="language-"><pre><code>|| ||| ||| |\n</code></pre></div><div class="language-"><pre><code>| | || ||| || ||||| | || ||||| | |||| || || ||| ||| | || || |||| | | || ||| || ||| ||| ||| |||||\n</code></pre></div><div class="language-"><pre><code>| || ||\n</code></pre></div><div class="language-"><pre><code>−0.6 −0.4 −0.2 0.0 0.2 0.4 0.6\n</code></pre></div><div class="language-"><pre><code>Bootstrap Samples\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>Bootstrap Probability of 0\n</code></pre></div><div class="language-"><pre><code>0.0 0.2 0.4 0.6 0.8 1.0\n</code></pre></div><div class="language-"><pre><code>β β^1\n1\n</code></pre></div><div class="language-"><pre><code>β β^2\n2\n</code></pre></div><div class="language-"><pre><code>β β^3\n3\n</code></pre></div><div class="language-"><pre><code>β^4 β^4\n</code></pre></div><div class="language-"><pre><code>β^5 β^5\n</code></pre></div><div class="language-"><pre><code>β^6 β^6\n</code></pre></div><div class="language-"><pre><code>β^7 β^7\n</code></pre></div><div class="language-"><pre><code>β^8 β^8\n</code></pre></div><div class="language-"><pre><code>β^9 β^9\n</code></pre></div><div class="language-"><pre><code>β^10 β^10\n</code></pre></div><p><strong>Figure 6.4</strong> <em>[Left] Boxplots of 1000 bootstrap realizations of</em> ̂ <em>β</em> ∗( <em>λ</em> ˆ <em>CV</em> ) <em>obtained by the nonparametric bootstrap, which corresponds to re-sampling from the empirical CDF F</em> ̂ <em>N. Comparing with the corresponding Bayesian posterior distribution in Figure 6.3, we see a close correspondence in this case. [Right] Proportion of times each coefficient is zero in the bootstrap distribution.</em></p><p>How do we assess the sampling distribution of <em>β</em> ̂(̂ <em>λCV</em> )? That is, we are inter- ested in the distribution of the random estimate <em>β</em> ̂(̂ <em>λCV</em> ) as a function of the <em>N</em> i.i.d. samples{( <em>xi,yi</em> )} <em>Ni</em> =1. The nonparametric bootstrap is one method for approximating this sampling distribution: in order to do so, it approxi- mates the cumulative distribution function <em>F</em> of the random pair ( <em>X,Y</em> ) by the empirical CDF <em>F</em> ̂ <em>N</em> defined by the <em>N</em> samples. We then draw <em>N</em> sam- ples from <em>F</em> ̂ <em>N</em> , which amounts to drawing <em>N</em> samples with replacement from the given dataset. Figure 6.4[left] shows boxplots of 1000 bootstrap realiza- tions <em>β</em> ̂∗( <em>λ</em> ˆ <em>CV</em> ) obtained in this way, by repeating steps 1–6 on each bootstrap sample.^2 There is a reasonable correspondence between this figure, and the corresponding Bayesian results in Figure 6.3. The right plot shows the propor- tion of times that each variable was exactly zero in the bootstrap distribution. None of the Bayesian posterior realizations are exactly zero, although often some are close to zero. (Theblassofunction has an argument that allows for variable selection via “reversible jump” MCMC, but this was not used here.) Similar to the right-hand plot, Meinshausen and B ̈uhlmann (2010) pro-</p><p>(^2) On a technical note, we implement the bootstrap with observation weights <em>w</em> ∗ <em>i</em> = <em>k/N</em> , with <em>k</em> = 0 <em>,</em> 1 <em>,</em> 2 <em>,...</em>. In cross-validation, the units are again the original <em>N</em> observations, which carry along with them their weights <em>w</em> ∗ <em>i</em>.</p><h6 id="_144-statistical-inference"><a class="header-anchor" href="#_144-statistical-inference" aria-hidden="true">#</a> 144 STATISTICAL INFERENCE</h6><div class="language-"><pre><code>−8 −6 −4 −2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.5\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.7\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>0.9\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>1.1\n</code></pre></div><div class="language-"><pre><code>Mean−Squared Error\n</code></pre></div><div class="language-"><pre><code>10 10 9 10 8 8 7 7 6 4 4 2\n</code></pre></div><div class="language-"><pre><code>−8 −6 −4 −2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.5\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.7\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>0.9\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>1.1\n</code></pre></div><div class="language-"><pre><code>Bootstrapped CV Curves\n</code></pre></div><div class="language-"><pre><code>Mean−Squared Error\n</code></pre></div><div class="language-"><pre><code>log(λ) log(λ)\n</code></pre></div><p><strong>Figure 6.5</strong> <em>[Left] Cross-validation curve for lasso on the diabetes data, with one- standard-error bands computed from the 10 realizations. The vertical line on the left corresponds to the minimizing value forλ. The line on the right corresponds to the</em> one-standard-error <em>rule; the biggest value ofλfor which the CV error is within one standard error of the minimizing value. [Right] 1000 bootstrap CV curves, with the average in red, and one-standard-error bands in blue. The rug-plot at the base shows the locations of the minima.</em></p><p>duce a <em>stability</em> plot for lasso under bootstrap resampling; as a function of <em>λ</em> they display what fraction of times a variable is nonzero in the bootstrapped coefficient paths. Figure 6.5 shows the bootstrapped cross-validation curves, and their min- ima. Not surprisingly, the bootstrapped minima have a wide spread, since the original CV curve is flat over a broad region. Interestingly, the bootstrap standard-error bands bear a close correspondence to those computed from the original CV fit in the left plot. Figure 6.6 shows pairwise plots of the boot- strapped coefficients. From such plots we can see, for example, how correlated variables can trade off with each other, both in value and their propensity for being zero. In Table 6.1, we show comparative timings in seconds for problems with <em>N</em> = 400 and different numbers of predictors. We generated 1000 bootstrap samples; for the Bayesian lasso we generated 2000 posterior samples, with the idea of discarding the first 1000 samples as a burn-in. While such comparisons depend on implementation details, the relative growth with <em>p</em> is informative. The Bayesian lasso is perhaps faster for small problems, but its complexity</p><h6 id="the-bootstrap-145"><a class="header-anchor" href="#the-bootstrap-145" aria-hidden="true">#</a> THE BOOTSTRAP 145</h6><div class="language-"><pre><code>−0.25 −0.10 0.10 0.25 −0.4 0.2 0.8 −0.1 0.2 −0.05 0.10\n</code></pre></div><div class="language-"><pre><code>−0.10 0.05\n</code></pre></div><div class="language-"><pre><code>−0.25 −0.10\n</code></pre></div><div class="language-"><pre><code>0.20 0.35\n</code></pre></div><div class="language-"><pre><code>0.10 0.25\n</code></pre></div><div class="language-"><pre><code>−1.0\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>−0.4 0.2 0.8\n</code></pre></div><div class="language-"><pre><code>−0.2 0.2\n</code></pre></div><div class="language-"><pre><code>−0.1 0.2\n</code></pre></div><div class="language-"><pre><code>0.2 0.4 0.6\n</code></pre></div><div class="language-"><pre><code>−0.10 0.05\n</code></pre></div><div class="language-"><pre><code>−0.05 0.10\n0.20 0.35 −1.0 0.0 −0.2 0.2 0.2 0.4 0.6\n</code></pre></div><div class="language-"><pre><code>β 1\n</code></pre></div><div class="language-"><pre><code>β 2\n</code></pre></div><div class="language-"><pre><code>β 3\n</code></pre></div><div class="language-"><pre><code>β 4\n</code></pre></div><div class="language-"><pre><code>β 5\n</code></pre></div><div class="language-"><pre><code>β 6\n</code></pre></div><div class="language-"><pre><code>β 7\n</code></pre></div><div class="language-"><pre><code>β 8\n</code></pre></div><div class="language-"><pre><code>β 9\n</code></pre></div><div class="language-"><pre><code>β 10\n</code></pre></div><p><strong>Figure 6.6</strong> <em>Pairwise plots of the bootstrapped coefficientsβ</em> ̂∗(ˆ <em>λCV</em> )<em>. The red points correspond to values that are zero on at least one coordinate for that plot. The samples</em><strong>x</strong> 5 <em>and</em> <strong>x</strong> 6 <em>have high correlation (</em> 0_._ 9 <em>); we see the corresponding negative correlation in their coefficients, with zero playing a prominent role.</em></p><div class="language-"><pre><code>Table 6.1 Timings for Bayesian lasso and boot-\nstrapped lasso, for four different problem sizes. The\nsample size isN = 400.\np Bayesian Lasso Lasso/Bootstrap\n10 3.3 secs 163.8 secs\n50 184.8 secs 374.6 secs\n100 28.6 mins 14.7 mins\n200 4.5 hours 18.1 mins\n</code></pre></div><h6 id="_146-statistical-inference"><a class="header-anchor" href="#_146-statistical-inference" aria-hidden="true">#</a> 146 STATISTICAL INFERENCE</h6><p>seems to scale asO( <em>p</em>^2 ). In contrast, the scaling of the bootstrap seems to be closer toO( <em>p</em> ), because it exploits the sparseness and convexity of the lasso. The above procedure used the <em>nonparametric bootstrap</em> , in which we esti- mate the unknown population <em>F</em> by the empirical distribution function <em>F</em> ̂ <em>N</em> , the nonparametric maximum likelihood estimate of <em>F</em>. Sampling from <em>F</em> ̂ <em>N</em> corresponds to sampling with replacement from the data. In contrast, the <em>parametric bootstrap</em> samples from a parametric estimate of <em>F</em> , or its corre- sponding density function <em>f</em>. In this example, we would fix <strong>X</strong> and obtain estimates <em>β</em> ̂and̂ <em>σ</em>^2 either from the full least-squares fit, or from the fitted lasso with parameter <em>λ</em>. We would then sample <strong>y</strong> values from the Gaussian model (6.1a), with <em>β</em> and <em>σ</em>^2 replaced by <em>β</em> ̂and̂ <em>σ</em>^2.</p><p>Using the full least-squares estimates for <em>β</em> ̂and̂ <em>σ</em>^2 , the parametric boot- strap results for our example are shown in Figure 6.7. They are similar to both the nonparametric bootstrap results and those from the Bayesian lasso. In general, we might expect that the parametric bootstrap would likely produce results even closer to the Bayesian lasso as compared to the nonparametric bootstrap, since the parametric bootstrap and Bayesian lasso both use the assumed parametric form for data distribution (6.1a). Note also that the use</p><p>of the full least squares estimates for <em>β</em> ̂and̂ <em>σ</em>^2 would not work when <em>p</em>  <em>N</em> , and we would need to generate a different dataset for each value of <em>λ</em>. This would slow down the computations considerably.</p><div class="language-"><pre><code>| ||||||| || | | |||| |||||| || |||| || || ||| |||| ||\n</code></pre></div><div class="language-"><pre><code>| | | |||\n</code></pre></div><div class="language-"><pre><code>|||||| ||\n</code></pre></div><div class="language-"><pre><code>|| |\n</code></pre></div><div class="language-"><pre><code>||| || ||| |||| ||| || ||| | |||||\n</code></pre></div><div class="language-"><pre><code>|||\n</code></pre></div><div class="language-"><pre><code>| |\n</code></pre></div><div class="language-"><pre><code>−0.6 −0.4 −0.2 0.0 0.2 0.4 0.6\n</code></pre></div><div class="language-"><pre><code>Bootstrap Samples\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>Bootstrap Probability of 0\n</code></pre></div><div class="language-"><pre><code>0.0 0.2 0.4 0.6 0.8 1.0\n</code></pre></div><div class="language-"><pre><code>β β^1\n1\n</code></pre></div><div class="language-"><pre><code>β β^2\n2\n</code></pre></div><div class="language-"><pre><code>β β^3\n3\n</code></pre></div><div class="language-"><pre><code>β β^4\n4\n</code></pre></div><div class="language-"><pre><code>β^5 β^5\n</code></pre></div><div class="language-"><pre><code>β^6 β^6\n</code></pre></div><div class="language-"><pre><code>β^7 β^7\n</code></pre></div><div class="language-"><pre><code>β^8 β^8\n</code></pre></div><div class="language-"><pre><code>β^9 β^9\n</code></pre></div><div class="language-"><pre><code>β^10 β^10\n</code></pre></div><p><strong>Figure 6.7</strong> <em>[Left] Boxplots of 1000 parametric bootstrap realizations ofβ</em> ̂∗(ˆ <em>λCV</em> )<em>. Comparing with the corresponding Bayesian posterior distribution in Figure 6.3, we again see a close correspondence. [Right] Proportion of times each coefficient is zero in the bootstrap distribution.</em></p><h6 id="post-selection-inference-for-the-lasso-147"><a class="header-anchor" href="#post-selection-inference-for-the-lasso-147" aria-hidden="true">#</a> POST-SELECTION INFERENCE FOR THE LASSO 147</h6><p>In summary, in this section we have compared the Bayesian and bootstrap approach on a Gaussian linear-regression problem, for which Bayesian software was available at the time of writing. As we move to GLMs and other models, the Bayesian technical complexities grow. The bootstrap, on the other hand, can be applied seamlessly in many situations. In a general sense, the similar results for the Bayesian lasso and lasso/bootstrap are not surprising. The histogram of values from the nonparametric bootstrap can be viewed as a kind of posterior-Bayes estimate under a noninformative prior in the multinomial model (Rubin 1981, Efron 1982). Which approach is better? Both the Bayesian and bootstrap approaches provide a way to assess variability of lasso estimates. The Bayesian approach is more principled but leans more heavily on parametric assumptions, as com- pared to the nonparametric bootstrap. The bootstrap procedure scales better computationally for large problems. Some further discussion of the relation- ship between Bayesian and bootstrap approaches is given in Efron (2011).</p><h3 id="_6-3-post-selection-inference-for-the-lasso"><a class="header-anchor" href="#_6-3-post-selection-inference-for-the-lasso" aria-hidden="true">#</a> 6.3 Post-Selection Inference for the Lasso</h3><p>In this section we present some relatively recent ideas on making inference after selection by adaptive methods such as the lasso and forward-stepwise regression. The first method we discuss in Section 6.3.1 pioneered a particu- lar line of research, and has been followed in rapid succession by a series of generalizations and improvements discussed in Section 6.3.2.</p><h4 id="_6-3-1-the-covariance-test"><a class="header-anchor" href="#_6-3-1-the-covariance-test" aria-hidden="true">#</a> 6.3.1 The Covariance Test</h4><p>In this section we describe a method proposed for assigning p-values to predic- tors as they are successively entered by the lasso. This method is based on the LAR algorithm and its piecewise construction of the path of lasso solutions (Section 5.6). Suppose that we are in the usual linear regression setup, with an outcome vector <strong>y</strong> ∈R <em>N</em> and matrix of predictor variables <strong>X</strong> ∈R <em>N</em> × <em>p</em> related by</p><div class="language-"><pre><code>y = X β +  ,  ∼ N ( 0 ,σ^2 I N × N ) , (6.3)\n</code></pre></div><p>where <em>β</em> ∈R <em>p</em> are unknown coefficients to be estimated. To understand the motivation for the covariance test, let’s first consider forward-stepwise regression. This procedure enters predictors one at a time, choosing the predictor that most decreases the residual sum of squares at each stage. Defining RSS <em>k</em> to be the residual sum of squares for the model containing <em>k</em> predictors, we can use this change in residual sum-of-squares to form a test statistic</p><div class="language-"><pre><code>Rk =\n</code></pre></div><h6 id="_1-101"><a class="header-anchor" href="#_1-101" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>σ^2\n</code></pre></div><div class="language-"><pre><code>(RSS k − 1 −RSS k ) (6.4)\n</code></pre></div><h6 id="_148-statistical-inference"><a class="header-anchor" href="#_148-statistical-inference" aria-hidden="true">#</a> 148 STATISTICAL INFERENCE</h6><p>(with <em>σ</em> assumed known for now), and compare it to a <em>χ</em>^21 distribution. Figure 6.8(a) shows the quantiles of <em>R</em> 1 from forward stepwise regression (the chi-squared statistic for the first predictor to enter) versus those of a <em>χ</em>^21 variate, in the fully null case ( <em>β</em> = 0). The observed quantiles are much larger than those of the <em>χ</em>^21 distribution. A test at the 5% level, for example, using the <em>χ</em>^21 cutoff of 3_._ 84, would have an actual type I error of about 39%.</p><div class="language-"><pre><code>ooooo\n</code></pre></div><div class="language-"><pre><code>ooooooooo\nooooooooooo\noooooooooooooo\nooooooooooooo\nooooooooooo\nooooooo\nooooooo\nooooo\nooooo\nooo\n</code></pre></div><div class="language-"><pre><code>ooo\n</code></pre></div><div class="language-"><pre><code>oo\n</code></pre></div><div class="language-"><pre><code>o\n</code></pre></div><div class="language-"><pre><code>oo\n</code></pre></div><div class="language-"><pre><code>o\n</code></pre></div><div class="language-"><pre><code>0 2 4 6 8 10\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>6\n</code></pre></div><div class="language-"><pre><code>8\n</code></pre></div><div class="language-"><pre><code>10\n</code></pre></div><div class="language-"><pre><code>χ^21\n</code></pre></div><div class="language-"><pre><code>R^1\n</code></pre></div><div class="language-"><pre><code>(a) Forward stepwise\n</code></pre></div><div class="language-"><pre><code>oooooooooooooooooooooooooo\nooooooooooooooooooo\nooooooooooooo\noooooooooo\noooooo\nooooo\nooooo\noooo\n</code></pre></div><div class="language-"><pre><code>ooo\n</code></pre></div><div class="language-"><pre><code>oo\n</code></pre></div><div class="language-"><pre><code>o\n</code></pre></div><div class="language-"><pre><code>oo\n</code></pre></div><div class="language-"><pre><code>o\n</code></pre></div><div class="language-"><pre><code>o\n</code></pre></div><div class="language-"><pre><code>o\n</code></pre></div><div class="language-"><pre><code>0 1 2 3 4 5\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>3\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>5\n</code></pre></div><div class="language-"><pre><code>Exp(1)\n</code></pre></div><div class="language-"><pre><code>T^1\n</code></pre></div><div class="language-"><pre><code>(b) Lasso\n</code></pre></div><p><strong>Figure 6.8</strong> <em>A simulation example withN</em> = 100 <em>observations andp</em> = 10 <em>orthogonal predictors andβ</em> = 0_. (a) a quantile-quantile plot, constructed over 1000 simulations, of the standard chi-squared statisticR_ 1 <em>in</em> (6.4) <em>, measuring the drop in residual sum- of-squares for the first predictor to enter in forward stepwise regression, versus theχ</em>^21 <em>distribution. The dashed vertical line marks the 95% quantile of theχ</em>^21 <em>distribution. (b) a quantile-quantile plot of the covariance test statisticT</em> 1 <em>in</em> (6.5) <em>for the first predictor to enter in the lasso path, versus its asymptotic nul l distribution Exp(1). The covariance test explicitly accounts for the adaptive nature of lasso modeling, whereas the chi-squared test is not appropriate for adaptively selected models as in forward-stepwise regression.</em></p><p>The reason for this is clear: the chi-squared test assumes that the models being compared are pre-specified, not chosen on the basis of the data. But the forward stepwise procedure has deliberately chosen the strongest predictor among all of the available choices, so it is not surprising that it yields a larger drop in training error than expected. It seems difficult to derive an appropriate p-value for forward stepwise regression, one that properly accounts for the adaptive nature of the fitting. For the first step and the test of the global null hypothesis, one can use a permutation distribution. For subsequent steps, it is not clear how to correctly carry out permutations. One can resort to sample splitting: we divide the data in half, compute the sequence of models on one half and then evaluate their significance on the other half. But this can lead to a significant loss of power, unless the sample size is large.</p><h6 id="post-selection-inference-for-the-lasso-149"><a class="header-anchor" href="#post-selection-inference-for-the-lasso-149" aria-hidden="true">#</a> POST-SELECTION INFERENCE FOR THE LASSO 149</h6><p>Surprisingly, it turns out that for the lasso, a simple test can be derived that properly accounts for the adaptivity. Denote the knots returned by the LAR algorithm (Algorithm 5.1 on page 119) by <em>λ</em> 1 <em>&gt; λ</em> 2 <em>... &gt; λK</em>. These are the values of the regularization parameter <em>λ</em> where there is a change in the set of active predictors. Suppose that we wish to test significance of the predictor entered by LAR at <em>λk</em>. LetA <em>k</em> − 1 be the active set (the predictors with nonzero coefficients) before this predictor was added and let the estimate at the end of this step be <em>β</em> ˆ( <em>λk</em> +1). We refit the lasso, keeping <em>λ</em> = <em>λk</em> +1but using just the variables inA <em>k</em> − 1. This yields the estimate <em>β</em> ˆA <em>k</em> − 1 ( <em>λk</em> +1). The <em>covariance test statistic</em> is defined by</p><div class="language-"><pre><code>Tk =\n</code></pre></div><h6 id="_1-102"><a class="header-anchor" href="#_1-102" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>σ^2\n</code></pre></div><h6 id="·-1"><a class="header-anchor" href="#·-1" aria-hidden="true">#</a> ·</h6><h6 id="-233"><a class="header-anchor" href="#-233" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>〈 y , X β ˆ( λk +1)〉−〈 y , X β ˆA k − 1 ( λk +1)〉\n</code></pre></div><h6 id="-234"><a class="header-anchor" href="#-234" aria-hidden="true">#</a> )</h6><h6 id="_6-5"><a class="header-anchor" href="#_6-5" aria-hidden="true">#</a> . (6.5)</h6><p>This statistic measures how much of the covariance between the outcome and the fitted model can be attributed to the predictor that has just entered the model; i.e., how much improvement there was over the interval ( <em>λk,λk</em> +1) in this measure. Interestingly, for forward-stepwise regression, the corresponding covariance statistic is equal to <em>Rk</em> (6.4); however, for the lasso this is not the case (Exercise 6.2). Remarkably, under the null hypothesis that all <em>k</em> −1 signal variables are in the model, and under general conditions on the model matrix <strong>X</strong> , for the predictor entered at the next step we have</p><div class="language-"><pre><code>Tk\nd\n→Exp(1) (6.6)\n</code></pre></div><p>as <em>N, p</em> → ∞. Figure 6.8(b) shows the quantile-quantile plot for <em>T</em> 1 ver- sus Exp(1). When <em>σ</em>^2 is unknown, we estimate it using the full model: ˆ <em>σ</em>^2 = <em>N</em>^1 − <em>p</em> RSS <em>p</em>. We then plug this into (6.5), and the exponential test be- comes an <em>F</em> 2 <em>,N</em> − <em>p</em> test. Table 6.2 shows the results of forward stepwise regression and LAR/lasso applied to the diabetes data. Only the first ten steps are shown in each case. We see that forward stepwise regression enters eight terms at level 0.05, while the covariance test enters only four. However as we argued above, the forward stepwise p-values are biased downward, and hence they are not trustworthy. In Exercise 6.3 we discuss a method for combining a set of sequential p-values to control the false discovery rate of the list of selected predictors. When applied to the covariance test at an FDR of 5%, it yields a model containing the first four predictors. For comparison, cross-validation estimated the optimal model size for prediction to be in the range of 7 to 14 predictors. Why is the mean of the forward-stepwise statistic <em>R</em> 1 much larger than one, while the mean of <em>T</em> 1 is approximately equal to one? The reason is <em>shrinkage</em> : the lasso picks the best predictor available at each stage, but does not fit it fully by least squares. It uses shrunken estimates of the coefficients, and this shrinkage compensates exactly for the inflation due to the selection. This test is also the natural analogue of the degrees of freedom result for the lasso and</p><h6 id="_150-statistical-inference"><a class="header-anchor" href="#_150-statistical-inference" aria-hidden="true">#</a> 150 STATISTICAL INFERENCE</h6><p><strong>Table 6.2</strong> <em>Results of forward stepwise regression and LAR/lasso applied to the di- abetes data introduced in Chapter 2. Only the first ten steps are shown in each case. The p-values are based on (6.4), (6.5), and (6.11), respectively. Values marked as 0 are&lt;</em> 0_._ 01_._</p><div class="language-"><pre><code>Forward Stepwise LAR/lasso\nStep Term p-value Term p-value\nCovariance Spacing\n1 bmi 0 bmi 0 0\n2 ltg 0 ltg 0 0\n3 map 0 map 0 0.01\n4 age:sex 0 hdl 0.02 0.02\n5 bmi:map 0 bmi:map 0.27 0.26\n6 hdl 0 age:sex 0.72 0.67\n7 sex 0 glu^2 0.48 0.13\n8 glu^2 0.02 bmi^2 0.97 0.86\n9 age^2 0.11 age:map 0.88 0.27\n10 tc:tch 0.21 age:glu 0.95 0.44\n</code></pre></div><p>LAR, discussed in Section 2.5. The lasso with <em>k</em> nonzero coefficients has <em>k</em> degrees of freedom in expectation, and LAR spends one degree of freedom in each segment ( <em>λk</em> +1 <em>,λk</em> ) along the path. The covariance test has mean equal to one, the degrees of freedom per step. In a sense, the Exp(1) distribution is the analogue of the <em>χ</em>^21 distribution, for adaptive fitting. The exponential limiting distribution for the covariance test (6.5) requires certain conditions on the data matrix <strong>X</strong> , namely that the signal variables (having nonzero true coefficients) are not too correlated with the noise vari- ables. These conditions are similar to those needed for support recovery for the lasso (Chapter 11). In the next section we discuss a more general scheme that gives the <em>spacing test</em> , whose null distribution holds exactly for finite <em>N</em> and <em>p</em> , and works for any <strong>X</strong>.</p><h4 id="_6-3-2-a-general-scheme-for-post-selection-inference"><a class="header-anchor" href="#_6-3-2-a-general-scheme-for-post-selection-inference" aria-hidden="true">#</a> 6.3.2 A General Scheme for Post-Selection Inference</h4><p>Here we discuss a general scheme for inference after selection—one that yields exact p-values and confidence intervals in the Gaussian case. It can deal with any procedure for which the selection events can be characterized by a set of linear inequalities in <strong>y</strong>. In other words, the selection event can be written as { <strong>Ay</strong> ≤ <em>b</em> }for some matrix <strong>A</strong> and vector <em>b</em>. In particular, it can be applied to successive steps of the LAR algorithm, where it gives an exact (finite sample) form of the covariance test. Similarly, it can be applied to forward stepwise regression, and to the lasso at a fixed choice of the regularization parameter <em>λ</em>. Why can the selection events for these procedures be written in the form { <strong>Ay</strong> ≤ <em>b</em> }? This is easiest to see for forward-stepwise regression. In this case we</p><h6 id="post-selection-inference-for-the-lasso-151"><a class="header-anchor" href="#post-selection-inference-for-the-lasso-151" aria-hidden="true">#</a> POST-SELECTION INFERENCE FOR THE LASSO 151</h6><p>take <em>b</em> = 0. At the first step, forward-stepwise regression chooses the predictor whose absolute inner product with <strong>y</strong> is the largest (see Figure 6.10 for an illustration). This can be expressed by forming 2( <em>p</em> −1) rows in the matrix <strong>A</strong> , each computing a difference of inner products, once each for the positive and negative directions. Similarly, at the next step we add 2( <em>p</em> −2) rows contrasting the inner product between the selected predictor and the other <em>p</em> −2 predictors, and so on. The lasso solution at a fixed value of <em>λ</em> is characterized by an active set of variablesA, along with the signs of their coefficients. Again, it turns out that the selection event that led to this particular combination can be written in the form{ <strong>Ay</strong> ≤ <em>b</em> }for some <strong>A</strong> and <em>b</em>. That is, the set{ <strong>y</strong> | <strong>Ay</strong> ≤ <em>b</em> }corresponds to the values of the outcome vector <strong>y</strong> that would yield this same collection of active variables and signs (with <strong>X</strong> fixed) (see Lee, Sun, Sun and Taylor (2016), and Exercise 6.10). The same is true for the LAR algorithm after its <em>kth</em> step. Now suppose that <strong>y</strong> ∼ <em>N</em> ( <strong><em>μ</em></strong> <em>,σ</em>^2 <strong>I</strong> <em>N</em> × <em>N</em> ), and that we want to make infer- ences conditional on the event{ <strong>Ay</strong> ≤ <em>b</em> }. In particular, we wish to make inferences about <strong><em>η</em></strong> <em>T</em> <strong><em>μ</em></strong> , where <strong><em>η</em></strong> might depend on the selection event. With lasso, LAR, or forward-stepwise regression having selected this set, we can now make inference statements about the selected variables. For example, we could be interested in the (ordinary) regression coefficients of <strong>y</strong> on <strong>X</strong> A, namely <em>θ</em> ˆ= ( <strong>X</strong> <em>T</em> A <strong>X</strong> A)−^1 <strong>X</strong> <em>T</em> A <strong>y</strong>. These correspond to the population parameters <em>θ</em> = ( <strong>X</strong> <em>T</em> A <strong>X</strong> A)−^1 <strong>X</strong> <em>T</em> A <strong><em>μ</em></strong> , the coefficients in the projection of <strong><em>μ</em></strong> on <strong>X</strong> A. So here <strong><em>η</em></strong> <em>T</em> <strong><em>μ</em></strong> could correspond to one of these coefficients, and hence <strong><em>η</em></strong> is one of the columns of <strong>X</strong> A( <strong>X</strong> <em>T</em> A <strong>X</strong> A)−^1. We pursue this example in Section 6.3.2.1. Lee et al. (2016) and Taylor, Lockhart, Tibshirani 2 and Tibshirani (2014) show that</p><div class="language-"><pre><code>{ Ay ≤ b }={V−( y )≤ η T y ≤V+( y ) , V^0 ( y )≥ 0 } , (6.7)\n</code></pre></div><p>and furthermore, <strong><em>η</em></strong> <em>T</em> <strong>y</strong> and (V−( <strong>y</strong> ) <em>,</em> V+( <strong>y</strong> ) <em>,</em> V^0 ( <strong>y</strong> )) are statistically indepen- dent. See Figure 6.9 for a geometric view of this surprising result, known as the <em>polyhedral lemma</em>. The three values on the right in (6.7) are computed via</p><div class="language-"><pre><code>α =\n</code></pre></div><div class="language-"><pre><code>A η\n‖ η ‖^22\n</code></pre></div><div class="language-"><pre><code>V−( y ) = max\nj : αj&lt; 0\n</code></pre></div><div class="language-"><pre><code>bj −( Ay ) j + αj η T y\nαj\n</code></pre></div><div class="language-"><pre><code>V+( y ) = min\nj : αj&gt; 0\n</code></pre></div><div class="language-"><pre><code>bj −( Ay ) j + αj η T y\nαj\nV^0 ( y ) = min\nj : αj =0\n( bj −( Ay ) j )\n</code></pre></div><h6 id="_6-8"><a class="header-anchor" href="#_6-8" aria-hidden="true">#</a> (6.8)</h6><p>(Exercise 6.7). Hence the selection event{ <strong>Ay</strong> ≤ <em>b</em> }is equivalent to the event that <strong><em>η</em></strong> <em>T</em> <strong>y</strong> falls into a certain range, a range depending on <strong>A</strong> and <em>b</em>. This equivalence and the independence means that the conditional inference on</p><h6 id="_152-statistical-inference"><a class="header-anchor" href="#_152-statistical-inference" aria-hidden="true">#</a> 152 STATISTICAL INFERENCE</h6><div class="language-"><pre><code>V−(y) V+(y)\nPη⊥y\n</code></pre></div><div class="language-"><pre><code>Pηy\n</code></pre></div><div class="language-"><pre><code>ηTy\n</code></pre></div><div class="language-"><pre><code>y\n</code></pre></div><div class="language-"><pre><code>η\n</code></pre></div><div class="language-"><pre><code>{Ay≤b}\n</code></pre></div><p><strong>Figure 6.9</strong> <em>Schematic il lustrating the polyhedral lemma (6.7), for the caseN</em> = 2 <em>and</em> ‖ <strong><em>η</em></strong> ‖ 2 = 1_. The yel low region is the selection event_ { <strong>Ay</strong> ≤ <em>b</em> }<em>. We decompose</em> <strong>y</strong><em>as the sum of two terms: its projectionP</em> <strong><em>η</em></strong> <strong>y</strong> <em>onto</em> <strong><em>η</em></strong> <em>(with coordinate</em> <strong><em>η</em></strong> <em>T</em> <strong>y</strong> <em>) and its projection onto the</em> ( <em>N</em> −1) <em>-dimensional subspace orthogonal to</em> <strong><em>η</em></strong> <em>:</em> <strong>y</strong> = <em>P</em> <strong><em>η</em></strong> <strong>y</strong> + <em>P</em> <strong><em>η</em></strong> ⊥ <strong>y</strong><em>. Conditioning onP</em> <strong><em>η</em></strong> ⊥ <strong>y</strong> <em>, we see that the event</em> { <strong>Ay</strong> ≤ <em>b</em> } <em>is equivalent to the event</em> {V−( <strong>y</strong> )≤ <strong><em>η</em></strong> <em>T</em> <strong>y</strong> ≤ V+( <strong>y</strong> )}<em>. Furthermore</em> V+( <strong>y</strong> ) <em>and</em> V−( <strong>y</strong> ) <em>are independent of</em> <strong><em>η</em></strong> <em>T</em> <strong>y</strong><em>since they are functions ofP</em> <strong><em>η</em></strong> ⊥ <strong>y</strong> <em>only, which is independent of</em> <strong>y</strong><em>.</em></p><p><strong><em>η</em></strong> <em>T</em> <strong><em>μ</em></strong> can be made using the truncated distribution of <strong><em>η</em></strong> <em>T</em> <strong>y</strong> , a truncated normal distribution. To use this fact, we define the cumulative distribution function (CDF) of a truncated normal distribution with support confined to [ <em>c,d</em> ]:</p><div class="language-"><pre><code>Fμ,σc,d 2 ( x ) =\nΦ(( x − μ ) /σ )−Φ(( c − μ ) /σ )\nΦ(( d − μ ) /σ )−Φ(( c − μ ) /σ )\n</code></pre></div><h6 id="_6-9"><a class="header-anchor" href="#_6-9" aria-hidden="true">#</a> , (6.9)</h6><p>with Φ the CDF of the standard Gaussian. Now the CDF of a random variable, evaluated at the value of that random variable, has a uniform distribution. Hence we can write</p><div class="language-"><pre><code>F V\n</code></pre></div><div class="language-"><pre><code>− , V+\nη T μ , σ^2 ‖ η ‖^22\n</code></pre></div><h6 id="-235"><a class="header-anchor" href="#-235" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>η T y\n</code></pre></div><h6 id="-236"><a class="header-anchor" href="#-236" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>|{ Ay ≤ b }∼U(0 , 1). (6.10)\n</code></pre></div><p>This result is used to make conditional inferences about any linear functional <strong><em>η</em></strong> <em>T</em> <strong><em>μ</em></strong>. For example, we can compute a p-value for testing <strong><em>η</em></strong> <em>T</em> <strong><em>μ</em></strong> = 0. We can also construct a 1− <em>α</em> level selection interval for <em>θ</em> = <strong><em>η</em></strong> <em>T</em> <strong><em>μ</em></strong> by inverting this</p><p>test, as follows. Let <em>P</em> ( <em>θ</em> ) = <em>F</em> V</p><div class="language-"><pre><code>− , V+\nθ, σ^2 ‖ η ‖^22\n</code></pre></div><h6 id="-237"><a class="header-anchor" href="#-237" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>η T y\n</code></pre></div><h6 id="-238"><a class="header-anchor" href="#-238" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>|{ Ay ≤ b }. The lower boundary\n</code></pre></div><p>of the interval is the largest value of <em>θ</em> such that 1− <em>P</em> ( <em>θ</em> )≤ <em>α/</em> 2, and the upper boundary is the smallest value of <em>θ</em> such that <em>P</em> ( <em>θ</em> )≤ <em>α/</em> 2.</p><p><em>Example 6.1.</em> To help understand these results, we present an example. We simulated <em>N</em> = 60 observations from the model <em>Y</em> =</p><p>∑ <em>p j</em> =1 <em>Xjβj</em> + <em>Z</em> , with <em>X</em> 1 <em>,X</em> 2 <em>,...,Xp, Z</em> ∼ <em>N</em> (0 <em>,</em> 1), and each standardized to have sample mean zero and unit <em>`</em> 2 norm. We considered the global null case with all <em>βj</em> = 0, and</p><h6 id="post-selection-inference-for-the-lasso-153"><a class="header-anchor" href="#post-selection-inference-for-the-lasso-153" aria-hidden="true">#</a> POST-SELECTION INFERENCE FOR THE LASSO 153</h6><div class="language-"><pre><code>x 1\n</code></pre></div><p>x 2 ∞</p><div class="language-"><pre><code>hx 1 ,yi\n</code></pre></div><div class="language-"><pre><code>V−(y)\n</code></pre></div><div class="language-"><pre><code>y\nV−(y)\n</code></pre></div><div class="language-"><pre><code>x 1\n</code></pre></div><div class="language-"><pre><code>x 2\n</code></pre></div><div class="language-"><pre><code>x 3\n</code></pre></div><div class="language-"><pre><code>∞\nhx 1 ,yi\n</code></pre></div><div class="language-"><pre><code>y\n</code></pre></div><div class="language-"><pre><code>Figure 6.10 Selection regions in Example 6.1 for whichλ 1 =〈 x 1 , y 〉. Left panel: two\northogonal predictors; right panel: three correlated predictors. The red line indicates\nthe part of the setP η ⊥ y + t η inside the selection region. In the left panel, V−( y ) =\n〈 x 2 , y 〉 , while in the right it isλ 2.\n</code></pre></div><div class="language-"><pre><code>found the predictor j 1 having largest absolute inner product with y. This is the\nfirst variable to enter the LAR or lasso path. We wish to make inference on λ 1 ,\nthe value of the largest knot in LAR, under the global null hypothesis. Thus\nη = x j 1 and η T y is the attained inner product (for simplicity we condition\non a positive sign for the inner-product). Note that with our standardization,\nη T y = x Tj 1 y is also the simple least-squares coefficient of y on the chosen x j 1 ,\nand so we are also making (conditional) inference on the population coefficient\nin the simple regression of y on x j 1. We chose five scenarios with number of\npredictors p ∈ { 2 , 5 , 10 , 20 , 50 }. We also considered two correlation patterns\nfor the predictors: uncorrelated and pairwise correlation 0. 5. Figure 6.10 il-\nlustrates the corresponding version of Figure 6.9 for the two situations. The\nupper bound in all cases isV+=∞, and the lower boundV−depends on y in\neach simulation. In the orthogonal case (left panel), conditioning on P η ⊥ y re-\nduces to conditioning on the values of| x Tk y |, for all predictors k not achieving\nthe maximum absolute inner product. Hence the lower bound on η T y is the\nsecond-largest among these. The right panel shows the nonorthogonal case,\nwith correlations between the Xj. Here the situation is slightly more complex,\nbut nevertheless a simple formula can be used to deriveV−( y ): it turns out\nto be λ 2 , the second knot in the LAR sequence (Exercise 6.11). Figure 6.11\nshows the resulting truncated normal densities from (6.10), averaged over 100\nsimulations. We plotted the density for the average value ofV−over the simu-\nlations. The colored squares along the bottom show the average largest inner\nproduct λ 1 = η T y in each setting. In the lower panel, with larger p , the effec-\ntive number of variables is smaller due to the correlation, so the maximum is\nsmaller as well. We pursue this example further in Section 6.3.2.2. ♦\nThis general mechanism (6.10) allows one to make inferences about any\n</code></pre></div><h6 id="_154-statistical-inference"><a class="header-anchor" href="#_154-statistical-inference" aria-hidden="true">#</a> 154 STATISTICAL INFERENCE</h6><div class="language-"><pre><code>0 1 2 3 4\n</code></pre></div><div class="language-"><pre><code>0.0 0.5 1.0 1.5 2.0\n</code></pre></div><div class="language-"><pre><code>Density\n</code></pre></div><div class="language-"><pre><code>p=2\np=5\np=10\np=20\np=50\n</code></pre></div><div class="language-"><pre><code>Uncorrelated Predictors\n</code></pre></div><div class="language-"><pre><code>0 1 2 3 4\n</code></pre></div><div class="language-"><pre><code>0.0 0.5 1.0 1.5 2.0\n</code></pre></div><div class="language-"><pre><code>Density\n</code></pre></div><div class="language-"><pre><code>Pairwise Correlation 0.5\n</code></pre></div><p><strong>Figure 6.11</strong> <em>Simulation:N</em> = 60 <em>observations from the model withX</em> 1 <em>,X</em> 2 <em>,...Xp</em> ∼ <em>N</em> (0 <em>,</em> 1) <em>,Y</em> =</p><h6 id="∑-31"><a class="header-anchor" href="#∑-31" aria-hidden="true">#</a> ∑</h6><p><em>jXijβj</em> + <em>ZwithZ</em> ∼ <em>N</em> (0 <em>,</em> 1) <em>, with allβj</em> = 0 <em>; two different predictor- correlation settings. The selection chooses the predictorj</em> 1 <em>having largest absolute inner product with</em> <strong>y</strong><em>. Shown is the truncated density on the left-hand side of (6.10) forp</em> = 2 <em>,</em> 5 <em>,</em> 10 <em>,</em> 20 <em>,</em> 50_. The colored squares along the bottom show the average largest inner product in each setting._</p><p>linear functional <strong><em>η</em></strong> <em>T</em> <strong><em>μ</em></strong> ; for example, inferences about any parameter <strong><em>η</em></strong> <em>T</em> <strong><em>μ</em></strong> at a given step of the LAR algorithm, or at a lasso solution computed at <em>λ</em>. The form of <strong>A</strong> and <em>b</em> is different depending on the setting, but otherwise the construction is the same. We illustrate two applications in the next two sections.</p><h5 id="_6-3-2-1-fixed-λ-inference-for-the-lasso"><a class="header-anchor" href="#_6-3-2-1-fixed-λ-inference-for-the-lasso" aria-hidden="true">#</a> 6.3.2.1 Fixed- λ Inference for the Lasso</h5><p>Consider the solution to the lasso, at some fixed value of <em>λ</em>. We can apply result (6.10) by constructing <strong>A</strong> and <em>b</em> so that the event{ <strong>Ay</strong> ≤ <em>b</em> }represents the set of outcome vectors <strong>y</strong> that yield the observed active set and signs of the predictors selected by the lasso at <em>λ</em>. These inequalities derive from the sub- gradient conditions that characterize the solution (Exercise 6.10). This yields an active setAof variables, and we can now make conditional inference on the population regression coefficients of <strong>y</strong> on <strong>X</strong> A, for example. This means we will perform a separate conditional analysis for <strong><em>η</em></strong> equal to each of the columns of <strong>X</strong> A( <strong>X</strong> <em>T</em> A <strong>X</strong> A)−^1. Hence we can obtain exact p-values and confidence intervals for the parameters of the active set in the lasso solution at <em>λ</em>. These quantities</p><h6 id="post-selection-inference-for-the-lasso-155"><a class="header-anchor" href="#post-selection-inference-for-the-lasso-155" aria-hidden="true">#</a> POST-SELECTION INFERENCE FOR THE LASSO 155</h6><p>have the correct type-I error and coverage <em>conditional on the membership and signs of the active set.</em>^3</p><div class="language-"><pre><code>−30\n</code></pre></div><div class="language-"><pre><code>−10\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>10\n</code></pre></div><div class="language-"><pre><code>20\n</code></pre></div><div class="language-"><pre><code>30\n</code></pre></div><div class="language-"><pre><code>Predictor\n</code></pre></div><div class="language-"><pre><code>Coefficient\n</code></pre></div><div class="language-"><pre><code>bmi map hdl ltg glu^2 age:sex bmi:map\n</code></pre></div><div class="language-"><pre><code>OLS interval\nLasso interval\n</code></pre></div><p><strong>Figure 6.12</strong> <em>Lasso model fit to the diabetes data. The solution atλ</em> = 7 <em>yields a model with seven nonzero coefficients. Shown are the 95% confidence intervals for the least-squares fit using the chosen variables. The OLS intervals ignore the selection, while the lasso intervals are exact under a Gaussian assumption, and condition on the selection event. Disclosure:λwas selected by cross-validation (1 SE rule), and σ</em>^2 <em>in (6.10) was estimated using the residuals from the ful l regression on al l 64 variables.</em></p><p>Figure 6.12 shows the result of fixed- <em>λ</em> lasso inference for the diabetes data with <em>λ</em> = 7; seven variables were selected. Notice that we now focus attention on the OLS regression coefficients using the reduced model containing only those seven predictors. The blue intervals are based on the usual multiple regression normal theory, ignoring the fact that we used the data to select the seven variables from the full set of 64. The red post-selection intervals were constructed by inverting relationship (6.10), and take the selection into account. We see that these two sets of intervals are similar for the larger coefficients, but the selection-adjusted ones are (appropriately) longer for the smaller coefficients. How did we choose <em>λ</em> = 7? Here we cheated a bit, and used ten-fold cross- validation (using the one-standard-error rule). In practice one would need to condition on this selection event as well, which would add considerably more complexity to the selection set. Simulations suggest this does not widen the confidence intervals substantially. In the next section, we discuss conditional inference at the LAR sequence{ <em>λk</em> }, which limits the set of <em>λ</em> s to the knots in</p><p>(^3) Lee et al. (2016) also discuss inference without conditioning on the signs, by considering the union of all regions with the same active set.</p><h6 id="_156-statistical-inference"><a class="header-anchor" href="#_156-statistical-inference" aria-hidden="true">#</a> 156 STATISTICAL INFERENCE</h6><p>the lasso path. We also needed to estimate <em>σ</em> in (6.10); since <em>N &gt; p</em> , we used the root mean-squared error from the full regression on all 64 predictors for this purpose.</p><h5 id="_6-3-2-2-the-spacing-test-for-lar"><a class="header-anchor" href="#_6-3-2-2-the-spacing-test-for-lar" aria-hidden="true">#</a> 6.3.2.2 The Spacing Test for LAR</h5><p>Here we apply the inference procedure (6.10) to successive steps of the LAR algorithm. We already visited the first step in Example 6.1 on page 152, for testing the global null hypothesis. There we set <strong><em>η</em></strong> <em>T</em> 1 <strong>y</strong> = <em>λ</em> 1 = max <em>j</em> |〈 <strong>x</strong> <em>j,</em> <strong>y</strong> 〉|, and the test amounts to testing if this maximum covariance exceeds what we expect by chance. We saw thatV−= <em>λ</em> 2 ,V+= +∞, and hence the resulting test can be written in a very simple form:</p><h6 id="r-1-1−-f-v"><a class="header-anchor" href="#r-1-1−-f-v" aria-hidden="true">#</a> R 1 = 1− F V</h6><div class="language-"><pre><code>− , V+\n0 ,σ^2 ( λ^1 |{ Ay ≤ b }) =\n</code></pre></div><div class="language-"><pre><code>1 −Φ( λ 1 /σ )\n1 −Φ( λ 2 /σ )\n</code></pre></div><h6 id="∼-u-0-1-6-11"><a class="header-anchor" href="#∼-u-0-1-6-11" aria-hidden="true">#</a> ∼ U (0 , 1). (6.11)</h6><p>Remarkably, the uniform distribution above holds <em>exactly</em> for finite <em>N</em> and <em>p</em> , and for any <strong>X</strong>. This is known as the <em>spacing</em> test (Taylor et al. 2014) for the global null hypothesis: it is a nonasymptotic version of the covariance test, and is asymptotically equivalent to it (Exercise 6.5). The spacing test is a monotone function of <em>λ</em> 1 − <em>λ</em> 2 : the larger this spacing, the smaller the p-value. Similarly, there is a more general form of the spacing test for testing that the partial regression coefficient of the variable added at any given LAR step is zero. These tests are based on the successive values for <em>λk</em> , and result in expressions more complex than Equation (6.11). In detail, if variable <strong>x</strong> <em>jk</em> is chosen at the <em>kth</em> step in the LAR algorithm, one can show that the corresponding knot <em>λk</em> is given by <em>λk</em> = <strong><em>η</em></strong> <em>Tk</em> <strong>y</strong> , with</p><div class="language-"><pre><code>η k =\n</code></pre></div><div class="language-"><pre><code>P ⊥A k − 1 x jk\nsk − x Tjk X A k − 1 ( X T A k − 1 X A k − 1 )−^1 s A k − 1\n</code></pre></div><h6 id="_6-12"><a class="header-anchor" href="#_6-12" aria-hidden="true">#</a> (6.12)</h6><p>(Exercise 6.8). HereA <em>k</em> − 1 indexes the active set after <em>k</em> −1 steps, and</p><div class="language-"><pre><code>P ⊥A k − 1 = I N − X A k − 1 ( X T A k − 1 X A k − 1 )−^1 X A k − 1\n</code></pre></div><p>is the residual projection operator that “adjusts” <strong>x</strong> <em>jk</em> for <strong>X</strong> A <em>k</em> − 1. Finally, <em>sk</em> and <em>s</em> A <em>k</em> − 1 are the signs for the coefficients for variables <em>k</em> and those indexed by A <em>k</em> − 1 (the latter being a ( <em>k</em> −1)-vector). Using this value of <strong><em>η</em></strong> , the spacing test follows from the general inference procedure outlined above, culminating in (6.10) on page 152. The matrix <strong>A</strong> at knot <em>λk</em> has considerably more rows than in the fixed- <em>λ</em> case, since we are conditioning on the entire sequence{ <em>λ`</em> } <em>k</em> 1. Nevertheless the computations are quite manageable, and one can compute exact p-values as well as confidence intervals for the chosen variables, as in the fixed- <em>λ</em> case. Taylor et al. (2014) give some simplified versions of the general spacing test—approximations to the exact case, that empirically are very close, and</p><h6 id="post-selection-inference-for-the-lasso-157"><a class="header-anchor" href="#post-selection-inference-for-the-lasso-157" aria-hidden="true">#</a> POST-SELECTION INFERENCE FOR THE LASSO 157</h6><p>asymptotically are equivalent (also with the covariance test). The most ap- pealing of these has the form</p><div class="language-"><pre><code>Rk =\n</code></pre></div><h6 id="φ"><a class="header-anchor" href="#φ" aria-hidden="true">#</a> Φ</h6><h6 id="-239"><a class="header-anchor" href="#-239" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>λk − 1\nσ ‖ η k ‖ 2\n</code></pre></div><h6 id="-240"><a class="header-anchor" href="#-240" aria-hidden="true">#</a> )</h6><h6 id="−φ"><a class="header-anchor" href="#−φ" aria-hidden="true">#</a> −Φ</h6><h6 id="-241"><a class="header-anchor" href="#-241" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>λk\nσ ‖ η k ‖ 2\n</code></pre></div><h6 id="-242"><a class="header-anchor" href="#-242" aria-hidden="true">#</a> )</h6><h6 id="φ-1"><a class="header-anchor" href="#φ-1" aria-hidden="true">#</a> Φ</h6><h6 id="-243"><a class="header-anchor" href="#-243" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>λk − 1\nσ ‖ η k ‖ 2\n</code></pre></div><h6 id="-244"><a class="header-anchor" href="#-244" aria-hidden="true">#</a> )</h6><h6 id="−φ-1"><a class="header-anchor" href="#−φ-1" aria-hidden="true">#</a> −Φ</h6><h6 id="-245"><a class="header-anchor" href="#-245" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>λk +1\nσ ‖ η k ‖ 2\n</code></pre></div><h6 id="_6-13"><a class="header-anchor" href="#_6-13" aria-hidden="true">#</a> ) , (6.13)</h6><p>which is an exact generalization of (6.11), usingV−= <em>λk</em> − 1 andV+= <em>λk</em> +1. It is easy to see that the term of interest (top-right in (6.13)) is</p><div class="language-"><pre><code>θ ̃ k = λk\nσ ‖ η ‖ 2\n</code></pre></div><h6 id="-246"><a class="header-anchor" href="#-246" aria-hidden="true">#</a> =</h6><div class="language-"><pre><code>η Tk y\nσ ‖ η ‖ 2\n</code></pre></div><h6 id="_6-14"><a class="header-anchor" href="#_6-14" aria-hidden="true">#</a> (6.14)</h6><p>is the (absolute) standardized partial regression coefficient for <strong>x</strong> <em>jk</em> in the pres- ence of <strong>X</strong> A <em>k</em> − 1 (Exercise 6.9); this view shows that testing for <em>λk</em> amounts to testing for this partial regression coefficient. The rightmost column of Table 6.2 shows the result of this more general spacing test applied to the diabetes data. Qualitatively the results look similar to those from the covariance test. Although the spacing test and fixed- <em>λ</em> approaches are similar in their con- struction, and are both exact, they are different in an important way. In par- ticular, the spacing test applies to each step of the sequential LAR procedure, and uses specific <em>λ</em> values (the knots). In contrast, the fixed- <em>λ</em> inference can be applied at any value of <em>λ</em> , but then treats this value as fixed. Hence it ignores any additional variability caused by choosing <em>λ</em> from the data.</p><h4 id="_6-3-3-what-hypothesis-is-being-tested"><a class="header-anchor" href="#_6-3-3-what-hypothesis-is-being-tested" aria-hidden="true">#</a> 6.3.3 What Hypothesis Is Being Tested?</h4><p>In adaptive testing, this question is tricky. The covariance test uses a set of conditional hypotheses: at each stage of LAR, we are testing whether the coef- ficients of all other predictors not yet in the model are zero. This is sometimes called the <em>complete null</em> hypothesis It turns out that the spacing test has a different focus. At the first step, it tests the global null hypothesis, as does the covariance test. But at subsequent steps, it tests whether the partial correlation of the given predictor entered at that step is zero, adjusting for other variables that are currently in the model. This is sometimes called the <em>incremental nul l</em> hypothesis. Unlike the covariance test, it does not try to assess the overall correctness of the current model. The fixed- <em>λ</em> test is similar; it conditions on the current active set of predictors and tests whether the coefficient of <em>any</em> given predictor is zero in the projected model. In contrast, Section 6.4 below discusses a procedure which forms confidence intervals for the <em>population</em> regression parameters in the full model.</p><h6 id="_158-statistical-inference"><a class="header-anchor" href="#_158-statistical-inference" aria-hidden="true">#</a> 158 STATISTICAL INFERENCE</h6><h4 id="_6-3-4-back-to-forward-stepwise-regression"><a class="header-anchor" href="#_6-3-4-back-to-forward-stepwise-regression" aria-hidden="true">#</a> 6.3.4 Back to Forward Stepwise Regression</h4><p>At the beginning of this section, we complained that na ̈ıve inference for forward-stepwise regression ignores the effects of selection, as in Figure 6.8(a) and the left side of Table 6.2. Coming full circle, we note that the general inference procedure outlined in Section 6.3.2 can in fact be applied to forward stepwise regression, providing proper selective inference for that procedure as well. In that case, the constraint matrix <strong>A</strong> is somewhat complicated, con- taining approximately 2 <em>pk</em> rows at step <em>k</em>. However the resulting procedure is computationally tractable: details are in Taylor et al. (2014) and Loftus and Taylor (2014).</p><h3 id="_6-4-inference-via-a-debiased-lasso"><a class="header-anchor" href="#_6-4-inference-via-a-debiased-lasso" aria-hidden="true">#</a> 6.4 Inference via a Debiased Lasso</h3><p>The aim of the method that we describe here is quite different from those discussed in Section 6.3. It does not attempt to make inferences about the partial regression coefficients in models derived by LAR or the lasso. Instead it directly estimates confidence intervals for the full set of population regres- sion parameters, under an assumed linear model. To do so, it uses the lasso estimate^4 as a starting point and applies a debiasing operation to yield an estimate that can be used for constructing confidence intervals. Suppose we assume that the linear model <strong>y</strong> = <strong>X</strong> <em>β</em> + <strong><em></em></strong> is correct, and we want confidence intervals for the components{ <em>βj</em> } <em>p</em> 1. Then if <em>N &gt; p</em> , we can simply fit the full model by least squares and use standard intervals from least-squares theory</p><div class="language-"><pre><code>β ̂ j ± z ( α ) vjσ, ˆ (6.15)\n</code></pre></div><p>where <em>β</em> ̂is the OLS estimate, <em>vj</em>^2 =</p><h6 id="-247"><a class="header-anchor" href="#-247" aria-hidden="true">#</a> (</h6><h6 id="x-t-x"><a class="header-anchor" href="#x-t-x" aria-hidden="true">#</a> X T X</h6><h6 id="−-1-3"><a class="header-anchor" href="#−-1-3" aria-hidden="true">#</a> )− 1</h6><div class="language-"><pre><code>jj , ˆ σ\n</code></pre></div><p>(^2) =∑ <em>i</em> ( <em>yi</em> − <em>y</em> ˆ <em>i</em> ) (^2) <em>/</em> ( <em>N</em> − <em>p</em> ), and <em>zα</em> is the <em>α</em> -percentile of the standard normal distribution. However this approach does not work when <em>N &lt; p</em>. One proposal that has been suggested (Zhang and Zhang 2014, B ̈uhlmann 2013, van de Geer, B ̈uhlmann, Ritov and Dezeure 2013, Javanmard and Montanari 2014), is to use a debiased version of the lasso estimator, namely <em>β</em> ̂ <em>d</em> = <em>β</em> ̂ <em>λ</em> +^1 <em>N</em> <strong>ΘX</strong><em>T</em> ( <strong>y</strong> − <strong>X</strong> <em>β</em> ̂ <em>λ</em> ) <em>,</em> (6.16) where <em>β</em> ̂ <em>λ</em> is the lasso estimate at <em>λ</em> , and <strong>Θ</strong> is an approximate inverse of <strong>Σ</strong> ̂=^1 <em>N</em> <strong>X</strong><em>T</em> <strong>X</strong>. (^5) From this, we can write <em>β</em> ̂ <em>d</em> = <em>β</em> +^1 <em>N</em> <strong>ΘX</strong></p><h6 id="t-i"><a class="header-anchor" href="#t-i" aria-hidden="true">#</a> T  + ( I</h6><div class="language-"><pre><code>p −\n1\nN ΘX\n</code></pre></div><div class="language-"><pre><code>T X )( β ̂\nλ − β )\n︸ ︷︷ ︸\n∆ ˆ\n</code></pre></div><h6 id="_6-17"><a class="header-anchor" href="#_6-17" aria-hidden="true">#</a> (6.17)</h6><p>(^4) Fit using a value of <em>λ</em> based on consistency considerations. (^5) If <em>N</em> ≥ <em>p</em> , then <strong>Θ</strong> − (^1) =^1 <em>N</em> <strong>X</strong><em>T</em> <strong>X</strong> and (6.16) would be exactly unbiased for <em>β</em>. However when <em>N &lt; p</em> , <strong>X</strong> <em>T</em> <strong>X</strong> <em>/N</em> is not invertible and we try to find an approximate inverse.</p><h6 id="inference-via-a-debiased-lasso-159"><a class="header-anchor" href="#inference-via-a-debiased-lasso-159" aria-hidden="true">#</a> INFERENCE VIA A DEBIASED LASSO 159</h6><p>with <strong><em></em></strong> ∼ <em>N</em> ( <strong>0</strong> <em>,σ</em>^2 <strong>I</strong> <em>p</em> ). These authors provide (different) estimates of <strong>Θ</strong> so that‖ <strong>∆</strong> ˆ‖∞ → 0. From Equation (6.17), one can use the approximation</p><p><em>β</em> ̂ <em>d</em> ∼ <em>N</em> ( <em>β,σ</em>^2 <em>N</em> <strong>Θ ΣΘ</strong> ̂ <em>T</em> ) to form confidence intervals for the components <em>βj</em>. The debiasing operation (6.16) can be viewed as an approximate Newton step for optimizing the residual sum of squares, starting at the lasso estimate <em>β</em> (Exercise 6.6). There have been different proposals for estimating <strong>Θ</strong> :</p><ul><li>van de Geer et al. (2013) estimate <strong>Θ</strong> using neighborhood-based methods to impose sparsity on the components (see Chapter 9 for details on sparse graph estimation).</li><li>Javanmard and Montanari (2014) use a different approach: for each <em>j</em> they define <em>mj</em> to be the solution to the convex program</li></ul><div class="language-"><pre><code>minimize\nm ∈R p\nmT Σ ̂ m (6.18)\n</code></pre></div><div class="language-"><pre><code>subject to ‖ Σ ̂ m − ej ‖∞≤ γ, (6.19)\n</code></pre></div><div class="language-"><pre><code>with ej being the jth unit vector. Then they set\n</code></pre></div><div class="language-"><pre><code>Θ ̂:= ( m 1 ,m 2 ,...,mp ). (6.20)\n</code></pre></div><div class="language-"><pre><code>This tries to make both Σ ̂ Θ ̂≈ I and the variances of β ̂ jd small.\n</code></pre></div><div class="language-"><pre><code>−20\n</code></pre></div><div class="language-"><pre><code>−10\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>10\n</code></pre></div><div class="language-"><pre><code>20\n</code></pre></div><div class="language-"><pre><code>30\n</code></pre></div><div class="language-"><pre><code>Predictors\n</code></pre></div><div class="language-"><pre><code>Coefficients\n</code></pre></div><div class="language-"><pre><code>ltg bmi\nmapsex\nage:sex\n</code></pre></div><div class="language-"><pre><code>hdl\n</code></pre></div><div class="language-"><pre><code>Lasso estimate\nDebiased estimate\n95% CI\n</code></pre></div><p><strong>Figure 6.13</strong> <em>Diabetes data: Lasso estimates, debiased lasso estimates, and confi- dence intervals from the debiasing approach. These intervals have not been adjusted for multiple comparisons. The first 6 predictors have intervals not containing zero: when Bonferroni-adjusted, this number drops to three.</em></p><p>Figure 6.13 shows the result of applying the debiasing procedure of Javanmard and Montanari (2014) to the diabetes data. For six of the predictors, the</p><h6 id="_160-statistical-inference"><a class="header-anchor" href="#_160-statistical-inference" aria-hidden="true">#</a> 160 STATISTICAL INFERENCE</h6><p>95% confidence intervals do not contain zero. However these intervals are not corrected for multiple comparisons; if we instead use a Bonferroni-adjusted level of 0_._ 05 <em>/</em> 64, then the number of significant predictors drops to three. The top three predictors agree with those from the covariance and spacing tests of Table 6.2; the fourth predictor (sex) is not entered until step seven by the forward stepwise algorithm of Table 6.2, and not in the first ten steps by the other two procedures.</p><h3 id="_6-5-other-proposals-for-post-selection-inference"><a class="header-anchor" href="#_6-5-other-proposals-for-post-selection-inference" aria-hidden="true">#</a> 6.5 Other Proposals for Post-Selection Inference</h3><p>The PoSI method (Berk, Brown, Buja, Zhang and Zhao 2013, “Post-Selection Inference”) fits the selected submodel, and then adjusts the standard (non- adaptive) confidence intervals by accounting for all possible models that might have been delivered by the selection procedure. The adjustment is not a func- tion of the particular search method used to find the given model. This can be both an advantage and a disadvantage. On the positive side, one can apply the method to published results for which the search procedure is not speci- fied by the authors, or there is doubt as to whether the reported procedure is an accurate account of what was actually done. On the negative side, it can produce very wide (conservative) confidence intervals in order to achieve its robustness property. In detail, consider again the linear model <strong>y</strong> = <strong>X</strong> <em>β</em> + <strong><em></em></strong> , and suppose that a model-selection procedureMchooses a submodel <em>M</em> , with estimate <em>β</em> ̂ <em>M</em>. The authors of PoSI argue that inferences should most naturally be made not about the true underlying parameter vector <em>β</em> , but rather the parameters in the projection of <strong>X</strong> <em>β</em> onto <strong>X</strong> <em>M</em> :</p><div class="language-"><pre><code>βM = ( X TM X M )−^1 X TM X β. (6.21)\n</code></pre></div><p>This approach was also adopted with the conditional inference discussed in Section 6.3.2. Consider a confidence interval for the <em>jth</em> component of <em>βM</em> of the form</p><div class="language-"><pre><code>CI j · M = β ̂ j · M ± Kσv ˆ j · M, (6.22)\n</code></pre></div><p>with <em>v</em>^2 <em>j</em> · <em>M</em> = ( <strong>X</strong> <em>TM</em> <strong>X</strong> <em>M</em> )− <em>jj</em>^1. Then the PoSI procedure delivers a constant <em>K</em> so that</p><div class="language-"><pre><code>Pr( βj · M ∈CI j · M )≥ 1 − 2 α (6.23)\n</code></pre></div><p>over all possible model selection proceduresM. The value of <em>K</em> is a function of the data matrix <strong>X</strong> and the maximum number of nonzero components allowed in√ <em>βM</em> , but not the outcome vector <strong>y</strong>. The authors show that <em>K</em> grows like 2 log( <em>p</em> ) for orthogonal designs but can grow as quickly as</p><h6 id="√-14"><a class="header-anchor" href="#√-14" aria-hidden="true">#</a> √</h6><p><em>p</em> for nonorthog- onal designs. Note that any individual parameter in any projected submodel of the form (6.21) can be written as <em>aTβ</em> , with least-squares estimate <em>aTβ</em> ̂, where <em>β</em> ̂is the</p><h6 id="bibliographic-notes-161"><a class="header-anchor" href="#bibliographic-notes-161" aria-hidden="true">#</a> BIBLIOGRAPHIC NOTES 161</h6><p>least-squares estimate for the full model. Scheff ́e (1953) provides a way to obtain simultaneous inference for <em>al l</em> such linear combinations:</p><div class="language-"><pre><code>Pr\n</code></pre></div><h6 id="-248"><a class="header-anchor" href="#-248" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>sup\na\n</code></pre></div><div class="language-"><pre><code>[ aT ( β ̂− β )]^2\naT ( X T X )−^1 a · σ ˆ^2\n≤ KSch^2\n</code></pre></div><h6 id="-249"><a class="header-anchor" href="#-249" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>= 1− 2 α. (6.24)\n</code></pre></div><p>Assuming that the full model is correct, with Gaussian errors, it can be shown that <em>KSch</em> =</p><h6 id="√-15"><a class="header-anchor" href="#√-15" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>pFp,N − p, 1 − 2 α , which provides the\n</code></pre></div><h6 id="√-16"><a class="header-anchor" href="#√-16" aria-hidden="true">#</a> √</h6><p><em>p</em> upper bound referred to above. The PoSI authors show that using numerical methods and direct search, smaller values of <em>K</em> can be found for actual model matrices <strong>X</strong> , in particular when the user limits the search to all models within a certain maximum size. For the diabetes data, Andreas Buja computed for us the value of <em>K</em> for submodels of size 5 (this took a little less than 2 hours of computing time). The resulting values of <em>K</em> were 4.21 (90%), 4.42 (95%), and 4.85 (99%). At the 95% level, this yielded four significant predictorsbmi,map,hdl, andltg. This is one more predictor than we obtain from the lasso method of Figure 6.12, if the latter intervals were adjusted for multiplicity. The PoSI intervals have advantages compared to the lasso method of Fig- ure 6.12 in that they don’t require <em>σ</em> to be known or <em>λ</em> to be fixed. On the other hand, the confidence intervals from PoSI can be very wide. In the di- abetes dataset there are four very strong predictors: their lasso intervals are essentially unaffected by the selection and look much like the standard least- squares intervals. Even with a Bonferroni adjustment from 0.05 to 0.01, the intervals have approximate length± 2_._ 33 · <em>σvj</em> · <em>M</em> compared to± 4_._ 42 · <em>σvj</em> · <em>M</em> for PoSI. However the authors of PoSI make the point that their method provides much stronger protection against all kinds of (unreported) things that people actually do with their data, like fishing for models that have lots of significant predictors. A major limitation of PoSI at this time is computation. According to the authors, with parallel computation the current problem could probably be solved for models up to size 7 or 8 out of 64, but no larger.</p><h3 id="bibliographic-notes-3"><a class="header-anchor" href="#bibliographic-notes-3" aria-hidden="true">#</a> Bibliographic Notes</h3><p>Our discussion of the Bayesian Lasso is based on Park and Casella (2008). The bootstrap is due to Efron (1979); the book by Efron and Tibshirani (1993) is a comprehensive reference. The connection between Bayesian methods and the bootstrap is explored in various papers (Rubin 1981, Efron 1982, Efron 2011). The covariance test was introduced in Lockhart, Taylor, Tibshirani 2 and Tibshirani (2014); the discussion following that paper is a valuable resource on model selection. This work was extended to general models and exact tests in Taylor, Loftus and Tibshirani 2 (2016). The spacing test is proposed in Taylor et al. (2014), while Lee et al. (2016) derive the fixed <em>λ</em> inference procedure for the lasso. Taylor et al. (2014) and Loftus and Taylor (2014) propose tests for forward stepwise regression, the latter including categorical variables via the group lasso penalty. Grazier G’Sell, Wager, Chouldechova and Tibshirani</p><h6 id="_162-statistical-inference"><a class="header-anchor" href="#_162-statistical-inference" aria-hidden="true">#</a> 162 STATISTICAL INFERENCE</h6><p>(2015) propose FDR-controlling procedures for sequential testing and apply them to the model selection p-values described here. Grazier G’Sell, Taylor and Tibshirani (2013) develop a covariance test for the graphical lasso while Choi, Taylor and Tibshirani (2014) do the same for principal components. Fithian, Sun and Taylor (2014) provide a general theoretical framework for conditional inference after model selection, with particular emphasis on expo- nential families. The “debiasing approach” (Section 6.4) was proposed by a number of au- thors. For example, Zhang and Zhang (2014) derive confidence intervals for contrasts of high-dimensional regression coefficients, by replacing the usual score vector with the residual from a relaxed projection (i.e., the residual from sparse linear regression). B ̈uhlmann (2013) constructs p-values for coef- ficients in high-dimensional regression models, starting with ridge estimation and then employing a bias-correction term that uses the lasso. This initial work was followed by van de Geer et al. (2013), Javanmard and Montanari (2014), and Javanmard and Montanari (2013), who all present approaches for debiasing the lasso estimate based on estimates of the inverse covariance matrix of the predictors. (The latter work focuses on the special case of a predictor matrix <strong>X</strong> with i.i.d. Gaussian rows; the first two consider a general matrix <strong>X</strong> .) These debiased lasso estimates are asymptotically normal, which allows one to compute p-values both marginally for an individual coefficient, and simultaneously for a group of coefficients. The PoSI (Post-Selection In- ference) method was proposed in Berk et al. (2013).</p><h3 id="exercises-4"><a class="header-anchor" href="#exercises-4" aria-hidden="true">#</a> Exercises</h3><p>Ex. 6.1</p><div class="language-"><pre><code>(a) Show that in the orthonormal design setting X T X = I p × p , the covariance\ntest (6.5) reduces to the simple form\n</code></pre></div><div class="language-"><pre><code>Tk =\n</code></pre></div><h6 id="_1-103"><a class="header-anchor" href="#_1-103" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>σ^2\n</code></pre></div><div class="language-"><pre><code>· λk ( λk − λk +1). (6.25)\n</code></pre></div><div class="language-"><pre><code>for all steps k.\n(b) Show that for general X , the covariance test (6.5) reduces to (6.25) for\nthe first step ( k = 1)\n</code></pre></div><p>Ex. 6.2 Show that <em>Rk</em> in Equation (6.4) can be written as a covariance statistic</p><div class="language-"><pre><code>Rk =\n</code></pre></div><h6 id="_1-104"><a class="header-anchor" href="#_1-104" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>σ^2\n</code></pre></div><h6 id="·-2"><a class="header-anchor" href="#·-2" aria-hidden="true">#</a> ·</h6><h6 id="-250"><a class="header-anchor" href="#-250" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>〈 y , X β ˆ k 〉−〈 y , X β ˆ k − 1 〉\n</code></pre></div><h6 id="-251"><a class="header-anchor" href="#-251" aria-hidden="true">#</a> )</h6><h6 id="_6-26"><a class="header-anchor" href="#_6-26" aria-hidden="true">#</a> , (6.26)</h6><p>where <em>β</em> ˆ <em>k</em> is the coefficient vector after <em>k</em> steps of forward stepwise regression (with the coefficients of those variables not included set to 0).</p><h6 id="exercises-163"><a class="header-anchor" href="#exercises-163" aria-hidden="true">#</a> EXERCISES 163</h6><p>Ex. 6.3 <em>Sequential control of FDR</em>. Suppose that we carry out tests of a set of hypotheses <em>H</em> 01 <em>,H</em> 02 <em>,...,H</em> 0 <em>m</em> , using p-values <em>p</em> 1 <em>,p</em> 2 <em>,...,pm</em>. Let the ordered p-values be <em>p</em> (1) <em>&lt; p</em> (2) <em>&lt; ... &lt; p</em> ( <em>m</em> ). If we apply a procedure that rejects <em>R</em> of the hypotheses and there are <em>V</em> false positives among these, then the <em>false discovery rate</em> of the procedure is defined to be E( <em>V/R</em> ). Given a target FDR of <em>α</em> , the Benjamini–Hochberg (BH) procedure (Benjamini and Hochberg 1995) rejects the <em>R</em> hypotheses with the smallest <em>R</em> p-values, where <em>R</em> is the largest <em>j</em> such that <em>p</em> ( <em>j</em> )≤ <em>α</em> · <em>j/m</em>. If the p-values are independent, this procedure has an FDR of at most <em>α</em>.</p><div class="language-"><pre><code>(a) Compute the univariate regression coefficients β ̂ j and standard errors\nsê j for each predictor in thediabetesdata. Hence obtain approximate\nnormal scores zj = β ̂ j/ sê j and associated (two)-tailed p-values. Apply the\nBH procedure to find a list of significant predictors at an FDR of 5%.\n(b) Now suppose that our hypotheses have to be considered in order. That\nis, we must reject a contiguous initial block of K of the hypotheses H 01 , H 02 ,\n...,HK 0 (or we could reject none of them). The covariance or spacing test\nare examples of this. The BH procedure cannot be applied in this setting, as\nit does not respect the ordering. For example in Table 6.2, the BH procedure\nmight tell us to reject the null hypothesis forltg, but not reject that for\nbmi. This is not helpful, because we seek a model consisting of the first k\npredictors that enter, for some k ≥0. There is a generalization of the BH\nprocedure that can be applied here. Let the p-values from the covariance or\nspacing test be p 1 ,p 2 ,...pm and let rk =−\n</code></pre></div><div class="language-"><pre><code>∑ k\nj =1log(1− pj ) /k. Then the\nso-called ForwardStop rule rejects p 1 ,p 2 ,...p ˆ k whereˆ k is the largest k such\nthat rk ≤ α (Grazier G’Sell et al. 2015). Apply the ForwardStop rule to the\ncovariance or spacing test p-values with a target FDR of 5%.\n</code></pre></div><p>Ex. 6.4 Here we derive a fact about the multivariate normal distribution, and then in (c) we apply it to derive the spacing test for LAR in the global null case. Suppose that the random vector <em>Z</em> = ( <em>Z</em> 1 <em>,...,Zp</em> ) follows the multivariate normal distribution <em>N</em> (0 <em>,</em> <strong>Σ</strong> ) with Σ <em>jj</em> = 1 for all <em>j</em>.</p><div class="language-"><pre><code>(a) Let\n( j 1 ,s 1 ) = arg max\nj ∈{ 1 , 2 ,...,p } , s ∈{− 1 , 1 }\n</code></pre></div><div class="language-"><pre><code>( sZj )\n</code></pre></div><div class="language-"><pre><code>and assume that these indices are uniquely attained. Define the random\nvariables\n</code></pre></div><div class="language-"><pre><code>Mj = max\n1 ≤ i ≤ p, i 6 = j, s ∈{− 1 , 1 }\n</code></pre></div><h6 id="-252"><a class="header-anchor" href="#-252" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>sZi − s Σ ijZj\n1 − ssj Σ ij\n</code></pre></div><h6 id="-253"><a class="header-anchor" href="#-253" aria-hidden="true">#</a> }</h6><h6 id="_6-27"><a class="header-anchor" href="#_6-27" aria-hidden="true">#</a> . (6.27)</h6><div class="language-"><pre><code>with sj = arg max s ∈{− 1 , 1 }( sZj ). Show that Mj is independent of Zj , for\nj = 1 , 2 ,...p.\n(b) Let Φ( x ) be the CDF of a standard Gaussian, and\n</code></pre></div><div class="language-"><pre><code>U ( z,m ) =\n1 −Φ( z )\n1 −Φ( m )\n</code></pre></div><h6 id="_6-28"><a class="header-anchor" href="#_6-28" aria-hidden="true">#</a> . (6.28)</h6><h6 id="_164-statistical-inference"><a class="header-anchor" href="#_164-statistical-inference" aria-hidden="true">#</a> 164 STATISTICAL INFERENCE</h6><div class="language-"><pre><code>Verify that j 1 = j if and only if Zj ≥ Mj , and prove that U ( Zj 1 ,Mj 1 ) is\nuniformly distributed on (0 , 1).\n(c) In the LAR procedure with standardized predictors, let Σ = N^1 X T X , and\nZj = N^1 x Tj y. Show that λ 1 = max j,s ( sZj ) and λ 2 = Mj 1 (difficult). Hence\nderive the spacing test (6.11).\n</code></pre></div><p>Ex. 6.5 Show that as <em>N,p</em> →∞, the covariance test (6.5) and the spacing test (6.11) are asymptotically equivalent. [Hint: send <em>λ</em> 2 →∞at a rate such that <em>λ</em> 1 <em>/λ</em> 2 →1 and apply Mill’s ratio.]</p><p>Ex. 6.6 Consider the residual sum of squares function <em>J</em> ( <em>β</em> ) =‖ <strong>y</strong> − <strong>X</strong> <em>β</em> ‖^2 and construct a Newton step for minimizing <em>J</em> ( <em>β</em> ) of the form</p><div class="language-"><pre><code>βnew ← β +\n</code></pre></div><h6 id="∂j"><a class="header-anchor" href="#∂j" aria-hidden="true">#</a> ( ∂J</h6><div class="language-"><pre><code>∂β\n</code></pre></div><p>)− (^1) <em>∂J ∂β</em></p><h6 id="_6-29"><a class="header-anchor" href="#_6-29" aria-hidden="true">#</a> (6.29)</h6><p>where <em>β</em> is the lasso estimate at some <em>λ</em>. Show that this has the form (6.16) with ( <strong>X</strong> <em>T</em> <strong>X</strong> )−^1 replaced by the estimate <strong>Θ</strong> ̂from (6.20).</p><p>Ex. 6.7 <em>General inference for the LAR algorithm and the lasso.</em> Let <strong>y</strong> ∼ <em>N</em> ( <strong><em>μ</em></strong> <em>,σ</em>^2 <strong>I</strong> ), and consider the distribution of <strong>y</strong> conditional on the selection event{ <strong>Ay</strong> ≤ <em>b</em> }.</p><div class="language-"><pre><code>(a) Show that\n</code></pre></div><div class="language-"><pre><code>{ Ay ≤ b }={V−( y )≤ η T y ≤V+( y ) , V^0 ( y )≥ 0 } (6.30)\n</code></pre></div><div class="language-"><pre><code>with the variables above defined as follows:\n</code></pre></div><div class="language-"><pre><code>α =\nA η\n‖ η ‖^2\n</code></pre></div><div class="language-"><pre><code>V−( y ) = max\nj : αj&lt; 0\n</code></pre></div><div class="language-"><pre><code>bj −( Ay ) j + αj η T y\nαj\n</code></pre></div><div class="language-"><pre><code>V+( y ) = min\nj : αj&gt; 0\n</code></pre></div><div class="language-"><pre><code>bj −( Ay ) j + αj η T y\nαj\nV^0 ( y ) = min\nj : αj =0\n( bj −( Ay ) j ) (6.31)\n</code></pre></div><div class="language-"><pre><code>[Hint: subtract E( Ay | η T y ) from both sides of the inequality Ay ≤ b. Sim-\nplify and examine separately the cases αj&lt; 0 , = 0 and &gt; 0.]\n(b) Let\n</code></pre></div><div class="language-"><pre><code>Fμ,σc,d 2 ( x ) =\n</code></pre></div><div class="language-"><pre><code>Φ(( x − μ ) /σ )−Φ(( c − μ ) /σ )\nΦ(( d − μ ) /σ )−Φ(( c − μ ) /σ )\n</code></pre></div><h6 id="_6-32"><a class="header-anchor" href="#_6-32" aria-hidden="true">#</a> . (6.32)</h6><div class="language-"><pre><code>This is the truncated normal distribution, with support on [ c,d ]. Show that\n</code></pre></div><div class="language-"><pre><code>F V\n</code></pre></div><div class="language-"><pre><code>− , V+\nη T μ , σ^2 ‖ η ‖^2 ( η\n</code></pre></div><div class="language-"><pre><code>T y )|{ Ay ≤ b }∼U(0 , 1). (6.33)\n</code></pre></div><h6 id="exercises-165"><a class="header-anchor" href="#exercises-165" aria-hidden="true">#</a> EXERCISES 165</h6><div class="language-"><pre><code>This result can be used to make inferences about parameter η T μ at a given\nstep of the LAR algorithm, or for a lasso solution computed at a fixed value\nof λ.\n(c) Use result (6.33) to provide an alternate proof of the spacing test re-\nsult (6.11).\n</code></pre></div><p>Ex. 6.8 The <em>kth</em> knot in the LAR algorithm is the value <em>λk</em> at which the <em>kth</em> variable enters the model. At <em>λk</em> the coefficient of this variable is zero (about to grow from zero). Using the KKT optimality conditions, verify expression (6.12).</p><p>Ex. 6.9 With <strong><em>η</em></strong> <em>k</em> defined in (6.12), show that <em>θ</em> ̃ <em>k</em> in (6.14) is the absolute standardized partial regression coefficient of <strong>y</strong> on <strong>x</strong> <em>jk</em> , adjusted for <strong>X</strong> A <em>k</em> − 1.</p><p>Ex. 6.10 Consider a solution to the lasso problem</p><div class="language-"><pre><code>minimize\nβ\n</code></pre></div><h6 id="_1-105"><a class="header-anchor" href="#_1-105" aria-hidden="true">#</a> 1</h6><h6 id="_2-69"><a class="header-anchor" href="#_2-69" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ y − X β ‖^22 + λ ‖ β ‖ 1 ,\n</code></pre></div><p>and let <em>E</em> ⊂ { 1 <em>,...,p</em> }denote a candidate active set, and <em>sE</em> ∈ {− 1 <em>,</em> 1 }| <em>E</em> | the signs of the active variables. The KKT conditions corresponding to any solution <em>β</em> ̂ <em>E</em> with the same <em>E</em> and <em>SE</em> are given by</p><div class="language-"><pre><code>− X TE ( y − X Eβ ̂ E ) + λsE = 0 (6.34)\n− X T − E ( y − X Eβ ̂ E ) + λs − E = 0 , (6.35)\n</code></pre></div><p>with sign( <em>β</em> ̂ <em>E</em> ) = <em>sE</em> and‖ <em>s</em> − <em>E</em> ‖∞ <em>&lt;</em> 1. Eliminate <em>β</em> ̂ <em>E</em> in these equations, and show that the set of values of <strong>y</strong> with solution characterized by ( <em>E, sE</em> ) can be defined by a set of linear inequalities</p><div class="language-"><pre><code>Ay ≤ b.\n</code></pre></div><p>Ex. 6.11 Consider the setup in Example 6.1, and assume <strong>x</strong> <em>Tj</em> 1 <strong>y</strong> is positive. Using simple inequalities, derive an expression forV−( <strong>y</strong> ). Show that this is equal to <em>λ</em> 2 , the second LAR knot.</p><div class="language-"><pre><code>Chapter 7\n</code></pre></div><h2 id="matrix-decompositions"><a class="header-anchor" href="#matrix-decompositions" aria-hidden="true">#</a> Matrix Decompositions,</h2><h2 id="approximations-and-completion"><a class="header-anchor" href="#approximations-and-completion" aria-hidden="true">#</a> Approximations, and Completion</h2><h3 id="_7-1-introduction"><a class="header-anchor" href="#_7-1-introduction" aria-hidden="true">#</a> 7.1 Introduction</h3><p>This chapter is devoted to problems of the following type: given data in the form of an <em>m</em> × <em>n</em> matrix <strong>Z</strong> ={ <em>zij</em> }, find a matrix <strong>Z</strong> ̂that approximates <strong>Z</strong> in a suitable sense. One purpose might be to gain an understanding of the matrix <strong>Z</strong> through an approximation <strong>Z</strong> ̂that has simple structure. Another goal might be to impute or fill in any missing entries in <strong>Z</strong> , a problem known as <em>matrix completion</em>. Our general approach is to consider estimators based on optimization prob- lems of the form</p><div class="language-"><pre><code>̂ Z = arg min\nM ∈R m × n\n</code></pre></div><div class="language-"><pre><code>‖ Z − M ‖^2 F subject to Φ( M )≤ c, (7.1)\n</code></pre></div><p>where‖ · ‖^2 <em>F</em> is the (squared) Frobenius norm of a matrix (defined as the element-wise sum of squares), and Φ(·) is a constraint function that encourages ̂ <strong>Z</strong> to be sparse in some general sense. The manner in which we impose sparsity</p><p>leads to a variety of useful procedures, many of which are discussed in this chapter. One can regularize the overall approximating matrix̂ <strong>Z</strong> , or factor it and regularize the components of its factorization. Of course, there are variations: for instance, the observed matrix <strong>Z</strong> might have missing entries, so that the squared Frobenius norm‖·‖^2 <em>F</em> is modified accordingly. In other settings, we might impose multiple constraints on the approximating matrix <strong>Z</strong> ̂. Table 7.1 provides a summary of the methods discussed in this chapter. Method (a) is based on a simple <em>`</em> 1 -norm constraint on all of the entries on the matrix <strong>Z</strong> ̂; this constraint leads to a soft-thresholded version of the origi- nal matrix—that is, the optimal solution to our general problem (7.1) takes the form <em>z</em> ̂ <em>ij</em> = sign( <em>zij</em> )(| <em>zij</em> | − <em>γ</em> )+, where the scalar <em>γ &gt;</em> 0 is chosen so</p><p>that</p><div class="language-"><pre><code>∑ m\ni =1\n</code></pre></div><div class="language-"><pre><code>∑ n\nj =1| z ̂ ij |= c. The resulting estimate\n̂ Z can be useful in the con-\n</code></pre></div><p>text of sparse covariance estimation. Method (b) bounds the rank of <strong>Z</strong> ̂, or in other words, the number of nonzero singular values in̂ <strong>Z</strong>. Although the ma- trix approximation problem (7.1) with such a rank constraint is nonconvex,</p><div class="language-"><pre><code>167\n</code></pre></div><h6 id="_168-matrix-decompositions"><a class="header-anchor" href="#_168-matrix-decompositions" aria-hidden="true">#</a> 168 MATRIX DECOMPOSITIONS</h6><div class="language-"><pre><code>Table 7.1 Different formulations for the matrix approximation problem.\n</code></pre></div><div class="language-"><pre><code>Constraint Resulting method\n(a) ‖ Z ̂‖ ` 1 ≤ c Sparse matrix approximation\n(b) rank(̂ Z )≤ k Singular value decomposition\n(c) ‖ Z ̂‖? ≤ c Convex matrix approximation\n(d) Z ̂= UDV T ,\nΦ 1 ( u j )≤ c 1 , Φ 2 ( v k )≤ c 2 Penalized SVD\n(e) Z ̂= LR T,\nΦ 1 ( L )≤ c 1 , Φ 2 ( R )≤ c 2 Max-margin matrix factorization\n(f) Z ̂= L + S ,\nΦ 1 ( L )≤ c 1 , Φ 2 ( S )≤ c 2 Additive matrix decomposition\n</code></pre></div><p>the optimal solution is easily found by computing the singular value decom- position (SVD) and truncating it to its top <em>k</em> components. In method (c), we relax the rank constraint to a <em>nuclear norm</em> constraint, namely an upper bound on the sum of the singular values of the matrix. The nuclear norm is a convex matrix function, so that the problem in (c) is convex and can be solved by computing the SVD, and soft-thresholding its singular values. This modification—from a rank constraint in (b) to the nuclear norm constraint in (c)—becomes important when the methods are applied to matrices with miss- ing elements. In such settings, we can solve the corresponding problem (c) exactly, whereas methods based on (b) are more difficult to solve in general. The approach in (d) imposes penalties on the left and right singular vectors of <strong>Z</strong> ̂. Examples of the penalty functions or regularizers Φ 1 and Φ 2 include the usual <em><code>_ 2 or _</code></em> 1 norms, the latter choice yielding sparsity in the elements of the singular vectors. This property is useful for problems where interpretation of the singular vectors is important. Approach (e) imposes penalties directly on the components of the LR-matrix factorization; although ostensibly similar to approach (d), we will see it is closer to (c) when Φ 1 and Φ 2 are the Frobenius norm. Finally, approach (f) seeks an additive decomposition of the matrix, imposing penalties on both components in the sum. Matrix decompositions also provide an approach for constructing sparse versions of popular multivariate statistical methods such as principal compo- nent analysis, canonical correlation analysis and linear discriminant analysis. In this case, the matrix <strong>Z</strong> is not the raw data, but is derived from the raw data. For example, principal components are based on the sample covariance matrix (or the column-centered data matrix), canonical correlation uses the cross-products matrix from two sets of measurements, while clustering starts with inter-point distances. We discuss these multivariate methods, and related approaches to these problems, in Chapter 8.</p><h6 id="the-singular-value-decomposition-169"><a class="header-anchor" href="#the-singular-value-decomposition-169" aria-hidden="true">#</a> THE SINGULAR VALUE DECOMPOSITION 169</h6><h3 id="_7-2-the-singular-value-decomposition"><a class="header-anchor" href="#_7-2-the-singular-value-decomposition" aria-hidden="true">#</a> 7.2 The Singular Value Decomposition</h3><p>Given an <em>m</em> × <em>n</em> matrix <strong>Z</strong> with <em>m</em> ≥ <em>n</em> , its <em>singular value decomposition</em> takes the form</p><div class="language-"><pre><code>Z = UDV T. (7.2)\n</code></pre></div><p>This decomposition is standard in numerical linear algebra, and many algo- rithms exist for computing it efficiently (see, for example, the book by Golub and Loan (1996)). Here <strong>U</strong> is an <em>m</em> × <em>n</em> orthogonal matrix ( <strong>U</strong> <em>T</em> <strong>U</strong> = <strong>I</strong> <em>n</em> ) whose columns <strong>u</strong> <em>j</em> ∈R <em>m</em> are called the <em>left singular vectors</em>. Similarly, the matrix <strong>V</strong> is an <em>n</em> × <em>n</em> orthogonal matrix ( <strong>V</strong> <em>T</em> <strong>V</strong> = <strong>I</strong> <em>n</em> ) whose columns <strong>v</strong> <em>j</em> ∈R <em>n</em> are called the <em>right singular vectors</em>. The <em>n</em> × <em>n</em> matrix <strong>D</strong> is diagonal, with diag- onal elements <em>d</em> 1 ≥ <em>d</em> 2 ≥ ··· ≥ <em>dn</em> ≥0 known as the <em>singular values</em>. If these diagonal entries{ <em>d<code>_ } _n</code></em> =1are unique, then so are <strong>U</strong> and <strong>V</strong> , up to column-wise sign flips. If the columns of <strong>Z</strong> (the variables) are centered, then the right sin- gular vectors{ <strong>v</strong> <em>j</em> } <em>nj</em> =1define the <em>principal components</em> of <strong>Z</strong>. Consequently, the unit vector <strong>v</strong> 1 yields the linear combination <strong>s</strong> 1 = <strong>Zv</strong> 1 with highest sample variance among all possible choices of unit vectors. Here <strong>s</strong> 1 is called the <em>first principal component</em> of <strong>Z</strong> , and <strong>v</strong> 1 is the corresponding <em>direction</em> or <em>loading</em> vector. Similarly, <strong>s</strong> 2 = <strong>Zv</strong> 2 is the second principal component, with maximal sample variance among all linear combinations uncorrelated with <strong>s</strong> 1 , and so on. See Exercise 7.1 and Section 8.2.1 for further details. The singular value decomposition provides a solution to the rank- <em>q</em> matrix approximation problem. Suppose <em>r</em> ≤rank( <strong>Z</strong> ), and let <strong>D</strong> <em>r</em> be a diagonal ma- trix with all but the first <em>r</em> diagonal entries of the diagonal matrix <strong>D</strong> set to zero. Then the optimization problem</p><div class="language-"><pre><code>minimize\nrank( M )= r\n</code></pre></div><h6 id="‖-z-−-m-‖-f-7-3"><a class="header-anchor" href="#‖-z-−-m-‖-f-7-3" aria-hidden="true">#</a> ‖ Z − M ‖ F (7.3)</h6><p>actually has a closed form solution̂ <strong>Z</strong> <em>r</em> = <strong>UD</strong> <em>r</em> <strong>V</strong> <em>T</em> , a decomposition known as the rank- <em>r</em> SVD (see Exercise 7.2). The estimate <strong>Z</strong> ̂ <em>r</em> is sparse in the sense that all but <em>r</em> singular values are zero. A fuller discussion of the SVD—in the context of principal components analysis—is given in Section 8.2.1.</p><h3 id="_7-3-missing-data-and-matrix-completion"><a class="header-anchor" href="#_7-3-missing-data-and-matrix-completion" aria-hidden="true">#</a> 7.3 Missing Data and Matrix Completion</h3><p>What if some of the entries of <strong>Z</strong> are missing? In general, the problem of filling in or imputing missing values in a matrix is known as <em>matrix comple- tion</em> (Laurent 2001). Of course, the matrix completion problem is ill-specified unless we impose additional constraints on the unknown matrix <strong>Z</strong> , and one common choice is a rank constraint. Low-rank forms of matrix completion arise in the problem of collaborative filtering and can be used to build recom- mender systems. The SVD provides an effective method for matrix completion. Formally, suppose that we observe all entries of the matrix <strong>Z</strong> indexed by the subset</p><h6 id="_170-matrix-decompositions"><a class="header-anchor" href="#_170-matrix-decompositions" aria-hidden="true">#</a> 170 MATRIX DECOMPOSITIONS</h6><p>Ω⊂{ 1 <em>,...,m</em> }×{ 1 <em>,...,n</em> }. Given such observations, a natural approach is to seek the lowest rank approximating matrix <strong>Z</strong> ̂that interpolates the observed entries of <strong>Z</strong> —namely</p><div class="language-"><pre><code>minimize rank( M ) subject to mij = zij for all ( i,j )∈Ω. (7.4)\n</code></pre></div><p>Unlike its fully observed counterpart, this rank-minimization problem is com- putationally intractable (NP-hard), and cannot be solved in general even for moderately large matrices. In addition, forcing the estimate <strong>M</strong> to interpolate each of the observed entries <em>zij</em> will often be too harsh and can lead to overfitting; it is gener- ally better to allow <strong>M</strong> to make some errors on the observed data as well. Accordingly, consider the optimization problem</p><div class="language-"><pre><code>minimize rank( M ) subject to\n</code></pre></div><h6 id="∑-32"><a class="header-anchor" href="#∑-32" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>( i,j )∈Ω\n</code></pre></div><div class="language-"><pre><code>( zij − mij )^2 ≤ δ , (7.5)\n</code></pre></div><p>or its equivalent form</p><div class="language-"><pre><code>minimize\nrank( M )≤ r\n</code></pre></div><h6 id="∑-33"><a class="header-anchor" href="#∑-33" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>( i,j )∈Ω\n</code></pre></div><div class="language-"><pre><code>( zij − mij )^2. (7.6)\n</code></pre></div><p>In words, we seek the matrix̂ <strong>Z</strong> = <strong>Z</strong> ̂ <em>r</em> of rank at most <em>r</em> that best approximates the observed entries of our matrix <strong>Z</strong> , with the other entries of <strong>Z</strong> ̂ <em>r</em> serving to fill in the missing values. The family of solutions generated by varying <em>δ</em> in optimization problem (7.5) is the same as that generated by varying <em>r</em> in problem (7.6). Unfortunately, both optimization problems (7.5) and (7.6) are nonconvex, and so exact solutions are in general not available. However, there are useful heuristic algorithms that can be used to find local minima. For instance, sup- pose that we start with an initial guess for the missing values, and use them to complete <strong>Z</strong>. We then compute the rank- <em>r</em> SVD approximation of the filled-in matrix as in (7.3), and use it to provide new estimates for the missing values. This process is repeated till convergence. The missing value imputation for a missing entry <em>xij</em> is simply the ( <em>i,j</em> ) <em>th</em> entry of the final rank- <em>r</em> approximation</p><p><strong>Z</strong> ̂. See Mazumder, Hastie and Tibshirani (2010) for further details. In Sec- tion 7.3.2, we discuss convex relaxations of these optimization problems based on the nuclear norm, for which exact solutions are available.</p><h4 id="_7-3-1-the-netflix-movie-challenge"><a class="header-anchor" href="#_7-3-1-the-netflix-movie-challenge" aria-hidden="true">#</a> 7.3.1 The Netflix Movie Challenge</h4><p>The Netflix movie-rating challenge has become one of the canonical examples for matrix completion (Bennett and Lanning 2007). Netflix is a movie-rental company that launched a competition in 2006 to try to improve their sys- tem for recommending movies to their customers. The Netflix dataset has <em>n</em> = 17 <em>,</em> 770 movies (columns) and <em>m</em> = 480 <em>,</em> 189 customers (rows). Customers</p><h6 id="missing-data-and-matrix-completion-171"><a class="header-anchor" href="#missing-data-and-matrix-completion-171" aria-hidden="true">#</a> MISSING DATA AND MATRIX COMPLETION 171</h6><div class="language-"><pre><code>Figure 7.1 The Netflix competition leaderboard at the close of the competition.\n</code></pre></div><h6 id="_172-matrix-decompositions"><a class="header-anchor" href="#_172-matrix-decompositions" aria-hidden="true">#</a> 172 MATRIX DECOMPOSITIONS</h6><p>have rated some of the movies on a scale from 1 to 5, where 1 is worst and 5 is best. The data matrix is very sparse with “only” 100 million (1%) of the ratings present in the training set. The goal is to predict the ratings for unrated movies, so as to better recommend movies to customers. In 2006, the “Cinematch” algorithm used by Netflix had a root-mean-square error of 0.9525 over a large test set. A competition was held starting in 2006, with the winner being the first algorithm that could improve this RMSE by at least 10%. The competition was finally won in 2009 by a large group of researchers called “Bellkor’s Pragmatic Chaos,” which was the combined effort of three individual groups. The winning algorithm used a combination of a large num- ber of statistical techniques, but as with many of the competing algorithms, the SVD played a central role. Figure 7.1 shows the leaderboard at the close of the competition.</p><p><strong>Table 7.2</strong> <em>Excerpt of the Netflix movie rating data. The movies are rated from 1 (worst) to 5 (best). The symbol</em> • <em>represents a missing value: a movie that was not rated by the corresponding customer.</em></p><div class="language-"><pre><code>Dirty DancingMeet the ParentsTop GunThe Sixth SenseCatch Me If You CanThe Royal TenenbaumsCon AirBig FishThe MatrixA Few Good Men\nCustomer 1 • • • • 4 • • • • •\nCustomer 2 • • 3 • • • 3 • • 3\nCustomer 3 • 2 • 4 • • • • 2 •\nCustomer 4 3 • • • • • • • • •\nCustomer 5 5 5 • • 4 • • • • •\nCustomer 6 • • • • • 2 4 • • •\nCustomer 7 • • 5 • • • • 3 • •\nCustomer 8 • • • • • 2 • • • 3\nCustomer 9 3 • • • 5 • • 5 • •\nCustomer 10 • • • • • • • • • •\n</code></pre></div><p>A low rank model provides a good heuristic for rating movies: in particular, suppose that we model the rating of user <em>i</em> on movie <em>j</em> by a model of the form</p><div class="language-"><pre><code>zij =\n</code></pre></div><div class="language-"><pre><code>∑ r\n</code></pre></div><div class="language-"><pre><code>` =1\n</code></pre></div><div class="language-"><pre><code>ci`gj` + wij, (7.7)\n</code></pre></div><p>or in matrix form <strong>Z</strong> = <strong>CG</strong> <em>T</em> + <strong>W</strong> , where <strong>C</strong> ∈R <em>m</em> × <em>r</em> and <strong>G</strong> ∈R <em>n</em> × <em>r</em>. In this model, there are <em>rgenres</em> of movies, and corresponding to each is a <em>clique</em> of viewers who like them. Here viewer <em>i</em> has a membership weight of <em>ci<code>_ for the _</code>th</em> clique, and the genre associated with this clique has a score <em>gj`</em> for movie <em>j</em>. The overall user rating is obtained by summing these products over</p><h6 id="missing-data-and-matrix-completion-173"><a class="header-anchor" href="#missing-data-and-matrix-completion-173" aria-hidden="true">#</a> MISSING DATA AND MATRIX COMPLETION 173</h6><p><em>`</em> (cliques/genres), and then adding some noise. Table 7.2 shows the data for the ten customers and ten movies with the most ratings. The competition identified a “probe set” of ratings, about 1_._ 4 million of the entries, for testing purposes. These were not a random draw, rather movies that had appeared chronologically later than most. Figure 7.2 shows the root- mean-squared error over the training and test sets as the rank of the SVD was varied. Also shown are the results from an estimator based on nuclear norm regularization, discussed in the next section. Here we double centered the training data, by removing row and column means. This amounts to fitting the model</p><div class="language-"><pre><code>zij = αi + βj +\n</code></pre></div><div class="language-"><pre><code>∑ r\n</code></pre></div><div class="language-"><pre><code>` =1\n</code></pre></div><div class="language-"><pre><code>ci`gj` + wij ; (7.8)\n</code></pre></div><p>However, the row and column means can be estimated separately, using a simple two-way ANOVA regression model (on unbalanced data).</p><div class="language-"><pre><code>0 50 100 150 200\n</code></pre></div><div class="language-"><pre><code>0.7\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>0.9\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>Rank\n</code></pre></div><div class="language-"><pre><code>RMSE\n</code></pre></div><div class="language-"><pre><code>Train\nTest\n</code></pre></div><div class="language-"><pre><code>0.65 0.70 0.75 0.80 0.85 0.90\n</code></pre></div><div class="language-"><pre><code>0.95\n</code></pre></div><div class="language-"><pre><code>0.96\n</code></pre></div><div class="language-"><pre><code>0.97\n</code></pre></div><div class="language-"><pre><code>0.98\n</code></pre></div><div class="language-"><pre><code>0.99\n</code></pre></div><div class="language-"><pre><code>1.00\n</code></pre></div><div class="language-"><pre><code>Training RMSE\n</code></pre></div><div class="language-"><pre><code>Test RMSE\n</code></pre></div><div class="language-"><pre><code>Hard−Impute\nSoft−Impute\n</code></pre></div><div class="language-"><pre><code>Netflix Competition Data\n</code></pre></div><p><strong>Figure 7.2</strong> <em>Left: Root-mean-squared error for the Netflix training and test data for the iterated-SVD (</em> Hard-Impute <em>) and the convex spectral-regularization algorithm (</em> Soft-Impute <em>). Each is plotted against the rank of the solution, an imperfect cal- ibrator for the regularized solution. Right: Test error only, plotted against training error, for the two methods. The training error captures the amount of fitting that each method performs. The dotted line represents the baseline “Cinematch” score.</em></p><p>While the iterated-SVD method is quite effective, it is not guaranteed to find the optimal solution for each rank. It also tends to overfit in this example, when compared to the regularized solution. In the next section, we present a convex relaxation of this setup that leads to an algorithm with guaranteed convergence properties.</p><h6 id="_174-matrix-decompositions"><a class="header-anchor" href="#_174-matrix-decompositions" aria-hidden="true">#</a> 174 MATRIX DECOMPOSITIONS</h6><h4 id="_7-3-2-matrix-completion-using-nuclear-norm"><a class="header-anchor" href="#_7-3-2-matrix-completion-using-nuclear-norm" aria-hidden="true">#</a> 7.3.2 Matrix Completion Using Nuclear Norm</h4><p>A convenient convex relaxation of the nonconvex objective function (7.4) is given by</p><div class="language-"><pre><code>minimize‖ M ‖? subject to mij = zij for all ( i,j )∈Ω, (7.9)\n</code></pre></div><p>where‖ <strong>M</strong> ‖<em>?</em> is the nuclear norm, or the sum of the singular values of <strong>M</strong>. It is also sometimes called the trace norm.^1 Figure 7.3 shows the level set of the nuclear norm of a symmetric 2×2 matrix, and depicts the convex problem (7.9).^2 The nuclear norm is a convex relaxation of the rank of a matrix, and hence problem (7.9) is convex (Fazel 2002). Specifically, as shown in Exercise 7.3, it is a semi-definite program (SDP), a particular class of convex programs for which special purpose solvers can be applied. The underlying convexity is also theoretically useful, since one can characterize the properties of the observed matrix and sample size under which the method succeeds in exactly reconstructing the matrix, as discussed in Section 7.3.3.</p><div class="language-"><pre><code>M\n</code></pre></div><div class="language-"><pre><code>Feasible set\n</code></pre></div><div class="language-"><pre><code>∥\n∥\n∥\n∥\n</code></pre></div><div class="language-"><pre><code>[\nxy\nyz\n</code></pre></div><div class="language-"><pre><code>]∥\n∥\n∥\n∥\n∗\n</code></pre></div><div class="language-"><pre><code>≤δ\n</code></pre></div><p><strong>Figure 7.3</strong> <em>The blue cylinder shows the level set of the nuclear norm unit-bal l for a symmetric</em> 2 × 2 <em>matrix. The tangent plane is the feasible setz</em> = <em>z</em> 0 <em>for the matrix imputation problem where we observezand wish to imputexandy. The point Mis the solution that we seek, leading to the minimum value forδ. This figure is analogous to the lasso estimation picture in Figure 2.2 of Chapter 2.</em></p><p>In practice, however, it is unrealistic to model the observed entries as being noiseless. Accordingly, a more practical method is based on the following</p><p>(^1) This terminology can be confusing: for symmetric, positive semi-definite matrices, the trace is the sum of the eigenvalues. For general matrices, “trace norm” refers to trace √ <strong>A</strong> <em>T</em> <strong>A</strong> , which is the sum of the singular values. (^2) Thanks to Emmanuel Candes and Benjamin Recht for providing Figure 7.3.</p><h6 id="missing-data-and-matrix-completion-175"><a class="header-anchor" href="#missing-data-and-matrix-completion-175" aria-hidden="true">#</a> MISSING DATA AND MATRIX COMPLETION 175</h6><p>relaxed version of the program (7.9):</p><div class="language-"><pre><code>minimize\nM\n</code></pre></div><h6 id="-34"><a class="header-anchor" href="#-34" aria-hidden="true">#</a> </h6><h6 id="-28"><a class="header-anchor" href="#-28" aria-hidden="true">#</a> </h6><h6 id="-28"><a class="header-anchor" href="#-28" aria-hidden="true">#</a> </h6><h6 id="_1-106"><a class="header-anchor" href="#_1-106" aria-hidden="true">#</a> 1</h6><h6 id="_2-70"><a class="header-anchor" href="#_2-70" aria-hidden="true">#</a> 2</h6><h6 id="∑-34"><a class="header-anchor" href="#∑-34" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>( i,j )∈Ω\n</code></pre></div><div class="language-"><pre><code>( zij − mij )^2 + λ ‖ M ‖?\n</code></pre></div><h6 id="-29"><a class="header-anchor" href="#-29" aria-hidden="true">#</a> </h6><h6 id="-28"><a class="header-anchor" href="#-28" aria-hidden="true">#</a> </h6><h6 id="-28"><a class="header-anchor" href="#-28" aria-hidden="true">#</a> </h6><h6 id="_7-10"><a class="header-anchor" href="#_7-10" aria-hidden="true">#</a> , (7.10)</h6><p>called <em>spectral regularization</em>. As in our relaxation from problem (7.4) to (7.6), this modification allows for solutionŝ <strong>Z</strong> that do not fit the observed entries exactly, thereby reducing potential overfitting in the case of noisy entries. The parameter <em>λ</em> is a tuning parameter that must be chosen from the data, typically by cross-validation. As in the previous section, we do not necessarily require the error</p><h6 id="∑-35"><a class="header-anchor" href="#∑-35" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>( i,j )∈Ω( zij − mij )\n</code></pre></div><p>(^2) to be zero, and this will only occur for a sufficiently small value of <em>λ</em>. There is a simple algorithm for solving (7.10), similar to the iterated SVD for missing-data imputation in the previous section. First consider the case where there is no missing data, so that the set Ω of observed entries includes all <em>m</em> · <em>n</em> pairs ( <em>i,j</em> )∈{ 1 <em>,...,m</em> }×{ 1 <em>,...,n</em> }. Then to solve (7.10), we simply compute the SVD of <strong>Z</strong> , soft-threshold the singular values by <em>λ</em> , and reconstruct the matrix. This observation leads to an obvious procedure for the setup with missing data. We start with an initial guess for the missing values, compute the (full rank) SVD, and then soft-threshold its singular values by an amount <em>λ</em>. We reconstruct the corresponding SVD approximation and obtain new estimates for the missing values. This process is repeated until convergence. In order to describe this procedure more precisely, we require some more notation. Given an observed subset Ω of matrix entries, we can define the projection operatorPΩ:R <em>m</em> × <em>n</em> 7→R <em>m</em> × <em>n</em> as follows: [PΩ( <strong>Z</strong> )] <em>ij</em> =</p><h6 id="-254"><a class="header-anchor" href="#-254" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>zij if ( i,j )∈Ω\n0 if ( i,j )∈ / Ω ,\n</code></pre></div><h6 id="_7-11"><a class="header-anchor" href="#_7-11" aria-hidden="true">#</a> (7.11)</h6><p>so thatPΩreplaces the missing entries in <strong>Z</strong> with zeros, and leaves the observed entries alone. With this definition, we have the equivalence ∑</p><div class="language-"><pre><code>( i,j )∈Ω\n</code></pre></div><div class="language-"><pre><code>( zij − mij )^2 =‖PΩ( Z )−PΩ( M )‖^2 F. (7.12)\n</code></pre></div><p>Given the singular value decomposition^3 <strong>W</strong> = <strong>UDV</strong> <em>T</em> of a rank- <em>r</em> matrix <strong>W</strong> , we define its soft-thresholded version as</p><div class="language-"><pre><code>Sλ ( W )≡ UD λ V T where D λ = diag [( d 1 − λ )+ ,..., ( dr − λ )+] (7.13)\n</code></pre></div><p>(note that the soft-threshholding can reduce the rank even further). Using this operator, the procedure for solving (7.10) is given in Algorithm 7.1. This algorithm was proposed and studied by Mazumder et al. (2010), where</p><p>(^3) If a matrix has rank <em>r &lt;</em> min( <em>m,n</em> ), we assume its SVD is represented in the truncated form, discarding the singular values of zero, and the corresponding left and right vectors.</p><h6 id="_176-matrix-decompositions"><a class="header-anchor" href="#_176-matrix-decompositions" aria-hidden="true">#</a> 176 MATRIX DECOMPOSITIONS</h6><div class="language-"><pre><code>Algorithm 7.1 Soft-Impute for matrix completion.\n</code></pre></div><ol><li>Initialize <strong>Z</strong> old= <strong>0</strong> and create a decreasing grid <em>λ</em> 1 <em>&gt; ... &gt; λK</em>.</li><li>For each <em>k</em> = 1 <em>,...,K</em> , set <em>λ</em> = <em>λk</em> and iterate until convergence:</li></ol><div class="language-"><pre><code>Compute Z ̂ λ ←S λ\n</code></pre></div><h6 id="-255"><a class="header-anchor" href="#-255" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>P Ω( Z ) + P Ω⊥( Z old)\n</code></pre></div><h6 id="-256"><a class="header-anchor" href="#-256" aria-hidden="true">#</a> )</h6><h6 id="-257"><a class="header-anchor" href="#-257" aria-hidden="true">#</a> .</h6><div class="language-"><pre><code>Update Z old← Z ̂ λ\n</code></pre></div><ol start="3"><li>Output the sequence of solutions <strong>Z</strong> ̂ <em>λ</em> 1 <em>,...,</em> <strong>Z</strong> ̂ <em>λK.</em></li></ol><div class="language-"><pre><code>its convergence to the global solution is established. In Exercise 7.4, the reader\nis asked to verify that a fixed point of the algorithm satisfies the zero sub-\ngradient equations associated with the objective function (7.10). It can also\nbe derived as a first-order Nesterov algorithm (see Exercise 7.5). Each itera-\ntion requires an SVD computation of a (potentially large) dense matrix, even\nthoughPΩ( Z ) is sparse. For “Netflix-sized” problems, such large dense matri-\nces can typically not even be stored in memory (68Gb with 8 bytes per entry).\nNote, however, that we can write\n</code></pre></div><div class="language-"><pre><code>PΩ( Z ) +PΩ⊥( Z old) =PΩ( Z )−PΩ( Z old)\n︸ ︷︷ ︸\nsparse\n</code></pre></div><div class="language-"><pre><code>+ Z ︸︷︷old︸\nlow rank\n</code></pre></div><h6 id="_7-14"><a class="header-anchor" href="#_7-14" aria-hidden="true">#</a> . (7.14)</h6><div class="language-"><pre><code>The first component is sparse, with|Ω|nonmissing entries. The second com-\nponent is a soft-thresholded SVD, so can be represented using the correspond-\ning components. Moreover, for each component, we can exploit their special\nstructure to efficiently perform left and right multiplications by a vector, and\nthereby apply iterative Lanczos methods to compute a (low rank) SVD effi-\nciently. It can be shown that this iterative algorithm converges to the solution\nof the problem\n</code></pre></div><div class="language-"><pre><code>minimize\nM ∈R m × n\n</code></pre></div><h6 id="-258"><a class="header-anchor" href="#-258" aria-hidden="true">#</a> {</h6><h6 id="_1-107"><a class="header-anchor" href="#_1-107" aria-hidden="true">#</a> 1</h6><h6 id="_2-71"><a class="header-anchor" href="#_2-71" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖PΩ( Z )−PΩ( M )‖^2 F + λ ‖ M ‖?\n</code></pre></div><h6 id="-259"><a class="header-anchor" href="#-259" aria-hidden="true">#</a> }</h6><h6 id="_7-15"><a class="header-anchor" href="#_7-15" aria-hidden="true">#</a> , (7.15)</h6><div class="language-"><pre><code>which is another way of writing the objective function in (7.10).\nFigure 7.2 shows the results ofSoft-Imputeapplied to the Netflix ex-\nample. We see that the regularization has paid off, since it outperforms the\niterated SVD algorithmHard-Impute. It takes longer to overfit, and because\nof the regularization, is able to use a higher rank solution. Taking advantage of\nthe warm starts in Algorithm 7.1, it took under 5 hours of computing to pro-\nduce the solution path in Figure 7.2, using the R packagesoftImpute(Hastie\nand Mazumder 2013). See also Figure 7.5 in Section 7.3.3, which illustrates\nthe performance of theSoft-Imputealgorithm for noisy matrix completion\nover a range of different ranks and sample sizes. We discuss this figure at more\nlength in that section.\nIn terms of convergence speed, Mazumder et al. (2010) show that the\nSoft-Imputealgorithm is guaranteed to converge at least sub-linearly, mean-\ning thatO(1 /δ ) iterations are sufficient to compute a solution that is δ -close\n</code></pre></div><h6 id="missing-data-and-matrix-completion-177"><a class="header-anchor" href="#missing-data-and-matrix-completion-177" aria-hidden="true">#</a> MISSING DATA AND MATRIX COMPLETION 177</h6><p>to the global optimum. In the absence of additional structure (such as strong convexity), this is the fastest rate that can be expected from a first-order gra- dient method (Nemirovski and Yudin 1983). Interestingly, in certain settings, it can be shown that simple first-order methods converge at a much faster geometric rate, meaning thatO(log(1 <em>/δ</em> )) iterations are sufficient to compute a <em>δ</em> -optimum. For instance, Agarwal, Negahban and Wainwright (2012 <em>a</em> ) an- alyze an algorithm closely related to theSoft-Imputealgorithm; they show that under the same conditions that guarantee good statistical performance of the nuclear norm estimator, this first-order algorithm is guaranteed to con- verge at the geometric rate.</p><h4 id="_7-3-3-theoretical-results-for-matrix-completion"><a class="header-anchor" href="#_7-3-3-theoretical-results-for-matrix-completion" aria-hidden="true">#</a> 7.3.3 Theoretical Results for Matrix Completion</h4><p>There are a variety of theoretical results for matrix completion using nuclear- norm regularization. Beginning with the simpler “no-noise” case, suppose that we sample <em>N</em> entries of a <em>p</em> × <em>p</em> matrix uniformly at random. How large does <em>N</em> need to be, as a function of the matrix dimension <em>p</em> and rank <em>r</em> , for the nuclear norm relaxation (7.9) to recover the matrix exactly? Of course, this is always possible if <em>N</em> ≥ <em>p</em>^2 , so that our interest is in guarantees based on <em>N</em>  <em>p</em>^2 samples. A first easy observation is that if there are no observed entries in some row (or column) of the matrix, then it is impossible to recover the matrix exactly, even if it is rank one. In Exercise 7.8, we show how this argument implies that any method—not only nuclear norm relaxation—needs at least <em>N &gt; p</em> log <em>p</em> samples, even for a rank one matrix. This phenomenon is an instance of the famous “coupon collector” problem (Erdos and Renyi 1961). As for the effect of the rank, note that we need roughlyO( <em>rp</em> ) parameters to specify an arbitrary <em>p</em> × <em>p</em> matrix with rank <em>r</em> , since it hasO( <em>r</em> ) singular vectors, each with <em>p</em> components. As we will see, under certain restrictions on the “coherence” of the matrices, nuclear norm relaxation succeeds in exact recovery based on a sample size just a logarithmic factor larger. Coherence measures the extent to which the singular vectors of a matrix are aligned with the standard basis. In order to appreciate the need for coherence constraints, consider the rank-one matrix <strong>Z</strong> = <strong>e</strong> 1 <strong>e</strong> <em>T</em> 1 , with a single one in its upper left corner, as shown on the left side of Equation (7.16) below:</p><h6 id="z"><a class="header-anchor" href="#z" aria-hidden="true">#</a> Z =</h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="_1-0-0-0"><a class="header-anchor" href="#_1-0-0-0" aria-hidden="true">#</a> 1 0 0 0</h6><h6 id="_0-0-0-0"><a class="header-anchor" href="#_0-0-0-0" aria-hidden="true">#</a> 0 0 0 0</h6><h6 id="_0-0-0-0-1"><a class="header-anchor" href="#_0-0-0-0-1" aria-hidden="true">#</a> 0 0 0 0</h6><h6 id="_0-0-0-0-2"><a class="header-anchor" href="#_0-0-0-0-2" aria-hidden="true">#</a> 0 0 0 0</h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><div class="language-"><pre><code> and Z\n</code></pre></div><h6 id="′"><a class="header-anchor" href="#′" aria-hidden="true">#</a> ′ =</h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>v 1 v 2 v 3 v 4\n0 0 0 0\n0 0 0 0\n0 0 0 0\n</code></pre></div><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="-7-16"><a class="header-anchor" href="#-7-16" aria-hidden="true">#</a> . (7.16)</h6><p>If we are allowed to observe only <em>N</em>  <em>p</em>^2 entries of this matrix, with the entries chosen uniformly at random, then with high probability, we will <em>not</em> observe the single nonzero entry, and hence have no hope of distinguishing it from the all-zeroes matrix. Similar concerns apply to a matrix of the form</p><h6 id="_178-matrix-decompositions"><a class="header-anchor" href="#_178-matrix-decompositions" aria-hidden="true">#</a> 178 MATRIX DECOMPOSITIONS</h6><p><strong>Z</strong> ′= <strong>e</strong> 1 <strong>v</strong> <em>T</em> , where <strong>v</strong> ∈R <em>p</em> is an arbitrary <em>p</em> vector, as shown on the right side of Equation (7.16). Thus, any theoretical guarantees on nuclear norm regulariza- tion must somehow account for these pathological cases. Both the matrices <strong>Z</strong> and <strong>Z</strong> ′have maximal coherence with the standard basis ofR^4 , meaning that some subset of their left and/or right singular vectors are perfectly aligned with some standard basis vector <strong>e</strong> <em>j</em>. One way to exclude troublesome matrices is by drawing matrices from some random ensemble; for instance, we might construct a random matrix of the form <strong>Z</strong> =</p><p>∑ <em>r j</em> =1 <strong>a</strong> <em>j</em> <strong>b</strong><em>T j</em> , where the random vectors <strong>a</strong> <em>j</em> ∼ <em>N</em> (0 <em>,Ip</em> ) and <strong>b</strong> <em>j</em> ∼ <em>N</em> (0 <em>,Ip</em> ) are all independently drawn. Such random matrices are ex- tremely unlikely to have singular vectors that are highly coherent with stan- dard basis vectors. For this ensemble, Gross (2011) shows that with high probability over the randomness in the ensemble and sampling, the nuclear norm relaxation succeeds in exact recovery if the number of samples satisfies</p><div class="language-"><pre><code>N ≥ Crp log p, (7.17)\n</code></pre></div><p>where <em>C &gt;</em> 0 is a fixed universal constant. See also Cand<code>es and Recht (2009) for earlier but somewhat weaker guarantees. More generally, it is possible to give exact recovery guarantees in which the pre-factor _C_ depends on the singular vector incoherence, as measured by the maximal alignment between the singular vectors and the standard basis. We refer the reader to the papers by Cand</code>es and Recht (2009), Gross (2011), and Recht (2011) for further details on results of this type, as well as to Keshavan, Oh and Montanari (2009) for related results on a slightly different estimator. We carried out a small simulation study to better understand what result (7.17) is saying. We generated matrices <strong>U</strong> <em>,</em> <strong>V</strong> each of size <em>p</em> × <em>r</em> and with i.i.d standard normal entries and defined <strong>Z</strong> = <strong>UV</strong> <em>T</em>. Then we set to missing a fixed proportion of entries, and appliedSoft-Imputewith <em>λ</em> chosen small enough so that‖PΩ( <strong>Z</strong> − <strong>Z</strong> ̂)‖^2 F <em>/</em> ‖PΩ( <strong>Z</strong> )‖^2 F <em>&lt;</em> 10 −^5 ; in other words, the observed entries are (effectively) reproduced. Then we checked to see if</p><div class="language-"><pre><code>‖P⊥Ω( Z − Z ̂)‖^22 / ‖PΩ⊥( Z )‖^22 &lt; 10 −^5 , (7.18)\n</code></pre></div><p>that is, the missing data was interpolated. The process was repeated 100 times for various values of the rank <em>r</em> and the proportion set to missing. The proportion of times that the missing data was successfully interpolated is shown in Figure 7.4. We see that when the rank is a small fraction of the matrix dimension, one can reproduce the missing entries with fairly high probability. But this gets significantly more difficult when the true rank is higher. Of course, the “exact” setting is often not realistic, and it might be more reasonable to assume some subset of the entries are observed with additional noise, as in observation model (7.7)—that is, <strong>Z</strong> = <strong>L</strong> ∗+ <strong>W</strong> , where <strong>L</strong> ∗has rank <em>r</em>. In this setting, exact matrix completion is not generally possible, and we would be interested in how well we can approximate the low-rank matrix <strong>L</strong> ∗using the</p><h6 id="missing-data-and-matrix-completion-179"><a class="header-anchor" href="#missing-data-and-matrix-completion-179" aria-hidden="true">#</a> MISSING DATA AND MATRIX COMPLETION 179</h6><div class="language-"><pre><code>0.0 0.2 0.4 0.6\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>Proportion Missing\n</code></pre></div><div class="language-"><pre><code>Probability of Exact Completion\n</code></pre></div><div class="language-"><pre><code>Rank 1\n</code></pre></div><div class="language-"><pre><code>p=20\np=40\n</code></pre></div><div class="language-"><pre><code>0.0 0.2 0.4 0.6\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>Proportion Missing\n</code></pre></div><div class="language-"><pre><code>Probability of Exact Completion\n</code></pre></div><div class="language-"><pre><code>Rank 5\n</code></pre></div><p><strong>Figure 7.4</strong> <em>Convex matrix completion in the no-noise setting. Shown are probabili- ties of exact completion (mean</em> ± <em>one standard error) as a function of the proportion missing, forn</em> × <em>nmatrices withn</em> ∈{ 20 <em>,</em> 40 }<em>. The true rank of the complete matrix is one in the left panel and five in the right panel.</em></p><p>estimator (7.10). Singular vector incoherence conditions are less appropriate for noisy observations, because they are not robust to small perturbations. To understand this issue, suppose that we start with a matrix <strong>B</strong> that has rank <em>r</em> −1, Frobenius norm one, and is <em>maximal ly incoherent</em> , meaning that all its singular vectors are orthogonal to the standard basis vectors. Recalling the troublesome matrix <strong>Z</strong> from Equation (7.16), now consider the perturbed matrix <strong>L</strong> ∗= <strong>B</strong> + <em>δ</em> <strong>Z</strong> for some <em>δ &gt;</em> 0. The matrix <strong>L</strong> ∗always has rank <em>r</em> , and no matter how small we choose the parameter <em>δ</em> , it is always <em>maximal ly coherent</em> , since it has the standard basis vector <strong>e</strong> 1 ∈R <em>p</em> as one of its singular vectors. An alternative criterion that is not sensitive to such small perturbations is based on the “spikiness” ratio of a matrix (Negahban and Wainwright 2012).</p><p>In particular, for any nonzero matrix <strong>L</strong> ∈R <em>p</em> × <em>p</em> , we define <em>α</em> sp( <strong>L</strong> ) = <em>p</em> ‖‖ <strong>LL</strong> ‖‖F∞,</p><p>where‖ <strong>L</strong> ‖∞is the element-wise maximum absolute value of the matrix entries. This ratio is a measure of the uniformity (or lack thereof) in the spread of the matrix entries; it ranges between 1 and <em>p</em>. For instance, any matrix <strong>L</strong> with all equal entries has <em>α</em> sp( <strong>L</strong> ) = 1, the minimal value, whereas the spikiest possible matrix such as <strong>Z</strong> from Equation (7.16) achieves the maximal spikiness ratio <em>α</em> sp( <strong>Z</strong> ) = <em>p</em>. In contrast to singular vector incoherence, the spikiness ratio involves the singular values (as well as the vectors). Thus, the matrix <strong>L</strong> ∗= <strong>B</strong> + <em>δ</em> <strong>Z</strong>. will have a low spikiness ratio whenever the perturbation <em>δ &gt;</em> 0 is sufficiently small. For the nuclear-norm regularized estimator (7.10) with a bound on the</p><h6 id="_180-matrix-decompositions"><a class="header-anchor" href="#_180-matrix-decompositions" aria-hidden="true">#</a> 180 MATRIX DECOMPOSITIONS</h6><p>spikiness ratio, Negahban and Wainwright (2012) show that the estimate <strong>L</strong> ̂ satisfies a bound of the form</p><div class="language-"><pre><code>‖̂ L − L ∗‖^2 F\n‖ L ∗‖^2 F\n</code></pre></div><div class="language-"><pre><code>≤ C max\n</code></pre></div><h6 id="-260"><a class="header-anchor" href="#-260" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>σ^2 , α^2 sp( L ∗)\n</code></pre></div><div class="language-"><pre><code>} rp log p\nN\n</code></pre></div><h6 id="_7-19"><a class="header-anchor" href="#_7-19" aria-hidden="true">#</a> (7.19)</h6><p>with high probability over the sampling pattern, and random noise (assumed i.i.d., zero-mean with all moments finite, and variance <em>σ</em>^2 ). See also Keshavan, Montanari and Oh (2010) and Koltchinskii, Lounici and Tsybakov (2011), who prove related guarantees for slightly different estimators.</p><div class="language-"><pre><code>Rank Ratio\n</code></pre></div><div class="language-"><pre><code>Observation Ratio\n</code></pre></div><div class="language-"><pre><code>Performance of Nuclear Norm Regularization\n</code></pre></div><div class="language-"><pre><code>0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\n</code></pre></div><div class="language-"><pre><code>1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n</code></pre></div><div class="language-"><pre><code>0.1\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.3\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.5\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.7\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>0.9\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><p><strong>Figure 7.5</strong> <em>Performance of the nuclear-norm regularized estimator</em> (7.10) <em>, solved via the</em> Soft-Impute <em>algorithm, for noisy matrix completion under the model</em> (7.7) <em>with matrix</em> <strong>L</strong> ∗= <strong>CG</strong> <em>Tof rankr. Plots of the relative Frobenius norm error</em> ‖̂ <strong>L</strong> − <strong>L</strong> ∗‖^2 F <em>/</em> ‖ <strong>L</strong> ∗‖^2 F <em>forp</em> = 50 <em>as a function of the rank ratioδ</em> = <em>r</em> log <em>ppand observation ratioν</em> = <em>Np</em> 2 <em>, corresponding to the fraction of observed entries in ap</em> × <em>pmatrix. Observations were of the linear form</em> (7.7) <em>withwij</em> ∼ <em>N</em> (0 <em>,σ</em>^2 ) <em>whereσ</em> = 1 <em>/</em> 4 <em>, and we used the</em> Soft-Impute <em>algorithm to solve the program</em> (7.10) <em>withλ/N</em> = 2 <em>σ</em></p><p>√ <em>p N, the latter choice suggested by theory. The theory also predicts that the Frobenius error should be low as long asν</em> % <em>δ, a prediction confirmed in this plot.</em></p><p>In order to better understand the guarantee (7.19), we carried out a sim- ulation. Let us define the ratio <em>ν</em> = <em>pN</em> 2 ∈(0 <em>,</em> 1), corresponding to the fraction</p><p>of observed entries in a <em>p</em> × <em>p</em> matrix, and the rank ratio <em>δ</em> = <em>r</em> log <em>pp</em> , corre- sponding to the relative rank of the matrix (up to a logarithmic factor). For a constant noise variance and spikiness ratio, the bound predicts that the es- timator (7.10) should have low relative mean-squared error whenever <em>ν &gt; δ</em>. Figure 7.5 confirms this prediction, and shows that the theory is actually somewhat conservative; see the figure caption for further details.</p><h6 id="missing-data-and-matrix-completion-181"><a class="header-anchor" href="#missing-data-and-matrix-completion-181" aria-hidden="true">#</a> MISSING DATA AND MATRIX COMPLETION 181</h6><div class="language-"><pre><code>0.0 0.2 0.4 0.6\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>1.5\n</code></pre></div><div class="language-"><pre><code>2.0\n</code></pre></div><div class="language-"><pre><code>2.5\n</code></pre></div><div class="language-"><pre><code>3.0\n</code></pre></div><div class="language-"><pre><code>3.5\n</code></pre></div><div class="language-"><pre><code>Proportion Missing\n</code></pre></div><div class="language-"><pre><code>Average Relative Error\n</code></pre></div><div class="language-"><pre><code>Rank 1\n</code></pre></div><div class="language-"><pre><code>0.0 0.2 0.4 0.6\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>1.5\n</code></pre></div><div class="language-"><pre><code>2.0\n</code></pre></div><div class="language-"><pre><code>2.5\n</code></pre></div><div class="language-"><pre><code>3.0\n</code></pre></div><div class="language-"><pre><code>3.5\n</code></pre></div><div class="language-"><pre><code>Proportion Missing\n</code></pre></div><div class="language-"><pre><code>Average Relative Error\n</code></pre></div><div class="language-"><pre><code>Rank 5\n</code></pre></div><p><strong>Figure 7.6</strong> <em>Matrix completion via</em> Soft-Impute <em>in the noisy setting. The plots show the imputation error from matrix completion as a function of the proportion missing, for</em> 40 × 40 <em>matrices. Shown are the mean absolute error(</em> ± <em>one standard error) over 100 simulations, al l relative to the noise standard deviation. In each case we chose the penalty parameter to minimize the imputation error, and the results would be somewhat worse if that parameter were chosen by cross-validation. The true rank of the complete matrix is one in the left panel and five in the right panel. The average absolute size of each matrix entry was 0.80 and 1.77 in the left and right panels, respectively.</em></p><p>Figure 7.6 is another illustration of the imputation error from matrix com- pletion in the noisy setting. Here we useSoft-Imputeon 40×40 matrices, with entries generated from a standard Gaussian matrix with rank <em>r</em> = 1 or 5, plus noise of standard deviation <em>σ</em> = 0_._ 5. We see that for rank one, we can impute the missing values with average error close to <em>σ</em> even when the proportion missing is as high as 50%. However when the true rank increases to five, the procedure starts to break down at about 30% missing.</p><h4 id="_7-3-4-maximum-margin-factorization-and-related-methods"><a class="header-anchor" href="#_7-3-4-maximum-margin-factorization-and-related-methods" aria-hidden="true">#</a> 7.3.4 Maximum Margin Factorization and Related Methods</h4><p>Here we discuss a class of techniques that are close in spirit to the method of the previous section. These are known as <em>maximum margin matrix factoriza- tion</em> methods (MMMF), and use a factor model for approximating the matrix <strong>Z</strong> (Rennie and Srebro 2005).^4 Consider a matrix factorization of the form <strong>M</strong> = <strong>AB</strong> <em>T</em> , where <strong>A</strong> and <strong>B</strong> are <em>m</em> × <em>r</em> and <em>n</em> × <em>r</em> , respectively. One way to</p><p>(^4) The “maximum margin” refers to the particular margin-based loss used by these au- thors; although we use squared-error loss, our focus is on the penalty, so we use the same acronym.</p><h6 id="_182-matrix-decompositions"><a class="header-anchor" href="#_182-matrix-decompositions" aria-hidden="true">#</a> 182 MATRIX DECOMPOSITIONS</h6><p>estimate such a factorization is by solving the optimization problem</p><div class="language-"><pre><code>minimize\nA ∈R m × r\nB ∈R n × r\n</code></pre></div><h6 id="-261"><a class="header-anchor" href="#-261" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>‖PΩ( Z )−PΩ( AB T )‖^2 F+ λ\n</code></pre></div><h6 id="-262"><a class="header-anchor" href="#-262" aria-hidden="true">#</a> (</h6><h6 id="‖-a-‖-2-f-‖-b-‖-2-f"><a class="header-anchor" href="#‖-a-‖-2-f-‖-b-‖-2-f" aria-hidden="true">#</a> ‖ A ‖^2 F+‖ B ‖^2 F</h6><h6 id="-263"><a class="header-anchor" href="#-263" aria-hidden="true">#</a> )}</h6><h6 id="_7-20"><a class="header-anchor" href="#_7-20" aria-hidden="true">#</a> . (7.20)</h6><p>Interestingly, this problem turns out to be equivalent to the nuclear norm regularized problem (7.10) for sufficiently large <em>r</em> , in a way that we now make precise. First, for any matrix <strong>M</strong> , it can be shown (Rennie and Srebro 2005, Mazumder et al. 2010) that</p><div class="language-"><pre><code>‖ M ‖? = min\nA ∈R m × r, B ∈R n × r\nM = AB T\n</code></pre></div><h6 id="_1-108"><a class="header-anchor" href="#_1-108" aria-hidden="true">#</a> 1</h6><h6 id="_2-72"><a class="header-anchor" href="#_2-72" aria-hidden="true">#</a> 2</h6><h6 id="-264"><a class="header-anchor" href="#-264" aria-hidden="true">#</a> (</h6><h6 id="‖-a-‖-2-f-‖-b-‖-2-f-1"><a class="header-anchor" href="#‖-a-‖-2-f-‖-b-‖-2-f-1" aria-hidden="true">#</a> ‖ A ‖^2 F+‖ B ‖^2 F</h6><h6 id="-265"><a class="header-anchor" href="#-265" aria-hidden="true">#</a> )</h6><h6 id="_7-21"><a class="header-anchor" href="#_7-21" aria-hidden="true">#</a> (7.21)</h6><p>As shown in Exercise 7.6, the solution to the problem (7.21) need not be unique. However, the equivalence (7.21) implies that the family of solutions</p><p><strong>M</strong> ̂= <strong>A</strong> ̂ <strong>B</strong> ̂ <em>T</em> of the biconvex problems (7.20) for <em>r</em> ≥min( <em>m,n</em> ) are the same as those for the family of convex problems (7.10). To be more specific, we have the following result:</p><p><em>Theorem 1.</em> Let <strong>Z</strong> be an <em>m</em> × <em>n</em> matrix with observed entries indexed by Ω.</p><p>(a) The solutions to the MMMF criterion (7.20) with <em>r</em> = min{ <em>m,n</em> }and the nuclear norm regularized criterion (7.10) coincide for all <em>λ</em> ≥0. (b) For some fixed <em>λ</em> ∗ <em>&gt;</em> 0, suppose that the objective (7.10) has an opti- mal solution with rank <em>r</em> ∗. Then for any optimal solution ( <strong>A</strong> ̂ <em>,</em> <strong>B</strong> ̂) to the problem (7.20) with <em>r</em> ≥ <em>r</em> ∗and <em>λ</em> = <em>λ</em> ∗, the matrix̂ <strong>M</strong> = <strong>A</strong> ̂ <strong>B</strong> ̂ <em>T</em> is an op- timal solution for the problem (7.10). Consequently, the solution space of the objective (7.10) is contained in that of (7.20). TheMMMFcriterion (7.20) defines a two-dimensional family of models indexed by the pair ( <em>r,λ</em> ), while theSoft-Imputecriterion (7.10) defines a one-dimensional family. In light of Theorem 1, this family is a special path</p><p>in the two-dimensional grid of solutions</p><h6 id="-266"><a class="header-anchor" href="#-266" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>A ̂( r,λ ) , B ̂( r,λ )\n</code></pre></div><h6 id="-267"><a class="header-anchor" href="#-267" aria-hidden="true">#</a> )</h6><p>. Figure 7.7 depicts</p><p>the situation. AnyMMMFmodel at parameter combinations above the red points are redundant, since their fit is the same at the red point. However, in practice the red points are not known toMMMF, nor is the actual rank of the solution. Further orthogonalization of <strong>A</strong> ̂and <strong>B</strong> ̂would be required to reveal the rank, which would only be approximate (depending on the convergence criterion of theMMMFalgorithm). In summary, the formulation (7.10) is preferable for two reasons: it is convex and it does both rank reduction and regularization at the same time. Using (7.20) we need to choose the rank of the approximation and the regularization parameter <em>λ</em>. In a related approach, Keshavan et al. (2010) propose the criterion</p><div class="language-"><pre><code>‖PΩ( Z )−PΩ( USV T )‖^2 F+ λ ‖ S ‖^2 F , (7.22)\n</code></pre></div><p>to be minimized over the triplet ( <strong>U</strong> <em>,</em> <strong>V</strong> <em>,</em> <strong>S</strong> ), where <strong>U</strong> <em>T</em> <strong>U</strong> = <strong>V</strong> <em>T</em> <strong>V</strong> = <strong>I</strong> <em>r</em> and</p><h6 id="missing-data-and-matrix-completion-183"><a class="header-anchor" href="#missing-data-and-matrix-completion-183" aria-hidden="true">#</a> MISSING DATA AND MATRIX COMPLETION 183</h6><div class="language-"><pre><code>−4 −2 0 2 4 6\n</code></pre></div><div class="language-"><pre><code>0 20 40 60 80 100\n</code></pre></div><div class="language-"><pre><code>Rank\n</code></pre></div><div class="language-"><pre><code>r\n</code></pre></div><div class="language-"><pre><code>logλ\n</code></pre></div><p><strong>Figure 7.7</strong> <em>Comparison of the parameter space for</em> MMMF <em>(gray and black points), and</em> Soft-Impute <em>(red points) for a simple example.</em> MMMF <em>solutions for ranks above the red points are identical to the</em> Soft-Impute <em>solutions at the red points and hence the gray points are redundant. On the other hand, fixing a rank for</em> MMMF <em>(for a givenλ) that is less than that of the</em> Soft-Impute <em>solution leads to a non- convex problem.</em></p><p><strong>S</strong> is an <em>r</em> × <em>r</em> matrix. For a fixed rank <em>r</em> , they minimize the criterion (7.22) by gradient descent. This criterion is similar to the MMMF criterion (7.20), except that the matrices <strong>U</strong> <em>,</em> <strong>V</strong> are constrained to be orthonormal, so that the “signal” and corresponding regularization are shifted to the (full) matrix <strong>S</strong>. Like MMMF, the problem is nonconvex so that gradient descent is not guaranteed to converge to the global optimum; moreover, it must be solved separately for different values of the rank <em>r</em>. Keshavan et al. (2010) provide some asymptotic theory for the estima- tor (7.22) when applied to noisy matrix completion, using a scaling in which the aspect ratio <em>m/n</em> converges to some constant <em>α</em> ∈(0 <em>,</em> 1). Here is a rough description of one such result. Consider an <em>m</em> × <em>n</em> matrix <strong>Z</strong> that can be written as a sum of the form <strong>Z</strong> = <strong>UΣV</strong> + <strong>W</strong> , where <strong>Σ</strong> ∈R <em>r</em> × <em>r</em> is a diagonal matrix. Here the term <strong>W</strong> is a random matrix with i.i.d. entries, each with zero mean and variance <em>σ</em>^2</p><h6 id="√-17"><a class="header-anchor" href="#√-17" aria-hidden="true">#</a> √</h6><p><em>mn</em>. Each entry of the matrix <strong>Z</strong> is assumed to be observed independently with probability <em>ρ</em>. Let <strong>Z</strong> ̂be the estimate obtained by mini- mizing the criterion (7.22) using the optimal value for <em>λ</em>. For this criterion,</p><p>Keshavan et al. (2010) show that the relative error‖ <strong>Z</strong> ̂− <strong>Z</strong> ‖^2 F <em>/</em> ‖ <strong>Z</strong> ‖^2 Fconverges in probability to a quantity 1− <em>c</em> ( <em>ρ</em> ) as as <em>m/n</em> → <em>α</em> ∈(0 <em>,</em> 1). The constant <em>c</em> ( <em>ρ</em> ) is zero if <em>σ</em>^2 <em>/ρ</em> ≥max <em>jj</em> Σ <em>jj</em> and nonzero otherwise. This shows that the estimator undergoes a phase transition: if the noise and probability of missing entries are low relative to the signal strength, then the missing entries can be recovered successfully. Otherwise they are essentially useless in reconstructing the missing entries. Full details may be found in Keshavan et al. (2009) and Keshavan et al. (2010).</p><h6 id="_184-matrix-decompositions"><a class="header-anchor" href="#_184-matrix-decompositions" aria-hidden="true">#</a> 184 MATRIX DECOMPOSITIONS</h6><h3 id="_7-4-reduced-rank-regression"><a class="header-anchor" href="#_7-4-reduced-rank-regression" aria-hidden="true">#</a> 7.4 Reduced-Rank Regression</h3><p>In this section we briefly revisit a topic touched on in Section 4.3, namely <em>mul- tivariate regression</em>. We have vector-valued responses <em>yi</em> ∈R <em>K</em> and covariates <em>xi</em> ∈R <em>p</em> , and we wish to build a series of <em>K</em> linear regression models. With <em>N</em> observations on ( <em>yi, xi</em> ), we can write these regression models in matrix form as <strong>Y</strong> = <strong>XΘ</strong> + <strong>E</strong> <em>,</em> (7.23)</p><p>with <strong>Y</strong> ∈R <em>N</em> × <em>K</em> , <strong>X</strong> ∈R <em>N</em> × <em>p</em> , <strong>Θ</strong> ∈R <em>p</em> × <em>K</em> a matrix of coefficients, and <strong>E</strong> ∈ R <em>N</em> × <em>K</em> a matrix of errors. The simplest approach would be to fit <em>K</em> separate models, perhaps via the lasso or elastic net. However, the idea is that the responses may have a lot in common, and these similarities can be used to <em>borrow strength</em> when fitting the <em>K</em> regression models. In Section 4.3, we used the group lasso to select variables simultaneously for each response; i.e., we used the group lasso to set whole rows of <strong>Θ</strong> to zero. In this section we instead assume <strong>Θ</strong> has low rank. The same ideas underlie <em>multitask</em> machine learning. Hence we entertain models of the form <strong>Y</strong> = <strong>XAB</strong> <em>T</em> + <strong>E</strong> <em>,</em> (7.24)</p><p>with <strong>A</strong> ∈R <em>p</em> × <em>r</em> and <strong>B</strong> ∈R <em>K</em> × <em>r</em>. One can think of having <em>r &lt; K</em> derived features <strong>Z</strong> = <strong>XA</strong> ̂ which are then distributed among the responses via <em>K</em> separate regressions <strong>Y</strong> ̂ = <strong>ZB</strong> ̂ <em>T</em>. Although fitting (7.24) by least-squares is a nonconvex optimization problem, with <em>N &gt; p</em> closed-form solutions are available through a form of canonical-correlation analysis (Hastie et al. 2009).</p><p><em>Example 7.1.</em> As an example, we consider the problem of video denoising. Fig- ure 7.8 shows four representative images of a video taken by a helicopter flying over the desert. Each column <em>j</em> of the matrix <strong>Y</strong> represents an image frame (in a vectorized form) at a time <em>k</em> , and the full matrix <strong>Y</strong> represents a video con- sisting of <em>K</em> image frames. The <em>p</em> columns of <strong>X</strong> represent a dictionary of image basis functions (e.g.,unions of orthonormal bases; see Chapter 10). Imposing a low rank model on <strong>Θ</strong> is reasonable when the video sequence changes relatively slowly over time (as they do in this sequence), so that most of its variation can be described by linear combinations of a small number of representative images. Figure 7.9 shows the SVD computed using <em>K</em> = 100 frames from the video in Figure 7.8; although the matrix <strong>Y</strong> is not exactly low-rank, its singular values decay rapidly, suggesting that it can be well-approximated by a low- rank matrix. ♦</p><p>As before, the nuclear norm is a useful convex penalty for enforcing low- rank structure on an estimate. In this case we would solve the optimization problem minimize <strong>Θ</strong> ∈R <em>p</em> × <em>K</em></p><h6 id="-268"><a class="header-anchor" href="#-268" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>‖ Y − XΘ ‖^2 F + λ ‖ Θ ‖?\n</code></pre></div><h6 id="-269"><a class="header-anchor" href="#-269" aria-hidden="true">#</a> }</h6><h6 id="_7-25"><a class="header-anchor" href="#_7-25" aria-hidden="true">#</a> , (7.25)</h6><p>and for sufficiently large values of <em>λ</em> the solution <strong>Θ</strong> ̂would have rank less than min( <em>N,K</em> ).</p><h6 id="a-general-matrix-regression-framework-185"><a class="header-anchor" href="#a-general-matrix-regression-framework-185" aria-hidden="true">#</a> A GENERAL MATRIX REGRESSION FRAMEWORK 185</h6><p><strong>Figure 7.8</strong> <em>Four</em> 352 × 640 <em>image frames from a video sequence of a helicopter flying over the desert. Each image was converted to a vector in</em> R <em>NwithN</em> = 352×640 = 225280 <em>elements, and represented by one column of the matrix</em> <strong>Y</strong><em>.</em></p><h3 id="_7-5-a-general-matrix-regression-framework"><a class="header-anchor" href="#_7-5-a-general-matrix-regression-framework" aria-hidden="true">#</a> 7.5 A General Matrix Regression Framework</h3><p>In this section we present a general “trace” regression framework, that includes matrix completion and reduced-rank regression as special cases. This general framework allows for a unified theoretical treatment. Let’s start with matrix completion. Let <strong>M</strong> represent a model underlying a partially observed <em>m</em> × <em>n</em> matrix <strong>Z</strong> that we wish to complete. Then we consider observations ( <strong>X</strong> <em>i,yi</em> ) <em>,i</em> = 1 <em>,</em> 2 <em>,...,</em> |Ω|from the model</p><div class="language-"><pre><code>yi = trace( X Ti M ) + εi. (7.26)\n</code></pre></div><p>Here <strong>X</strong> <em>i</em> are <em>m</em> × <em>n</em> matrices and <em>yi</em> and <em>εi</em> are scalars. The observation model (7.26) can be viewed as a regression with inputs the matrices <strong>X</strong> <em>i</em> and outputs the <em>yi</em>. The trace inner product on matrices plays the role of an or- dinary inner product on vectors, but otherwise everything is conceptually the same as in a usual regression model.^5 To relate this to matrix completion, let [ <em>a</em> ( <em>i</em> ) <em>,b</em> ( <em>i</em> )] be the row-column indices of the matrix entry observed in observation <em>i</em>. We then define</p><p>(^5) Recall that if <strong>A</strong> and <strong>B</strong> are both <em>m</em> × <em>n</em> matrices, trace( <strong>A</strong> <em>T</em> <strong>B</strong> ) =∑ <em>m i</em> =1 ∑ <em>n j</em> =1 <em>aijbij.</em></p><h6 id="_186-matrix-decompositions"><a class="header-anchor" href="#_186-matrix-decompositions" aria-hidden="true">#</a> 186 MATRIX DECOMPOSITIONS</h6><div class="language-"><pre><code>0 20 40 60 80 100\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>Index\n</code></pre></div><div class="language-"><pre><code>Singular Value\n</code></pre></div><div class="language-"><pre><code>SVD for K=100 Frames\n</code></pre></div><p><strong>Figure 7.9</strong> <em>Singular values of the matrix</em> <strong>Y</strong> ∈R <em>p</em> × <em>KusingK</em> = 100 <em>frames from the video sequence. Note the rapid decay, showing that a low-rank approximation is possible.</em></p><p><strong>X</strong> <em>i</em> = <em>ena</em> ( <em>i</em> ) <em>emb</em> ( <em>i</em> ) <em>T</em> , where <em>em`</em> ∈R <em>m</em> denotes the unit <em>m</em> -vector with a single</p><p>one in the <em>`th</em> coordinate, so that <strong>X</strong> <em>i</em> is zero everywhere except at position [ <em>a</em> ( <em>i</em> ) <em>,b</em> ( <em>i</em> )]. With this choice, we have trace( <strong>X</strong> <em>Ti</em> <strong>M</strong> ) = <em>ma</em> ( <em>i</em> ) <em>b</em> ( <em>i</em> ), so that the observation model (7.26) provides us with certain entries of <strong>M</strong> —those in the training set Ω—each contaminated with noise <em>εi</em>. Our goal is to predict the un- observed entries in <strong>Z</strong> via <strong>M</strong> ̂, which can be thought of asE( <em>y</em> ∗| <strong>X</strong> ∗) for feature values <strong>X</strong> ∗that are distinct from those in the training set. The trace observation model (7.26) is also relevant in a more general set- ting, since with different choices of the covariate matrices{ <strong>X</strong> <em>i</em> }, it can also be used to model other types of matrix estimation problems involving low-rank constraints. The multiresponse regression model of the previous section is another example. The response and covariate vectors are linked via the equation <em>yi</em> = <strong>Θ</strong> <em>Txi</em> + <em>i</em> , where <strong>Θ</strong> ∈R <em>p</em> × <em>K</em> is a matrix of regression coefficients, and <em>i</em> ∈R <em>K</em> is a noise vector. Since each response <em>yi</em> is a <em>K</em> -vector of observations, it can be rewritten as a collection of <em>K</em> separate observations in the trace</p><p>form: if we set <strong>X</strong> <em>ij</em> = <em>xieKj T</em> where <em>eKj</em> ∈R <em>K</em> is the unit vector with a single one in the <em>jth</em> position, then the <em>jth</em> component of <em>yi</em> can be expressed in the form <em>yij</em> = trace( <strong>X</strong> <em>Tij</em> <strong>Θ</strong> ) + <em>ij</em>. In the context of multivariate regression, the matrix lasso takes the form</p><div class="language-"><pre><code>minimize\nΘ\n</code></pre></div><h6 id="-35"><a class="header-anchor" href="#-35" aria-hidden="true">#</a> </h6><h6 id="-29"><a class="header-anchor" href="#-29" aria-hidden="true">#</a> </h6><h6 id="-29"><a class="header-anchor" href="#-29" aria-hidden="true">#</a> </h6><h6 id="_1-109"><a class="header-anchor" href="#_1-109" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-14"><a class="header-anchor" href="#_2-n-14" aria-hidden="true">#</a> 2 N</h6><h6 id="∑-n-81"><a class="header-anchor" href="#∑-n-81" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><h6 id="∑-k-4"><a class="header-anchor" href="#∑-k-4" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>j =1\n</code></pre></div><h6 id="-270"><a class="header-anchor" href="#-270" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>yij −trace( X Tij Θ )\n</code></pre></div><h6 id="_2-73"><a class="header-anchor" href="#_2-73" aria-hidden="true">#</a> ) 2</h6><div class="language-"><pre><code>+ λ ‖ Θ ‖?\n</code></pre></div><h6 id="-30"><a class="header-anchor" href="#-30" aria-hidden="true">#</a> </h6><h6 id="-29"><a class="header-anchor" href="#-29" aria-hidden="true">#</a> </h6><h6 id="-29"><a class="header-anchor" href="#-29" aria-hidden="true">#</a> </h6><h6 id="_7-27"><a class="header-anchor" href="#_7-27" aria-hidden="true">#</a> . (7.27)</h6><p>Exercise 7.9 explores another example. See the papers by Yuan, Ekici, Lu and Monteiro (2007), Negahban and Wainwright (2011 <em>a</em> ), and Rohde and Tsybakov (2011) for further details and benefits of this unified approach. See</p><h6 id="penalized-matrix-decomposition-187"><a class="header-anchor" href="#penalized-matrix-decomposition-187" aria-hidden="true">#</a> PENALIZED MATRIX DECOMPOSITION 187</h6><p>also Bunea, She and Wegkamp (2011) for analysis of an alternative procedure for reduced-rank multivariate regression.</p><h3 id="_7-6-penalized-matrix-decomposition"><a class="header-anchor" href="#_7-6-penalized-matrix-decomposition" aria-hidden="true">#</a> 7.6 Penalized Matrix Decomposition</h3><p>Maximum-margin matrix factorization methods lead naturally to other forms of regularization such as the <em>`</em> 1 -penalized version</p><div class="language-"><pre><code>minimize\nU ∈R m × r, V ∈R n × r\nD ∈R r × r\n</code></pre></div><h6 id="-271"><a class="header-anchor" href="#-271" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>‖ Z − UDV T ‖^2 F+ λ 1 ‖ U ‖ 1 + λ 2 ‖ V ‖ 1\n</code></pre></div><h6 id="-272"><a class="header-anchor" href="#-272" aria-hidden="true">#</a> }</h6><h6 id="_7-28"><a class="header-anchor" href="#_7-28" aria-hidden="true">#</a> , (7.28)</h6><p>where <strong>D</strong> is diagonal and nonnegative. Here we assume that all values of <strong>Z</strong> are observed, and apply an <em>`</em> 1 penalty to the left and right singular vectors of the decomposition. The idea is to obtain sparse versions of the singular vectors for interpretability. Before discussing how to optimize the criterion (7.28), let’s see how it can be used. Returning to the Netflix example, we created a smaller matrix consisting of the 1000 users and the 100 movies, each with the most ratings. We imputed the missing values using an iterated rank 10 SVD (Section 7.3). Then we set the rank of <strong>U</strong> and <strong>V</strong> to two, and minimized a version of the criterion (7.28) for values of <em>λ</em> 1 and <em>λ</em> 2 that yielded a very sparse solution. The resulting solution <strong>V</strong> ̂had 12 nonzero entries, all with the same sign, cor- responding to the movies in Table 7.3. The first group looks like a mixture</p><p><strong>Table 7.3</strong> <em>Movies with nonzero loadings, from a two-dimensional penalized matrix decomposition.</em></p><div class="language-"><pre><code>First Component Second Component\nThe Wedding Planner Lord of the Rings: The Fellowship of the Ring\nGone in 60 Seconds The Last Samurai\nThe Fast and the Furious Lord of the Rings: The Two Towers\nPearl Harbor Gladiator\nMaid in Manhattan Lord of the Rings: The Return of the King\nTwo Weeks Notice\nHow to Lose a Guy in 10 Days\n</code></pre></div><p>of romantic comedies and action movies, while the second group consists of historical action/fantasy movies. How do we solve the optimization problem (7.28)? Let us first consider the one-dimensional case, written in the constrained rather than Lagrangian form:</p><div class="language-"><pre><code>minimize\nu ∈R m, v ∈R n\nd ≥ 0\n</code></pre></div><div class="language-"><pre><code>‖ Z − d uv T ‖^2 Fsubject to‖ u ‖ 1 ≤ c 1 and‖ v ‖ 1 ≤ c 2. (7.29)\n</code></pre></div><h6 id="_188-matrix-decompositions"><a class="header-anchor" href="#_188-matrix-decompositions" aria-hidden="true">#</a> 188 MATRIX DECOMPOSITIONS</h6><p>It turns out that the estimator (7.29) is not very useful, as it tends to produce solutions that are too sparse, as illustrated in Figure 7.10 (right panel). In order to fix this problem, we augment our formulation with additional <em>`</em> 2 - norm constraints, thereby obtaining the optimization problem</p><p>minimize <strong>u</strong> ∈R <em>m,</em> <strong>v</strong> ∈R <em>n d</em> ≥ 0</p><div class="language-"><pre><code>‖ Z − d uv T ‖^2 Fsubject to‖ u ‖ 1 ≤ c 1 ,‖ v ‖ 1 ≤ c 2 ,‖ u ‖ 2 ≤1,‖ v ‖ 2 ≤1.\n</code></pre></div><h6 id="_7-30"><a class="header-anchor" href="#_7-30" aria-hidden="true">#</a> (7.30)</h6><p>It may seem surprising that <em>adding</em> constraints can make the solution <em>sparse</em> , but Figure 7.10 provides some insight.</p><div class="language-"><pre><code>−1.5 −0.5 0.5 1.0 1.5\n</code></pre></div><div class="language-"><pre><code>−1.5\n</code></pre></div><div class="language-"><pre><code>−0.5\n</code></pre></div><div class="language-"><pre><code>0.5 1.0 1.5\n</code></pre></div><div class="language-"><pre><code>−1.5 −0.5 0.5 1.0 1.5\n</code></pre></div><div class="language-"><pre><code>−1.5\n</code></pre></div><div class="language-"><pre><code>−0.5\n</code></pre></div><div class="language-"><pre><code>0.5 1.0 1.5\n</code></pre></div><div class="language-"><pre><code>u 1 u 1\n</code></pre></div><div class="language-"><pre><code>u^2 u^2\n</code></pre></div><p><strong>Figure 7.10</strong> <em>A graphical representation of the<code>_ 1 _and</code></em> 2 <em>constraints on</em> <strong>u</strong> ∈R^2 <em>in the PMD(<code>_ 1 _,</code></em> 1 <em>) criterion. The constraints are as fol lows:</em> ‖ <strong>u</strong> ‖^22 ≤ 1 <em>and</em> ‖ <strong>u</strong> ‖ 1 ≤ <em>c. The gray lines indicate the coordinate axesu</em> 1 <em>andu</em> 2_._ Left panel: <em>The<code>_ 2 _constraint is the solid circle. For both the</code></em> 1 <em>and`</em> 2 <em>constraints to be active, the constraint radiuscmust be between</em> 1 <em>and</em></p><div class="language-"><pre><code>√\n2. The constraints ‖ u ‖ 1 = 1 and ‖ u ‖ 1 =\n</code></pre></div><p>√ 2 <em>are shown using dashed lines.</em> Right panel: <em>The<code>_ 2 _and</code></em> 1 <em>constraints on</em> <strong>u</strong> <em>are shown for somecbetween 1 and</em></p><p>√ 2_. Red dots indicate the points where both the<code>_ 1 _and the</code>_ 2 <em>constraints are active. The red contour shows the boundary of the constraint region. The black lines are the linear contours of the criterion</em> (7.30) <em>as a function of</em> <strong>u</strong> <em>, which increase as we move to the upper right in this example. The solid red arcs indicate the solutions that occur whenλ</em> 1 = 0 <em>in Algorithm 7.2 (<code>_ 2 _active,</code></em> 1 <em>not). The figure shows that in two dimensions, the points where both the<code>_ 1 _and</code></em> 2 <em>constraints are active have neitheru</em> 1 <em>noru</em> 2 <em>equal to zero. We also see that without the`</em> 2 <em>constraints, we would always end up at a corner; this would lead to trivial solutions.</em></p><p>If we fix the second component <strong>v</strong> , the criterion (7.30) is linear in <strong>u</strong>. Suppose that the linear contour of the criterion is angled as in Figure 7.10 and not exactly parallel to a side of the polyhedral constraint region. Then to solve the problem, we move the linear contour toward the top right as far as possible while still remaining inside the constraint region. The solution occurs at one</p><h6 id="penalized-matrix-decomposition-189"><a class="header-anchor" href="#penalized-matrix-decomposition-189" aria-hidden="true">#</a> PENALIZED MATRIX DECOMPOSITION 189</h6><div class="language-"><pre><code>of the open circles, or on the solid red contour. Notice that without the ` 2\nconstraint, the solution will occur at a corner of the polyhedron where only one\ncoefficient is nonzero. As shown in the left panel of Figure 7.10, the problem\nis well defined as long as 1≤ c 1 ≤\n</code></pre></div><h6 id="√-18"><a class="header-anchor" href="#√-18" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>m and 1≤ c 2 ≤\n</code></pre></div><h6 id="√-19"><a class="header-anchor" href="#√-19" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>n.\nSince the criterion (7.30) is biconvex, we can minimize it in an alternat-\ning fashion. It is easy to verify that the solution in each direction is a soft-\nthresholding operation. For example, the update for v ∈R n takes the form\n</code></pre></div><div class="language-"><pre><code>u ←\n</code></pre></div><div class="language-"><pre><code>S λ 1\n</code></pre></div><h6 id="-273"><a class="header-anchor" href="#-273" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Zv\n</code></pre></div><h6 id="-274"><a class="header-anchor" href="#-274" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>‖S λ 1\n</code></pre></div><h6 id="-275"><a class="header-anchor" href="#-275" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Zv\n</code></pre></div><h6 id="-276"><a class="header-anchor" href="#-276" aria-hidden="true">#</a> )</h6><h6 id="‖-2-2"><a class="header-anchor" href="#‖-2-2" aria-hidden="true">#</a> ‖ 2</h6><h6 id="_7-31"><a class="header-anchor" href="#_7-31" aria-hidden="true">#</a> . (7.31)</h6><div class="language-"><pre><code>Here we apply our soft-thresholding operatorSelement-wise on its vector\n</code></pre></div><div class="language-"><pre><code>Algorithm 7.2 Alternating soft-thresholding for rank-one penal-\nized matrix decomposition.\n</code></pre></div><ol><li>Set <strong>v</strong> to the top left singular vector from the SVD of <strong>Z</strong>.</li><li>Perform the update <strong>u</strong> ←</li></ol><div class="language-"><pre><code>S λ 1\n</code></pre></div><h6 id="-277"><a class="header-anchor" href="#-277" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Zv\n</code></pre></div><h6 id="-278"><a class="header-anchor" href="#-278" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>‖S λ 1\n</code></pre></div><h6 id="-279"><a class="header-anchor" href="#-279" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Zv\n</code></pre></div><h6 id="-280"><a class="header-anchor" href="#-280" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>‖ 2\n, with λ 1 being the smallest value such\nthat‖ u ‖ 1 ≤ c 1 ;\n</code></pre></div><ol start="3"><li>Perform the update <strong>v</strong> ←</li></ol><div class="language-"><pre><code>S λ 2\n</code></pre></div><h6 id="-281"><a class="header-anchor" href="#-281" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Z T u\n</code></pre></div><h6 id="-282"><a class="header-anchor" href="#-282" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>‖S λ 2\n</code></pre></div><h6 id="-283"><a class="header-anchor" href="#-283" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Z T u\n</code></pre></div><h6 id="-284"><a class="header-anchor" href="#-284" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>‖ 2\n, with λ 2 being the smallest value\nsuch that‖ v ‖ 1 ≤ c 2 ;\n</code></pre></div><ol start="4"><li>Iterate steps 2 and 3 until convergence.</li><li>Return <strong>u</strong> , <strong>v</strong> and <em>d</em> = <strong>u</strong> <em>T</em> <strong>Zv</strong>.</li></ol><div class="language-"><pre><code>argument. The threshold λ 1 in Equation (7.31) must be chosen adaptively\nto satisfy the constraints: it is set to zero if this results in‖ u ‖ 1 ≤ c 1 , and\notherwise λ 1 is chosen to be a positive constant such that‖ u ‖ 1 = c 1. (See\nExercise 7.7). The overall procedure is summarized in Algorithm 7.2. We note\nthat if c 1 &gt;\n</code></pre></div><h6 id="√-20"><a class="header-anchor" href="#√-20" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>m and c 2 &gt;\n</code></pre></div><h6 id="√-21"><a class="header-anchor" href="#√-21" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>n so that the ` 1 constraints have no effect,\nthen Algorithm 7.2 reduces to the power method for computing the largest\nsingular vectors of the matrix Z. See Section 5.9 for further discussion of\nthe (ordinary) power method. Some recent work has established theoretical\nguarantees for iterative soft thresholding updates related to Algorithm 7.2;\nsee the bibliographic section for further details.\nThe criterion (7.30) is quite useful and may be used with other penalties\n(in addition to the ` 1 -norm) for either u or v , such as the fused lasso penalty\n</code></pre></div><div class="language-"><pre><code>Φ( u ) =\n</code></pre></div><div class="language-"><pre><code>∑ m\n</code></pre></div><div class="language-"><pre><code>j =2\n</code></pre></div><div class="language-"><pre><code>| uj − uj − 1 | , (7.32)\n</code></pre></div><div class="language-"><pre><code>where u = ( u 1 ,u 2 ,...um ). This choice is useful for enforcing smoothness along\na one-dimensional ordering j = 1 , 2 ,...m , such as chromosomal position in\n</code></pre></div><h6 id="_190-matrix-decompositions"><a class="header-anchor" href="#_190-matrix-decompositions" aria-hidden="true">#</a> 190 MATRIX DECOMPOSITIONS</h6><div class="language-"><pre><code>a genomics application. Depending on this choice of penalty, the correspond-\ning minimization in Algorithm 7.2 must change accordingly. In addition, one\ncan modify Algorithm 7.2 to handle missing matrix entries, for example by\nomitting missing values when computing the inner products Zv and Z T u.\nTo obtain a multifactor penalized matrix decomposition, we apply the\nrank-one Algorithm (7.2) successively to the matrix Z , as given in Algo-\nrithm 7.3. If ` 1 penalties on u k and v k are not imposed—equivalently, if we\n</code></pre></div><div class="language-"><pre><code>Algorithm 7.3 Multifactor penalized matrix decomposition\n</code></pre></div><ol><li>Let <strong>R</strong> ← <strong>Z</strong>.</li><li>For <em>k</em> = 1 <em>,...K</em> :</li></ol><div class="language-"><pre><code>(a) Find u k , v k , and dk by applying the single-factor Algorithm 7.2 to\ndata R.\n(b) Update R ← R − dk u k v Tk.\n</code></pre></div><div class="language-"><pre><code>set λ 1 = λ 2 = 0 in Algorithm 7.2—then it can be shown that the K -factor\nPMD algorithm leads to the rank- K SVD of Z. In particular, the successive\nsolutions are orthogonal. With penalties present, the solutions are no longer\nin the column and row spaces of Z , and so the orthogonality does not hold.\nIt is important to note the difference between sparse matrix decomposition\nand matrix completion, discussed earlier. For successful matrix completion, we\nrequired that the singular vectors of Z have low coherence; that is, they need\nto be dense. In sparse matrix decomposition, we seek sparse singular vectors,\nfor interpretability. Matrix completion is not the primary goal in this case.\nUnlike the minimization of convex functions, alternating minimization of\nbiconvex functions is not guaranteed to find a global optimum. In special cases,\nsuch as the power method for computing the largest singular vector, one can\nshow that the algorithm converges to a desired solution, as long as the start-\ning vector is not orthogonal to this solution. But in general, these procedures\nare only guaranteed to move downhill to a partial local minimum of the func-\ntion; see Section 5.9 for discussion of this issue. Based on our experience,\nhowever, they behave quite well in practice, and some recent theoretical work\nprovides rigorous justification of this behavior. See the bibliographic section\nfor discussion.\nLee, Shen, Huang and Marron (2010) suggest the use of the penalized ma-\ntrix decomposition for biclustering of two-way data. In Chapter 8 we describe\napplications of the penalized matrix decomposition to derive penalized mul-\ntivariate methods such as sparse versions of principal components, canonical\ncorrelation, and clustering.\n</code></pre></div><h3 id="_7-7-additive-matrix-decomposition"><a class="header-anchor" href="#_7-7-additive-matrix-decomposition" aria-hidden="true">#</a> 7.7 Additive Matrix Decomposition</h3><div class="language-"><pre><code>In the problem of additive matrix decomposition, we seek to decompose a\nmatrix into the sum of two or more matrices. The components in this additive\n</code></pre></div><h6 id="additive-matrix-decomposition-191"><a class="header-anchor" href="#additive-matrix-decomposition-191" aria-hidden="true">#</a> ADDITIVE MATRIX DECOMPOSITION 191</h6><p>composition should have complementary structures; for instance, one of the most widely studied cases involves decomposing into the sum of a low-rank matrix with a sparse matrix (see also Section 9.5). Additive matrix decomposi- tions arise in a wide variety of applications, among them factor analysis, robust forms of PCA and matrix completion, and multivariate regression problems, as discussed below. Most of these applications can be described in terms of the noisy linear observation model <strong>Z</strong> = <strong>L</strong> ∗+ <strong>S</strong> ∗+ <strong>W</strong> , where the pair ( <strong>L</strong> ∗ <em>,</em> <strong>S</strong> ∗) specify the additive matrix decomposition into low rank and sparse components, and <strong>W</strong> is a noise matrix. In certain cases, we consider a slight generalization of this model, in which we observe a noisy version ofX( <strong>L</strong> ∗+ <strong>S</strong> ∗), whereXis some type of linear operator on the matrix sum (e.g., the projection operatorPΩ in the case of matrix completion, or multiplication via the model matrix <strong>X</strong> in the case of matrix regression. Given such observations, we consider estimators of the pair ( <strong>L</strong> ∗ <em>,</em> <strong>S</strong> ∗) based on the criterion</p><div class="language-"><pre><code>minimize\nL ∈R m × n\nS ∈R m × n\n</code></pre></div><h6 id="-285"><a class="header-anchor" href="#-285" aria-hidden="true">#</a> {</h6><h6 id="_1-110"><a class="header-anchor" href="#_1-110" aria-hidden="true">#</a> 1</h6><h6 id="_2-74"><a class="header-anchor" href="#_2-74" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ Z −( L + S )‖^2 F+ λ 1 Φ 1 ( L ) + λ 2 Φ 2 ( S )\n</code></pre></div><h6 id="-286"><a class="header-anchor" href="#-286" aria-hidden="true">#</a> }</h6><h6 id="_7-33"><a class="header-anchor" href="#_7-33" aria-hidden="true">#</a> , (7.33)</h6><p>where Φ 1 and Φ 2 are penalty functions each designed to enforce a different type of generalized sparsity. For instance, in the case of low rank and sparse matrices, we study the choices Φ 1 ( <strong>L</strong> ) =‖ <strong>L</strong> ‖<em>?</em> and Φ 2 ( <strong>S</strong> ) =‖ <strong>S</strong> ‖ 1. We now turn to some applications of additive matrix decompositions.</p><p><em>Factor Analysis with Sparse Noise:</em> Factor analysis is a widely used form of linear dimensionality reduction that generalizes principal component analy- sis. Factor analysis is easy to understand as a generative model: we generate random vectors <em>yi</em> ∈R <em>p</em> using the “noisy subspace” model</p><div class="language-"><pre><code>yi = μ + Γ ui + wi, for i = 1 , 2 ,...,N. (7.34)\n</code></pre></div><p>Here <em>μ</em> ∈R <em>p</em> is a mean vector, <strong>Γ</strong> ∈R <em>p</em> × <em>r</em> is a loading matrix, and the random vectors <em>ui</em> ∼ <em>N</em> (0 <em>,</em> <strong>I</strong> <em>r</em> × <em>r</em> ) and <em>wi</em> ∼ <em>N</em> (0 <em>,</em> <strong>S</strong> ∗) are independent. Each vector <em>yi</em> drawn from model (7.34) is obtained by generating a random element in the <em>r</em> -dimensional subspace spanned by the columns of <strong>Γ</strong>. Given <em>N</em> samples from this model, the goal is to estimate the column of the loading matrix <strong>Γ</strong> , or equivalently, the rank <em>r</em> matrix <strong>L</strong> ∗= <strong>ΓΓ</strong> <em>T</em> ∈R <em>p</em> × <em>p</em> that spans the column space of <strong>Γ</strong>. A simple calculation shows that the covariance matrix of <em>yi</em> has the form <strong>Σ</strong> = <strong>ΓΓ</strong> <em>T</em> + <strong>S</strong> ∗. Consequently, in the special case when <strong>S</strong> ∗= <em>σ</em>^2 <strong>I</strong> <em>p</em> × <em>p</em> , then the column span of <strong>Γ</strong> is equivalent to the span of the top <em>r</em> eigenvectors of <strong>Σ</strong> , and so we can recover it via standard principal components analysis. In particular, one way to do so is by computing the SVD of the data matrix <strong>Y</strong> ∈R <em>N</em> × <em>p</em> , as discussed in Section 7.2. The right singular vectors of <strong>Y</strong> spec- ify the eigenvectors of the sample covariance matrix, which is a consistent estimate of <strong>Σ</strong>.</p><h6 id="_192-matrix-decompositions"><a class="header-anchor" href="#_192-matrix-decompositions" aria-hidden="true">#</a> 192 MATRIX DECOMPOSITIONS</h6><p>What if the covariance matrix <strong>S</strong> ∗ is not a multiple of the identity? A typical assumption in factor analysis is that <strong>S</strong> ∗is diagonal, but with the noise variance depending on the component of the data. More generally, it might have nonzero entries off the diagonal as well, but perhaps a relatively small number, so that it could be represented as a sparse matrix. In such settings, we no longer have any guarantees that the top <em>r</em> eigenvectors of <strong>Σ</strong> are close to the column span of <strong>Γ</strong>. When this is not the case, PCA will be <em>inconsistent</em> — meaning that it will fail to recover the true column span even if we have an infinite sample size. Nonetheless, when <strong>S</strong> ∗ is a sparse matrix, the problem of estimating <strong>L</strong> ∗= <strong>ΓΓ</strong> <em>T</em> can be understood as an instance of our general observation model with <em>p</em> = <em>N</em>. In particular, given our observations{ <em>yi</em> } <em>Ni</em> =1, we can let our</p><p>observation matrix <strong>Z</strong> ∈R <em>p</em> × <em>p</em> be the sample covariance matrix <em>N</em>^1</p><h6 id="∑-n-82"><a class="header-anchor" href="#∑-n-82" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i − 1 yiy\n</code></pre></div><p><em>T i</em>. With this algebra, we can then write <strong>Z</strong> = <strong>L</strong> ∗+ <strong>S</strong> ∗+ <strong>W</strong> , where <strong>L</strong> ∗= <strong>ΓΓ</strong> <em>T</em> is of rank <em>r</em> , and the random matrix <strong>W</strong> is a re-centered form of Wishart noise—in particular, the zero-mean matrix <strong>W</strong> : = <em>N</em>^1</p><h6 id="∑-n-83"><a class="header-anchor" href="#∑-n-83" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1 yiy\nT\ni −\n</code></pre></div><h6 id="-287"><a class="header-anchor" href="#-287" aria-hidden="true">#</a> {</h6><h6 id="l-∗-s-∗"><a class="header-anchor" href="#l-∗-s-∗" aria-hidden="true">#</a> L ∗+ S ∗</h6><h6 id="-288"><a class="header-anchor" href="#-288" aria-hidden="true">#</a> }</h6><h6 id="-289"><a class="header-anchor" href="#-289" aria-hidden="true">#</a> .</h6><p><em>Robust PCA:</em> As discussed in Section 7.2, standard principal component analysis is based on performing an SVD of a (column centered) data ma- trix <strong>Z</strong> ∈R <em>N</em> × <em>p</em> , where row <em>i</em> represents the <em>ith</em> sample of a <em>p</em> -dimensional data vector. As shown there, the rank- <em>r</em> SVD can be obtained by minimizing the squared Frobenius norm‖ <strong>Z</strong> − <strong>L</strong> ‖^2 Fsubject to a rank constraint on <strong>L</strong>. What if some entries of the data matrix <strong>Z</strong> are corrupted? Or even worse, what if some subset of the rows (data vectors) are corrupted? Since PCA is based on a quadratic objective function, its solution (the rank <em>r</em> SVD) can be very sensitive to these types of perturbations. Additive matrix decompositions provide one way in which to introduce some robustness to PCA. In particular, instead of approximating <strong>Z</strong> with a low-rank matrix, we might approximate it with the sum <strong>L</strong> + <strong>S</strong> of a low- rank matrix with a sparse component to model the corrupted variables. In the case of element-wise corruption, the component <strong>S</strong> would be modeled as element-wise sparse, having relatively few nonzero entries, whereas in the more challenging setting of having entirely corrupted rows, it would be modeled as a row-sparse matrix. Given some target rank <em>r</em> and sparsity <em>k</em> , the direct approach would be to try and solve the optimization problem</p><div class="language-"><pre><code>minimize\nrank( L )≤ r\ncard( S )≤ k\n</code></pre></div><h6 id="_1-111"><a class="header-anchor" href="#_1-111" aria-hidden="true">#</a> 1</h6><h6 id="_2-75"><a class="header-anchor" href="#_2-75" aria-hidden="true">#</a> 2</h6><h6 id="‖-z-−-l-s-‖-2-f-7-35"><a class="header-anchor" href="#‖-z-−-l-s-‖-2-f-7-35" aria-hidden="true">#</a> ‖ Z −( L + S )‖^2 F. (7.35)</h6><p>Here card denotes a cardinality constraint, either the total number of nonzero entries (in the case of element-wise corruption), or the total number of nonzero rows (in the case of row-wise corruption). Of course, the criterion (7.35) is doubly nonconvex, due to both the rank and cardinality constraints, but a natural convex relaxation is provided by our general estimator (7.33) with Φ 1 ( <strong>L</strong> ) =‖ <strong>L</strong> ‖<em>?</em> and Φ 2 ( <strong>S</strong> ) =</p><h6 id="∑-36"><a class="header-anchor" href="#∑-36" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i,j | sij |for element-wise sparsity.\nFigure 7.11 shows an example of robust PCA with the above penalties,\n</code></pre></div><h6 id="additive-matrix-decomposition-193"><a class="header-anchor" href="#additive-matrix-decomposition-193" aria-hidden="true">#</a> ADDITIVE MATRIX DECOMPOSITION 193</h6><p>taken from an unpublished paper by Mazumder and Hastie and using images from Li, Huang, Gu and Tian (2004). The columns of the data matrix <strong>Z</strong> are frames from a video surveillance camera, and are noisy and have missing pixel values (next section). The last two columns show the reconstructed frames; the low-rank part represents the static background, while the sparse component changes in each frame, and in this case represent people moving.</p><div class="language-"><pre><code>True Image Training Image Low-Rank ( L ̂) Sparse (̂ S )\n</code></pre></div><p><strong>Figure 7.11</strong> <em>Video surveil lance. Shown are the true image, noisy training image with missing-values, the estimated low-rank part, and the sparse part aligned side by side. The true images were sampled from the sequence and include ones with varying il lumination and some benchmark test sequences. Despite the missingness and added noise the procedure succeeds in separating the moving components (people) from the fixed background.</em></p><p><em>Robust Matrix Completion:</em> Robustness is also a concern for matrix-com- pletion methods (Section 7.3), which are used in collaborative filtering and recommender systems. Ratings may be corrupted for various reasons: for in- stance, users might try to “game” the system (e.g., a movie star would like to have his/her movies more highly recommended by Netflix). Alternatively, a subset of users might simply be playing pranks with their ratings; for instance, in 2002, <em>The New York Times</em> reported how the Amazon system had been ma- nipulated by adversarial users, so that it would recommend a sex manual to users who expressed interest in Christian literature (Olsen 2002). As we did for robust PCA, we can build in robustness to matrix comple- tion by introducing a sparse component <strong>S</strong> to our representation. The nature of sparsity depends on how we model the adversarial behavior: if we believed that only a small fraction of <em>entries</em> were corrupted, then it would be appro- priate to impose element-wise sparsity via the <em>`</em> 1 -norm. On the other hand, if we wished to model users (rows) as being adversarial, then it would be ap-</p><h6 id="_194-matrix-decompositions"><a class="header-anchor" href="#_194-matrix-decompositions" aria-hidden="true">#</a> 194 MATRIX DECOMPOSITIONS</h6><p>propriate to impose a row-wise sparsity penalty, such as the group lasso norm ‖ <strong>S</strong> ‖ 1 <em>,</em> 2 =</p><div class="language-"><pre><code>∑ m\ni =1‖ S i ‖^2 , where S i ∈R\n</code></pre></div><div class="language-"><pre><code>n denotes the ith row of the matrix. This\n</code></pre></div><p>choice would lead to the following modification of our earlier estimator (7.10):</p><div class="language-"><pre><code>minimize\nL , S ∈R m × n\n</code></pre></div><h6 id="-36"><a class="header-anchor" href="#-36" aria-hidden="true">#</a> </h6><h6 id="-30"><a class="header-anchor" href="#-30" aria-hidden="true">#</a> </h6><h6 id="-30"><a class="header-anchor" href="#-30" aria-hidden="true">#</a> </h6><h6 id="_1-112"><a class="header-anchor" href="#_1-112" aria-hidden="true">#</a> 1</h6><h6 id="_2-76"><a class="header-anchor" href="#_2-76" aria-hidden="true">#</a> 2</h6><h6 id="∑-37"><a class="header-anchor" href="#∑-37" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>( i,j )∈Ω\n</code></pre></div><div class="language-"><pre><code>( zij −( L ij + S ij ))^2 + λ 1 ‖ L ‖? + λ 2\n</code></pre></div><div class="language-"><pre><code>∑ m\n</code></pre></div><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>‖ S i ‖ 2\n</code></pre></div><h6 id="-31"><a class="header-anchor" href="#-31" aria-hidden="true">#</a> </h6><h6 id="-30"><a class="header-anchor" href="#-30" aria-hidden="true">#</a> </h6><h6 id="-30"><a class="header-anchor" href="#-30" aria-hidden="true">#</a> </h6><h6 id="_7-36"><a class="header-anchor" href="#_7-36" aria-hidden="true">#</a> . (7.36)</h6><p>Exercise 7.10 shows an equivalence between this criterion and a nuclear-norm regularized robust Huber loss. Hence one can develop an algorithm along the lines ofSoft-Imputein Section 7.1, replacing the squared-error loss with the Huber loss. Figure 7.11 shows the results of this approach on some video- surveillance data.</p><p><em>Multivariate Regression:</em> Recall the multivariate linear regression model <em>yi</em> = <strong>Θ</strong> <em>Txi</em> + <em>i</em> , where <strong>Θ</strong> ∈R <em>p</em> × <em>K</em> is a matrix of regression coefficients used to predict the multivariate response vector <em>y</em> ∈R <em>K</em>. As discussed in Section 7.5, in one application of matrix regression, each column of the response matrix <strong>Y</strong> represents a vectorized image, so that the full matrix represents a video sequence consisting of <em>K</em> frames. The model matrix <strong>X</strong> represents <em>p</em> image basis functions, one per column; for example, an orthonormal basis of two- dimensional wavelets, at different scales and locations (see Section 10.2.3). Figure 7.8 showed that for certain types of video sequences, the matrix <strong>Y</strong> exhibits rapid decay in its singular values, and so can be well-approximated by a low-rank matrix. In a more realistic setting, a video sequence consists of both a background, and various types of foreground elements. The background component is of- ten slowly varying, so that the low-rank model is appropriate, whereas fore- ground elements vary more rapidly, and may disappear and reappear. (The “helicopter” sequence in Figure 7.8 can be viewed as pure background.) Con- sequently, a more realistic model for the video sequence is based on decom- position <strong>Θ</strong> = <strong>L</strong> + <strong>S</strong> , where <strong>L</strong> is low-rank, and <strong>S</strong> is a relatively sparse matrix. Active entries of <strong>S</strong> correspond to the basis functions (rows) and time posi- tions (columns) for representing the foreground elements that play a role in the video. Of course, these types of decompositions also arise in other applications of multivariate regression. In the general setting, we try to recover the decom- position using the estimator</p><div class="language-"><pre><code>minimize\nL , S\n</code></pre></div><h6 id="-37"><a class="header-anchor" href="#-37" aria-hidden="true">#</a> </h6><h6 id="-31"><a class="header-anchor" href="#-31" aria-hidden="true">#</a> </h6><h6 id="-31"><a class="header-anchor" href="#-31" aria-hidden="true">#</a> </h6><h6 id="_1-113"><a class="header-anchor" href="#_1-113" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-15"><a class="header-anchor" href="#_2-n-15" aria-hidden="true">#</a> 2 N</h6><h6 id="∑-n-84"><a class="header-anchor" href="#∑-n-84" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><h6 id="∑-k-5"><a class="header-anchor" href="#∑-k-5" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>j =1\n</code></pre></div><h6 id="-290"><a class="header-anchor" href="#-290" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>yij −trace( X Tij ( L + S ))\n</code></pre></div><h6 id="_2-77"><a class="header-anchor" href="#_2-77" aria-hidden="true">#</a> ) 2</h6><div class="language-"><pre><code>+ λ 1 ‖ L ‖? + λ 2 ‖ S ‖ 1\n</code></pre></div><h6 id="-32"><a class="header-anchor" href="#-32" aria-hidden="true">#</a> </h6><h6 id="-31"><a class="header-anchor" href="#-31" aria-hidden="true">#</a> </h6><h6 id="-31"><a class="header-anchor" href="#-31" aria-hidden="true">#</a> </h6><h6 id="_7-37"><a class="header-anchor" href="#_7-37" aria-hidden="true">#</a> (7.37)</h6><p>where <strong>X</strong> <em>ij</em> = <em>xieKj T</em> for <em>i</em> = 1 <em>,...,N</em> and <em>j</em> = 1 <em>,...,K</em>. Note that this is a nat- ural generalization of our earlier estimator (7.27) for nuclear-norm regularized multivariate regression.</p><h6 id="bibliographic-notes-195"><a class="header-anchor" href="#bibliographic-notes-195" aria-hidden="true">#</a> BIBLIOGRAPHIC NOTES 195</h6><h3 id="bibliographic-notes-4"><a class="header-anchor" href="#bibliographic-notes-4" aria-hidden="true">#</a> Bibliographic Notes</h3><p>Early work by Fazel (2002) studied the use of the nuclear norm as a sur- rogate for a rank constraint. Srebro, Alon and Jaakkola (2005) studied the nuclear norm as well as related relaxations of rank constraints in the context of matrix completion and collaborative filtering. Bach (2008) derived some asymptotic theory for consistency of nuclear norm regularization. Recht, Fazel and Parrilo (2010) derived nonasymptotic bounds on the performance of nu- clear norm relaxation in the compressed sensing observation model. See also the papers Negahban and Wainwright (2011 <em>a</em> ), Rohde and Tsybakov (2011) for nonasymptotic analysis of the nuclear norm relaxation for more general observation models. Maximum margin matrix factorization is discussed in Srebro and Jaakkola (2003), Srebro, Alon and Jaakkola (2005), and Srebro, Rennie and Jaakkola (2005). Spectral regularization and theSoft-Imputealgorithm were devel- oped by Mazumder et al. (2010). The penalized matrix decomposition is de- scribed in Witten, Tibshirani and Hastie (2009). Matrix completion using the nuclear norm has been studied by various authors, with initial results on prediction-error bounds by Srebro, Alon and Jaakkola (2005). The first the- oretical results on exact recovery with noiseless observations for exactly low- rank matrices were established by Cand<code>es and Recht (2009), with subsequent refinements by various authors. Gross (2011) developed a general dual-witness scheme for proving exactness of nuclear norm relaxations given noiseless ob- servations in arbitrary bases, generalizing the case of entry-wise sampling; see also Recht (2011) for related arguments. Keshavan et al. (2009) provide ex- act recovery guarantees for a slightly different two-stage procedure, involving trimming certain rows and columns of the matrix and then applying the SVD. The more realistic noisy observation model has also been studied by various authors (e.g., Cand</code>es and Plan (2010), Negahban and Wainwright (2012), Keshavan et al. (2010)). The problem of additive matrix decomposition was first considered by Chandrasekaran, Sanghavi, Parrilo and Willsky (2011) in the noiseless setting, who derived worst-case incoherence conditions sufficient for exact re- covery of an arbitrary low-rank/sparse pair. Subsequent work by Cand`es, Li, Ma and Wright (2011) studied the case of random sparse perturbations to the low-rank matrix, with applications to robust PCA. Xu, Caramanis and Sanghavi (2012) proposed an alternative approach to robust PCA, based on modeling the corruptions in terms of a row-sparse matrix. Chandrasekaran, Parrilo and Willsky (2012) developed the use of sparse/low-rank decomposi- tions for the problem of latent Gaussian graphical model selection. In the more general noisy setting, Hsu, Kakade and Zhang (2011) and Agarwal, Negahban and Wainwright (2012 <em>b</em> ) provide bounds on relatives of the estimator (7.33). A recent line of work has provide some theory for alternating mini- mization algorithms in application to particular nonconvex problems, in- cluding matrix completion (Netrapalli, Jain and Sanghavi 2013), phase re-</p><h6 id="_196-matrix-decompositions"><a class="header-anchor" href="#_196-matrix-decompositions" aria-hidden="true">#</a> 196 MATRIX DECOMPOSITIONS</h6><p>trieval (Netrapalli et al. 2013), mixtures of regression (Yi, Caramanis and Sanghavi 2014), and dictionary learning (Agarwal, Anandkumar, Jain, Ne- trapalli and Tandon 2014). These papers show that given suitable initial- izations, alternating minimization schemes do converge (with high probabil- ity) to estimates with similar statistical accuracy to a global minimum. Sim- ilarly, there are also theoretical guarantees for variants of the power method with soft thresholding for recovering sparse eigenvectors (Ma 2013, Yuan and Zhang 2013).</p><h3 id="exercises-5"><a class="header-anchor" href="#exercises-5" aria-hidden="true">#</a> Exercises</h3><p>Ex. 7.1 Recall the singular value decomposition (7.2) of a matrix.</p><div class="language-"><pre><code>(a) Show that the SVD of the column-centered matrix Z gives the principal\ncomponents of Z.\n(b) Show that the condition that successive PCs are uncorrelated is equiv-\nalent to the condition that the vectors{ v j }are orthogonal. What is the\nrelationship between the vectors{ s j }in Section 7.2 and the components of\nthe SVD?\n</code></pre></div><p>Ex. 7.2 In this exercise, we work through the proof of assertion (7.3), namely that <strong>Z</strong> ̂ <em>r</em> = arg min rank( <strong>M</strong> )= <em>r</em></p><h6 id="‖-z-−-m-‖-2-f"><a class="header-anchor" href="#‖-z-−-m-‖-2-f" aria-hidden="true">#</a> ‖ Z − M ‖^2 F ,</h6><p>where <strong>Z</strong> ̂ <em>r</em> = <strong>UD</strong> <em>r</em> <strong>V</strong> <em>T</em> is the SVD truncated to its top <em>r</em> components. (In detail, the SVD is given by <strong>Z</strong> = <strong>UDV</strong> <em>T</em> , and <strong>D</strong> <em>r</em> is the same as <strong>D</strong> except all but the first <em>r</em> diagonal elements are set to zero.) Here we assume that <em>m</em> ≤ <em>n</em> and rank( <strong>Z</strong> ) = <em>m</em>. We begin by noting that any rank <em>r</em> matrix <strong>M</strong> can be factored as <strong>M</strong> = <strong>QA</strong> , where <strong>Q</strong> ∈R <em>m</em> × <em>r</em> is an orthogonal matrix, and <strong>A</strong> ∈R <em>r</em> × <em>n</em>.</p><div class="language-"><pre><code>(a) Show that given Q , the optimal value for A is given by Q T Z.\n(b) Using part (a), show that minimizing‖ Z − M ‖^2 Fis equivalent to solving\nmaximize\nQ ∈R m × r\n</code></pre></div><div class="language-"><pre><code>trace( Q T ΣQ ) subject to Q T Q = I r , (7.38)\n</code></pre></div><div class="language-"><pre><code>where Σ = ZZ T.\n(c) Show that this is equivalent to the problem\nmaximize\nQ ∈R m × r\n</code></pre></div><div class="language-"><pre><code>trace( Q T D^2 Q ) subject to Q T Q = I r. (7.39)\n</code></pre></div><div class="language-"><pre><code>(d) Given an orthonormal matrix Q ∈R m × r , define H = QQ T with diagonal\nelements hii for i = 1 ,...,m. Show that hii ∈[0 , 1] and that\n</code></pre></div><div class="language-"><pre><code>∑ m\ni =1 hii = r.\nConclude that problem (7.39) is equivalent to\n</code></pre></div><div class="language-"><pre><code>maximize\n∑ hii ∈[0 , 1]\nm\ni =1 hii = r\n</code></pre></div><div class="language-"><pre><code>∑ m\n</code></pre></div><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>hiid^2 i. (7.40)\n</code></pre></div><h6 id="exercises-197"><a class="header-anchor" href="#exercises-197" aria-hidden="true">#</a> EXERCISES 197</h6><div class="language-"><pre><code>(e) Assuming that d^21 ≥ d^22 ≥ ...d^2 m ≥0, show that the solution to prob-\nlem (7.40) is obtained by setting h 11 = h 22 = ... = hrr = 1, and setting\nthe remaining coefficients zero. If the{ d^2 i }are strictly ordered, show that\nthis solution is unique.\n(f) Conclude that an optimal choice for Q in problem (7.38) is U 1 , the matrix\nformed from the first r columns of U. This completes the proof.\n</code></pre></div><p>Ex. 7.3</p><div class="language-"><pre><code>(a) ` 1 norm as an LP: For any vector β ∈R p , show that\n</code></pre></div><div class="language-"><pre><code>‖ β ‖ 1 = max\nu ∈R p\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>ujβj subject to‖ u ‖∞≤1. (7.41)\n</code></pre></div><div class="language-"><pre><code>This relation expresses the fact that the ` ∞norm is dual to the ` 1 norm.\n(b) Nuclear norm as an SDP: For any matrix B ∈R m × n , show that\n</code></pre></div><div class="language-"><pre><code>‖ B ‖? = max\nU ∈R m × n\n</code></pre></div><div class="language-"><pre><code>trace( U T B ) subject to‖ U ‖op≤1,\n</code></pre></div><div class="language-"><pre><code>where‖ U ‖opis the maximum singular value of the matrix U , a quantity\nknown as the spectral norm or ` 2 operator norm. This relation expresses\nthe fact that the spectral norm is dual to the nuclear norm. ( Hint: use the\nSVD of Z and cyclical properties of trace operator in order to reduce this\nto an instance of part (a).)\n(c) Given a matrix U ∈R m × n , show that the inequality‖ U ‖op≤1 is equiv-\nalent to the constraint (\nI m U\nU T I n\n</code></pre></div><h6 id="-291"><a class="header-anchor" href="#-291" aria-hidden="true">#</a> )</h6><h6 id="_0-7-42"><a class="header-anchor" href="#_0-7-42" aria-hidden="true">#</a>  0. (7.42)</h6><div class="language-"><pre><code>Since this constraint is a linear matrix inequality, it shows nuclear norm\nminimization can be formulated as an SDP. ( Hint: The Schur-complement\nformula might be useful.)\n</code></pre></div><p>Ex. 7.4 <em>Subgradients of the nuclear norm:</em> Subgradients, as previously defined in Section 5.2, extend the notion of a gradient to nondifferentiable functions.</p><div class="language-"><pre><code>(a) Given a matrix A ∈R m × n with rank r ≤min( m,n ), write its singular\nvalue decomposition as A = UDV T. With this notation, show that the\nsubgradient of the nuclear norm is\n</code></pre></div><div class="language-"><pre><code>∂ ‖ A ‖? =\n</code></pre></div><h6 id="-292"><a class="header-anchor" href="#-292" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>UV T + W | U T W = WV = 0 , ‖ W ‖op≤ 1\n</code></pre></div><h6 id="-293"><a class="header-anchor" href="#-293" aria-hidden="true">#</a> }</h6><h6 id="_7-43"><a class="header-anchor" href="#_7-43" aria-hidden="true">#</a> . (7.43)</h6><div class="language-"><pre><code>(b) Use part (a) to show that a fixed point of theSoft-Imputeproce-\ndure (7.1) satisfies the subgradient equation of the criterion (7.10).\n</code></pre></div><p>Ex. 7.5 From Chapter 5, recall our description (5.21) of Nesterov’s generalized gradient procedure. Show that theSoft-Imputeprocedure (7.1) corresponds to this algorithm applied to the criterion (7.10).</p><h6 id="_198-matrix-decompositions"><a class="header-anchor" href="#_198-matrix-decompositions" aria-hidden="true">#</a> 198 MATRIX DECOMPOSITIONS</h6><p>Ex. 7.6 Construct a solution to the maximum-margin problem (7.21), in the case rank( <strong>M</strong> ) = <em>r</em> ≤min( <em>m,n</em> ), of the form̂ <strong>M</strong> = <strong>A</strong> ̂ <em>m</em> × <em>r</em> <strong>B</strong> ̂ <em>Tr</em> × <em>n</em>. Show that this solution is not unique. Suppose we restrict <strong>A</strong> and <strong>B</strong> to have <em>r</em> ′ <em>&gt; r</em> columns. Show how solutions of this enlarged problem might not reveal the rank of <strong>M</strong>.</p><p>Ex. 7.7 Consider the convex optimization problem</p><div class="language-"><pre><code>maximize\nu ∈R p\nu T Zv subject to‖ u ‖ 2 ≤1 and‖ u ‖ 1 ≤ c. (7.44)\n</code></pre></div><p>Show that the solution is given by</p><div class="language-"><pre><code>u =\n</code></pre></div><div class="language-"><pre><code>S λ\n</code></pre></div><h6 id="-294"><a class="header-anchor" href="#-294" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Zv\n</code></pre></div><h6 id="-295"><a class="header-anchor" href="#-295" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>‖S λ\n</code></pre></div><h6 id="-296"><a class="header-anchor" href="#-296" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Zv\n</code></pre></div><h6 id="-297"><a class="header-anchor" href="#-297" aria-hidden="true">#</a> )</h6><h6 id="‖-2-3"><a class="header-anchor" href="#‖-2-3" aria-hidden="true">#</a> ‖ 2</h6><h6 id="_7-45"><a class="header-anchor" href="#_7-45" aria-hidden="true">#</a> , (7.45)</h6><p>where <em>λ</em> ≥0 is the smallest positive value such that‖ <strong>u</strong> ‖ 1 ≤ <em>c</em>.</p><p>Ex. 7.8 In this exercise, we demonstrate that, in the context of exact com- pletion of a <em>n</em> × <em>n</em> matrix <strong>M</strong> from noiseless entries, it is necessary to observe at least <em>N &gt; n</em> log <em>n</em> entries, even for a rank one matrix. We begin by noting that if we fail to observe any entries from some row (or column) of <strong>M</strong> , then it is impossible to recover <strong>M</strong> exactly (even if we restrict to incoherent matrices with rank one). We letFbe the event that there exists some row with no ob- served entries, under the sampling model in which we choose <em>N</em> entries from the matrix uniformly at random <em>with replacement.</em></p><div class="language-"><pre><code>(a) For each row j = 1 ,...,p , let Zj be a binary indicator variable for the\nevent that no entries of j are observed, and define Z =\n</code></pre></div><div class="language-"><pre><code>∑ n\nj =1 Zj. Show that\n</code></pre></div><h6 id="p-f-p-z-0-≥"><a class="header-anchor" href="#p-f-p-z-0-≥" aria-hidden="true">#</a> P[F] =P[ Z &gt; 0]≥</h6><h6 id="-298"><a class="header-anchor" href="#-298" aria-hidden="true">#</a> (</h6><h6 id="e-z"><a class="header-anchor" href="#e-z" aria-hidden="true">#</a> E[ Z ]</h6><h6 id="_2-78"><a class="header-anchor" href="#_2-78" aria-hidden="true">#</a> ) 2</h6><h6 id="e-z-2"><a class="header-anchor" href="#e-z-2" aria-hidden="true">#</a> E[ Z^2 ]</h6><h6 id="-299"><a class="header-anchor" href="#-299" aria-hidden="true">#</a> .</h6><div class="language-"><pre><code>( Hint: The Cauchy–Schwarz inequality could be useful.)\n(b) Show thatE[ Z ] = n (1− 1 /n ) N.\n(c) Show thatE[ ZiZj ]≤E[ Zi ]E[ Zj ] for i 6 = j.\n(d) Use parts (b) and (c) so show thatE[ Z^2 ]≤ n (1− 1 /n ) N + n^2 (1− 1 /n )^2 N.\n(e) Use the previous parts to show thatP[F] stays bounded away from zero\nunless N &gt; n log n.\n</code></pre></div><p>Ex. 7.9 Quadratic polynomial regression in high dimensions is dangerous, be- cause the number of parameters is proportional to the square of the dimension. Show how to represent this problem as a matrix regression (Section 7.5), and hence suggest how the parameter explosion can be controlled.</p><p>Ex. 7.10. In Exercise 2.11 of Chapter 2, we show that a regression model that allows for a sparse perturbation of each prediction is equivalent to a robust</p><h6 id="exercises-199"><a class="header-anchor" href="#exercises-199" aria-hidden="true">#</a> EXERCISES 199</h6><p>regression using Huber’s <em>ρ</em> function. Here we establish an analogous result for robust PCA. Recall the sparse plus low-rank version of PCA:</p><div class="language-"><pre><code>minimize\nL , S\n</code></pre></div><h6 id="_1-114"><a class="header-anchor" href="#_1-114" aria-hidden="true">#</a> 1</h6><h6 id="_2-79"><a class="header-anchor" href="#_2-79" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>‖ Z −( L + S )‖^2 F+ λ 1 ‖ L ‖? + λ 2 ‖ S ‖ 1. (7.46)\n</code></pre></div><p>Now consider a robustified version of PCA</p><div class="language-"><pre><code>minimize\nL\n</code></pre></div><h6 id="_1-115"><a class="header-anchor" href="#_1-115" aria-hidden="true">#</a> 1</h6><h6 id="_2-80"><a class="header-anchor" href="#_2-80" aria-hidden="true">#</a> 2</h6><h6 id="∑-n-85"><a class="header-anchor" href="#∑-n-85" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>ρ ( zij − `ij ; λ 2 ) + λ 1 ‖ L ‖ ?, (7.47)\n</code></pre></div><p>where</p><div class="language-"><pre><code>ρ ( t ; λ ) =\n</code></pre></div><h6 id="-300"><a class="header-anchor" href="#-300" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>λ | t |− λ^2 / 2 if| t | &gt; λ\nt^2 / 2 if| t |≤ λ.\n</code></pre></div><h6 id="_7-48"><a class="header-anchor" href="#_7-48" aria-hidden="true">#</a> (7.48)</h6><p>is Huber’s loss function. Show that problem (7.47) has the same solution for <strong>L</strong> as does problem (7.46).</p><div class="language-"><pre><code>Chapter 8\n</code></pre></div><h2 id="sparse-multivariate-methods"><a class="header-anchor" href="#sparse-multivariate-methods" aria-hidden="true">#</a> Sparse Multivariate Methods</h2><h3 id="_8-1-introduction"><a class="header-anchor" href="#_8-1-introduction" aria-hidden="true">#</a> 8.1 Introduction</h3><p>In this chapter, we discuss some popular methods for multivariate analysis and explore how they can be “sparsified”: that is, how the set of features can be reduced to a smaller set to yield more interpretable solutions. Many stan- dard multivariate methods are derived from the singular value decomposition of an appropriate data matrix. Hence, one systematic approach to sparse mul- tivariate analysis is through a sparse decomposition of the same data matrix. The penalized matrix decomposition of Section 7.6 is well-suited to this task, as it delivers sparse versions of the left and/or right singular vectors. For example, suppose that we have a data matrix <strong>X</strong> of dimension <em>N</em> × <em>p</em> , and assume that the columns each have mean zero. Then the principal components of <strong>X</strong> are derived from its singular value decomposition (SVD) <strong>X</strong> = <strong>UDV</strong> <em>T</em> : the columns of <strong>V</strong> are the principal component direction vectors (in order), and the columns of <strong>U</strong> are the standardized principal components. Hence we can derive <em>sparse</em> principal components by applying instead the penalized matrix decomposition to <strong>X</strong> , with sparsity enforced on the right vectors. In a similar way, many multivariate methods can be derived by appropriate application of the penalized matrix decomposition. These methods are summarized in Table 8.1.</p><div class="language-"><pre><code>Table 8.1 The penalized matrix decomposition of Section 7.6 applied\nto appropriate input matrices leads to sparse versions of classical mul-\ntivariate methods.\nInput Matrix Result\nData matrix sparse SVD and principal components\nVariance-covariance sparse principal components\nCross-products sparse canonical variates\nDissimilarity sparse clustering\nBetween-class covariance sparse linear discriminants\n</code></pre></div><div class="language-"><pre><code>201\n</code></pre></div><h6 id="_202-sparse-multivariate-methods"><a class="header-anchor" href="#_202-sparse-multivariate-methods" aria-hidden="true">#</a> 202 SPARSE MULTIVARIATE METHODS</h6><div class="language-"><pre><code>−3 −2 −1 0 1 2 3\n</code></pre></div><div class="language-"><pre><code>−3\n</code></pre></div><div class="language-"><pre><code>−2\n</code></pre></div><div class="language-"><pre><code>−1\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>3\n</code></pre></div><div class="language-"><pre><code>x 1\n</code></pre></div><div class="language-"><pre><code>x^2\n</code></pre></div><div class="language-"><pre><code>v 1\n</code></pre></div><div class="language-"><pre><code>zi 1\nxi\n</code></pre></div><p><strong>Figure 8.1</strong> <em>A two-dimensional illustration of principal components analysis, show- ing the first principal componentv</em> 1 ∈R^2 <em>of a col lection of data pointsxi</em> = ( <em>xi</em> 1 <em>,xi</em> 2 ) <em>, shown in green circles. Letting</em> ̄ <em>x</em> = ( ̄ <em>x</em> 1 <em>,x</em> ̄ 2 ) <em>T denote the sample mean, the line x</em> ̄+ <em>λTv</em> 1 <em>maximizes the variance of the projected points along the line, and min- imizes the total squared distance from each point to its orthogonal projection onto the line. Herezi</em> 1 = <em>ui</em> 1 <em>d</em> 1 <em>is the scalar valued representation of observationxiin the first principal component</em> <strong>z</strong> 1_._</p><h3 id="_8-2-sparse-principal-components-analysis"><a class="header-anchor" href="#_8-2-sparse-principal-components-analysis" aria-hidden="true">#</a> 8.2 Sparse Principal Components Analysis</h3><p>We begin our exploration with the problem of sparse principal component analysis, which is a natural extension of PCA well-suited to high-dimensional data. To set the stage, we first review principal components.</p><h4 id="_8-2-1-some-background"><a class="header-anchor" href="#_8-2-1-some-background" aria-hidden="true">#</a> 8.2.1 Some Background</h4><p>Given a data matrix <strong>X</strong> of dimension <em>N</em> × <em>p</em> , consisting of <em>N</em> vectors { <em>x</em> 1 <em>,...,xN</em> }inR <em>p</em> , principal component analysis provides a sequence of linear approximations, indexed by a rank <em>r</em> ≤min{ <em>p,N</em> }. There are two different but equivalent ways of viewing and deriving prin- cipal components. The first approach is based on the directions of <em>maximal variance</em>. Any unit-norm vector <em>α</em> ∈R <em>p</em> leads to a one-dimensional projection of the data, namely the <em>N</em> -vector <strong>X</strong> <em>α</em>.^1 Assuming that the columns of <strong>X</strong> have been centered, the sample variance of the projected data vector is given by</p><p>(^1) In this chapter we deal with multivariate methods applied to a data matrix <strong>X</strong> ∈R <em>N</em> × <em>p</em> ; we hence adhere to our convention of representing <em>N</em> -vectors and all matrices in boldface, and <em>p</em> -vectors in plain text.</p><h6 id="sparse-principal-components-analysis-203"><a class="header-anchor" href="#sparse-principal-components-analysis-203" aria-hidden="true">#</a> SPARSE PRINCIPAL COMPONENTS ANALYSIS 203</h6><p>̂Var( <strong>X</strong> <em>α</em> ) =^1 <em>N</em></p><h6 id="∑-n-86"><a class="header-anchor" href="#∑-n-86" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1( x\n</code></pre></div><div class="language-"><pre><code>T\niα )\n</code></pre></div><p>(^2). Principal components analysis finds the direction that maximizes the sample variance <em>v</em> 1 = arg max ‖ <em>α</em> ‖ 2 =1</p><h6 id="-301"><a class="header-anchor" href="#-301" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>̂Var( X α )\n</code></pre></div><h6 id="-302"><a class="header-anchor" href="#-302" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>= arg max\n‖ α ‖ 2 =1\n</code></pre></div><h6 id="-303"><a class="header-anchor" href="#-303" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>αT\n</code></pre></div><h6 id="x-t-x-1"><a class="header-anchor" href="#x-t-x-1" aria-hidden="true">#</a> X T X</h6><h6 id="n-28"><a class="header-anchor" href="#n-28" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>α\n</code></pre></div><h6 id="-304"><a class="header-anchor" href="#-304" aria-hidden="true">#</a> }</h6><h6 id="_8-1"><a class="header-anchor" href="#_8-1" aria-hidden="true">#</a> . (8.1)</h6><p>Hence the first principal component direction corresponds to the largest eigen- value of the sample covariance <strong>X</strong> <em>T</em> <strong>X</strong> <em>/N</em> , which provides the link to the notion of maximal variance at the population level. See Exercise 8.1 for further de- tails. Figure 8.1 illustrates the geometry of this optimization problem. The resulting projection <strong>z</strong> 1 = <strong>X</strong> <em>v</em> 1 is called the first principal component of the data <strong>X</strong> , and the elements of <em>v</em> 1 are called the <em>principal component loadings</em>. The vector <em>v</em> 1 is easily seen to be the right singular vector corresponding to the largest singular value <em>d</em> 1 of <strong>X</strong>. Similarly <strong>z</strong> 1 = <strong>u</strong> 1 <em>d</em> 1 , where <strong>u</strong> 1 is the corresponding left singular vector. Subsequent principal-component directions (eigen-vectors) <em>v</em> 2 <em>,v</em> 3 <em>,...,vp</em></p><p>correspond to maxima of̂Var( <strong>X</strong> <em>vj</em> ) subject to‖ <em>vj</em> ‖ 2 = 1 and <em>vj</em> orthogonal to <em>v</em> 1 <em>,...vj</em> − 1. This property also implies that the <strong>z</strong> <em>j</em> are mutually uncorrelated (see Exercise 8.2). In fact, after <em>r</em> steps of this procedure, we obtain a rank <em>r</em> matrix that solves the optimization problem</p><div class="language-"><pre><code>V r = arg max\nA : A T A = I r\n</code></pre></div><div class="language-"><pre><code>trace( A T X T XA ) (8.2)\n</code></pre></div><p>See Exercise 7.2 for further details on this property. Thus, even though they are defined sequentially, the collection of loading vectors in <strong>V</strong> <em>r</em> also maximize the total variance among all such collections. A second derivation of principal components is based on minimizing the <em>reconstruction error</em> associated with a particular generative model for the data. Suppose that the rows of the data matrix can be modeled as <em>xi</em> ≈ <em>f</em> ( <em>λi</em> ), where the function</p><div class="language-"><pre><code>f ( λ ) = μ + A rλ (8.3)\n</code></pre></div><p>parametrizes an affine set of dimension <em>r</em>. Here <em>μ</em> ∈R <em>p</em> is a location vector, <strong>A</strong> <em>r</em> ∈R <em>p</em> × <em>r</em> is a matrix with orthonormal columns corresponding to directions, and <em>λ</em> ∈R <em>r</em> is a parameter vector that varies over samples. It is natural to choose the parameters{ <em>μ,</em> <strong>A</strong> <em>r,</em> { <em>λi</em> } <em>Ni</em> =1}to minimize the average reconstruction error</p><div class="language-"><pre><code>1\nN\n</code></pre></div><h6 id="∑-n-87"><a class="header-anchor" href="#∑-n-87" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>‖ xi − μ − A rλi ‖^22. (8.4)\n</code></pre></div><p>This interpretation of PCA is illustrated in Figure 8.1. As we explore in Ex- ercise 8.3, when the data has been precentered (so that we may take <em>μ</em> = 0), the criterion (8.4) can be reduced to</p><div class="language-"><pre><code>1\nN\n</code></pre></div><h6 id="∑-n-88"><a class="header-anchor" href="#∑-n-88" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>‖ xi − A r A Trxi ‖^22 , (8.5)\n</code></pre></div><h6 id="_204-sparse-multivariate-methods"><a class="header-anchor" href="#_204-sparse-multivariate-methods" aria-hidden="true">#</a> 204 SPARSE MULTIVARIATE METHODS</h6><p>and the value of <strong>A</strong> <em>r</em> that minimizes the reconstruction error (8.5) can again be obtained from the singular value decomposition of the data matrix. In operational terms, we compute the SVD <strong>X</strong> = <strong>UDV</strong> <em>T</em> , and then form <strong>A</strong> ˆ <em>r</em> = <strong>V</strong> <em>r</em> by taking the <em>r</em> columns of <strong>V</strong> corresponding to the top <em>r</em> singular values. The estimates for <em>λi</em> are given by the rows of <strong>Z</strong> <em>r</em> = <strong>U</strong> <em>r</em> <strong>D</strong> <em>r</em>. So maximizing total variance within the affine surface corresponds to minimizing total distance from the surface. Again we observe that the successive solutions are <em>nested</em> ; this property is special, and is not necessarily inherited by the generalizations that we discuss in this chapter.</p><h4 id="_8-2-2-sparse-principal-components"><a class="header-anchor" href="#_8-2-2-sparse-principal-components" aria-hidden="true">#</a> 8.2.2 Sparse Principal Components</h4><p>We often interpret principal components by examining the loading vectors { <em>vj</em> } <em>rj</em> =1so as to determine which of the variables play a significant role. In this section, we discuss some methods for deriving principal components with sparse loadings. Such sparse principal components are especially useful when the number of variables <em>p</em> is large relative to the sample size. With a large number of variables, it is often desirable to select a smaller subset of relevant variables, as revealed by the loadings. At the theoretical level, in the <em>p</em>  <em>N</em> regime, ordinary PCA is known to break down very badly, in that the eigenvectors of the sample covariance need not be close to the population eigenvectors (Johnstone 2001). Imposing sparsity on the principal components makes the problem well-posed in the “large <em>p</em> , small <em>N</em> ” regime. In this section, we discuss a number of methods for obtaining sparse principal components, all based on lasso-type ( <em>`</em> 1 ) penalties. As with ordinary PCA, we start with an <em>N</em> × <em>p</em> data matrix <strong>X</strong> with centered columns. The proposed methods focus on either the maximum variance property of principal components, or minimum reconstruction error. For ease of exposition, we begin by discussing the rank- one case for each method, deferring the case of higher ranks until Section 8.2.3.</p><h5 id="_8-2-2-1-sparsity-from-maximum-variance"><a class="header-anchor" href="#_8-2-2-1-sparsity-from-maximum-variance" aria-hidden="true">#</a> 8.2.2.1 Sparsity from Maximum Variance</h5><p>We begin by discussing how the maximum variance characterization of PCA can be modified to incorporate sparsity. The most natural modification would be to impose an <em>`</em> 0 -restriction on the criterion, leading to the problem</p><div class="language-"><pre><code>maximize\n‖ v ‖ 2 =1\n</code></pre></div><h6 id="-305"><a class="header-anchor" href="#-305" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>vT X T X v\n</code></pre></div><h6 id="-306"><a class="header-anchor" href="#-306" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>subject to‖ v ‖ 0 ≤ t, (8.6)\n</code></pre></div><p>where‖ <em>v</em> ‖ 0 =</p><p>∑ <em>p j</em> =1I[ <em>vj</em>^6 = 0] simply counts the number of nonzeros in the vec- tor <em>v</em>. However, this problem is doubly nonconvex, since it involves maximizing (as opposed to minimizing) a convex function with a combinatorial constraint. TheSCoTLASSprocedure of Jolliffe, Trendafilov and Uddin (2003) is a natu- ral relaxation of this objective, based on replacing the <em><code>_ 0 -norm by the _</code></em> 1 -norm, leading to</p><div class="language-"><pre><code>maximize\n‖ v ‖ 2 =1\n</code></pre></div><h6 id="-307"><a class="header-anchor" href="#-307" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>vT X T X v\n</code></pre></div><h6 id="-308"><a class="header-anchor" href="#-308" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>subject to‖ v ‖ 1 ≤ t. (8.7)\n</code></pre></div><h6 id="sparse-principal-components-analysis-205"><a class="header-anchor" href="#sparse-principal-components-analysis-205" aria-hidden="true">#</a> SPARSE PRINCIPAL COMPONENTS ANALYSIS 205</h6><div class="language-"><pre><code>The ` 1 -constraint encourages some of the loadings to be zero and hence v\nto be sparse. Although the ` 1 -norm is convex, the overall problem remains\nnonconvex, and moreover is not well-suited to simple iterative algorithms.\nThere are multiple ways to address these challenges. One approach draws\non the SVD version of principal components; we re-express the problem, leav-\ning it nonconvex but leading to a computationally efficient algorithm for find-\ning local optima. Recall the penalized matrix criterion (7.28) on page 187;\napplying it with no constraint on u —that is, with c 1 =∞—leads to the\noptimization problem\n</code></pre></div><div class="language-"><pre><code>maximize\n‖ u ‖ 2 =‖ v ‖ 2 =1\n</code></pre></div><h6 id="-309"><a class="header-anchor" href="#-309" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>u T X v\n</code></pre></div><h6 id="-310"><a class="header-anchor" href="#-310" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>subject to‖ v ‖ 1 ≤ t. (8.8)\n</code></pre></div><div class="language-"><pre><code>Any optimal solution̂ v to this problem is also optimal for the originalSCoT-\nLASSprogram (8.7). The advantage of this reformulation is that the objective\nfunction (8.8) is biconvex in the pair ( u ,v ), so that we can apply alternating\nminimization to solve it—in particular, recall Algorithm 7.2 in Chapter 7 for\nthe penalized matrix decomposition. Doing so leads to Algorithm 8.1, which\nconsists of the following steps:\n</code></pre></div><div class="language-"><pre><code>Algorithm 8.1 Alternating algorithm for rank one sparse PCA.\n</code></pre></div><ol><li>Initialize <em>v</em> ∈R <em>p</em> with‖ <em>v</em> ‖ 2 = 1.</li><li>Repeat until changes in <strong>u</strong> and <em>v</em> are sufficiently small:</li></ol><div class="language-"><pre><code>(a) Update u ∈R N via u ←‖ XX vv ‖ 2.\n(b) Update v ∈R p via\n</code></pre></div><div class="language-"><pre><code>v ← v ( λ, u ) =\nS λ ( X T u )\n‖S λ ( X T u )‖ 2\n</code></pre></div><h6 id="_8-9"><a class="header-anchor" href="#_8-9" aria-hidden="true">#</a> , (8.9)</h6><div class="language-"><pre><code>where λ = 0 if‖ X T u ‖ 1 ≤ t , and otherwise λ &gt; 0 is chosen such that\n‖ v ( λ, u )‖ 1 = t.\n</code></pre></div><div class="language-"><pre><code>HereS λ ( x ) = sign( x ) (| x |− λ )+is the familiar soft-thresholding operator\nat level λ. In Exercise 8.6, we show that any fixed point of this algorithm rep-\nresents a local optimum of the criterion (8.7), and moreover, that the updates\ncan be interpreted as a minorization-maximization , or simply a minorization\nalgorithm for the objective function (8.7).\nAn alternative approach, taken by d’Aspremont, El Ghaoui, Jordan and\nLanckriet (2007), is to further relax theSCoTLASSobjective to a convex\nprogram, in particular by lifting it to a linear optimization problem over the\nspace of positive semidefinite matrices. Such optimization problems are known\nas semidefinite programs. In order to understand this method, let us begin\nwith an exact reformulation of the nonconvex objective function (8.7). By the\nproperties of the matrix trace, we can rewrite the quadratic form vT X T X v in\n</code></pre></div><h6 id="_206-sparse-multivariate-methods"><a class="header-anchor" href="#_206-sparse-multivariate-methods" aria-hidden="true">#</a> 206 SPARSE MULTIVARIATE METHODS</h6><p>terms of a trace operation—specifically</p><div class="language-"><pre><code>vT X T X v = trace( X T X vvT ). (8.10)\n</code></pre></div><p>In terms of the rank one matrix <strong>M</strong> = <em>vvT</em> , the constraint‖ <em>v</em> ‖^22 = 1 is equiv- alent to the linear constraint trace( <strong>M</strong> ) = 1, and the constraint‖ <em>v</em> ‖ 1 ≤ <em>t</em> can be expressed as trace(| <strong>M</strong> | <strong>E</strong> )≤ <em>t</em>^2 , where <strong>E</strong> ∈R <em>p</em> × <em>p</em> is a matrix of all ones, and| <strong>M</strong> |is the matrix obtained by taking absolute values entry-wise. Putting together the pieces, we conclude that the nonconvexSCoTLASSobjective has the equivalent reformulation</p><div class="language-"><pre><code>maximize\nM  0\n</code></pre></div><div class="language-"><pre><code>trace( X T X M )\n</code></pre></div><div class="language-"><pre><code>subject to trace( M ) = 1 , trace(| M | E )≤ t^2 , and rank( M ) = 1.\n</code></pre></div><h6 id="_8-11"><a class="header-anchor" href="#_8-11" aria-hidden="true">#</a> (8.11)</h6><p>By construction, any optimal solution to this problem is a positive semidefinite matrix of rank one, say <strong>M</strong> = <em>vvT</em> , and the vector <em>v</em> is an optimal solution to the original problem (8.7). However, the optimization problem (8.11) is still nonconvex, due to the presence of the constraint rank( <strong>M</strong> ) = 1. By dropping this constraint, we obtain the semidefinite program proposed by d’Aspremont et al. (2007), namely</p><div class="language-"><pre><code>maximize\nM  0\ntrace( X T XM )\n</code></pre></div><div class="language-"><pre><code>subject to trace( M ) = 1 , trace(| M | E )≤ t^2.\n</code></pre></div><h6 id="_8-12"><a class="header-anchor" href="#_8-12" aria-hidden="true">#</a> (8.12)</h6><p>Since this problem is convex, it has no local optima, and a global optimum can be obtained by various standard methods. These include interior point methods (Boyd and Vandenberghe 2004); see also d’Aspremont et al. (2007) for a special-purpose and more efficient method for solving it. In general, solving the SDP (8.12) is computationally more intensive than finding a local optimum of the biconvex criterion (8.8). However, since it is a convex relaxation of an exact reformulation, it has an attractive theoretical guarantee: if we solve the SDP and do obtain a rank-one solution, then we have in fact obtained the global optimum of the nonconvexSCoTLASScrite- rion. For various types of spiked covariance models, it can be shown that the SDP (8.12) will have a rank-one solution with high probability, as long as the sample size <em>N</em> is sufficiently large relative to the sparsity and dimension (but still allowing for <em>N</em>  <em>p</em> ); see Section 8.2.6 for further discussion. Thus, for all of these problems, we are guaranteed to have found the <em>global</em> optimum of theSCoTLASScriterion.</p><h5 id="_8-2-2-2-methods-based-on-reconstruction"><a class="header-anchor" href="#_8-2-2-2-methods-based-on-reconstruction" aria-hidden="true">#</a> 8.2.2.2 Methods Based on Reconstruction</h5><p>We now turn to methods for sparse PCA that are based on its reconstruction interpretation. In the case of a single sparse principal component, Zou, Hastie</p><h6 id="sparse-principal-components-analysis-207"><a class="header-anchor" href="#sparse-principal-components-analysis-207" aria-hidden="true">#</a> SPARSE PRINCIPAL COMPONENTS ANALYSIS 207</h6><p>and Tibshirani (2006) proposed the optimization problem</p><div class="language-"><pre><code>minimize\nθ,v ∈R p\n‖ θ ‖ 2 =1\n</code></pre></div><h6 id="-311"><a class="header-anchor" href="#-311" aria-hidden="true">#</a> {</h6><h6 id="_1-116"><a class="header-anchor" href="#_1-116" aria-hidden="true">#</a> 1</h6><h6 id="n-29"><a class="header-anchor" href="#n-29" aria-hidden="true">#</a> N</h6><h6 id="∑-n-89"><a class="header-anchor" href="#∑-n-89" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>‖ xi − θvTxi ‖^22 + λ 1 ‖ v ‖ 1 + λ 2 ‖ v ‖^22\n</code></pre></div><h6 id="-312"><a class="header-anchor" href="#-312" aria-hidden="true">#</a> }</h6><h6 id="_8-13"><a class="header-anchor" href="#_8-13" aria-hidden="true">#</a> , (8.13)</h6><p>where <em>λ</em> 1 <em>,λ</em> 2 are nonnegative regularization parameters. Let’s examine this formulation in more detail.</p><ul><li>If we set <em>λ</em> 1 = <em>λ</em> 2 = 0, then it is easy to show that the program (8.13) achieves its optimum at a pair <em>θ</em> ˆ= ˆ <em>v</em> = <em>v</em> 1 , corresponding to a maximum eigenvector of <strong>X</strong> <em>T</em> <strong>X</strong> , so that we recover the usual PCA solution.</li><li>When <em>p</em>  <em>N</em> the solution is not necessarily unique unless <em>λ</em> 2 <em>&gt;</em> 0. If we set <em>λ</em> 1 = 0, then for any <em>λ</em> 2 <em>&gt;</em> 0, the optimal solution̂ <em>v</em> is proportional to the largest principal component direction.</li><li>In the general setting with both <em>λ</em> 1 and <em>λ</em> 2 strictly positive, the <em>`</em> 1 -penalty weighted by <em>λ</em> 1 encourages sparsity of the loadings. Like the objective (8.8), criterion (8.13) is not jointly convex in <em>v</em> and <em>θ</em> , but is biconvex. Minimization over <em>v</em> with <em>θ</em> fixed is equivalent to an elastic- net problem (see Section 4.2) and can be computed efficiently. On the other hand, minimization over <em>θ</em> with <em>v</em> fixed has the simple solution</li></ul><div class="language-"><pre><code>θ =\n</code></pre></div><div class="language-"><pre><code>X T z\n‖ X T z ‖ 2\n</code></pre></div><h6 id="_8-14"><a class="header-anchor" href="#_8-14" aria-hidden="true">#</a> , (8.14)</h6><p>where <em>zi</em> = <em>vTxi</em> for <em>i</em> = 1 <em>,...,N</em> (see Exercise 8.8). Overall, this procedure is reasonably efficient, but not as simple as Algorithm 8.1, which involves just soft-thresholding. It turns out that the originalSCoTLASScriterion (8.7) and the regression- based objective function (8.13) are intimately related. Focusing on the rank- one case (8.13), consider the constrained as opposed to the Lagrangian form of the optimization problem—namely</p><div class="language-"><pre><code>minimize\n‖ v ‖ 2 =‖ θ ‖ 2 =1\n</code></pre></div><div class="language-"><pre><code>‖ X − X vθT ‖^2 F subject to‖ v ‖ 1 ≤ t. (8.15)\n</code></pre></div><p>If we add the extra <em><code>_ 1 -constraint‖ _θ_ ‖ 1 ≤ _t_ , then as shown in Exercise 8.7, the resulting optimization problem is equivalent to theSCoTLASScrite- rion (8.7). Consequently, it can be solved conveniently by Algorithm 8.1. Note that adding this _</code></em> 1 -constraint is quite natural, as it just symmetrizes the con- straints in problem (8.15).</p><h4 id="_8-2-3-higher-rank-solutions"><a class="header-anchor" href="#_8-2-3-higher-rank-solutions" aria-hidden="true">#</a> 8.2.3 Higher-Rank Solutions</h4><p>In Section 8.2.1, we presented a sequential approach for standard principal components analysis, based on successively solving the rank-one problem, re- stricting each candidate to be orthogonal to all previous solutions. This se- quential approach also solves the multirank problem (8.2).</p><h6 id="_208-sparse-multivariate-methods"><a class="header-anchor" href="#_208-sparse-multivariate-methods" aria-hidden="true">#</a> 208 SPARSE MULTIVARIATE METHODS</h6><p>How about in the sparse setting? This sequential approach is also used in theSCoTLASSprocedure, where each candidate solution for rank <em>k</em> is restricted to be orthogonal to all previous solutions for ranks <em>&lt; k</em>. However, here the sequential approach will typically not solve a multirank criterion. For the sparse PCA approach (8.8), we can apply the multifactor penal- ized matrix decomposition (7.3) of Chapter 7. Given the rank-one solution ( <strong>u</strong> 1 <em>,v</em> 1 <em>,d</em> 1 ), we simply compute the residual <strong>X</strong> ′= <strong>X</strong> − <em>d</em> 1 <strong>u</strong> 1 <em>vT</em> 1 and apply the rank-one Algorithm (8.8) to <strong>X</strong> ′to obtain the next solution.^2 Doing so ensures neither orthogonality of the principal components{( <strong>u</strong> 1 <em>d</em> 1 ) <em>,</em> ( <strong>u</strong> 2 <em>d</em> 2 ) <em>,...</em> ( <strong>u</strong> <em>kdk</em> )} nor the sparse loading vectors{ <em>v</em> 1 <em>,v</em> 2 <em>...,vk</em> }. But the solutions do tend to be somewhat orthogonal in practice. However, there is a subtle issue here: it is not clear that orthogonality of the vectors{ <em>v</em> 1 <em>,v</em> 2 <em>,...vk</em> }is desirable in the setting of sparse PCA, as orthogo- nality may be at odds with sparsity. Otherwise stated, enforcing orthogonality might result in less sparse solutions. A similar issue arises with sparse coding, as discussed in Section 8.2.5. Interestingly, one can modify the approach (8.8) to constrain the vectors <strong>u</strong> <em>j</em> to be orthogonal, with no such constraints on the vectors <em>vj</em>. This modification can improve the interpretability of the set of solutions while still allowing the <em>vj</em> to be sparse. In detail, consider the problem</p><div class="language-"><pre><code>maximize\nu k, vk\n</code></pre></div><h6 id="-313"><a class="header-anchor" href="#-313" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>u Tk X vk\n</code></pre></div><h6 id="-314"><a class="header-anchor" href="#-314" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>subject to‖ vk ‖ 2 ≤1,‖ vk ‖ 1 ≤ c ,\n</code></pre></div><div class="language-"><pre><code>and‖ u k ‖ 2 ≤1 with u Tk u j = 0 for all j = 1 ,...,k −1.\n</code></pre></div><h6 id="_8-16"><a class="header-anchor" href="#_8-16" aria-hidden="true">#</a> (8.16)</h6><p>The solution for <strong>u</strong> <em>k</em> with <em>vk</em> fixed is</p><div class="language-"><pre><code>u k =\n</code></pre></div><div class="language-"><pre><code>P ⊥ k − 1 X vk\n‖ P ⊥ k − 1 X vk ‖ 2\n</code></pre></div><h6 id="_8-17"><a class="header-anchor" href="#_8-17" aria-hidden="true">#</a> (8.17)</h6><p>where <strong>P</strong> ⊥ <em>k</em> − 1 = <strong>I</strong> −</p><p>∑ <em>k</em> − 1 <em>i</em> =1 <strong>u</strong> <em>i</em> <strong>u</strong><em>T i</em> , the projection onto the orthogonal complement of the space spanned by <strong>u</strong> 1 <em>,</em> <strong>u</strong> 2 <em>,...</em> <strong>u</strong> <em>k</em> − 1. This multifactor version of Algo- rithm 8.1 uses operation (8.17) in place of the rank-one projection <strong>u</strong> =‖ <strong>XX</strong> <em>vv</em> ‖ 2.</p><p>The approach (8.13) of Zou et al. (2006) can be generalized to <em>r &gt;</em> 1 components by minimizing the cost function</p><div class="language-"><pre><code>1\nN\n</code></pre></div><h6 id="∑-n-90"><a class="header-anchor" href="#∑-n-90" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>‖ xi − ΘV Txi ‖^22 +\n</code></pre></div><div class="language-"><pre><code>∑ r\n</code></pre></div><div class="language-"><pre><code>k =1\n</code></pre></div><div class="language-"><pre><code>λ 1 k ‖ vk ‖ 1 + λ 2\n</code></pre></div><div class="language-"><pre><code>∑ r\n</code></pre></div><div class="language-"><pre><code>k =1\n</code></pre></div><div class="language-"><pre><code>‖ vk ‖^22 , (8.18)\n</code></pre></div><p>subject to <strong>Θ</strong> <em>T</em> <strong>Θ</strong> = <strong>I</strong> <em>r</em> × <em>r</em>. Here <strong>V</strong> is a <em>p</em> × <em>r</em> matrix with columns{ <em>v</em> 1 <em>,...,vr</em> }, and <strong>Θ</strong> is also a matrix of dimension <em>p</em> × <em>r</em>. Although this objective func- tion (8.18) is not jointly convex in <strong>V</strong> and <strong>Θ</strong> , it is biconvex. Minimization over <strong>V</strong> with <strong>Θ</strong> fixed is equivalent to solving <em>r</em> separate elastic net problems and</p><p>(^2) Without sparsity constraints, this procedure would deliver exactly the usual sequence of principal components.</p><h6 id="sparse-principal-components-analysis-209"><a class="header-anchor" href="#sparse-principal-components-analysis-209" aria-hidden="true">#</a> SPARSE PRINCIPAL COMPONENTS ANALYSIS 209</h6><p>can be done efficiently. On the other hand, minimization over <strong>Θ</strong> with <strong>V</strong> fixed is a version of the so-called Procrustes problem, and can be solved by a simple SVD calculation (Exercise 8.10). These steps are alternated until convergence to a local optimum.</p><p><em>8.2.3.1 Il lustrative Application of Sparse PCA</em></p><p>Here we demonstrate sparse principal components on a dataset of digitized handwritten digits. We have a training set of <em>N</em> = 664 gray-scale images of handwritten sevens. Each image contains 16×16 pixels, leading to a data matrix <strong>X</strong> of dimension 664×256. Panel (a) of Figure 8.2 shows some ex-</p><div class="language-"><pre><code>(a)\n</code></pre></div><div class="language-"><pre><code>(b)\n</code></pre></div><p><strong>Figure 8.2</strong> <em>(a) A sample of handwritten sevens from the zip code database. (b) Top row: first four principal components for “sevens” data (color shades represent negative loadings as yel low and positive loadings as blue); Bottom two rows: first eight sparse principal components, constrained to be positive. These are superimposed on the average seven to enhance interpretability.</em></p><p>amples of these images, where panel (b) shows the results of sparse principal components, and contrasts them with standard PCA. The top row in panel (b) shows the first four standard principal components, which explain about 50% of the variance. To enhance interpretability, we compute sparse principal components with the loadings constrained to be nonnegative. In order to do so, we simply replace the soft-threshold operatorS <em>λ</em> ( <em>x</em> ) in Algorithm 8.1 by the nonnegative soft-threshold operatorS <em>λ</em> +( <em>x</em> ) = ( <em>x</em> − <em>λ</em> )+.</p><h6 id="_210-sparse-multivariate-methods"><a class="header-anchor" href="#_210-sparse-multivariate-methods" aria-hidden="true">#</a> 210 SPARSE MULTIVARIATE METHODS</h6><p>The first eight sparse principal components are shown in the middle and bottom rows, and also explain about 50% of the variation. While more com- ponents are needed to explain the same amount of variation, the individual components are simpler and potentially more interpretable. For example, the 2nd and 6th sparse components appear to be capturing the “notch” style used by some writers, for example in the top left image of Figure 8.2(a).</p><h4 id="_8-2-4-sparse-pca-via-fantope-projection"><a class="header-anchor" href="#_8-2-4-sparse-pca-via-fantope-projection" aria-hidden="true">#</a> 8.2.4 Sparse PCA via Fantope Projection</h4><p>Vu, Cho, Lei and Rohe (2013) propose another related approach to sparse PCA. Letting <strong>S</strong> = <strong>X</strong> <em>T</em> <strong>X</strong> <em>/N</em> , their proposal is to solve the semidefinite program</p><div class="language-"><pre><code>maximize\nZ ∈F p\n</code></pre></div><div class="language-"><pre><code>{trace( SZ )− λ ‖ Z ‖ 1 } (8.19)\n</code></pre></div><p>where the convex setF <em>p</em> ={ <strong>Z</strong> : <strong>0</strong>  <strong>Z</strong>  <strong>I</strong> <em>,</em> trace( <strong>Z</strong> ) = <em>p</em> }is known as a <em>Fantope</em>. When <em>p</em> = 1 the spectral norm bound inF <em>p</em> is redundant and (8.19) reduces to the direct approach of d’Aspremont et al. (2007). For <em>p &gt;</em> 1, al- though the penalty in (8.19) only implies entry-wise sparsity of the solution, it can be shown (Lei and Vu 2015) that the solution is able to consistently select the nonzero entries of the leading eigenvectors under appropriate conditions.</p><h4 id="_8-2-5-sparse-autoencoders-and-deep-learning"><a class="header-anchor" href="#_8-2-5-sparse-autoencoders-and-deep-learning" aria-hidden="true">#</a> 8.2.5 Sparse Autoencoders and Deep Learning</h4><p>In the neural network literature, an autoencoder generalizes the idea of prin- cipal components. Figure 8.3 provides a simple illustration of the idea, which is based on reconstruction, much like in the criterion (8.13). The autoen-</p><div class="language-"><pre><code>x 1\n</code></pre></div><div class="language-"><pre><code>x 2\n</code></pre></div><div class="language-"><pre><code>x 3\n</code></pre></div><div class="language-"><pre><code>x 4\n</code></pre></div><div class="language-"><pre><code>x 5\n</code></pre></div><div class="language-"><pre><code>xˆ 1\n</code></pre></div><div class="language-"><pre><code>xˆ 2\n</code></pre></div><div class="language-"><pre><code>xˆ 3\n</code></pre></div><div class="language-"><pre><code>xˆ 4\n</code></pre></div><div class="language-"><pre><code>xˆ 5\n</code></pre></div><div class="language-"><pre><code>Input\nlayer\n</code></pre></div><div class="language-"><pre><code>Hidden\nlayer\n</code></pre></div><div class="language-"><pre><code>Output\nlayer\nWT\n</code></pre></div><div class="language-"><pre><code>σ(WTx)\n</code></pre></div><div class="language-"><pre><code>W\n</code></pre></div><p><strong>Figure 8.3</strong> <em>Left: Network representation of an autoencoder used for unsupervised learning of nonlinear principal components. The middle layer of hidden units creates a bottleneck, and learns nonlinear representations of the inputs. The output layer is the transpose of the input layer, and so the network tries to reproduce the input data using this restrictive representation. Right: Images representing the estimated columns of</em> <strong>W</strong> <em>in an image modeling task.</em></p><h6 id="sparse-principal-components-analysis-211"><a class="header-anchor" href="#sparse-principal-components-analysis-211" aria-hidden="true">#</a> SPARSE PRINCIPAL COMPONENTS ANALYSIS 211</h6><p>coder is based on a <em>p</em> × <em>m</em> matrix of weights <strong>W</strong> with <em>m &lt; p</em> ; it is used to create <em>m</em> linear combinations of the input vector <em>x</em>. Each such linear combi- nation is passed through a nonlinear function <em>σ</em> , with the sigmoid function <em>σ</em> ( <em>t</em> ) = 1 <em>/</em> (1 + <em>e</em> − <em>t</em> ) being one typical choice, as represented in Figure 8.3 via the vector function <em>h</em> ( <em>x</em> ) = <em>σ</em> ( <strong>W</strong> <em>Tx</em> ). The output layer is then modeled as <strong>W</strong> <em>h</em> ( <em>x</em> ) = <strong>W</strong> <em>σ</em> ( <strong>W</strong> <em>Tx</em> ).^3 Given input vectors <em>xi</em> for <em>i</em> = 1 <em>,...,N</em> , the weight matrix <strong>W</strong> is then estimated by solving the (nonconvex) optimization problem</p><div class="language-"><pre><code>minimize\nW ∈R m × p\n</code></pre></div><h6 id="-315"><a class="header-anchor" href="#-315" aria-hidden="true">#</a> {</h6><h6 id="_1-117"><a class="header-anchor" href="#_1-117" aria-hidden="true">#</a> 1</h6><h6 id="_2-81"><a class="header-anchor" href="#_2-81" aria-hidden="true">#</a> 2</h6><h6 id="∑-n-91"><a class="header-anchor" href="#∑-n-91" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>‖ xi − W h ( xi )‖^2\n</code></pre></div><h6 id="-316"><a class="header-anchor" href="#-316" aria-hidden="true">#</a> }</h6><h6 id="_8-20"><a class="header-anchor" href="#_8-20" aria-hidden="true">#</a> . (8.20)</h6><p>If we restrict <em>σ</em> to be the identity function, then <em>h</em> ( <em>x</em> ) = <strong>W</strong> <em>Tx</em> and the solution to (8.20) is equivalent to principal components; i.e., <strong>WW</strong> <em>T</em> = <strong>V</strong> <em>m</em> <strong>V</strong> <em>Tm</em> , where <strong>V</strong> <em>m</em> is the <em>p</em> × <em>m</em> matrix consisting of the first <em>m</em> principal component load- ings (see Exercise 8.12). Here the bottleneck in the network imposes a rank constraint on <strong>W</strong> , forcing it to learn structure. In modeling high-dimensional signals such as images, the vectors <em>xi</em> might represent the pixels of a (sub) image. The columns of <strong>W</strong> represent a learned dictionary of image shapes, and <em>h</em> ( <em>xi</em> ) tries to represent <em>xi</em> in this basis. Now the bottleneck might be seen as an unnecessary restriction, since many slightly different shapes are likely in an image. The idea is to replace this restriction by imposing sparseness on the coefficients <em>h</em> ( <em>x</em> ), leading to so-called <em>sparse coding</em> (Olshausen and Field 1996). To build intuition, we first consider the linear case, but now with <em>m &gt; p</em>. In the optimization problem</p><div class="language-"><pre><code>minimize\nW ∈R p × m, { si } N 1 ∈R m\n</code></pre></div><h6 id="-317"><a class="header-anchor" href="#-317" aria-hidden="true">#</a> {</h6><h6 id="_1-118"><a class="header-anchor" href="#_1-118" aria-hidden="true">#</a> 1</h6><h6 id="_2-82"><a class="header-anchor" href="#_2-82" aria-hidden="true">#</a> 2</h6><h6 id="∑-n-92"><a class="header-anchor" href="#∑-n-92" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><h6 id="-318"><a class="header-anchor" href="#-318" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>‖ xi − W si ‖^22 + λ ‖ si ‖ 1\n</code></pre></div><h6 id="-319"><a class="header-anchor" href="#-319" aria-hidden="true">#</a> }</h6><h6 id="-320"><a class="header-anchor" href="#-320" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>subject to‖ W ‖^2 F≤ 1 ,\n</code></pre></div><h6 id="_8-21"><a class="header-anchor" href="#_8-21" aria-hidden="true">#</a> (8.21)</h6><p>the individual <em>si</em> are forced to be sparse through the <em>`</em> 1 -penalties. The columns of <strong>W</strong> are not constrained to be uncorrelated, and their total size is kept in bound by the Frobenius norm. Exercise 8.13 examines the sparse linear coder (8.21) in more detail, and develops a natural alternating algorithm for solving it. The right panel of Figure 8.3 illustrates a typical solution for <strong>W</strong> in an image modeling problem, where each <em>xi</em> is a vectorized version of an image. Each subimage represents a column of <strong>W</strong> (the codebook), and every image is modeled as a sparse superposition of elements of <strong>W</strong>. Modern sparse encoders used in deep learning generalize this formulation in several ways (Le et al. 2012):</p><ul><li>They use multiple hidden layers, leading to a hierarchy of dictionaries;</li><li>Nonlinearities that can be computed more rapidly than the sigmoid are used—for example <em>σ</em> ( <em>t</em> ) = <em>t</em> +.</li></ul><p>(^3) In practice, bias terms are also included in each linear combination; we omit them here for simplicity.</p><h6 id="_212-sparse-multivariate-methods"><a class="header-anchor" href="#_212-sparse-multivariate-methods" aria-hidden="true">#</a> 212 SPARSE MULTIVARIATE METHODS</h6><ul><li>More general sparseness penalties are imposed directly on the coefficients <em>h</em> ( <em>xi</em> ) in the problem (8.20).</li></ul><p>These models are typically fit by (stochastic) gradient descent, and often on very large databases of images (for example), using distributed computing with large clusters of processors. One important use of the sparse autoencoder is for <em>pretraining</em>. When fit- ting a supervised neural network to labelled data, it is often advantageous to first fit an autoencoder to the data without the labels and then use the resulting weights as starting values for fitting the supervised neural network (Erhan et al. 2010). Because the neural-network objective function is noncon- vex, these starting weights can significantly improve the quality of the final solution. Furthermore, if there is additional data available without labels, the autoencoder can make use of these data in the pretraining phase.</p><h4 id="_8-2-6-some-theory-for-sparse-pca"><a class="header-anchor" href="#_8-2-6-some-theory-for-sparse-pca" aria-hidden="true">#</a> 8.2.6 Some Theory for Sparse PCA</h4><p>Here we give a brief overview of how standard principal component analysis breaks down in the high-dimensional setting ( <em>p</em>  <em>N</em> ), and why some struc- tural assumption—such as sparsity in the principal components—is essential. One way of studying the behavior of (sparse) PCA is in terms of a <em>spiked covariance model</em> , meaning a <em>p</em> -dimensional covariance matrix of the form</p><h6 id="σ"><a class="header-anchor" href="#σ" aria-hidden="true">#</a> Σ =</h6><h6 id="∑-m-1"><a class="header-anchor" href="#∑-m-1" aria-hidden="true">#</a> ∑ M</h6><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>ωjθjθTj + σ^2 I p × p, (8.22)\n</code></pre></div><p>where the vectors{ <em>θj</em> } <em>Mj</em> =1 are orthonormal, and associated with positive</p><p>weights <em>ω</em> 1 ≥ <em>ω</em> 2 ≥···≥ <em>ωM&gt;</em> 0. By construction, the vectors{ <em>θj</em> } <em>Mj</em> =1are the top <em>M</em> eigenvectors of the population covariance, with associated eigenvalues { <em>σ</em>^2 + <em>ωj</em> } <em>Mj</em> =1. Given <em>N</em> i.i.d. samples{ <em>xi</em> } <em>Ni</em> =1from a zero-mean distribution with covari- ance <strong>Σ</strong> , standard PCA is based on estimating the span of{ <em>θj</em> } <em>Mj</em> =1using the</p><p>top <em>M</em> eigenvectors of the sample covariance matrix <strong>Σ</strong> ̂= <em>N</em>^1</p><h6 id="∑-n-93"><a class="header-anchor" href="#∑-n-93" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1 xix\n</code></pre></div><p><em>T i</em>. In the classical setting, in which the dimension <em>p</em> remains fixed while the sample size <em>N</em> →+∞, the sample covariance converges to the population covariance, so that the principal components are consistent estimators. More relevant for high-dimensional data analysis is a scaling in which <em>bothp</em> and <em>N</em> tend to infinity, with <em>p/N</em> → <em>c</em> ∈(0 <em>,</em> ∞) with <em>M</em> and the eigenvalues remaining fixed.^4 Under this scaling, the sample eigenvectors or principal components do not</p><p>converge to the population eigenvectors{ <em>θ</em> ( <em>p</em> ) <em>j</em> } <em>M j</em> =1. In fact, if the signal-to-noise ratios <em>ωj/σ</em>^2 are sufficiently small, the sample eigenvectors are asymptotically orthogonal to the population eigenvectors! This poor behavior is caused by</p><p>(^4) To be clear, for each <em>j</em> = 1 <em>,...,M</em> , we have a sequence{ <em>θ</em> ( <em>jp</em> )}of population eigenvectors, but we keep the signal-to-noise ratio <em>ωj/σ</em>^2 fixed, independently of ( <em>p,N</em> ).</p><h6 id="sparse-canonical-correlation-analysis-213"><a class="header-anchor" href="#sparse-canonical-correlation-analysis-213" aria-hidden="true">#</a> SPARSE CANONICAL CORRELATION ANALYSIS 213</h6><p>the <em>p</em> − <em>M</em> dimensions of noise in the spiked covariance model (8.22), which can swamp the signal when <em>N</em>  <em>p</em> ; see Johnstone and Lu (2009) for a precise statement of this phenomenon. Given the breakdown of high-dimensional PCA without any structure on the eigenvectors, we need to make additional assumptions. A number of au- thors have explored how sparsity can still allow for consistent estimation of principal components even when <em>p</em>  <em>N</em>. Johnstone and Lu (2009) propose a two-stage procedure, based on thresholding the diagonal of the sample covari- ance matrix in order to isolate the highest variance coordinates, and then per- forming PCA in the reduced-dimensional space. They prove consistency of this method even when <em>p/N</em> stays bounded away from zero, but allow only polyno- mial growth of <em>p</em> as a function of sample size. Amini and Wainwright (2009) analyze the variable selection properties of both diagonal thresholding and the semidefinite programming relaxation (8.12) of theSCoTLASSproblem (8.7). For a spiked covariance model (8.22) with a single leading eigenvector that is <em>k</em> -sparse, they show that the diagonal thresholding method (Johnstone and Lu 2009) succeeds in recovering sparsity pattern of the leading eigenvector if and only if the sample size <em>N</em>  <em>k</em>^2 log <em>p</em>. The SDP relaxation also performs correct variable selection with this scaling of the sample size, and in cer- tain settings, can succeed with fewer samples. Amini and Wainwright (2009) show that no method—even one based on exhaustively enumerating all the subsets—can succeed with fewer than <em>N</em>  <em>k</em> log <em>p</em> samples. Other authors have studied the estimation of the eigenspaces themselves in <em><code>_ 2 or related norms. Paul and Johnstone (2008) propose the augmented SPCA algorithm, a refinement of the two-stage method of Johnstone and Lu (2009); this algorithm is also analyzed by Birnbaum, Johnstone, Nadler and Paul (2013), who show that it achieves the minimax rates for models of weakly sparse eigenvectors in _</code>q</em> -balls. In independent work, Vu and Lei (2012) prove minimax lower bounds for the sparse PCA problem, and show that they can be achieved by computing the maximum eigenvalue of the sample covariance subject to an <em>`q</em> -constraint. Ma (2010, 2013) and Yuan and Zhang (2013) have studied algorithms for sparse PCA based on a combination of the power method (a classical iterative technique for computing eigenvectors) with intermediate soft-thresholding steps. When <em>M</em> = 1, the procedure of Ma (2013) is essentially the same as Algorithm 8.1, the only difference being the use of a fixed level <em>λ</em> in the soft-thresholding step, rather than the variable choice used in the latter to solve the bound version of the problem.</p><h3 id="_8-3-sparse-canonical-correlation-analysis"><a class="header-anchor" href="#_8-3-sparse-canonical-correlation-analysis" aria-hidden="true">#</a> 8.3 Sparse Canonical Correlation Analysis</h3><p>Canonical correlation analysis extends the idea of principal components anal- ysis to two data matrices. Suppose that we have data matrices <strong>X</strong> <em>,</em> <strong>Y</strong> of di- mensions <em>N</em> × <em>p</em> and <em>N</em> × <em>q</em> , respectively, with centered columns. Given two vectors <em>β</em> ∈R <em>p</em> and <em>θ</em> ∈R <em>q</em> , they define one-dimensional projections of the two datasets, namely the variates ( <em>N</em> -vectors) <strong>X</strong> <em>β</em> and <strong>Y</strong> <em>θ</em> , respectively. Canoni-</p><h6 id="_214-sparse-multivariate-methods"><a class="header-anchor" href="#_214-sparse-multivariate-methods" aria-hidden="true">#</a> 214 SPARSE MULTIVARIATE METHODS</h6><p>cal correlation analysis (CCA) chooses <em>β</em> and <em>θ</em> to maximize the correlation between these two variates. In detail, the sample covariance between <strong>X</strong> <em>β</em> and <strong>Y</strong> <em>θ</em> is given by</p><div class="language-"><pre><code>Cov(̂ X β, Y θ ) =^1\nN\n</code></pre></div><h6 id="∑-n-94"><a class="header-anchor" href="#∑-n-94" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>( xTiβ ) ( yiTθ ) =\n</code></pre></div><h6 id="_1-119"><a class="header-anchor" href="#_1-119" aria-hidden="true">#</a> 1</h6><h6 id="n-30"><a class="header-anchor" href="#n-30" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>βT X T Y θ. (8.23)\n</code></pre></div><p>where <em>xi</em> and <em>yi</em> are the <em>ith</em> rows of <strong>X</strong> and <strong>Y</strong> , respectively. CCA solves the problem</p><div class="language-"><pre><code>maximize\nβ ∈R p, θ ∈R q\n</code></pre></div><h6 id="-321"><a class="header-anchor" href="#-321" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>Cov(̂ X β, Y θ )\n</code></pre></div><h6 id="-322"><a class="header-anchor" href="#-322" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>subject tôVar( X β ) = 1 and̂Var( Y θ ) = 1.\n</code></pre></div><h6 id="_8-24"><a class="header-anchor" href="#_8-24" aria-hidden="true">#</a> (8.24)</h6><p>The solution set ( <em>β</em> 1 <em>,θ</em> 1 ) are called the first canonical vectors, and the cor- responding linear combinations <strong>z</strong> 1 = <strong>X</strong> <em>β</em> 1 and <strong>s</strong> 1 = <strong>Y</strong> <em>θ</em> 1 the first canonical variates. Subsequent pairs of variates can be found by restricting attention to vectors such that the resulting variates are uncorrelated with the earlier ones. All solutions are given by a generalized SVD of the matrix <strong>X</strong> <em>T</em> <strong>Y</strong> (see Exercise 8.14). Canonical correlation analysis fails when the sample size <em>N</em> is strictly less than max( <em>p,q</em> ): in this case, the problem is degenerate, and one can find meaningless solutions with correlations equal to one. One approach to avoid- ing singularity of the sample covariance matrices <em>N</em>^1 <strong>X</strong> <em>T</em> <strong>X</strong> and <em>N</em>^1 <strong>Y</strong> <em>T</em> <strong>Y</strong> is by imposing additional restrictions. For instance, the method of <em>ridge regular- ization</em> is based on adding some positive multiple <em>λ</em> of the identity to each sample covariance matrix; see Exercise 8.17 for further discussion. An alter- native method is based on taking only the diagonal entries of the sample covariance matrices, an approach that we adopt below. Sparse canonical vectors can be derived by imposing <em>`</em> 1 -constraints on <em>β</em> and <em>θ</em> in the criterion (8.24), leading to the modified objective</p><div class="language-"><pre><code>maximize\nβ, θ\n</code></pre></div><h6 id="-323"><a class="header-anchor" href="#-323" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>Cov(̂ X β, Y θ )\n</code></pre></div><h6 id="-324"><a class="header-anchor" href="#-324" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>subject to Var( X β ) = 1 , ‖ β ‖ 1 ≤ c 1 , Var( Y θ ) = 1 , ‖ θ ‖ 1 ≤ c 2.\n(8.25)\n</code></pre></div><p>Note that one can use either the bound form for the <em>`</em> 1 -constraints (as above), or add corresponding Lagrangian terms. For numerical solution of this prob- lem, we note that the standard CCA problem (8.24) can be solved by alternat- ing least squared regressions (see Exercises 8.14–8.17). Not surprisingly then, the sparse version (8.25) can be solved by alternating elastic-net procedures, as we explore in Exercise 8.19. The sparse formulation (8.25) is useful when <em>N &gt;</em> max( <em>p,q</em> ), but can fail in high-dimensional situations just as before. Again, ridging the individual</p><h6 id="sparse-canonical-correlation-analysis-215"><a class="header-anchor" href="#sparse-canonical-correlation-analysis-215" aria-hidden="true">#</a> SPARSE CANONICAL CORRELATION ANALYSIS 215</h6><p>covariance matrices will resolve the issue, and can be absorbed in the alternat- ing elastic-net regressions. When the dimensions are very high—as in genomic problems—the cross-covariance between <strong>X</strong> and <strong>Y</strong> is of primary interest, and the internal covariance among the columns of <strong>X</strong> and among the columns of <strong>Y</strong> are nuisance parameters which can add to the estimation variance. In this case, it is convenient to standardize the variables, and then assume the internal covariance matrices are the identity. We are thus led to the problem</p><div class="language-"><pre><code>maximize\nβ,θ\n</code></pre></div><div class="language-"><pre><code>Cov(̂ X β, Y θ )\n</code></pre></div><div class="language-"><pre><code>subject to‖ β ‖^2 ≤ 1 , ‖ θ ‖^2 ≤ 1 , ‖ β ‖ 1 ≤ c 1 , ‖ θ ‖ 1 ≤ c 2.\n</code></pre></div><h6 id="_8-26"><a class="header-anchor" href="#_8-26" aria-hidden="true">#</a> (8.26)</h6><p>This objective has the same form as the penalized matrix decomposition (7.6) previously discussed in Chapter 7, but using as input the data matrix <strong>X</strong> <em>T</em> <strong>Y</strong>. We can thus apply Algorithm 7.2 directly, using alternating soft-thresholding to compute the solutions. Higher-order sparse canonical variates are obtained from the higher-order PMD components: as in Algorithm 7.3, after computing a solution, we take residuals and then apply the procedure to what remains.</p><h4 id="_8-3-1-example-netflix-movie-rating-data"><a class="header-anchor" href="#_8-3-1-example-netflix-movie-rating-data" aria-hidden="true">#</a> 8.3.1 Example: Netflix Movie Rating Data</h4><p>Let us illustrate the behavior of sparse CCA by applying it to the Netflix movie-ratings data. As originally described in detail in Section 7.3.1, the full dataset consists of 17 <em>,</em> 770 movies and 480 <em>,</em> 189 customers. Customers have rated some (around 1%) of the movies on a scale from 1 to 5. For this example, we selected the <em>p</em> = <em>N</em> = 500 movies and customers with the most ratings, and imputed the missing values with the movie means. Among the 500 films, we identified those that were action movies (59 in all) and those that were romantic movies (73 in all). The remaining movies were discarded. We then applied the sparse CCA procedure to the data. The idea was to correlate each customer’s ratings on the action movies with their ratings on the romantic movies. We divided the 500 customers into two equal-sized training and test groups at random, and applied sparse CCA to the training set. We constrained the weight vectors to be nonnegative for interpretability. The movies receiving positive weights in the first sparse pair of components are shown in Table 8.3.1. Perhaps a movie buff could tell us why the ratings on these particular movies should correlate; for example, the action movies may be relatively “tame” compared to films likeTerminator. In Figure 8.4, we plot the average ratings for the seven action movies on the test set, plotted against the average rating for the 16 romantic movies for each customer. The correlation is quite high—about 0.7. Hence for a given customer, we can do a reasonable job of predicting his/her average rating for the seven action movies from his/her average rating on the 16 romantic movies, and vice versa.</p><h6 id="_216-sparse-multivariate-methods"><a class="header-anchor" href="#_216-sparse-multivariate-methods" aria-hidden="true">#</a> 216 SPARSE MULTIVARIATE METHODS</h6><div class="language-"><pre><code>Table 8.2 Smal l Netflix dataset: Action and romantic movies with nonzero\nweights in the first sparse canonical covariates.\nAction Movies\nSpeed S.W.A.T. Men in Black II\nThe Fast and the Furious Behind Enemy Lines Charlies Angels\nCon Air\nRomantic Movies\nWhat Women Want Ghost The Family Man\nThe Bodyguard Miss Congeniality Pretty Woman\nSister Act Dirty Dancing Runaway Bride\nJust Married Maid in Manhattan Two Weeks Notice\nLegally Blonde 2: Red 13 Going on 30 Father of the Bride\nLegally Blonde\n</code></pre></div><div class="language-"><pre><code>1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>3\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>5\n</code></pre></div><div class="language-"><pre><code>Action Movies\n</code></pre></div><div class="language-"><pre><code>Romantic Movies\n</code></pre></div><div class="language-"><pre><code>Average Ratings\n</code></pre></div><div class="language-"><pre><code>corr= 0.7\n</code></pre></div><p><strong>Figure 8.4</strong> <em>Sparse canonical correlation analysis applied to a subset of the Netflix movie rating data. The plot shows the average rating for the seven action movies on the test data versus the average for the 16 romantic movies having nonzero weights in the first sparse CCA components.</em></p><h6 id="sparse-linear-discriminant-analysis-217"><a class="header-anchor" href="#sparse-linear-discriminant-analysis-217" aria-hidden="true">#</a> SPARSE LINEAR DISCRIMINANT ANALYSIS 217</h6><h3 id="_8-4-sparse-linear-discriminant-analysis"><a class="header-anchor" href="#_8-4-sparse-linear-discriminant-analysis" aria-hidden="true">#</a> 8.4 Sparse Linear Discriminant Analysis</h3><p>Linear discriminant analysis (LDA) is an important technique for classifica- tion. There is a variety of different proposals for sparse linear discriminant analysis, in part because there are at least three different ways to approach classical discriminant analysis. These are the normal theory model, Fisher’s between-to-within variance criterion, and optimal scoring. In addition, in the high-dimensional regime <em>p</em>  <em>N</em> , some form of regularization is needed for the within-class covariance estimate, and the form of this estimate leads to different methods for sparse LDA.</p><h4 id="_8-4-1-normal-theory-and-bayes’-rule"><a class="header-anchor" href="#_8-4-1-normal-theory-and-bayes’-rule" aria-hidden="true">#</a> 8.4.1 Normal Theory and Bayes’ Rule</h4><p>Consider a response variable <em>G</em> falling into one of <em>K</em> classes{ 1 <em>,</em> 2 <em>,...,K</em> }, and a predictor vector <em>X</em> ∈R <em>p</em>. Suppose that <em>fk</em> ( <em>x</em> ) is the class-conditional density of <em>X</em> in class <em>G</em> = <em>k</em> , and let <em>πk</em> be the prior probability of class <em>k</em> , with</p><h6 id="∑-k-6"><a class="header-anchor" href="#∑-k-6" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1 πk = 1. A simple application of Bayes’ rule gives us\n</code></pre></div><div class="language-"><pre><code>Pr( G = k | X = x ) =\nπkfk ( x )\n∑ K\n` =1 π`f` ( x )\n</code></pre></div><h6 id="_8-27"><a class="header-anchor" href="#_8-27" aria-hidden="true">#</a> . (8.27)</h6><p>Suppose moreover that each class density is modeled as a multivariate Gaus- sian <em>N</em> ( <em>μk,</em> <strong>Σ</strong> <em>w</em> ), with density</p><div class="language-"><pre><code>fk ( x ) =\n</code></pre></div><h6 id="_1-120"><a class="header-anchor" href="#_1-120" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>(2 π ) p/^2 | Σ w |^1 /^2\n</code></pre></div><div class="language-"><pre><code>e −\n</code></pre></div><div class="language-"><pre><code>1\n2 ( x − μk )\n</code></pre></div><p><em>T</em> <strong>Σ</strong> − <em>w</em> (^1) ( <em>x</em> − <em>μk</em> ) <em>,</em> (8.28) based on a common covariance matrix <strong>Σ</strong> <em>w</em>. In comparing two classes <em>k</em> and <em><code>_ , it is sufficient to look at the log-ratio of their posterior probabilities (8.27), and we find that log Pr( _G_ = _k_ | _X_ = _x_ ) Pr( _G_ = _</code></em> | <em>X</em> = <em>x</em> ) = log <em>fk</em> ( <em>x</em> ) <em>f`</em> ( <em>x</em> )</p><ul><li>log <em>πk π<code>_ = log _πk π</code></em></li></ul><h6 id="−-18"><a class="header-anchor" href="#−-18" aria-hidden="true">#</a> −</h6><h6 id="_1-121"><a class="header-anchor" href="#_1-121" aria-hidden="true">#</a> 1</h6><h6 id="_2-83"><a class="header-anchor" href="#_2-83" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>( μk + μ` ) T Σ − w^1 ( μk − μ` )\n</code></pre></div><div class="language-"><pre><code>+ xT Σ − w^1 ( μk − μ` ) ,\n</code></pre></div><h6 id="_8-29"><a class="header-anchor" href="#_8-29" aria-hidden="true">#</a> (8.29)</h6><p>an equation linear in <em>x</em>. Consequently, the decision boundary between classes <em>k</em> and <em><code>_ —i.e., all vectors _x_ for which Pr( _G_ = _k_ | **X** = _x_ ) = Pr( _G_ = _</code></em> | <strong>X</strong> = <em>x</em> )— defines a hyperplane inR <em>p</em>. This statement holds for any pair of classes, so all the decision boundaries are linear. If we divideR <em>p</em> into regions that are classified as class 1, class 2, and so on, these regions will be separated by hyperplanes. Equation (8.29) shows us that these LDA models are also linear logistic regression models; the only difference is the way the parameters are estimated. In logistic regression, we use the conditional binomial/multinomial likelihoods, whereas estimation in LDA is based on the joint likelihood of <em>X</em> and <em>G</em> (Hastie</p><h6 id="_218-sparse-multivariate-methods"><a class="header-anchor" href="#_218-sparse-multivariate-methods" aria-hidden="true">#</a> 218 SPARSE MULTIVARIATE METHODS</h6><p>et al. 2009, Chapter 4). From Equation (8.29), we see that the <em>linear discrim- inant functions</em></p><div class="language-"><pre><code>δk ( x ) = xT Σ − w^1 μk −\n</code></pre></div><h6 id="_1-122"><a class="header-anchor" href="#_1-122" aria-hidden="true">#</a> 1</h6><h6 id="_2-84"><a class="header-anchor" href="#_2-84" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>μTk Σ − w^1 μk + log πk (8.30)\n</code></pre></div><p>provide an equivalent description of the decision rule, leading to the classifi- cation function <em>G</em> ( <em>x</em> ) = arg max <em>k</em> ∈{ 1 <em>,...,K</em> } <em>δk</em> ( <em>x</em> ). In practice, the parameters of the Gaussian class-conditional distributions are not known. However, given <em>N</em> samples{( <em>x</em> 1 <em>,g</em> 1 ) <em>,...,</em> ( <em>xN,gN</em> )}of feature- label pairs, we can estimate the parameters as follows. Let <em>Ck</em> denote the subset of indices <em>i</em> for which <em>gi</em> = <em>k</em> , and let <em>Nk</em> =| <em>Ck</em> |denote the total number of class- <em>k</em> samples. We then form the estimates <em>π</em> ̂ <em>k</em> = <em>Nk/N</em> , and</p><div class="language-"><pre><code>̂ μk =\n</code></pre></div><h6 id="_1-123"><a class="header-anchor" href="#_1-123" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>Nk\n</code></pre></div><h6 id="∑-38"><a class="header-anchor" href="#∑-38" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i ∈ Ck\n</code></pre></div><div class="language-"><pre><code>xi, and (8.31a)\n</code></pre></div><div class="language-"><pre><code>Σ ̂ w =^1\nN − K\n</code></pre></div><h6 id="∑-k-7"><a class="header-anchor" href="#∑-k-7" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1\n</code></pre></div><h6 id="∑-39"><a class="header-anchor" href="#∑-39" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i ∈ Ck\n</code></pre></div><div class="language-"><pre><code>( xi −̂ μk )( xi −̂ μk ) T. (8.31b)\n</code></pre></div><p>Note that <strong>Σ</strong> ̂ <em>w</em> is an unbiased estimate of the pooled within-class covariance. In the high-dimensional setting with <em>p &gt; N</em> , the sample within-class covari- ance matrix <strong>Σ</strong> ̂ <em>w</em> is singular, and so we must regularize it in order to proceed. As before, there are many ways to do so; later in this section, we describe an ap- proach based on quadratic regularization (Hastie, Buja and Tibshirani 1995). In very high dimensions, it is often effective to assume that predictors are uncorrelated, which translates into a diagonal form for <strong>Σ</strong> <em>w</em>. Doing so yields the so-called <em>naive Bayes</em> classifier, or alternatively <em>diagonal linear discrimi- nant analysis</em> (see Exercise 8.20). Lettinĝ <em>σ</em>^2 <em>j</em> = <em>s</em>^2 <em>j</em> be the pooled within-class variance for feature <em>j</em> , the estimated classification rule simplifies to</p><div class="language-"><pre><code>G ̂( x ) = arg min\n` =1 ,...,K\n</code></pre></div><h6 id="-38"><a class="header-anchor" href="#-38" aria-hidden="true">#</a> </h6><h6 id="-32"><a class="header-anchor" href="#-32" aria-hidden="true">#</a> </h6><h6 id="-32"><a class="header-anchor" href="#-32" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>( xj − μ ̂ j` )^2\n̂ σ^2 j\n</code></pre></div><div class="language-"><pre><code>−log ˆ πk\n</code></pre></div><h6 id="-33"><a class="header-anchor" href="#-33" aria-hidden="true">#</a> </h6><h6 id="-32"><a class="header-anchor" href="#-32" aria-hidden="true">#</a> </h6><h6 id="-32"><a class="header-anchor" href="#-32" aria-hidden="true">#</a> </h6><h6 id="_8-32"><a class="header-anchor" href="#_8-32" aria-hidden="true">#</a> , (8.32)</h6><p>known as the <em>nearest centroid rule</em>.</p><h4 id="_8-4-2-nearest-shrunken-centroids"><a class="header-anchor" href="#_8-4-2-nearest-shrunken-centroids" aria-hidden="true">#</a> 8.4.2 Nearest Shrunken Centroids</h4><p>Notice that the classification rule (8.32) will typically involve all features; when <em>p</em> is large, while one might expect that only a subset of these features is informative. This subset can be revealed by reparametrizing the model, and imposing a sparsity penalty. More specifically, suppose that we decompose the mean vector for class <em>k</em> into the sum <em>μk</em> = ̄ <em>x</em> + <em>αk</em> , where ̄ <em>x</em> = <em>N</em>^1</p><h6 id="∑-n-95"><a class="header-anchor" href="#∑-n-95" aria-hidden="true">#</a> ∑ N</h6><p><em>i</em> =1 <em>xi</em> is the overall mean vector, and <em>αk</em> ∈R <em>p, k</em> = 1 <em>,...,K</em> denotes the contrast for class <em>k</em> , together satisfying the constraint</p><h6 id="∑-k-8"><a class="header-anchor" href="#∑-k-8" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1 αk = 0. We then consider\n</code></pre></div><h6 id="sparse-linear-discriminant-analysis-219"><a class="header-anchor" href="#sparse-linear-discriminant-analysis-219" aria-hidden="true">#</a> SPARSE LINEAR DISCRIMINANT ANALYSIS 219</h6><p>optimizing the <em>`</em> 1 -regularized criterion</p><div class="language-"><pre><code>minimize\nαk ∈R p,k =1 ,...,K\n</code></pre></div><h6 id="-39"><a class="header-anchor" href="#-39" aria-hidden="true">#</a> </h6><h6 id="-33"><a class="header-anchor" href="#-33" aria-hidden="true">#</a> </h6><h6 id="-33"><a class="header-anchor" href="#-33" aria-hidden="true">#</a> </h6><h6 id="_1-124"><a class="header-anchor" href="#_1-124" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-16"><a class="header-anchor" href="#_2-n-16" aria-hidden="true">#</a> 2 N</h6><h6 id="∑-k-9"><a class="header-anchor" href="#∑-k-9" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1\n</code></pre></div><h6 id="∑-40"><a class="header-anchor" href="#∑-40" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i ∈ Ck\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>( xij − x ̄ j − αjk )^2\ns^2 j\n</code></pre></div><div class="language-"><pre><code>+ λ\n</code></pre></div><h6 id="∑-k-10"><a class="header-anchor" href="#∑-k-10" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><h6 id="√-22"><a class="header-anchor" href="#√-22" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>Nk\nsj\n</code></pre></div><div class="language-"><pre><code>| αjk |\n</code></pre></div><h6 id="-34"><a class="header-anchor" href="#-34" aria-hidden="true">#</a> </h6><h6 id="-33"><a class="header-anchor" href="#-33" aria-hidden="true">#</a> </h6><h6 id="-33"><a class="header-anchor" href="#-33" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>subject to\n</code></pre></div><h6 id="∑-k-11"><a class="header-anchor" href="#∑-k-11" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1 αjk = 0 for j = 1 ,...,p.\n(8.33)\n</code></pre></div><p>The solutions for <em>αjk</em> amount to simple soft-thresholding of particular class-wise contrasts. In detail, we define the contrasts</p><div class="language-"><pre><code>djk =\nx ̃ jk − ̄ xj\nmksj\n</code></pre></div><h6 id="_8-34"><a class="header-anchor" href="#_8-34" aria-hidden="true">#</a> , (8.34)</h6><p>where <em>x</em> ̃ <em>jk</em> = <em>N</em>^1 <em>k</em></p><h6 id="∑-41"><a class="header-anchor" href="#∑-41" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i ∈ Ck\n</code></pre></div><div class="language-"><pre><code>xij , the quantity ̄ xj denotes the jth component of the\n</code></pre></div><p>global mean ̄ <em>x</em> , and^5 <em>m</em>^2 <em>k</em> = <em>N</em>^1 <em>k</em> − <em>N</em>^1. We then apply the soft-thresholding op- erator</p><div class="language-"><pre><code>d ′ jk =S λ ( djk ) = sign( djk )(| djk |− λ )+ , (8.35a)\n</code></pre></div><p>and reverse the transformation to obtain the shrunken centroid estimates</p><div class="language-"><pre><code>̂ μ ′ jk = ̄ xj + mksjd ′ kj. (8.35b)\n</code></pre></div><p>Finally, we use these shrunken centroids for the estimates for <em>μjk</em> in the nearest centroid rule (8.32). Suppose for a given feature <em>j</em> , the contrasts <em>d</em> ′ <em>jk</em> are set to zero by the soft- thresholding for each of the <em>k</em> classes. Then that feature does not participate in the nearest centroid rule (8.32), and is ignored. In this way, the nearest shrunken centroid procedure does automatic feature selection. Alternatively, a feature might have <em>d</em> ′ <em>jk</em> = 0 for some classes but not others, and hence would only play a role for those classes. The nearest shrunken centroid classifier is very useful for high-dimensional classification problems, like those that occur in genomic and proteomic data. The publicly available software (Hastie, Tibshirani, Narasimhan and Chu 2003) includes some additional bells and whistles: a small constant <em>s</em> 0 is added to each <em>sj</em> , to stabilize the contrasts when <em>sj</em> is close to zero; class- specific shrinkage rates, to name a few. Figure 8.5 shows the results of this procedure applied to some Lymphoma cancer data (Tibshirani, Hastie, Narasimhan and Chu 2003). These data con- sist of expression measurements on 4026 genes from samples of 59 lymphoma patients. The samples are classified into diffuse large B-cell lymphoma (DL- BCL), follicular lymphoma (FL), and chronic lymphocytic lymphoma (CLL).</p><p>(^5) The quantity <em>mk</em> is a standardization constant, based on the variance of the numerator, which makes <em>djk</em> a t-statistic.</p><h6 id="_220-sparse-multivariate-methods"><a class="header-anchor" href="#_220-sparse-multivariate-methods" aria-hidden="true">#</a> 220 SPARSE MULTIVARIATE METHODS</h6><div class="language-"><pre><code>−1.5 −0.5 0.5 1.0 1.5\n</code></pre></div><div class="language-"><pre><code>DLCL (27)\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>1000\n</code></pre></div><div class="language-"><pre><code>2000\n</code></pre></div><div class="language-"><pre><code>3000\n</code></pre></div><div class="language-"><pre><code>4000\n</code></pre></div><div class="language-"><pre><code>−1.5 −0.5 0.5 1.0 1.5\n</code></pre></div><div class="language-"><pre><code>FL (5)\n</code></pre></div><div class="language-"><pre><code>−1.5 −0.5 0.5 1.0 1.5\n</code></pre></div><div class="language-"><pre><code>CLL (7)\n</code></pre></div><div class="language-"><pre><code>Gene\n</code></pre></div><div class="language-"><pre><code>Centroids: Average Expression Centered at Overall Centroid\n</code></pre></div><div class="language-"><pre><code>0 1 2 3 4 5\nAmount of Shrinkage λ\n</code></pre></div><div class="language-"><pre><code>Error\n</code></pre></div><div class="language-"><pre><code>4026 3146 1774 867 557 354 212 128 78 48 21 13 3\n</code></pre></div><div class="language-"><pre><code>Size\n</code></pre></div><div class="language-"><pre><code>0.0 0.2 0.4 0.6 0.8\n</code></pre></div><div class="language-"><pre><code>Train\nCV\nTest\n</code></pre></div><p><strong>Figure 8.5</strong> <em>Results of nearest-shrunken-centroid classification on some Lymphoma data, with three classes. The top plot shows the class-specific mean expression for each gene (gray lines), and their shrunken versions (blue). Most of the genes are shrunk to the overal l mean (0 here). The lower plot shows training, cross-validated, and test misclassification error as a function of the shrinkage thresholdλ. The chosen model includes 79 genes, and makes 0 test errors.</em></p><h6 id="sparse-linear-discriminant-analysis-221"><a class="header-anchor" href="#sparse-linear-discriminant-analysis-221" aria-hidden="true">#</a> SPARSE LINEAR DISCRIMINANT ANALYSIS 221</h6><p>The data are divided into a training set of 39 (27, 5, 7) samples, and a test set of 20. The genes have been organized by hierarchical clustering. All but 79 of the genes have been shrunk to zero. Notice that the deviations of the smaller classes are larger, since the biggest class DLBCL mostly determines the overall mean. In Section 8.4.3.1, we compare the nearest shrunken centroid classifier to a sparse version of Fisher’s linear discriminant analysis, discussed next.</p><h4 id="_8-4-3-fisher’s-linear-discriminant-analysis"><a class="header-anchor" href="#_8-4-3-fisher’s-linear-discriminant-analysis" aria-hidden="true">#</a> 8.4.3 Fisher’s Linear Discriminant Analysis</h4><p>A different approach to sparse discriminant analysis arises from Fisher’s dis- criminant framework. Here the idea is to produce low-dimensional projections of the data that preserve the class separation. Although these projections are primarily intended for visualization, one can also perform Gaussian classifica- tion in the subspace produced. Let <strong>X</strong> be an <em>N</em> × <em>p</em> matrix of observations, and assume that its columns, corresponding to features, have been standardized to have mean zero. Given such an observation matrix, we seek a low-dimensional projection such that the <em>between-class</em> variance is large relative to the <em>within-class</em> variance. As before, let <strong>Σ</strong> ̂ <em>w</em> be the pooled within-class covariance matrix and̂ <em>μk</em> the class- specific centroids. The between-class covariance matrix <strong>Σ</strong> ̂ <em>b</em> is the covariance matrix of these centroids, given by</p><div class="language-"><pre><code>Σ ̂ b =\n</code></pre></div><h6 id="∑-k-12"><a class="header-anchor" href="#∑-k-12" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1\n</code></pre></div><div class="language-"><pre><code>̂ πk ̂ μkμ ̂ Tk, (8.36)\n</code></pre></div><p>treating them as multivariate observations with mass ˆ <em>πk</em>. Note that</p><div class="language-"><pre><code>Σ ̂ t =^1\nN\nX T X = Σ ̂ b + Σ ̂ w. (8.37)\n</code></pre></div><p>For now we assume that <strong>Σ</strong> ̂ <em>w</em> is of full rank (which implies that <em>p</em> ≤ <em>N</em> ); we treat the non-full rank case below. For a linear combination <strong>z</strong> = <strong>X</strong> <em>β</em> , Fisher’s between-to-within variance criterion is captured by the ratio</p><div class="language-"><pre><code>R ( β ) =\n</code></pre></div><div class="language-"><pre><code>βT Σ ̂ bβ\nβT Σ ̂ wβ\n</code></pre></div><h6 id="_8-38"><a class="header-anchor" href="#_8-38" aria-hidden="true">#</a> , (8.38)</h6><p>which is to be maximized. Fisher’s LDA proceeds by sequentially solving the following problem:</p><div class="language-"><pre><code>maximize\nβ ∈R p\n</code></pre></div><h6 id="-325"><a class="header-anchor" href="#-325" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>βT Σ ̂ bβ\n</code></pre></div><h6 id="-326"><a class="header-anchor" href="#-326" aria-hidden="true">#</a> }</h6><p>such that <em>βT</em> <strong>Σ</strong> ̂ <em>wβ</em> ≤1, and <em>βT</em> <strong>Σ</strong> ̂ <em>wβ</em> ̂ <em><code>_ = 0 for all _</code> &lt; k</em>. (8.39) for <em>k</em> = 1 <em>,</em> 2 <em>,...,</em> min( <em>K</em> − 1 <em>,p</em> ). Although the problem (8.39) is generally writ- ten with the inequality constraint replaced with an equality constraint, the two programs are equivalent if <strong>Σ</strong> ̂ <em>w</em> has full rank. The solution <em>β</em> ̂ <em>k</em> is called</p><h6 id="_222-sparse-multivariate-methods"><a class="header-anchor" href="#_222-sparse-multivariate-methods" aria-hidden="true">#</a> 222 SPARSE MULTIVARIATE METHODS</h6><p>the <em>kthdiscriminant vector</em> , and <strong>z</strong> <em>k</em> = <strong>X</strong> <em>β</em> ̂ <em>k</em> the corresponding discriminant variable. Note that LDA essentially does principal components on the class centroids, but using a normalization metric that respects the within-class vari- ances (Hastie et al. 2009, Chapter 4). In practice, we do not need to solve the problem sequentially, because as with PCA we can get all the solutions with a single eigen-decomposition: the first <em>k</em> discriminant vectors are the <em>k</em> leading</p><p>eigenvectors of <strong>Σ</strong> ̂</p><p>− 1 <em>w</em> <strong>Σ</strong> ̂ <em>b</em>. Witten and Tibshirani (2011) proposed a way to “sparsify” the objec- tive (8.39), in particular by solving</p><div class="language-"><pre><code>maximize\nβ\n</code></pre></div><h6 id="-40"><a class="header-anchor" href="#-40" aria-hidden="true">#</a> </h6><h6 id="-34"><a class="header-anchor" href="#-34" aria-hidden="true">#</a> </h6><h6 id="-34"><a class="header-anchor" href="#-34" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>βT Σ ̂ bβ − λ\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>σ ˆ j | βj |\n</code></pre></div><h6 id="-35"><a class="header-anchor" href="#-35" aria-hidden="true">#</a> </h6><h6 id="-34"><a class="header-anchor" href="#-34" aria-hidden="true">#</a> </h6><h6 id="-34"><a class="header-anchor" href="#-34" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>subject to βT Σ ̃ wβ ≤ 1 , (8.40)\n</code></pre></div><p>where ˆ <em>σ</em>^2 <em>j</em> is the <em>jth</em> diagonal element of <strong>Σ</strong> ̂ <em>w</em> , and <strong>Σ</strong> ̃ <em>w</em> is a positive definite es-</p><p>timate for <strong>Σ</strong> <em>w</em>. This produces a first sparse discriminant vector <em>β</em> ̂ 1 with level of sparsity determined by the choice of <em>λ</em>. Further components can be suc- cessively found by first removing the current solution from <strong>Σ</strong> ̂ <em>b</em> before solving problem (8.40); see the reference for details. The choice for the regularized within-covariance matrix <strong>Σ</strong> ̃ <em>w</em> depends on the setting. In some problems, we might choose <strong>Σ</strong> ̃ <em>w</em> to encourage spatial smooth- ness, for example when the data are images. Then we can take <strong>Σ</strong> ̃ <em>w</em> = <strong>Σ</strong> ̂ <em>w</em> + <strong>Ω</strong> where <strong>Ω</strong> penalizes differences in spatially nearby values. This idea is stud- ied in the <em>flexible and penalized discriminant analysis</em> approach of Hastie, Tibshirani and Buja (1994) and Hastie et al. (1995). In the sparse setting, this is conveniently implemented using the optimal-scoring approach of Sec- tion 8.4.4. In other cases we only require that <strong>Σ</strong> ̃ <em>w</em> makes the sample esti- mate <strong>Σ</strong> ̂ <em>w</em> positive definite, and for that purpose we can use a ridged version <strong>Σ</strong> ̃ <em>w</em> = <strong>Σ</strong> ̂ <em>w</em> + <em></em> diag( <strong>Σ</strong> ̂ <em>w</em> ) for some <em> &gt;</em> 0. One simple case of particular interest is where <strong>Σ</strong> ̃ <em>w</em> is taken to be a di- agonal matrix, for example diag( <strong>Σ</strong> ̂w). Then problem (8.40) can be cast as a penalized matrix decomposition applied to the between covariance matrix <strong>Σ</strong> ̂ <em>b</em> , and Algorithm 7.2 can be applied. In this case, with <em>K</em> = 2 classes, this method gives a solution that is similar to nearest shrunken centroids: details are in Witten and Tibshirani (2011, Section 7.2). With more than two classes, the two approaches are different. Nearest shrunken centroids produce sparse contrasts between each class and the overall mean, while the sparse LDA approach (8.40) produces sparse discriminant vectors for more general class contrasts. This distinction is explored in the next example.</p><h5 id="_8-4-3-1-example-simulated-data-with-five-classes"><a class="header-anchor" href="#_8-4-3-1-example-simulated-data-with-five-classes" aria-hidden="true">#</a> 8.4.3.1 Example: Simulated Data with Five Classes</h5><p>We created two artificial scenarios to contrast the nearest shrunken centroids approach with sparse discriminant analysis (8.40). Figure 8.6 shows the results of nearest shrunken centroids classifier applied to the two different simulated</p><h6 id="sparse-linear-discriminant-analysis-223"><a class="header-anchor" href="#sparse-linear-discriminant-analysis-223" aria-hidden="true">#</a> SPARSE LINEAR DISCRIMINANT ANALYSIS 223</h6><div class="language-"><pre><code>1 2 3 4 5\n</code></pre></div><div class="language-"><pre><code>12\n</code></pre></div><div class="language-"><pre><code>34\n</code></pre></div><div class="language-"><pre><code>56\n</code></pre></div><div class="language-"><pre><code>78\n</code></pre></div><p>(^109) 1112 1314 1516 1718 1920 2122 2324 2526 2728 2930 Scenario 1 1 2 3 4 5 12 34 56 79 1011 1213 1415 1617 1819 2053 172233 271512 542671 700750 835925 954 Scenario 2 <strong>Figure 8.6</strong> <em>Results of nearest shrunken centroid classifier applied to two different simulated datasets, as described in the text. Those features (rows in each plot) with nonzero estimated contrasts are shown. The length of each horizontal line segment is proportional to the size of the contrast, with positive values to the right and negative values to the left.</em> datasets. In both cases there are <em>N</em> = 100 observations, with 20 observations falling into each of <em>K</em> = 5 classes involving <em>p</em> = 1000 features.</p><ol><li>In the first scenario, the first 10 features are two units higher in class 1, features 11–20 are two units higher in class 3, and features 21–30 are two units lower in class 5. Thus, higher values in each of the first three block of 10 features characterize classes 1, 3, and 5.</li><li>In the second scenario, features 1–10 are one unit higher in classes 3–5 versus 1,2 and features 11–20 are one unit higher in class 2 and one unit lower in class 1. Hence higher values for the first 10 features discriminate classes 3–5 versus 1 and 2, while higher values for the second 10 features discriminate between classes 1 versus 2. We applied the nearest shrunken centroid classifier, using cross-validation to choose the shrinkage parameter and show the features with nonzero esti- mated contrasts. The length of each horizontal line segment is proportional to the size of the contrast, with positive values to the right and negative values to the left. In the left panel, we see that nearest shrunken centroids has clearly revealed the structure of the data, while in the right panel, the structure does not come through as clearly. Figure 8.7 shows the results of rank-2 sparse linear discriminant analysis with diagonal within-class covariance in each of the two scenarios. In the first scenario (top row), the discriminant projection cleanly separates classes 1, 3, and 5 from the rest, but the discriminant load- ings (top right) are forced to combine three pieces of information into two vectors, and hence give a cloudy picture. Of course one could use more than</li></ol><h6 id="_224-sparse-multivariate-methods"><a class="header-anchor" href="#_224-sparse-multivariate-methods" aria-hidden="true">#</a> 224 SPARSE MULTIVARIATE METHODS</h6><div class="language-"><pre><code>−6 −4 −2 0 2 4 6 8\n</code></pre></div><div class="language-"><pre><code>−6\n</code></pre></div><div class="language-"><pre><code>−4\n</code></pre></div><div class="language-"><pre><code>−2\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>6\n</code></pre></div><div class="language-"><pre><code>8\n</code></pre></div><div class="language-"><pre><code>First Discriminant Score\n</code></pre></div><div class="language-"><pre><code>Second Discriminant Score\n</code></pre></div><div class="language-"><pre><code>2\n55\n</code></pre></div><div class="language-"><pre><code>3\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><p>(^42) 5 3 1 5 4 5 3 24 3 2 3 4 4 2 3 5 3 1 4 5 1 4 1 (^25) 4 2 5 4 1 3 1 4 3 2 1 2 2 5 1 2 1 1 3 1 5 1 3 2 2 5 5 3 2 5 3 4 3 4 4 5 5 3 3 2 5 3 4 3 11 2 1 2 5 3 4 2 5 1 4 3 1 4 4 5 (^44) 1 1 2 1 0 5 10 15 20 25 30 −0.3 −0.1 0.1 0.2 0.3 Predictor Discriminant Loading ● ● ● ●●●●●●● ● ● ● ● ●● ●●●● ● ●● ●●● ● ● ●● ● ●● ● ●●● ● ● ● ● ● ● ●● ● ● ● ●●● ● First ComponentSecond Component Scenario 1 −4 −2 0 2 4 −4 −2 0 2 First Discriminant Score Second Discriminant Score 1 1 2 2 2 4 2 34 3 5 2 3 1 5 1 4 5 3 4 3 1 2 5 4 1 3 2 2 4 2 4 5 3 (^22) 5 4 1 4 3 44 3 (^55) 2 3 4 2 (^333) 5 (^544) 5 1 3 5 5 5 2 3 5 3 11 5 1 1 4 4 2 5 2 11 2 5 3 (^21) 4 1 3 5 4 5 3 1 4 2 1 4 1 1 3 2 0 5 10 15 20 25 30 −0.5 −0.4 −0.3 −0.2 −0.1 0.0 Predictor Discriminant Loading ●●● ● ● ● ● ● ● ● ● ●● ●● ● ● ● ● Scenario 2 <strong>Figure 8.7</strong> <em>Rank two sparse linear discriminant analysis with diagonal within-class covariance, applied to the same two scenarios (top and bottom panels) as in Fig- ure 8.6. The projections onto the first two sparse discriminant vectors is shown in the left panels, while the right panels show the discriminant weights or loadings.</em></p><h6 id="sparse-linear-discriminant-analysis-225"><a class="header-anchor" href="#sparse-linear-discriminant-analysis-225" aria-hidden="true">#</a> SPARSE LINEAR DISCRIMINANT ANALYSIS 225</h6><p><em>K</em> = 2 sparse components and this would help in this example, but this ap- proach is less attractive if high-order discriminants are required. The second scenario is well suited to sparse LDA, as it cleanly separates classes 1 and 2 from the rest (bottom left), and reveals the features responsible for this separation (bottom right).</p><h4 id="_8-4-4-optimal-scoring"><a class="header-anchor" href="#_8-4-4-optimal-scoring" aria-hidden="true">#</a> 8.4.4 Optimal Scoring</h4><p>A third approach to the derivation of linear discriminant analysis is called <em>optimal scoring</em>. It is based on a recasting of the problem in terms of a mul- tivariate linear regression, where the codes for the output classes are chosen “optimally,” as we detail next. Suppose that the membership of the samples are coded using a binary-valued <em>N</em> × <em>K</em> indicator matrix <strong>Y</strong> , with entries</p><div class="language-"><pre><code>yik =\n</code></pre></div><h6 id="-327"><a class="header-anchor" href="#-327" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>1 if observation i belongs to class k\n0 otherwise.\n</code></pre></div><p>Using this notation, optimal scoring involves solving the sequence of problems for <em>k</em> = 1 <em>,...,K</em> , each of the form</p><p>( <em>β</em> ̂ <em>k,</em> ̂ <em>θk</em> ) = arg min <em>βk</em> ∈R <em>p,θk</em> ∈R <em>K</em></p><h6 id="-328"><a class="header-anchor" href="#-328" aria-hidden="true">#</a> {</h6><h6 id="_1-125"><a class="header-anchor" href="#_1-125" aria-hidden="true">#</a> 1</h6><h6 id="n-31"><a class="header-anchor" href="#n-31" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>‖ Y θk − X βk ‖^22\n</code></pre></div><h6 id="-329"><a class="header-anchor" href="#-329" aria-hidden="true">#</a> }</h6><h6 id="_8-41"><a class="header-anchor" href="#_8-41" aria-hidden="true">#</a> (8.41)</h6><div class="language-"><pre><code>such that θTk Y T Y θk = 1 and θkT Y T Y θj = 0 for all j = 1 , 2 ,...,k −1.\n</code></pre></div><p>The optimal solution <em>β</em> ̂ <em>k</em> of this problem turns out to be proportional to the solution of the Fisher linear discriminant criterion (8.39) (Breiman and Ihaka 1984, Hastie et al. 1995). This equivalence is not too surprising. With just <em>K</em> = 2 classes, it is well known that the linear regression of a binary response ̃ <em>y</em> = <strong>Y</strong> <em>θ</em> (with arbitrary coding <em>θ</em> ) on <strong>X</strong> gives the same coefficient vector as linear discriminant analysis (up to a proportionality factor). For example, see Exercise 4.2 of Hastie et al. (2009). With more than two classes, a regression of <strong>y</strong> ̃ <em><code>_ on **X** will differ according to how we assign numerical scores _θ</code>k</em> to the classes. We obtain the particular solution from linear regression that is equivalent to linear discriminant analysis by optimizing over the choice of scores, as in the problem (8.41). As with the other methods for sparse discriminant analysis, adding an <em>`</em> 1 -penalty to the criterion (8.41) yields the modified optimization problem</p><div class="language-"><pre><code>minimize\nβk ∈R p,θk ∈R K\n</code></pre></div><h6 id="-330"><a class="header-anchor" href="#-330" aria-hidden="true">#</a> {</h6><h6 id="_1-126"><a class="header-anchor" href="#_1-126" aria-hidden="true">#</a> 1</h6><h6 id="n-32"><a class="header-anchor" href="#n-32" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>‖ Y θk − X βk ‖^22 + βTk Ω βk + λ ‖ βk ‖ 1\n</code></pre></div><h6 id="-331"><a class="header-anchor" href="#-331" aria-hidden="true">#</a> }</h6><h6 id="_8-42"><a class="header-anchor" href="#_8-42" aria-hidden="true">#</a> (8.42)</h6><div class="language-"><pre><code>such that θkT Y T Y θk = 1 and θTk Y T Y θj = 0 for all j = 1 , 2 ,...,k − 1\n</code></pre></div><p>(Leng 2008, Clemmensen, Hastie, Witten and Ersboll 2011). In addition to the <em>`</em> 1 -penalty with nonnegative regularization weight <em>λ</em> , we have also added a quadratic penalty defined by a positive semidefinite matrix <strong>Ω</strong> , equivalent</p><h6 id="_226-sparse-multivariate-methods"><a class="header-anchor" href="#_226-sparse-multivariate-methods" aria-hidden="true">#</a> 226 SPARSE MULTIVARIATE METHODS</h6><p>to the elastic net penalty in the special case <strong>Ω</strong> = <em>γ</em> <strong>I</strong>. The resulting dis- criminant vectors will be sparse if the regularization weight <em>λ</em> on the <em>`</em> 1 - penalty is sufficiently large. At the other extreme, if <em>λ</em> = 0, then minimizing the criterion (8.42) is equivalent to the <em>penalized discriminant analysis</em> pro- posal of Hastie et al. (1995). Although the criterion is nonconvex (due to the quadratic constraints), a local optimum can be obtained via alternating min- imization, using the elastic net to solve for <em>β</em>. In fact, if any convex penalties are applied to the discriminant vectors in the optimal scoring criterion (8.41), then it is easy to apply alternating minimization to solve the resulting prob- lem. Moreover, there is a close connection between this approach and sparse Fisher LDA (8.40). In particular, they are essentially equivalent if we take <strong>Σ</strong> ̃= <strong>Σ</strong> ̂ <em>w</em> + <strong>Ω</strong>. The qualification “essentially” is needed due to nonconvexity: we can only say that a stationary point for the one problem is also a stationary point for the other (see Exercise 8.22 for details). Whether one approaches the sparse discriminant problem through Fisher LDA (8.40) or optimal scoring (8.42) depends on the nature of the problem. When <em>p</em>  <em>N</em> and the features are not structured—a category containing many genomic problems—it is most attractive to set <strong>Σ</strong> ̃ <em>w</em> equal to diag( <strong>Σ</strong> ˆ <em>w</em> ). Since this matrix is positive definite, we can take <strong>Ω</strong> = <strong>0</strong> , and the resulting problem is easily solved via the soft-thresholding algorithm for penalized ma- trix decomposition. When the problem has a spatial or temporal structure, the matrix <strong>Ω</strong> can be chosen to encourage spatial or temporal smoothness of the solution. In this case the optimal scoring approach is attractive, since the quadratic term can be absorbed into the quadratic loss. Otherwise, the matrix <strong>Ω</strong> can be chosen to be a diagonal matrix, as in the next example, and again optimal scoring is convenient. Both methods are implemented in packages inR:penalizedLDA(Witten 2011) for the criterion (8.40), andsparseLDA (Clemmensen 2012) for the criterion (8.42).</p><h5 id="_8-4-4-1-example-face-silhouettes"><a class="header-anchor" href="#_8-4-4-1-example-face-silhouettes" aria-hidden="true">#</a> 8.4.4.1 Example: Face Silhouettes</h5><p>We illustrate sparse discriminant analysis based on the objective func- tion (8.42) with a morphometric example taken from Clemmensen et al. (2011). The dataset consisting of 20 adult male and 19 adult female face- silhouettes. Following the work of Thodberg and Olafsdottir (2003), we apply a minimum description length (MDL) approach to annotate the silhouettes, and then perform Procrustes’ alignment on the resulting 65 MDL landmarks of ( <em>x,y</em> )-coordinates. These 65 pairs are vectorized resulting in <em>p</em> = 130 spatial features. We set <strong>Ω</strong> = <strong>I</strong> in the criterion (8.42); in this case the spatial features are already smooth, and the ridge penalty <strong>I</strong> is sufficient to deal with the strong spatial autocorrelations. For training, we used 22 of the silhouettes (11 female and 11 male), which left 17 silhouettes for testing (8 female and 9 male). The left and middle panels of Figure 8.8 illustrate the two classes of silhouettes. Leave-one-out cross validation was performed on the training data, esti- mating an optimal value of <em>λ</em> that yielded 10 nonzero features. Since there are</p><h6 id="sparse-clustering-227"><a class="header-anchor" href="#sparse-clustering-227" aria-hidden="true">#</a> SPARSE CLUSTERING 227</h6><p><strong>Figure 8.8</strong> <em>The silhouettes and the 65</em> ( <em>x,y</em> ) <em>-coordinates for females (left) and males (middle). Right: The mean shape of the silhouettes, and the 10</em> ( <em>x,y</em> ) <em>-coordinates in the SDA model. The superimposed dots indicate the landmarks retained in the sparse discriminant vector. The arrows il lustrate the directions of the differences between male and female classes.</em></p><p>two classes, there is only one sparse direction. The nonzero weights are shown in the right panel of Figure 8.8. The few landmarks included in the model are placed near high curvature points in the silhouettes, suggesting that the important gender differences are located in these regions. The training and test classification rates were both 82%.</p><h3 id="_8-5-sparse-clustering"><a class="header-anchor" href="#_8-5-sparse-clustering" aria-hidden="true">#</a> 8.5 Sparse Clustering</h3><p>In this section, we discuss methods for clustering observations that employ sparsity to filter out the uninformative features. We first give a brief back- ground on clustering; more details can be found, for example, in Hastie et al. (2009, Chapter 14).</p><h4 id="_8-5-1-some-background-on-clustering"><a class="header-anchor" href="#_8-5-1-some-background-on-clustering" aria-hidden="true">#</a> 8.5.1 Some Background on Clustering</h4><p>Suppose we wish to group or cluster a collection of <em>N</em> observations on <em>p</em> fea- tures, where <em>p</em>  <em>N</em>. Our goal is to find groups of observations that are similar with respect to the <em>p</em> features. A standard method for doing this is called “hi- erarchical clustering.” More precisely, we are referring to agglomerative (or bottom-up) hierarchical clustering. This method starts with the individual observations, and then merges or agglomerates the pair that are closest ac- cording to some metric, with the Euclidean distance over the <em>p</em> features being one common choice. This process is continued, with the closest pair grouped together at each stage. Along the way, we consider merging not only individ- ual pairs of observations, but also merging clusters of observations that were created at previous steps, with individual observations or other clusters. For this, we need to define a linkage measure—the distance between two clusters. Some common choices include <em>average linkage</em> , which define the distance be- tween two clusters as the average distance between any two observations, one in each cluster; <em>complete linkage</em> , which uses the maximum pairwise distance; and <em>single linkage</em> , which uses the minimum pairwise distance.</p><h6 id="_228-sparse-multivariate-methods"><a class="header-anchor" href="#_228-sparse-multivariate-methods" aria-hidden="true">#</a> 228 SPARSE MULTIVARIATE METHODS</h6><h5 id="_8-5-1-1-example-simulated-data-with-six-classes"><a class="header-anchor" href="#_8-5-1-1-example-simulated-data-with-six-classes" aria-hidden="true">#</a> 8.5.1.1 Example: Simulated Data with Six Classes</h5><p>The top panel of Figure 8.9 shows an example of hierarchical clustering ap- plied to some artificial data with 120 observations and 2000 features. The figure shows the result of hierarchical clustering using Euclidean distance and complete linkage. The clustering tree or <em>dendrogram</em> summarizes the sequence of merges, leading to a single cluster at the top. The colors of the leaves of the tree were not used in the clustering, and are explained below. Now suppose that the observations vary only over a subset of the features. Then we would like to isolate that subset both for interpretation and to im- prove the clustering. In the top panel of Figure 8.9, the data were actually generated so that the average levels of the first 200 features varied over six predefined classes, with the remaining 1800 features being standard Gaussian noise. These classes are not used in the clustering, but after carrying out the clustering, we have colored the leaves of the dendrogram according to the true class. We see that hierarchical clustering is confused by the uninformative features, and does a poor job of clustering the observations into classes. In this instance we would like to isolate the informative subset of features both for interpretability and to improve the clustering. One way of doing that is described next.</p><h4 id="_8-5-2-sparse-hierarchical-clustering"><a class="header-anchor" href="#_8-5-2-sparse-hierarchical-clustering" aria-hidden="true">#</a> 8.5.2 Sparse Hierarchical Clustering</h4><p>We now describe an approach that introduces sparsity and feature selection to this problem. Given a data matrix <strong>X</strong> ∈R <em>N</em> × <em>p</em> , standard clustering based on Euclidean distance uses the dissimilarity measure <em>Di,i</em> ′=</p><p>∑ <em>p j</em> =1 <em>di,i</em> ′ <em>,j</em> with <em>di,i</em> ′ <em>,j</em> = ( <em>xij</em> − <em>xi</em> ′ <em>j</em> )^2. The idea here is to find a set of feature weights <em>wj</em> ≥ 0 and use these to define a weighted dissimilarity measure <em>D</em> ̃ <em>i,i</em> ′=</p><p>∑ <em>p j</em> =1 <em>wjdi,i</em> ′ <em>,j</em> ; we want each weight to reflect the importance of that feature. Finally, this modified dissimilarity matrix is used as input into hierarchical clustering. Denote by <strong>∆</strong> the <em>N</em>^2 × <em>p</em> matrix with column <em>j</em> containing the <em>N</em>^2 pair- wise dissimilarities for feature <em>j</em>. Then <strong>∆</strong> 1 is the vectorized version of <em>Di,i</em> ′, and likewise <strong>∆</strong> <em>w</em> the vectorized version of <em>D</em> ̃ <em>i,i</em> ′. We now seek the vector <em>w</em> , subject to sparsity and normalization restrictions, that recovers most of the variability in <strong>∆</strong>. This requirement leads to the penalized matrix decomposi- tion problem (Witten et al. 2009) (see Section 7.6):</p><div class="language-"><pre><code>maximize\nu ∈R N^2 , w ∈R p\n</code></pre></div><h6 id="-332"><a class="header-anchor" href="#-332" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>u T ∆ w\n</code></pre></div><h6 id="-333"><a class="header-anchor" href="#-333" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>subject to‖ u ‖ 2 ≤1,‖ w ‖ 2 ≤1,\n</code></pre></div><div class="language-"><pre><code>‖ w ‖ 1 ≤ s , and w 0.\n</code></pre></div><h6 id="_8-43"><a class="header-anchor" href="#_8-43" aria-hidden="true">#</a> (8.43)</h6><p>Notice that <em>wj</em> is a weight on the dissimilarity matrix for feature <em>j</em>. Given the optimal solution <em>w</em> ̂, we rearrange the elements of <strong>∆</strong> <em>w</em> into a <em>N</em> × <em>N</em> matrix, and perform hierarchical clustering on this reweighted dissimilarity matrix. The result is a sparse hierarchical clustering of the data. In Figure 8.9, we see that sparse clustering isolates the informative features (bottom panel) and</p><h6 id="sparse-clustering-229"><a class="header-anchor" href="#sparse-clustering-229" aria-hidden="true">#</a> SPARSE CLUSTERING 229</h6><div class="language-"><pre><code>58\n</code></pre></div><div class="language-"><pre><code>62\n</code></pre></div><div class="language-"><pre><code>66\n</code></pre></div><div class="language-"><pre><code>70\n</code></pre></div><div class="language-"><pre><code>Standard clustering\n</code></pre></div><div class="language-"><pre><code>0.000\n</code></pre></div><div class="language-"><pre><code>0.010\n</code></pre></div><div class="language-"><pre><code>0.020\n</code></pre></div><div class="language-"><pre><code>Sparse clustering\n</code></pre></div><div class="language-"><pre><code>0 500 1000 1500 2000\n</code></pre></div><div class="language-"><pre><code>0.00\n</code></pre></div><div class="language-"><pre><code>0.04\n</code></pre></div><div class="language-"><pre><code>0.08\n</code></pre></div><div class="language-"><pre><code>0.12\n</code></pre></div><div class="language-"><pre><code>Feature\n</code></pre></div><div class="language-"><pre><code>Feature weight\n</code></pre></div><div class="language-"><pre><code>Estimated feature weights\n</code></pre></div><p><strong>Figure 8.9</strong> <em>Standard and sparse clustering applied to a simulated example. The data were generated so that the average levels of the first 200 features varied over six predefined classes, while the remaining 1800 features were noise features with the same distribution over the classes. The top two panels show the result of standard hierarchical clustering and sparse clustering, respectively. We used complete linkage in each case. The class of each sample is indicated by the color of each leaf, and was not used by the clustering procedures. The bottom panel shows the estimated weight given to each feature by the sparse clustering procedure.</em></p><h6 id="_230-sparse-multivariate-methods"><a class="header-anchor" href="#_230-sparse-multivariate-methods" aria-hidden="true">#</a> 230 SPARSE MULTIVARIATE METHODS</h6><p>uses this information to correctly cluster the observations into the predefined groups (middle panel).</p><h4 id="_8-5-3-sparse-k-means-clustering"><a class="header-anchor" href="#_8-5-3-sparse-k-means-clustering" aria-hidden="true">#</a> 8.5.3 Sparse K -Means Clustering</h4><p>Another commonly used method of clustering is called “ <em>K</em> -means.” Here we predefine the number of groups <em>K</em> and then try to partition the observations into <em>K</em> homogeneous groups. Each group is summarized by a centroid, with each observation assigned to the group with the closest centroid. In detail, the <em>K</em> -means algorithm maintains a partitionC={ <em>C</em> 1 <em>,...,CK</em> } of the index set{ 1 <em>,</em> 2 <em>,...,N</em> }, where subset <em>Ck</em> corresponds to those observa- tions currently assigned to class <em>k</em>. It chooses these partitions by minimizing the within cluster sum of squares:</p><h6 id="w-c"><a class="header-anchor" href="#w-c" aria-hidden="true">#</a> W (C) =</h6><h6 id="∑-k-13"><a class="header-anchor" href="#∑-k-13" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1\n</code></pre></div><h6 id="∑-42"><a class="header-anchor" href="#∑-42" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i ∈ Ck\n</code></pre></div><div class="language-"><pre><code>‖ xi − x ̄ k ‖^22. (8.44)\n</code></pre></div><p>Here <em>xi</em> is the <em>ith</em> observation and ̄ <em>xk</em> is a <em>p</em> -vector equal to the average of all observations in cluster <em>k</em>. The collection{ <em>x</em> ̄ <em>k</em> } <em>K</em> 1 is referred to as the <em>codebook</em> in the compression literature. The <em>encoderτ</em> ( <em>i</em> ) assigns each observation <em>xi</em> to the cluster <em>k</em> whose centroid is closest to it. Hence <em>Ck</em> ={ <em>i</em> : <em>τ</em> ( <em>i</em> ) = <em>k</em> }. The standard algorithm for <em>K</em> -means clustering alternates over optimizing forC and{ <em>x</em> ̄ 1 <em>,...,</em> ̄ <em>xK</em> }, and is guaranteed to find a local minimum of <em>W</em> (C). Since</p><div class="language-"><pre><code>∑\n</code></pre></div><div class="language-"><pre><code>i,i ′∈ Ck\n</code></pre></div><div class="language-"><pre><code>‖ xi − xi ′‖^22 = 2 Nk\n</code></pre></div><h6 id="∑-43"><a class="header-anchor" href="#∑-43" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i ∈ Ck\n</code></pre></div><div class="language-"><pre><code>‖ xi − x ̄ k ‖^22 , (8.45)\n</code></pre></div><p>with <em>Nk</em> =| <em>Ck</em> |, one can alternatively derive K-means clustering using a squared Euclidean dissimilarity matrix <em>Di,i</em> ′. For general dissimilarity ma- trices, <em>K-medoids</em> clustering is a natural generalization (Hastie et al. 2009, for example). It might seem reasonable to define as a criterion for sparse <em>K</em> -means clus- tering the minimum of the weighted within-cluster sum of squares:</p><div class="language-"><pre><code>minimize\nC , w ∈R p\n</code></pre></div><h6 id="-41"><a class="header-anchor" href="#-41" aria-hidden="true">#</a> </h6><h6 id="-35"><a class="header-anchor" href="#-35" aria-hidden="true">#</a> </h6><h6 id="-35"><a class="header-anchor" href="#-35" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>wj\n</code></pre></div><h6 id="∑-k-14"><a class="header-anchor" href="#∑-k-14" aria-hidden="true">#</a> (∑ K</h6><div class="language-"><pre><code>k =1\n</code></pre></div><h6 id="_1-127"><a class="header-anchor" href="#_1-127" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>Nk\n</code></pre></div><h6 id="∑-44"><a class="header-anchor" href="#∑-44" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i,i ′∈ Ck\n</code></pre></div><div class="language-"><pre><code>di,i ′ ,j\n</code></pre></div><h6 id="-334"><a class="header-anchor" href="#-334" aria-hidden="true">#</a> )</h6><h6 id="-36"><a class="header-anchor" href="#-36" aria-hidden="true">#</a> </h6><h6 id="-35"><a class="header-anchor" href="#-35" aria-hidden="true">#</a> </h6><h6 id="-35"><a class="header-anchor" href="#-35" aria-hidden="true">#</a> </h6><h6 id="-335"><a class="header-anchor" href="#-335" aria-hidden="true">#</a> .</h6><p>We still need to add constraints on <em>w</em> , to make the problem meaningful. Adding the constraints‖ <em>w</em> ‖ 2 ≤1,‖ <em>w</em> ‖ 1 ≤ <em>s</em> as well as the nonnegativity constraint <em>w</em> 0 makes the problem convex in <em>w</em> , but leads to the pathological solution <em>w</em> ̂= 0. On the other hand, the triplet of constraints‖ <em>w</em> ‖ 2 ≥ 1 <em>,</em> ‖ <em>w</em> ‖ 1 ≥ <em>s,w</em>  0 would lead to a potentially useful solution, but make the problem nonconvex in <em>w</em>. Witten and Tibshirani (2010) proposed a modified criterion that focuses</p><h6 id="sparse-clustering-231"><a class="header-anchor" href="#sparse-clustering-231" aria-hidden="true">#</a> SPARSE CLUSTERING 231</h6><p>instead on the between-cluster sum of squares:</p><div class="language-"><pre><code>maximize\nC , w ∈R p\n</code></pre></div><h6 id="-42"><a class="header-anchor" href="#-42" aria-hidden="true">#</a> </h6><h6 id="-36"><a class="header-anchor" href="#-36" aria-hidden="true">#</a> </h6><h6 id="-36"><a class="header-anchor" href="#-36" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>wj\n</code></pre></div><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="-1-1"><a class="header-anchor" href="#-1-1" aria-hidden="true">#</a> ^1</h6><h6 id="n-33"><a class="header-anchor" href="#n-33" aria-hidden="true">#</a> N</h6><h6 id="∑-n-96"><a class="header-anchor" href="#∑-n-96" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><h6 id="∑-n-97"><a class="header-anchor" href="#∑-n-97" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i ′=1\n</code></pre></div><div class="language-"><pre><code>di,i ′ ,j −\n</code></pre></div><h6 id="∑-k-15"><a class="header-anchor" href="#∑-k-15" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1\n</code></pre></div><h6 id="_1-128"><a class="header-anchor" href="#_1-128" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>Nk\n</code></pre></div><h6 id="∑-45"><a class="header-anchor" href="#∑-45" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i,i ′∈ Ck\n</code></pre></div><div class="language-"><pre><code>di,i ′ ,j\n</code></pre></div><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="-37"><a class="header-anchor" href="#-37" aria-hidden="true">#</a> </h6><h6 id="-36"><a class="header-anchor" href="#-36" aria-hidden="true">#</a> </h6><h6 id="-36"><a class="header-anchor" href="#-36" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>subject to‖ w ‖ 2 ≤ 1 , ‖ w ‖ 1 ≤ s,w  0.\n</code></pre></div><h6 id="_8-46"><a class="header-anchor" href="#_8-46" aria-hidden="true">#</a> (8.46)</h6><p>When <em>wj</em> = 1 for all <em>j</em> , we can see from condition (8.45) that the second term is equal to 2 <em>W</em> (C), and hence this approach is equivalent to <em>K</em> -means. This problem is now convex in <em>w</em> and generally has an interesting solution. It can be solved by a simple alternating algorithm. WithC= ( <em>C</em> 1 <em>,...,CK</em> ) fixed, the minimization over <em>w</em> is a convex problem, with solutions given by soft- thresholding. With <em>w</em> fixed, optimization with respect toCleads to a weighted <em>K</em> -means clustering algorithm. Details are given in Exercise 8.11.</p><h4 id="_8-5-4-convex-clustering"><a class="header-anchor" href="#_8-5-4-convex-clustering" aria-hidden="true">#</a> 8.5.4 Convex Clustering</h4><p>The method of <em>K</em> -means clustering and its sparse generalization lead to prob- lems that are biconvex but not jointly convex, and hence it is difficult to guarantee that a global solution has been attained. Here we present a dif- ferent formulation of clustering that yields a convex program, and represents an interesting alternative to <em>K</em> -means and hierarchical clustering. Unlike the methods of the previous section which use sparsity to do feature selection, this method uses a form of sparsity to determine the number and memberships of the clusters. In this approach, each of the <em>N</em> observations <em>xi</em> ∈R <em>p</em> is assigned a proto- type <em>ui</em> ∈R <em>p</em>. We then minimize the objective function</p><div class="language-"><pre><code>J ( u 1 ,u 2 ,...,uN ) =\n</code></pre></div><h6 id="_1-129"><a class="header-anchor" href="#_1-129" aria-hidden="true">#</a> 1</h6><h6 id="_2-85"><a class="header-anchor" href="#_2-85" aria-hidden="true">#</a> 2</h6><h6 id="∑-n-98"><a class="header-anchor" href="#∑-n-98" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>‖ xi − ui ‖^2 + λ\n</code></pre></div><h6 id="∑-46"><a class="header-anchor" href="#∑-46" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i&lt;i ′\n</code></pre></div><div class="language-"><pre><code>wii ′‖ ui − ui ′‖ q (8.47)\n</code></pre></div><p>for some fixed <em>λ</em> ≥0, and some <em>q</em> -norm (typically <em>q</em> = 1 or <em>q</em> = 2). This criterion seeks prototypes that are close to the data points, but not too far from one another. The weights <em>wii</em> ′can be equal to 1, or can be a function of the distance between observations <em>i</em> and <em>i</em> ′. We note that this problem is convex for <em>q</em> ≥1. Consider for example the natural choice <em>q</em> = 2 (group lasso). Then the penalty term shrinks prototype vectors toward each other, and the distance between many pairs will be equal to zero for a sufficiently large value of <em>λ</em>. Each distinct prototype <em>u</em> ̂ <em>i</em> in the solution represents a cluster; however, as shown in the example of Figure 8.10, we should not think of it as a typical prototype or centroid of that cluster. In this example, there are two classes each containing 50 spherical Gaussian data points, with their means separated by three units in both directions. Here we used <em>q</em> = 2 and weight function <em>wii</em> ′ = exp(−‖ <em>xi</em> − <em>xi</em> ′‖^2 ). The colors in the right panel coincide with the</p><h6 id="_232-sparse-multivariate-methods"><a class="header-anchor" href="#_232-sparse-multivariate-methods" aria-hidden="true">#</a> 232 SPARSE MULTIVARIATE METHODS</h6><div class="language-"><pre><code>0 2 4\n</code></pre></div><div class="language-"><pre><code>−1\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>3\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>0 2 4\n</code></pre></div><div class="language-"><pre><code>−1\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>3\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>X 1 X 1\n</code></pre></div><div class="language-"><pre><code>X^2 X^2\n</code></pre></div><div class="language-"><pre><code>λ= 3. 27 λ= 12. 65\n</code></pre></div><p><strong>Figure 8.10</strong> <em>Convex clustering applied to data generated from two spherical Gaus- sian populations separated by three units in each direction. We show two solutions from a path of 50 values ofλ; the solution on the right was the smallest value ofλ that yielded two clusters, and in this case identified the true clusters. Pointsxiare associated with prototypes</em> ̂ <em>μiof the same color. The estimated prototypes need not be close to the centroids of their clusters.</em></p><p>true clusters; further details are given in the caption. The convexity of the objective function as well as its ability to choose the number of clusters and the informative features, makes this approach attractive. The next example is taken from Chi and Lange (2014), on the problem of clustering mammals based on their dentition. Eight different kinds of teeth were counted for each of 27 mammals: the number of top incisors, bottom incisors, top canines, bottom canines, top premolars, bottom premolars, top molars, and bottom molars. Figure 8.11 shows the resulting clustering path over <em>λ</em> using kernel-based weights <em>wii</em> ′. For visualization, the prototypes have been projected onto the first two principal components. The continuous path of solutions provides an appealing summary of the similarity among the mam- mals. Both these examples were produced using thecvxclusterpackage in R (Chi and Lange 2014). The path of solutions creates a tree, which in this example is rather similar to that produced by hierarchical clustering with average linkage.</p><h3 id="bibliographic-notes-5"><a class="header-anchor" href="#bibliographic-notes-5" aria-hidden="true">#</a> Bibliographic Notes</h3><p>Jolliffe et al. (2003) proposed the originalSCoTLASScriterion (8.7) for sparse PCA; the reformulation and alternating updates (8.9) were proposed by Wit- ten et al. (2009). d’Aspremont et al. (2007) proposed the semidefinite pro- gramming relaxation (8.12) of the nonconvexSCoTLASScriterion; Amini and</p><h6 id="bibliographic-notes-233"><a class="header-anchor" href="#bibliographic-notes-233" aria-hidden="true">#</a> BIBLIOGRAPHIC NOTES 233</h6><div class="language-"><pre><code>oppossum\n</code></pre></div><div class="language-"><pre><code>htailmole\n</code></pre></div><div class="language-"><pre><code>commonmole\n</code></pre></div><div class="language-"><pre><code>brownbat\nshairbat\npigmybat\nhousebat\nredbat\n</code></pre></div><div class="language-"><pre><code>armadillo\n</code></pre></div><div class="language-"><pre><code>pika\nssrabbit\n</code></pre></div><div class="language-"><pre><code>beavergraysquirrel\n</code></pre></div><div class="language-"><pre><code>packrat\n</code></pre></div><div class="language-"><pre><code>coyote\n</code></pre></div><div class="language-"><pre><code>wolf\n</code></pre></div><div class="language-"><pre><code>civetcat\nraccoonriverotterweasel\n</code></pre></div><div class="language-"><pre><code>seaotter\n</code></pre></div><div class="language-"><pre><code>jaguar\n</code></pre></div><div class="language-"><pre><code>furseal\n</code></pre></div><div class="language-"><pre><code>walrus\nephantseal\n</code></pre></div><div class="language-"><pre><code>elkdeer\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n● ●\n●\n●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n●\n●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n● ●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n●\n</code></pre></div><div class="language-"><pre><code>●●\n</code></pre></div><div class="language-"><pre><code>−2\n</code></pre></div><div class="language-"><pre><code>−1\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>−2 0 2 4\nPrincipal Component 1\n</code></pre></div><div class="language-"><pre><code>Principal Component 2\n</code></pre></div><p><strong>Figure 8.11</strong> <em>Mammal data: path of convex clustering solutions using the crite- rion (8.47), and a grid of values ofλ. Asλincreases, the prototypes unite to form a smal ler set.</em></p><p>Wainwright (2009) develop some theory for the variable selection properties of this relaxation for sparse PCA. Zou et al. (2006) proposed the reconstruction- based criterion (8.13). Johnstone (2001) studied the high-dimensional asymp- totics of ordinary PCA, and proposed the spiked covariance model (8.22). Johnstone and Lu (2009) and Birnbaum et al. (2013) study various types of two-stage procedures for estimating sparse principal components. Birn- baum et al. (2013) and Vu and Lei (2012) derive minimax lower bounds on <em><code>_ 2 -estimation error for sparse PCA when the eigenvectors belong _</code>q</em> -balls. Ma (2010, 2013) and Yuan and Zhang (2013) have studied iterative algorithms for sparse PCA based on combining the power method with soft thresholding. Berthet and Rigollet (2013) study the detection problem for high-dimensional sparse PCA, and establish a computational hardness result related to the random <em>k</em> -clique problem. Olshausen and Field (1996) proposed the version of sparse coding discussed in Section 8.2.5. There is a large literature on deep learning; see Le et al. (2012) and references therein for some recent approaches. There is a variety of papers that explore sparse canonical correlation anal- ysis, for example Parkhomenko, Tritchler and Beyene (2009), Waaijenborg,</p><h6 id="_234-sparse-multivariate-methods"><a class="header-anchor" href="#_234-sparse-multivariate-methods" aria-hidden="true">#</a> 234 SPARSE MULTIVARIATE METHODS</h6><p>Vers ́elewel de Witt Hamer and Zwinderman (2008), Hardoon and Shawe- Taylor (2011), Parkhomenko et al. (2009), Witten et al. (2009), Witten and Tibshirani (2009), and Lykou and Whittaker (2010). Dudoit, Fridlyand and Speed (2002) provide comparison of different classification methods for mi- croarray data, including diagonal LDA. Nearest shrunken centroids was pro- posed by Tibshirani, Hastie, Narasimhan and Chu (2001) and Tibshirani et al. (2003). Sparse discriminant analysis from Fisher’s framework and op- timal scoring were explored in Trendafilov and Jolliffe (2007), Leng (2008), Clemmensen et al. (2011), and Witten and Tibshirani (2011). Minorization algorithms are discussed in Lange, Hunter and Yang (2000), Lange (2004), and Hunter and Lange (2004). Sparse hierarchical and <em>k</em> -means clustering are presented in Witten and Tibshirani (2010), while convex clustering was pro- posed by Pelckmans, De Moor and Suykens (2005) and Hocking, Vert, Bach and Joulin (2011).</p><h3 id="exercises-6"><a class="header-anchor" href="#exercises-6" aria-hidden="true">#</a> Exercises</h3><p>Ex. 8.1 In this exercise, we consider some elementary properties of principal component analysis.</p><div class="language-"><pre><code>(a) Show that the first principal component is a maximal eigenvector of the\nsample covariance matrix N^1 X T X.\n(b) Suppose that the rows of X , denoted{ x 1 ,...,xN }are drawn i.i.d. ac-\ncording to a zero-mean distributionP, and suppose that Σ = Cov( x ) has\na unique maximal eigenvalue λ. Explain why, for large sample size N , one\nmight expect̂ v to approach\n</code></pre></div><div class="language-"><pre><code>v ∗= arg max\n‖ v ‖ 2 =1\n</code></pre></div><div class="language-"><pre><code>Var( vTx ) , where x ∼P. (8.48)\n</code></pre></div><p>Ex. 8.2 Consider the principal component criterion (8.1), and the definition of the vectors <em>v</em> 1 , <em>v</em> 2 , and so on.</p><div class="language-"><pre><code>(a) Show that the principal components z j are mutually uncorrelated.\n(b) Show that instead of using orthogonality of the vj to define the sequence\nof principal component directions, we can instead use the uncorrelatedness\nof the z j.\n</code></pre></div><p>Ex. 8.3 Consider the reconstruction error objective (8.4) with <em>μ</em> = 0.</p><div class="language-"><pre><code>(a) For fixed V r , show that the optimal choice of the reconstruction weights\nis λi = V Trxi for each i = 1 ,...,N.\n(b) Use part (a) to show that the optimal V r maximizes the criterion\nV Tr X T XV , and conclude that it can be obtained via the truncated SVD\n(as described in the main text).\n</code></pre></div><h6 id="exercises-235"><a class="header-anchor" href="#exercises-235" aria-hidden="true">#</a> EXERCISES 235</h6><p>Ex. 8.4 Consider criterion (8.8) and Algorithm 8.1 for finding a solution. By partially maximizing w.r.t. <strong>u</strong> with‖ <strong>u</strong> ‖ 2 ≤1, show that any stationary value of <em>v</em> is also a stationary value for theSCoTLASScriterion (8.7).</p><p>Ex. 8.5 Consider the problem</p><div class="language-"><pre><code>minimize\nu ,v,d\n</code></pre></div><div class="language-"><pre><code>‖ X − d u vT ‖ F\n</code></pre></div><div class="language-"><pre><code>subject to‖ u ‖ 1 ≤ c 1 , ‖ v ‖ 1 ≤ c 2 , ‖ u ‖ 2 = 1 , ‖ v ‖ 2 = 1 , d ≥ 0 ,\n</code></pre></div><h6 id="_8-49"><a class="header-anchor" href="#_8-49" aria-hidden="true">#</a> (8.49)</h6><p>where <strong>X</strong> is <em>N</em> × <em>p</em> , and assume 1≤ <em>c</em> 1 ≤</p><h6 id="√-23"><a class="header-anchor" href="#√-23" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>N , 1≤ c 2 ≤\n</code></pre></div><h6 id="√-24"><a class="header-anchor" href="#√-24" aria-hidden="true">#</a> √</h6><p><em>p</em>. Show that a solution <strong>u</strong> 1 , <em>v</em> 1 , <em>d</em> 1 is also the solution to the following problem:</p><div class="language-"><pre><code>maximize\nu ,v\nu T X v\n</code></pre></div><div class="language-"><pre><code>subject to‖ u ‖ 1 ≤ c 1 , ‖ v ‖ 1 ≤ c 2 , ‖ u ‖ 2 = 1 , ‖ v ‖ 2 = 1 ,\n</code></pre></div><h6 id="_8-50"><a class="header-anchor" href="#_8-50" aria-hidden="true">#</a> (8.50)</h6><p>and <em>d</em> 1 = <strong>u</strong> <em>T</em> 1 <strong>X</strong> <em>v</em> 1.</p><p>Ex. 8.6 In this exercise, we explore some properties of theSCoTLASScrite- rion (8.7).</p><div class="language-"><pre><code>(a) Use the Cauchy–Schwarz inequality to show that a fixed point of Algo-\nrithm 8.1 is a local minimum of theSCoTLASScriterion (8.7).\n(b) Notice that the Cauchy–Schwarz inequality implies that\n</code></pre></div><div class="language-"><pre><code>vT X T X v ≥\n( v ( m )\n</code></pre></div><div class="language-"><pre><code>T\nX T X v )^2\nv ( m )\nT\nX T X v ( m )\n</code></pre></div><h6 id="_8-51"><a class="header-anchor" href="#_8-51" aria-hidden="true">#</a> , (8.51)</h6><div class="language-"><pre><code>and equality holds when v = v ( m ). So ( v\n</code></pre></div><div class="language-"><pre><code>( m ) T X T X v ) 2\nv ( m ) T X T X v ( m )minorizes v\n</code></pre></div><div class="language-"><pre><code>T X T X v at\n</code></pre></div><div class="language-"><pre><code>v ( m ). Hence show that the MM algorithm (Section 5.8) using this minoriza-\ntion function yields Algorithm 8.1.\n</code></pre></div><p>Ex. 8.7 Show that the solution to the problem (8.15), with the additional constraint‖ <em>θ</em> ‖ 1 ≤ <em>t</em> , is also a solution to theSCoTLASSproblem (8.7).</p><p>Ex. 8.8 Consider the problem</p><div class="language-"><pre><code>minimize\nθ :‖ θ ‖ 2 =1\n</code></pre></div><h6 id="∑-n-99"><a class="header-anchor" href="#∑-n-99" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>‖ xi − θzi ‖^22 , (8.52)\n</code></pre></div><p>where the vectors{ <em>xi</em> } <em>Ni</em> =1 and <em>θ</em> are all <em>p</em> -dimensional, and the variables { <em>zi</em> } <em>Ni</em> =1are scalars. Show that the optimal solution is unique and given by ̂ <em>θ</em> = <strong>X</strong> <em>T</em> <strong>z</strong> ‖ <strong>X</strong> <em>T</em> <strong>z</strong> ‖ 2.</p><p>Ex. 8.9 Show that the vectors <strong>u</strong> <em>k</em> in (8.17) solve the multifactor sparse PCA problem (8.16).</p><h6 id="_236-sparse-multivariate-methods"><a class="header-anchor" href="#_236-sparse-multivariate-methods" aria-hidden="true">#</a> 236 SPARSE MULTIVARIATE METHODS</h6><p>Ex. 8.10 Consider the reconstruction criterion (8.13) for sparse principal com- ponents.</p><div class="language-"><pre><code>(a) With V fixed, derive the solution for Θ.\n(b) Show that when the λ 1 k = 0 for all k the iterations are stationary w.r.t.\nany set of k principal components of X ; in particular, if the algorithm is\nstarted at the largest k principal components, it will not move from them.\n(c) Show under the conditions in (b) that the criterion is maximized by\nΘ = V and both are equal to V k , the matrix consisting of the largest k\nprincipal components of X.\n(d) For a solution V k in (c), show that V k R is also a solution, for any k × k\northogonal matrix R.\n</code></pre></div><p>Consequently, this version of sparse principal components is similar to the Varimax method (Kaiser 1958) of rotating factors to achieve sparsity.</p><p>Ex. 8.11 <em>SparseK-means clustering algorithm</em>. Consider the objective func- tion (8.46).</p><div class="language-"><pre><code>(a) Show that with w fixed, optimization with respect toC= ( C 1 ,...,CK )\nyields the problem\n</code></pre></div><div class="language-"><pre><code>minimize\nC 1 ,...,CK\n</code></pre></div><h6 id="-43"><a class="header-anchor" href="#-43" aria-hidden="true">#</a> </h6><h6 id="-37"><a class="header-anchor" href="#-37" aria-hidden="true">#</a> </h6><h6 id="-37"><a class="header-anchor" href="#-37" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>wj (\n</code></pre></div><h6 id="∑-k-16"><a class="header-anchor" href="#∑-k-16" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1\n</code></pre></div><h6 id="_1-130"><a class="header-anchor" href="#_1-130" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>Nk\n</code></pre></div><h6 id="∑-47"><a class="header-anchor" href="#∑-47" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i,i ′∈ Ck\n</code></pre></div><div class="language-"><pre><code>( xij − xi ′ j )^2 )\n</code></pre></div><h6 id="-38"><a class="header-anchor" href="#-38" aria-hidden="true">#</a> </h6><h6 id="-37"><a class="header-anchor" href="#-37" aria-hidden="true">#</a> </h6><h6 id="-37"><a class="header-anchor" href="#-37" aria-hidden="true">#</a> </h6><h6 id="_8-53"><a class="header-anchor" href="#_8-53" aria-hidden="true">#</a> . (8.53)</h6><div class="language-"><pre><code>This can be thought of as K -means clustering with weighted data. Give a\nsketch of its solution.\n(b) With C 1 ,...,CK fixed, we optimize with respect to w yielding\n</code></pre></div><div class="language-"><pre><code>maximize\nw ∈R p\n</code></pre></div><h6 id="-44"><a class="header-anchor" href="#-44" aria-hidden="true">#</a> </h6><h6 id="-38"><a class="header-anchor" href="#-38" aria-hidden="true">#</a> </h6><h6 id="-38"><a class="header-anchor" href="#-38" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>wj\n</code></pre></div><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><h6 id="-1-2"><a class="header-anchor" href="#-1-2" aria-hidden="true">#</a> ^1</h6><h6 id="n-34"><a class="header-anchor" href="#n-34" aria-hidden="true">#</a> N</h6><h6 id="∑-n-100"><a class="header-anchor" href="#∑-n-100" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><h6 id="∑-n-101"><a class="header-anchor" href="#∑-n-101" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i ′=1\n</code></pre></div><div class="language-"><pre><code>( xij − xi ′ j )^2 −\n</code></pre></div><h6 id="∑-k-17"><a class="header-anchor" href="#∑-k-17" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1\n</code></pre></div><h6 id="_1-131"><a class="header-anchor" href="#_1-131" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>Nk\n</code></pre></div><h6 id="∑-48"><a class="header-anchor" href="#∑-48" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i,i ′∈ CK\n</code></pre></div><div class="language-"><pre><code>( xij − xi ′ j )^2\n</code></pre></div><h6 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> </h6><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="-39"><a class="header-anchor" href="#-39" aria-hidden="true">#</a> </h6><h6 id="-38"><a class="header-anchor" href="#-38" aria-hidden="true">#</a> </h6><h6 id="-38"><a class="header-anchor" href="#-38" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>such that‖ w ‖ 2 = 1,‖ w ‖ 1 ≤ s , and wj ≥0. (8.54)\n</code></pre></div><div class="language-"><pre><code>This is a simple convex problem of the form\n</code></pre></div><div class="language-"><pre><code>maximize\nw ∈R p\n</code></pre></div><h6 id="-336"><a class="header-anchor" href="#-336" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>wTa\n</code></pre></div><h6 id="-337"><a class="header-anchor" href="#-337" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>such that‖ w ‖ 2 = 1,‖ w ‖ 1 ≤ s , and w 0. (8.55)\n</code></pre></div><div class="language-"><pre><code>Give the details of its solution.\n</code></pre></div><p>Ex. 8.12 Consider the optimization problem:</p><div class="language-"><pre><code>minimize\nA , B ∈R p × m\n</code></pre></div><h6 id="n-35"><a class="header-anchor" href="#n-35" aria-hidden="true">#</a> { N</h6><h6 id="∑-49"><a class="header-anchor" href="#∑-49" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>‖ xi − AB Txi ‖^2\n</code></pre></div><h6 id="-338"><a class="header-anchor" href="#-338" aria-hidden="true">#</a> }</h6><h6 id="_8-56"><a class="header-anchor" href="#_8-56" aria-hidden="true">#</a> , (8.56)</h6><p>where <em>xi</em> ∈R <em>p, i</em> = 1 <em>,...,N</em> are the rows of <strong>X</strong> , and <em>m &lt;</em> min( <em>N,p</em> ). Show that the solution satisfies <strong>A</strong> ˆ <strong>B</strong> ˆ <em>T</em> = <strong>V</strong> <em>m</em> <strong>V</strong> <em>Tm</em> , where <strong>V</strong> <em>m</em> is the matrix of the first <em>m</em> right-singular vectors of <strong>X</strong>.</p><h6 id="exercises-237"><a class="header-anchor" href="#exercises-237" aria-hidden="true">#</a> EXERCISES 237</h6><p>Ex. 8.13 Consider the sparse encoder specified in the optimization prob- lem (8.21). Develop a simple alternating algorithm for solving this problem. Give some details for each step.</p><p>Ex. 8.14 <em>Canonical correlation via alternating regression:</em> Consider two ran- dom vectors <em>X</em> ∈R <em>m</em>^1 and <em>Y</em> ∈R <em>m</em>^2 with covariance matrices <strong>Σ</strong> 11 and <strong>Σ</strong> 22 ,</p><p>respectively, and cross-covariance <strong>Σ</strong> 12. Define <strong>L</strong> = <strong>Σ</strong> −^12 11 <strong>Σ</strong>^12 <strong>Σ</strong></p><p>−^12 22. Denote by <em>γi</em> and <em>τi</em> the left and right singular vectors of <strong>L</strong> , ordered by their singular values <em>ρi</em>.</p><div class="language-"><pre><code>(a) Show that the vectors β 1 and θ 1 maximizing Corr( Xβ,Y θ ) are given by\n</code></pre></div><div class="language-"><pre><code>β 1 = Σ −\n</code></pre></div><p>(^12) 11 <em>γ</em>^1 <em>θ</em> 1 = <strong>Σ</strong> −^12 22 <em>τ</em>^1 <em>,</em></p><h6 id="_8-57"><a class="header-anchor" href="#_8-57" aria-hidden="true">#</a> (8.57)</h6><div class="language-"><pre><code>and the maximal correlation is ρ 1.\n(b) Now consider the analogous problem based on data matrices X and Y of\ndimension N × p and N × q , respectively, each centered to have zero-mean\ncolumns. In this setting, the canonical correlation estimates are obtained\nsimply by replacing Σ 11 , Σ 22 , and Σ 12 by their sample estimates.\nBased on this formulation, show that the optimal sample canonical vectors\nare given by β 1 = ( X T X )−\n</code></pre></div><p>(^12) <em>γ</em> 1 and <em>θ</em> 1 = ( <strong>Y</strong> <em>T</em> <strong>Y</strong> )− (^12) <em>τ</em> 1 , where <em>γ</em> 1 and <em>τ</em> 1 are the leading left and right singular vectors of the matrix ( <strong>X</strong> <em>T</em> <strong>X</strong> )− (^12) <strong>X</strong> <em>T</em> <strong>Y</strong> ( <strong>Y</strong> <em>T</em> <strong>Y</strong> )− (^12) <em>.</em> (c) Denote the first canonical variates by <strong>z</strong> 1 = <strong>X</strong> <em>β</em> 1 and <strong>s</strong> 1 = <strong>Y</strong> <em>θ</em> 1 , both <em>N</em> -vectors. Let <strong>H</strong> <em>X</em> = <strong>X</strong> ( <strong>X</strong> <em>T</em> <strong>X</strong> )−^1 <strong>X</strong> <em>T</em> be the projection onto the column space of <strong>X</strong> ; likewise <strong>H</strong> <em>Y</em>. Show that <strong>H</strong> <em>X</em> <strong>s</strong> 1 = <em>ρ</em> 1 <strong>z</strong> 1 <em>,</em> and <strong>H</strong> <em>Y</em> <strong>z</strong> 1 = <em>ρ</em> 1 <strong>s</strong> 1_._ Consequently, alternately regressing onto <strong>X</strong> and <strong>Y</strong> until convergence yields a solution of the maximal canonical correlation problem. Ex. 8.15 In Exercise 8.14, we found that the leading pair of canonical variates can be found by alternating least-squares regressions. Having solved for the leading canonical variates, how could you modify this procedure to produce the second pair of canonical variates ( <strong>z</strong> 2 <em>,</em> <strong>s</strong> 2 )? Propose a modification and prove that it works. Show how to extend this approach to find all subsequent pairs. Ex. 8.16 <em>CCA via optimal scoring:</em> Given data matrices <strong>X</strong> and <strong>Y</strong> as in Exer- cise 8.14, both with mean-centered columns and both with full column rank, consider the problem minimize <em>β,θ</em> ‖ <strong>Y</strong> <em>θ</em> − <strong>X</strong> <em>β</em> ‖^22 subject to</p><h6 id="_1-132"><a class="header-anchor" href="#_1-132" aria-hidden="true">#</a> 1</h6><h6 id="n-36"><a class="header-anchor" href="#n-36" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>‖ Y θ ‖ 2 = 1. (8.58)\n</code></pre></div><h6 id="_238-sparse-multivariate-methods"><a class="header-anchor" href="#_238-sparse-multivariate-methods" aria-hidden="true">#</a> 238 SPARSE MULTIVARIATE METHODS</h6><div class="language-"><pre><code>(a) Characterize the solution to this problem, by first solving for β with θ\nfixed, and then solving for θ.\n(b) Show that the optimal solution is given bŷ θ = θ 1 and β ̂= ρ 1 β 1 , where\nβ 1 and θ 1 are the first pair of canonical vectors, and ρ 1 the largest canonical\ncorrelation.\n(c) Show in addition that‖ Y θ 1 − X β 1 ‖^22 = 1− ρ^21. This equivalence shows\nthat solving the optimal scoring problem is equivalent to solving the CCA\nproblem.\n(d) Describe how to find subsequent canonical solutions, uncorrelated with\nthe earlier solutions. Show how this can be achieved by transforming the\ndata matrices X and Y.\n(e) Does the problem change if we include a constraint‖ X β ‖ 2 = 1?\n</code></pre></div><p>Ex. 8.17 <em>Low-rank CCA:</em> Suppose that at least one of the matrices in Exer- cise 8.14 or 8.16 do not have full column rank. (For instance, this degeneracy will occur whenever <em>N &lt;</em> min( <em>p,q</em> ).)</p><div class="language-"><pre><code>(a) Show that ρ 1 = 1, and the CCA problem has multiple optima.\n(b) Suppose that Y is full column rank, but X is not. You add a ridge con-\nstraint to (8.58), and solve\n</code></pre></div><div class="language-"><pre><code>minimize\nβ,θ\n‖ Y θ − X β ‖^22 + λ ‖ β ‖^22 subject to‖ Y θ ‖ 2 = 1. (8.59)\n</code></pre></div><div class="language-"><pre><code>Is this problem degenerate? Characterize the solution.\n(c) Show that the solution in (b) is equivalent to applying CCA to X and\nY , except that the optimal solution β ̂satisfies the normalization condition\n1\nN\nβ ̂ T ( X T X + λ I ) β ̂= 1, corresponding to normalization by a type of ridged\ncovariance estimate.\n</code></pre></div><p>Ex. 8.18 For data matrices <strong>X</strong> and <strong>Y</strong> with centered columns, consider the optimization problem</p><div class="language-"><pre><code>maximize\nβ,θ\n</code></pre></div><h6 id="-339"><a class="header-anchor" href="#-339" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>Cov(̂ X β, Y θ )− λ 1 ‖ β ‖^22 − λ 2 ‖ β ‖^22 − λ ′ 1 ‖ θ ‖ 1 − λ ′ 2 ‖ θ ‖ 2\n</code></pre></div><h6 id="-340"><a class="header-anchor" href="#-340" aria-hidden="true">#</a> }</h6><h6 id="_8-60"><a class="header-anchor" href="#_8-60" aria-hidden="true">#</a> . (8.60)</h6><p>Using the results of Exercise 8.14, outline how to solve this problem using alternating elastic-net fitting operations in place of the least-squares regres- sions.</p><p>Ex. 8.19 <em>Sparse canonical correlation analysis:</em> Consider the optimal scoring problem (8.58) from Exercise 8.16, but augmented with <em>`</em> 1 constraints:</p><div class="language-"><pre><code>minimize\nβ,θ\n</code></pre></div><h6 id="-341"><a class="header-anchor" href="#-341" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>‖ Y θ − X β ‖^22 + λ 1 ‖ β ‖ 1 + λ 2 ‖ θ ‖ 1\n</code></pre></div><h6 id="-342"><a class="header-anchor" href="#-342" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>subject to‖ X β ‖ 2 = 1. (8.61)\n</code></pre></div><p>Using the results of Exercises 8.14, 8.16, and 8.17, outline how to solve this problem using alternating elastic-net regressions in place of the least-squares regressions.</p><h6 id="exercises-239"><a class="header-anchor" href="#exercises-239" aria-hidden="true">#</a> EXERCISES 239</h6><p>Ex. 8.20 Consider the multivariate Gaussian setup in Section 8.4.1, but as- sume a different covariance matrix <strong>Σ</strong> <em>k</em> in each class.</p><div class="language-"><pre><code>(a) Show that the discriminant functions δk are quadratic functions of x.\nWhat can you say about the decision boundaries?\n(b) Suppose that the covariance matrices Σ k are assumed to be diagonal,\nmeaning that the features X are conditionally independent in each class.\nDescribe the decision boundary between class k and ` for this naive Bayes\nclassifier.\n</code></pre></div><p>Ex. 8.21 This exercise relates to the nearest shrunken centroids problem of Section 8.4.2. Consider the <em>`</em> 1 -regularized criterion</p><p>minimize ̄ <em>μ</em> ∈R <em>p αk</em> ∈R <em>p,k</em> = 1 <em>,...,p</em></p><h6 id="-45"><a class="header-anchor" href="#-45" aria-hidden="true">#</a> </h6><h6 id="-39"><a class="header-anchor" href="#-39" aria-hidden="true">#</a> </h6><h6 id="-39"><a class="header-anchor" href="#-39" aria-hidden="true">#</a> </h6><h6 id="_1-133"><a class="header-anchor" href="#_1-133" aria-hidden="true">#</a> 1</h6><h6 id="_2-86"><a class="header-anchor" href="#_2-86" aria-hidden="true">#</a> 2</h6><h6 id="∑-k-18"><a class="header-anchor" href="#∑-k-18" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1\n</code></pre></div><h6 id="∑-50"><a class="header-anchor" href="#∑-50" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i ∈ Ck\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>( xij − μ ̄ j − αjk )^2\ns^2 j\n+ λ\n</code></pre></div><h6 id="∑-k-19"><a class="header-anchor" href="#∑-k-19" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><h6 id="√-25"><a class="header-anchor" href="#√-25" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>Nk\nsj\n| αjk |\n</code></pre></div><h6 id="-40"><a class="header-anchor" href="#-40" aria-hidden="true">#</a> </h6><h6 id="-39"><a class="header-anchor" href="#-39" aria-hidden="true">#</a> </h6><h6 id="-39"><a class="header-anchor" href="#-39" aria-hidden="true">#</a> </h6><h6 id="-343"><a class="header-anchor" href="#-343" aria-hidden="true">#</a> .</h6><h6 id="_8-62"><a class="header-anchor" href="#_8-62" aria-hidden="true">#</a> (8.62)</h6><p>Here we have decomposed each class mean into an overall mean plus a class- wise contrast from the overall mean, and we have weighted the penalties by the class sizes and within-class standard deviations for each feature.</p><div class="language-"><pre><code>(a) Show that replacing ̄ μj by ̄ xj , corresponding to the overall mean for\nfeature j , yields the shrinkage scheme (8.35a), apart from a term 1 /N in\nmk.\n(b) Show that part (a) does not yield a solution to the criterion (8.33) unless\nwe restrict ̄ μj as above.\n(c) A more natural criterion would add the constraints\n</code></pre></div><h6 id="∑-k-20"><a class="header-anchor" href="#∑-k-20" aria-hidden="true">#</a> ∑ K</h6><div class="language-"><pre><code>k =1 αjk = 0 for\nall j = 1 ,...p. Discuss the solution to this problem, and whether it can\ncoincide with the solution from part (a).\n</code></pre></div><p>Ex. 8.22 Show that the penalized Fisher’s discriminant problem (8.40) and the penalized optimal scoring problem (8.42) are equivalent in the sense that any stationary point for one problem is also a stationary point for the other problem. (Clemmensen et al. 2011, Witten and Tibshirani 2011).</p><div class="language-"><pre><code>Chapter 9\n</code></pre></div><h2 id="graphs-and-model-selection"><a class="header-anchor" href="#graphs-and-model-selection" aria-hidden="true">#</a> Graphs and Model Selection</h2><h3 id="_9-1-introduction"><a class="header-anchor" href="#_9-1-introduction" aria-hidden="true">#</a> 9.1 Introduction</h3><p>Probabilistic graphical models provide a useful framework for building par- simonious models for high-dimensional data. They are based on an interplay between probability theory and graph theory, in which the properties of an underlying graph specify the conditional independence properties of a set of random variables. In typical applications, the structure of this graph is not known, and it is of interest to estimate it based on samples, a problem known as graphical model selection. In this chapter, we discuss a variety of methods based on <em>`</em> 1 -regularization designed for this purpose.</p><h3 id="_9-2-basics-of-graphical-models"><a class="header-anchor" href="#_9-2-basics-of-graphical-models" aria-hidden="true">#</a> 9.2 Basics of Graphical Models</h3><p>We begin by providing a brief introduction to the basics of graphical models; for more details, we refer the reader to the references cited in the bibliographic notes at the end of the chapter. Any collection <em>X</em> = ( <em>X</em> 1 <em>,X</em> 2 <em>,...,Xp</em> ) of random variables can be associated with the vertex set <em>V</em> ={ 1 <em>,</em> 2 <em>,...,p</em> } of some underlying graph. The essential idea of a graphical model is to use the structure of the underlying graph—either its clique structure or its cut set structure—in order to constrain the distribution of the random vector <em>X</em>. We now make these notions more precise.</p><h4 id="_9-2-1-factorization-and-markov-properties"><a class="header-anchor" href="#_9-2-1-factorization-and-markov-properties" aria-hidden="true">#</a> 9.2.1 Factorization and Markov Properties</h4><p>An ordinary graph <em>G</em> consists of a vertex set <em>V</em> ={ 1 <em>,</em> 2 <em>,...,p</em> }, and an edge set <em>E</em> ⊂ <em>V</em> × <em>V</em>. In this chapter, we focus exclusively on undirected graphical models, meaning that there is no distinction between an edge ( <em>s,t</em> )∈ <em>E</em> , and the edge ( <em>t,s</em> ). In contrast, <em>directed acyclic graphs</em> (DAGs) are the most popular form of graph in which the edges have directionality. In general, such directed graphs are more difficult to handle than undirected graphs, and we do not cover them here. But we do note that some methods for computation in undirected graphs can be helpful in the DAG case: see the bibliographic notes for some references. A <em>graph cliqueC</em> ⊆ <em>V</em> is a fully-connected subset of the vertex set, meaning</p><div class="language-"><pre><code>241\n</code></pre></div><h6 id="_242-graphs-and-model-selection"><a class="header-anchor" href="#_242-graphs-and-model-selection" aria-hidden="true">#</a> 242 GRAPHS AND MODEL SELECTION</h6><p>that ( <em>s,t</em> )∈ <em>E</em> for all <em>s,t</em> ∈ <em>C</em>. A clique is said to be maximal if it is not strictly contained within any other clique. For instance, any single vertex{ <em>s</em> }is itself a clique, but it is not maximal unless <em>s</em> is an isolated vertex (meaning that it participates in no edges). We useCto denote the set of all cliques in the graph, both maximal and nonmaximal; see Figure 9.1(a) for an illustration of graph cliques.</p><div class="language-"><pre><code>3\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>5\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>3 4\n</code></pre></div><div class="language-"><pre><code>5\n</code></pre></div><div class="language-"><pre><code>6\n</code></pre></div><div class="language-"><pre><code>7\nA BC\n</code></pre></div><div class="language-"><pre><code>D\n</code></pre></div><div class="language-"><pre><code>A\n</code></pre></div><div class="language-"><pre><code>B\n</code></pre></div><div class="language-"><pre><code>S\n</code></pre></div><div class="language-"><pre><code>(a) (b)\n</code></pre></div><p><strong>Figure 9.1</strong> <em>(a) Il lustration of cliques in a graph: each of the four subsets indicated are cliques. SetsAandBare</em> 3 <em>-cliques, whereasCandDare</em> 2 <em>-cliques, more com- monly known as edges. Al l of these cliques are maximal. (b) Il lustration of a vertex cut setS: with the vertices inSremoved, the graph is broken into two subcomponents AandB.</em></p><h5 id="_9-2-1-1-factorization-property"><a class="header-anchor" href="#_9-2-1-1-factorization-property" aria-hidden="true">#</a> 9.2.1.1 Factorization Property</h5><p>We now describe how the clique structure of a graph can be used to constrain the probability distribution of the random vector ( <em>X</em> 1 <em>,...,Xp</em> ) indexed by the graph vertices. For a given clique <em>C</em> ∈C, a <em>compatibility functionψC</em> is a real-valued function of the subvector <em>xC</em> : = ( <em>xs, s</em> ∈ <em>C</em> ), taking positive real values. Given a collection of such compatibility functions, we say that the probability distributionP <em>factorizes overG</em> if it has the decomposition</p><div class="language-"><pre><code>P( x 1 ,...,xp ) =\n</code></pre></div><h6 id="_1-134"><a class="header-anchor" href="#_1-134" aria-hidden="true">#</a> 1</h6><h6 id="z-1"><a class="header-anchor" href="#z-1" aria-hidden="true">#</a> Z</h6><h6 id="∏"><a class="header-anchor" href="#∏" aria-hidden="true">#</a> ∏</h6><div class="language-"><pre><code>C ∈C\n</code></pre></div><div class="language-"><pre><code>ψC ( xC ). (9.1)\n</code></pre></div><p>Here the quantity <em>Z</em> , known as the partition function, is given by the sum <em>Z</em> =</p><h6 id="∑-51"><a class="header-anchor" href="#∑-51" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>x ∈X p\n</code></pre></div><h6 id="∏-1"><a class="header-anchor" href="#∏-1" aria-hidden="true">#</a> ∏</h6><p><em>C</em> ∈C <em>ψC</em> ( <em>xC</em> ). Thus, it ensures thatPis properly normalized, and so defines a valid probability distribution. As a particular example, any probability distribution that factorizes over the graph in Figure 9.1(a) must have the form</p><div class="language-"><pre><code>P( x 1 ,...,x 7 ) =\n</code></pre></div><h6 id="_1-135"><a class="header-anchor" href="#_1-135" aria-hidden="true">#</a> 1</h6><h6 id="z-2"><a class="header-anchor" href="#z-2" aria-hidden="true">#</a> Z</h6><div class="language-"><pre><code>ψ 123 ( x 1 ,x 2 ,x 3 ) ψ 345 ( x 3 ,x 4 ,x 5 ) ψ 46 ( x 4 ,x 6 ) ψ 57 ( x 5 ,x 7 ) ,\n(9.2)\n</code></pre></div><p>for some choice of the compatibility functions{ <em>ψ</em> 123 <em>, ψ</em> 345 <em>, ψ</em> 46 <em>, ψ</em> 57 }.</p><h6 id="basics-of-graphical-models-243"><a class="header-anchor" href="#basics-of-graphical-models-243" aria-hidden="true">#</a> BASICS OF GRAPHICAL MODELS 243</h6><p>A factorization of the form (9.1) is practically significant, since it can lead to substantial savings, in both storage and computation, if the clique sizes are not too large. For instance, if each variable <em>Xs</em> is binary, then a generic probability distribution over the vector <em>X</em> ∈ {− 1 <em>,</em> +1} <em>p</em> requires specifying 2 <em>p</em> −1 nonnegative numbers, and so grows exponentially in the graph size. On the other hand, for a clique-based factorization, the number of degrees of freedom is at most|C| 2 <em>c</em> , where <em>c</em> is the maximum cardinality of any clique. Thus, for the clique-based factorization, the complexity grows exponentially in the maximum clique size <em>c</em> , but only linearly in the number of cliques |C|. Luckily, many practical models of interest can be specified in terms of cliques with bounded size, in which case the clique-based representation yields substantial gains.</p><h5 id="_9-2-1-2-markov-property"><a class="header-anchor" href="#_9-2-1-2-markov-property" aria-hidden="true">#</a> 9.2.1.2 Markov Property</h5><p>We now turn to an alternative way in which the graph structure can be used to constrain the distribution of <em>X</em> , based on its cut sets (see Figure 9.1(b)). In particular, consider a cut set <em>S</em> that separates the graph into disconnected components <em>A</em> and <em>B</em> , and let us introduce the symbol⊥⊥to denote the relation <em>“is conditionally independent of.”</em> With this notation, we say that the random vector <em>X</em> is <em>Markov with respect toG</em> if</p><div class="language-"><pre><code>XA ⊥⊥ XB | XS for all cut sets S ⊂ V. (9.3)\n</code></pre></div><p>The graph in Figure 9.1(b) is an example showing this conditional indepen- dence relation. Markov chains provide a particular illustration of this property; naturally, they are based on a chain-structured graph, with edge set</p><div class="language-"><pre><code>E ={(1 , 2) , (2 , 3) ,..., ( p − 1 ,p )}.\n</code></pre></div><p>In this graph, any single vertex <em>s</em> ∈{ 2 <em>,</em> 3 <em>,...,p</em> − 1 }forms a cut set, separating the graph into the past <em>P</em> ={ 1 <em>,...,s</em> − 1 }and the future <em>F</em> ={ <em>s</em> + 1 <em>,...,p</em> }. For these cut sets, the Markov property (9.3) translates into the fact that, for a Markov chain, the future <em>XF</em> is conditionally independent of the past <em>XP</em> given the present <em>Xs</em>. Of course, graphs with more structure have correspondingly more complex cut sets, and thus more interesting conditional-independence properties.</p><p><em>9.2.1.3 Equivalence of Factorization and Markov Properties</em></p><p>A remarkable fact, known as the Hammersley–Clifford theorem, is that for any strictly positive distribution (i.e., for whichP( <em>x</em> ) <em>&gt;</em> 0 for all <em>x</em> ∈X <em>p</em> ), the two characterizations are equivalent: namely, the distribution of <em>X</em> factorizes according to the graph <em>G</em> (as in Equation (9.1)) if and only if the random vector <em>X</em> is Markov with respect to the graph (as in Equation (9.3)). See the bibliographic section for further discussion of this celebrated theorem.</p><h6 id="_244-graphs-and-model-selection"><a class="header-anchor" href="#_244-graphs-and-model-selection" aria-hidden="true">#</a> 244 GRAPHS AND MODEL SELECTION</h6><h4 id="_9-2-2-some-examples"><a class="header-anchor" href="#_9-2-2-some-examples" aria-hidden="true">#</a> 9.2.2 Some Examples</h4><p>We present some examples to provide a concrete illustration of these proper- ties.</p><h5 id="_9-2-2-1-discrete-graphical-models"><a class="header-anchor" href="#_9-2-2-1-discrete-graphical-models" aria-hidden="true">#</a> 9.2.2.1 Discrete Graphical Models</h5><p>We begin by discussing the case of a discrete graphical model, in which random variables <em>Xs</em> at each vertex <em>s</em> ∈ <em>V</em> take values in a discrete spaceX <em>s</em>. The simplest example is the binary case, say withX <em>s</em> ={− 1 <em>,</em> +1}. Given a graph <em>G</em> = ( <em>V,E</em> ), one might consider the family of probability distributions</p><div class="language-"><pre><code>P θ ( x 1 ,...,xp ) = exp\n</code></pre></div><h6 id="∑-52"><a class="header-anchor" href="#∑-52" aria-hidden="true">#</a> {∑</h6><div class="language-"><pre><code>s ∈ V\n</code></pre></div><div class="language-"><pre><code>θsxs +\n</code></pre></div><h6 id="∑-53"><a class="header-anchor" href="#∑-53" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>( s,t )∈ E\n</code></pre></div><div class="language-"><pre><code>θstxsxt − A ( θ )\n</code></pre></div><h6 id="-344"><a class="header-anchor" href="#-344" aria-hidden="true">#</a> }</h6><h6 id="_9-4"><a class="header-anchor" href="#_9-4" aria-hidden="true">#</a> , (9.4)</h6><p>parametrized by the vector <em>θ</em> ∈R| <em>V</em> |+| <em>E</em> |. For later convenience, here we have introduced the notation <em>A</em> ( <em>θ</em> ) = log <em>Z</em> ( <em>θ</em> ), reflecting the dependence of the normalization constant on the parameter vector <em>θ</em>. This family of distributions is known as the <em>Ising model</em> , since it was originally used by Ising (1925) to model the behavior of magnetic materials; see the bibliographic section for further discussion. Figure 9.2 shows simulations from three different Ising models.</p><div class="language-"><pre><code>(a) (b) (c)\n</code></pre></div><p><strong>Figure 9.2</strong> <em>Samples generated from Ising models based on a graph withp</em> = 1024 <em>nodes. For il lustrative purposes, the resulting vectorx</em> ∈ {+1 <em>,</em> 1 }^1024 <em>is plotted as a</em> 32 × 32 <em>binary image. Panels (a) through (c) correspond to three very different distributions. The samples were drawn by running the Gibbs sampler.</em></p><p>The Ising model has been used to model social networks, for exam- ple the voting behavior of politicians. In this context, the random vector ( <em>X</em> 1 <em>,X</em> 2 <em>,...,Xp</em> ) represents the set of votes cast by a set of <em>p</em> politicians on a particular bill. We assume that politician <em>s</em> provides either a “yes” vote ( <em>Xs</em> = +1) or a “no” vote ( <em>Xs</em> =−1) on the bill. With the voting results for <em>N</em> bills, we can make inferences on the joint distribution of <em>X</em>. In the fac- torization (9.4), a parameter <em>θs&gt;</em> 0 indicates that politician <em>s</em> is more likely (assuming fixed values of other politicians’ votes) to vote “yes” on any given bill, with the opposite interpretation holding in the case <em>θs&lt;</em> 0. On the other</p><h6 id="basics-of-graphical-models-245"><a class="header-anchor" href="#basics-of-graphical-models-245" aria-hidden="true">#</a> BASICS OF GRAPHICAL MODELS 245</h6><p>hand, for any given pair <em>s,t</em> that are joined by an edge, a weight <em>θst&gt;</em> 0 means that with the behavior of all politicians held fixed, politicians <em>s</em> and <em>t</em> are more likely to share the same vote (i.e., both yes or both no) than to disagree; again, the opposite interpretation applies to the setting <em>θst&lt;</em> 0. See Figure 9.7 on page 257 for an application to voting-record data. Many extensions of the Ising model are possible. First, the factoriza- tion (9.4) is limited to cliques of size at most two (i.e., edges). By allowing terms over cliques of size up to size three, one obtains the family of models</p><div class="language-"><pre><code>P θ ( x ) = exp\n</code></pre></div><h6 id="∑-54"><a class="header-anchor" href="#∑-54" aria-hidden="true">#</a> {∑</h6><div class="language-"><pre><code>s ∈ V\n</code></pre></div><div class="language-"><pre><code>θsxs +\n</code></pre></div><h6 id="∑-55"><a class="header-anchor" href="#∑-55" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>( s,t )∈ E\n</code></pre></div><div class="language-"><pre><code>θstxsxt +\n</code></pre></div><h6 id="∑-56"><a class="header-anchor" href="#∑-56" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>( s,t,u )∈ E 3\n</code></pre></div><div class="language-"><pre><code>θstuxsxtxu − A ( θ )\n</code></pre></div><h6 id="-345"><a class="header-anchor" href="#-345" aria-hidden="true">#</a> }</h6><h6 id="_9-5"><a class="header-anchor" href="#_9-5" aria-hidden="true">#</a> (9.5)</h6><p>where <em>E</em> 3 is some subset of vertex triples. This factorization can be extended up to subsets of higher order, and in the limit (where we allow an interaction term among all <em>p</em> variables simultaneously), it is possible to specify the dis- tribution of any binary vector. In practice, of most interest are models based on relatively local interactions, as opposed to such a global interaction. Another extension of the Ising model is to allow for non-binary variables, for instance <em>Xs</em> ∈{ 0 <em>,</em> 1 <em>,</em> 2 <em>,...,m</em> − 1 }for some <em>m &gt;</em> 2. In this case, one might consider the family of distributions</p><div class="language-"><pre><code>P θ ( x 1 ,...,xp ) = exp\n</code></pre></div><h6 id="∑-57"><a class="header-anchor" href="#∑-57" aria-hidden="true">#</a> {∑</h6><div class="language-"><pre><code>s ∈ V\n</code></pre></div><div class="language-"><pre><code>m ∑− 1\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>θs ; j I[ xs = j ] +\n</code></pre></div><h6 id="∑-58"><a class="header-anchor" href="#∑-58" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>( s,t )∈ E\n</code></pre></div><div class="language-"><pre><code>θst I[ xs = xt ]− A ( θ )\n</code></pre></div><h6 id="-346"><a class="header-anchor" href="#-346" aria-hidden="true">#</a> }</h6><h6 id="-347"><a class="header-anchor" href="#-347" aria-hidden="true">#</a> ,</h6><h6 id="_9-6"><a class="header-anchor" href="#_9-6" aria-hidden="true">#</a> (9.6)</h6><p>where the indicator functionI[ <em>xs</em> = <em>j</em> ] takes the value 1 when <em>xs</em> = <em>j</em> , and 0 otherwise. When the weight <em>θst &gt;</em> 0, the edge-based indicator function I[ <em>xs</em> = <em>xt</em> ] acts as a smoothness prior, assigning higher weight to pairs ( <em>xs,xt</em> ) that agree. The model (9.6) has found many uses in computer vision, for instance in image denoising and disparity computation problems. All the models discussed here have a parallel life in the statistics and biostatistics literature, where they are referred to as log-linear models for multiway tables. However, in that setting the number of variables is typically quite small. In Section 9.4.3 we discuss a general class of pairwise-Markov models for mixed data—allowing for both continuous and discrete variables.</p><h5 id="_9-2-2-2-gaussian-graphical-models"><a class="header-anchor" href="#_9-2-2-2-gaussian-graphical-models" aria-hidden="true">#</a> 9.2.2.2 Gaussian Graphical Models</h5><p>Let <em>X</em> ∼ <em>N</em> ( <em>μ,</em> <strong>Σ</strong> ) be a Gaussian distribution in <em>p</em> dimensions, with mean vector <em>μ</em> ∈R <em>p</em> and covariance matrix <strong>Σ</strong> :</p><div class="language-"><pre><code>P μ, Σ ( x ) =\n</code></pre></div><h6 id="_1-136"><a class="header-anchor" href="#_1-136" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>(2 π )\n</code></pre></div><div class="language-"><pre><code>p\n</code></pre></div><p>(^2) det[ <strong>Σ</strong> ]^12 <em>e</em> − (^12) ( <em>x</em> − <em>μ</em> ) <em>T</em> <strong>Σ</strong> − (^1) ( <em>x</em> − <em>μ</em> )</p><p><em>.</em> (9.7)</p><h6 id="_246-graphs-and-model-selection"><a class="header-anchor" href="#_246-graphs-and-model-selection" aria-hidden="true">#</a> 246 GRAPHS AND MODEL SELECTION</h6><p>If we view the multivariate Gaussian distribution as a particular type of ex- ponential family, then ( <em>μ,</em> <strong>Σ</strong> ) are known as the mean <em>parameters</em> of the family. In order to represent the multivariate Gaussian as a graphical model, it is convenient to consider instead its parametrization in terms of the so-called canonical parameters, say a vector <em>γ</em> ∈R <em>p</em> and <strong>Θ</strong> ∈R <em>p</em> × <em>p</em>. Any nondegenerate multivariate Gaussian—meaning whenever <strong>Σ</strong> is strictly positive definite—can be represented in the form</p><div class="language-"><pre><code>P γ, Θ ( x ) = exp\n</code></pre></div><div class="language-"><pre><code>{∑ p\n</code></pre></div><div class="language-"><pre><code>s =1\n</code></pre></div><div class="language-"><pre><code>γsxs −\n</code></pre></div><h6 id="_1-137"><a class="header-anchor" href="#_1-137" aria-hidden="true">#</a> 1</h6><h6 id="_2-87"><a class="header-anchor" href="#_2-87" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>s,t =1\n</code></pre></div><div class="language-"><pre><code>θstxsxt − A ( Θ )\n</code></pre></div><h6 id="-348"><a class="header-anchor" href="#-348" aria-hidden="true">#</a> }</h6><h6 id="_9-8"><a class="header-anchor" href="#_9-8" aria-hidden="true">#</a> , (9.8)</h6><p>where <em>A</em> ( <strong>Θ</strong> ) =−^12 log det[ <strong>Θ</strong> <em>/</em> (2 <em>π</em> )], so that</p><h6 id="∫"><a class="header-anchor" href="#∫" aria-hidden="true">#</a> ∫</h6><p>P <em>γ,</em> <strong>Θ</strong> ( <em>x</em> ) <em>dx</em> = 1. Our choice of the rescaling by− 1 <em>/</em> 2 in the factorization (9.8) is to ensure that the matrix <strong>Θ</strong> has a concrete interpretation. In particular, with this scaling, as shown in Exercise 9.1, we have the relation <strong>Θ</strong> = <strong>Σ</strong> −^1 , so that <strong>Θ</strong> corresponds to the <em>inverse covariance, precision or concentration matrix</em>. The representation (9.8) is especially convenient, because it allows us to discuss the factorization properties directly in terms of the sparsity pattern of the precision matrix <strong>Θ</strong>. In particular, whenever <em>X</em> factorizes according to the graph <em>G</em> , then based on the factorization (9.8), we must have <strong>Θ</strong> <em>st</em> = 0 for</p><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>3\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>5\n</code></pre></div><div class="language-"><pre><code>1 2 3 4 5\n</code></pre></div><div class="language-"><pre><code>5\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>3\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>ZeropatternofΘ\n</code></pre></div><div class="language-"><pre><code>(a) (b)\n</code></pre></div><p><strong>Figure 9.3</strong> <em>(a) An undirected graphGon five vertices. (b) Associated sparsity pat- tern of the precision matrix</em> <strong>Θ</strong><em>. White squares correspond to zero entries.</em></p><p>any pair ( <em>s,t</em> )∈ <em>/E</em> , which sets up a correspondence between the zero pattern of <strong>Θ</strong> and the edge structure <em>E</em> of the underlying graph. See Figure 9.3 for an illustration of this correspondence.</p><h3 id="_9-3-graph-selection-via-penalized-likelihood"><a class="header-anchor" href="#_9-3-graph-selection-via-penalized-likelihood" aria-hidden="true">#</a> 9.3 Graph Selection via Penalized Likelihood</h3><p>We now turn to the problem of graph selection, and the use of <em>`</em> 1 -regularized likelihood methods for solving it. The problem itself is simply stated: suppose</p><h6 id="graph-selection-via-penalized-likelihood-247"><a class="header-anchor" href="#graph-selection-via-penalized-likelihood-247" aria-hidden="true">#</a> GRAPH SELECTION VIA PENALIZED LIKELIHOOD 247</h6><p>that we are given a collection <strong>X</strong> ={ <em>x</em> 1 <em>,...,xN</em> }of samples from a graphical model, but the underlying graph structure is unknown. How to use the data to select the correct graph with high probability? Here we discuss the use of likelihood-based methods in conjunction with <em><code>_ 1 -regularization for this pur- pose. This section discusses methods based on the global likelihood function of the graphical model. In the Gaussian case, this approach leads to tractable methods for model selection based on a log-determinant convex program with _</code></em> 1 -regularization. On the other hand, in the discrete case, this approach is computationally tractable only for relatively small graphs, or graphs with special structure.</p><h4 id="_9-3-1-global-likelihoods-for-gaussian-models"><a class="header-anchor" href="#_9-3-1-global-likelihoods-for-gaussian-models" aria-hidden="true">#</a> 9.3.1 Global Likelihoods for Gaussian Models</h4><p>We start with model selection for the Gaussian graphical model, a problem that is also known as covariance selection. Since our primary interest is in estimating the graph structure, we assume that the distribution has mean zero, so that under the parametrization (9.8), we need only consider the symmetric precision matrix <strong>Θ</strong> ∈R <em>p</em> × <em>p</em>. Suppose <strong>X</strong> represents samples from a zero-mean multivariate Gaussian with precision matrix <strong>Θ</strong>. Based on some straightforward algebra (see Ex- ercise 9.2 for details), it can be seen that (up to a constant) the rescaled log-likelihoodL( <strong>Θ</strong> ; <strong>X</strong> ) of the multivariate Gaussian takes the form</p><h6 id="l-θ-x"><a class="header-anchor" href="#l-θ-x" aria-hidden="true">#</a> L( Θ ; X ) =</h6><h6 id="_1-138"><a class="header-anchor" href="#_1-138" aria-hidden="true">#</a> 1</h6><h6 id="n-37"><a class="header-anchor" href="#n-37" aria-hidden="true">#</a> N</h6><h6 id="∑-n-102"><a class="header-anchor" href="#∑-n-102" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>logP Θ ( xi )\n</code></pre></div><div class="language-"><pre><code>= log det Θ −trace( SΘ ) , (9.9)\n</code></pre></div><p>where <strong>S</strong> = <em>N</em>^1</p><h6 id="∑-n-103"><a class="header-anchor" href="#∑-n-103" aria-hidden="true">#</a> ∑ N</h6><p><em>i</em> =1 <em>xix T i</em> is the empirical covariance matrix. The log-determ- inant function is defined on the space of symmetric matrices as</p><div class="language-"><pre><code>log det( Θ ) =\n</code></pre></div><h6 id="∑-59"><a class="header-anchor" href="#∑-59" aria-hidden="true">#</a> {∑</h6><div class="language-"><pre><code>p\nj =1log( λj ( Θ )) if Θ ^0\n−∞; otherwise,\n</code></pre></div><h6 id="_9-10"><a class="header-anchor" href="#_9-10" aria-hidden="true">#</a> (9.10)</h6><p>where <em>λj</em> ( <strong>Θ</strong> ) is the <em>jth</em> eigenvalue of <strong>Θ</strong>. In Exercise 9.2, we explore some ad- ditional properties of this function. The objective function (9.9) is an instance of a log-determinant program, a well-studied class of optimization problems. It is strictly concave, so that the maximum—when it is achieved—must be unique, and defines the maximum likelihood estimate <strong>Θ</strong> ̂ML, denoted MLE for short. By classical theory, the maximum likelihood estimate <strong>Θ</strong> ̂MLconverges to the true precision matrix as the sample size <em>N</em> tends to infinity. Thus, at least in principle, one could use a thresholded version of <strong>Θ</strong> ̂MLto specify an edge set, and thereby perform Gaussian graphical model selection. However, for prob- lems frequently arising in practice, the number of nodes <em>p</em> may be comparable to or larger than the sample size <em>N</em> , in which case the MLE does not exist.</p><h6 id="_248-graphs-and-model-selection"><a class="header-anchor" href="#_248-graphs-and-model-selection" aria-hidden="true">#</a> 248 GRAPHS AND MODEL SELECTION</h6><p>Indeed, the empirical correlation matrix <strong>S</strong> must be rank-degenerate whenever <em>N &lt; p</em> , which implies that the MLE fails to exist (see Exercise 9.2(c)). Hence we must consider suitably constrained or regularized forms of the MLE. More- over, irrespective of the sample size, we may be interested in constraining the estimated precision matrix to be sparse, and hence easier to interpret. If we are seeking Gaussian graphical models based on relatively sparse graphs, then it is desirable to control the number of edges, which can be measured by the <em>`</em> 0 -based quantity</p><div class="language-"><pre><code>ρ 0 ( Θ ) =\n</code></pre></div><h6 id="∑-60"><a class="header-anchor" href="#∑-60" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>s 6 = t\n</code></pre></div><div class="language-"><pre><code>I[ θst 6 = 0] , (9.11)\n</code></pre></div><p>whereI[ <em>θst</em> 6 = 0] is a 0-1-valued indicator function. Note that by construction, we have <em>ρ</em> 0 ( <strong>Θ</strong> ) = 2| <em>E</em> |, where| <em>E</em> |is the number of edges in the graph uniquely defined by <strong>Θ</strong>. We could then consider the optimization problem</p><div class="language-"><pre><code>Θ ̂∈arg max\nΘ  0\nρ 0 ( Θ )≤ k\n</code></pre></div><h6 id="-349"><a class="header-anchor" href="#-349" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>log det( Θ )−trace( SΘ )\n</code></pre></div><h6 id="-350"><a class="header-anchor" href="#-350" aria-hidden="true">#</a> }</h6><h6 id="_9-12"><a class="header-anchor" href="#_9-12" aria-hidden="true">#</a> . (9.12)</h6><p>Unfortunately, the <em>`</em> 0 -based constraint defines a highly nonconvex constraint</p><p>set, essentially formed as the union over all</p><div class="language-"><pre><code>(( p 2 )\nk\n</code></pre></div><h6 id="-351"><a class="header-anchor" href="#-351" aria-hidden="true">#</a> )</h6><p>possible subsets of <em>k</em> edges. For this reason, it is natural to consider the convex relaxation obtained by replacing the <em><code>_ 0 constraint with the corresponding _</code></em> 1 -based constraint. Doing so leads to the following convex program</p><div class="language-"><pre><code>Θ ̂∈arg max\nΘ  0\n</code></pre></div><h6 id="-352"><a class="header-anchor" href="#-352" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>log det Θ −trace( SΘ )− λρ 1 ( Θ )\n</code></pre></div><h6 id="-353"><a class="header-anchor" href="#-353" aria-hidden="true">#</a> }</h6><h6 id="_9-13"><a class="header-anchor" href="#_9-13" aria-hidden="true">#</a> , (9.13)</h6><p>where <em>ρ</em> 1 ( <strong>Θ</strong> ) =</p><h6 id="∑-61"><a class="header-anchor" href="#∑-61" aria-hidden="true">#</a> ∑</h6><p><em>s</em> 6 = <em>t</em> | <em>θst</em> |is simply the <em>`</em>^1 -norm of the off-diagonal entries of <strong>Θ</strong>. The problem (9.13) can be formulated as an instance of a log-determinant program; it is thus a convex program, often referred to as the <em>graphical lasso</em>. Since this is a convex program, one can use generic interior point methods for its solution, as in Vandenberghe, Boyd and Wu (1998). However this is not efficient for large problems. More natural are first-order block coordinate- descent approaches, introduced by d’Aspremont, Banerjee and El Ghaoui (2008) and refined by Friedman, Hastie and Tibshirani (2008). The latter authors call this the <em>graphical lasso</em> algorithm. It has a simple form which also connects it to the neighborhood-based regression approach, discussed in Section 9.4.</p><h4 id="_9-3-2-graphical-lasso-algorithm"><a class="header-anchor" href="#_9-3-2-graphical-lasso-algorithm" aria-hidden="true">#</a> 9.3.2 Graphical Lasso Algorithm</h4><p>The subgradient equation corresponding to (9.13) is</p><div class="language-"><pre><code>Θ −^1 − S − λ · Ψ = 0 , (9.14)\n</code></pre></div><p>with the symmetric matrix <strong>Ψ</strong> having diagonal elements zero, <em>ψjk</em> = sign( <em>θjk</em> ) if <em>θjk</em> 6 = 0, else <em>ψjk</em> ∈[− 1 <em>,</em> 1] if <em>θjk</em> = 0.</p><h6 id="graph-selection-via-penalized-likelihood-249"><a class="header-anchor" href="#graph-selection-via-penalized-likelihood-249" aria-hidden="true">#</a> GRAPH SELECTION VIA PENALIZED LIKELIHOOD 249</h6><p>We now consider solving this system by blockwise coordinate descent. To this end we consider partitioning all the matrices into one column versus the rest; for convenience we pick the last column:</p><h6 id="θ"><a class="header-anchor" href="#θ" aria-hidden="true">#</a> Θ =</h6><h6 id="-354"><a class="header-anchor" href="#-354" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>Θ 11 θ 12\nθ T 12 θ 22\n</code></pre></div><h6 id="-355"><a class="header-anchor" href="#-355" aria-hidden="true">#</a> ]</h6><h6 id="s"><a class="header-anchor" href="#s" aria-hidden="true">#</a> , S =</h6><h6 id="-356"><a class="header-anchor" href="#-356" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>S 11 s 12\ns T 12 s 22\n</code></pre></div><h6 id="-357"><a class="header-anchor" href="#-357" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>, etc. (9.15)\n</code></pre></div><p>Denote by <strong>W</strong> the current working version of <strong>Θ</strong> −^1 , with partitions as in (9.15). Then fixing all but the last row and column and using partitioned inverses, (9.14) leads to <strong>W</strong> 11 <strong><em>β</em></strong> − <strong>s</strong> 12 + <em>λ</em> · <strong><em>ψ</em></strong> 12 = <strong>0</strong> <em>,</em> (9.16)</p><p>where <strong><em>β</em></strong> =− <strong><em>θ</em></strong> 12 <em>/θ</em> 22. Here we have fixed the <em>pth</em> row and column: <strong>W</strong> 11 is the ( <em>p</em> −1)×( <em>p</em> −1) block of <strong>Θ</strong> −^1 , and <strong>s</strong> 12 and <strong><em>θ</em></strong> 12 are <em>p</em> −1 nondiagonal elements of the <em>pth</em> row and columns of <strong>S</strong> and <strong>Θ</strong>. Finally <em>θ</em> 22 is the <em>pth</em> diagonal element of <strong>Θ</strong>. These details are derived in Exercise 9.6.^1 It can be seen that (9.16) is equivalent to a modified version of the esti- mating equations for a lasso regression. Consider the usual regression setup with outcome variables <strong>y</strong> and predictor matrix <strong>Z</strong>. In that problem, the lasso minimizes 1 2 <em>N</em> ( <strong>y</strong> − <strong>Z</strong> <strong><em>β</em></strong> )</p><div class="language-"><pre><code>T ( y − Z β ) + λ ·‖ β ‖\n1 (9.17)\n</code></pre></div><p>The subgradient equations are</p><div class="language-"><pre><code>1\nN Z\n</code></pre></div><div class="language-"><pre><code>T Z β − 1\nN Z\n</code></pre></div><div class="language-"><pre><code>T y + λ ·sign( β ) = 0. (9.18)\n</code></pre></div><p>Comparing to (9.16), we see that <em>N</em>^1 <strong>Z</strong> <em>T</em> <strong>y</strong> is the analog of <strong>s</strong> 12 , and <em>N</em>^1 <strong>Z</strong> <em>T</em> <strong>Z</strong> cor- responds to <strong>W</strong> 11 , the estimated cross-product matrix from our current model. Thus we can solve each blockwise step (9.16) using a modified algorithm for the lasso, treating each variable as the response variable and the other <em>p</em> − 1 variables as the predictors. It is summarized in Algorithm 9.1. Friedman et al. (2008) use the pathwise-coordinate-descent approach to solve the modified lasso problems at each stage, and for a decreasing series of values for <em>λ</em>. [This corresponds to the “covariance” version of their lasso algorithm, as implemented in theglmnetpackage inRandmatlab(Friedman et al. 2010 <em>b</em> ).] From Equation (9.14) we see that the diagonal elements <em>wjj</em> of the solution matrix <strong>W</strong> are simply <em>sjj</em> , and these are fixed in Step 1 of Algorithm 9.1.^2</p><p>(^1) On a historical note, it turns out that this algorithm is <em>not</em> block-coordinate descent on <strong>Θ</strong> in (9.14) (as originally intended in Friedman et al. (2008)), but instead amounts to a block coordinate-descent step for the convex dual of problem (9.13). This is implied in Banerjee, El Ghaoui and d’Aspremont (2008), and is detailed in Mazumder and Hastie (2012). The dual variable is effectively <strong>W</strong> = <strong>Θ</strong> −^1. These latter authors derive alternative coordinate descent algorithms for the primal problem (see Exercise 9.7). In some cases, this gives better numerical performance than the original graphical lasso procedure. (^2) An alternative formulation of the problem can be posed, where we penalize the diagonal of <strong>Θ</strong> as well as the off-diagonal terms. Then the diagonal elements <em>wjj</em> of the solution matrix are <em>sjj</em> + <em>λ</em> , and the rest of the algorithm is unchanged.</p><h6 id="_250-graphs-and-model-selection"><a class="header-anchor" href="#_250-graphs-and-model-selection" aria-hidden="true">#</a> 250 GRAPHS AND MODEL SELECTION</h6><div class="language-"><pre><code>Algorithm 9.1 Graphical Lasso.\n</code></pre></div><ol><li>Initialize <strong>W</strong> = <strong>S</strong>. Note that the diagonal of <strong>W</strong> is unchanged in what follows.</li><li>Repeat for <em>j</em> = 1 <em>,</em> 2 <em>,...p,</em> 1 <em>,</em> 2 <em>,...p,...</em> until convergence:</li></ol><div class="language-"><pre><code>(a) Partition the matrix W into part 1: all but the jth row and column,\nand part 2: the jth row and column.\n(b) Solve the estimating equations W 11 β − s 12 + λ ·sign( β ) = 0 using a\ncyclical coordinate-descent algorithm for the modified lasso.\n(c) Update w 12 = W 11 β ˆ\n</code></pre></div><ol start="3"><li>In the final cycle (for each <em>j</em> ) solve forˆ <strong><em>θ</em></strong> 12 = − <strong><em>β</em></strong> ˆ· <em>θ</em> ˆ 22 , with 1 <em>/θ</em> ˆ 22 = <em>w</em> 22 − <strong>w</strong> <em>T</em> 12 <strong><em>β</em></strong> ˆ.</li></ol><div class="language-"><pre><code>The graphical lasso algorithm is fast, and can solve a moderately sparse\nproblem with 1000 nodes in less than a minute. It is easy to modify the algo-\nrithm to have edge-specific penalty parameters λjk ; note also that λjk =∞\nwill force θ ˆ jk to be zero.\nFigure 9.4 illustrates the path algorithm using a simple example. We gen-\nerated 20 observations from the model of Figure 9.3, with\n</code></pre></div><h6 id="θ-1"><a class="header-anchor" href="#θ-1" aria-hidden="true">#</a> Θ =</h6><h6 id="-----"><a class="header-anchor" href="#-----" aria-hidden="true">#</a>      </h6><h6 id="_2-0-6-0-0-0-5"><a class="header-anchor" href="#_2-0-6-0-0-0-5" aria-hidden="true">#</a> 2 0. 6 0 0 0. 5</h6><h6 id="_0-6-2-−-0-4-0-3-0"><a class="header-anchor" href="#_0-6-2-−-0-4-0-3-0" aria-hidden="true">#</a> 0. 6 2 − 0. 4 0. 3 0</h6><h6 id="_0-−-0-4-2-−-0-2-0"><a class="header-anchor" href="#_0-−-0-4-2-−-0-2-0" aria-hidden="true">#</a> 0 − 0. 4 2 − 0. 2 0</h6><h6 id="_0-0-3-−-0-2-2-0"><a class="header-anchor" href="#_0-0-3-−-0-2-2-0" aria-hidden="true">#</a> 0 0. 3 − 0. 2 2 0</h6><h6 id="_0-5-0-0-−-0-2-2"><a class="header-anchor" href="#_0-5-0-0-−-0-2-2" aria-hidden="true">#</a> 0. 5 0 0 − 0. 2 2</h6><h6 id="-----"><a class="header-anchor" href="#-----" aria-hidden="true">#</a>      </h6><h6 id="_9-19"><a class="header-anchor" href="#_9-19" aria-hidden="true">#</a> . (9.19)</h6><div class="language-"><pre><code>Shown are the graphical lasso estimates for a range of λ values, plotted against\nthe ` 1 norm of the solution Θ ˆ λ. The true values of Θ are indicated on the\nright. The solutions are spot on, but this is not surprising since the solution on\nthe right (no regularization) is S −^1 which equals Θ (Exercise 9.3). In the right\npanel, we have added independent Gaussian noise to each element of the data\nmatrix (with standard deviation 0. 05). Now we see that the estimate is not\nnearly as accurate; in fact, the nonzero support is never recovered correctly\nalong the path.\nSome further points about the Gaussian graphical model:\n</code></pre></div><ul><li>A simpler approach would be to use the observed covariance <strong>S</strong> 11 in place of <strong>W</strong> 11 : this requires just a single pass through the predictors, carrying out a lasso regression of each variable <em>Xj</em> on the others. This is called <em>neighbor-</em><em>hood selection</em> (Meinshausen and B ̈uhlmann 2006). Like the graphical lasso algorithm, this yields a consistent estimate of the support of <strong>Θ</strong> , but is not guaranteed to produce a positive definite estimate <strong>Θ</strong> ˆ. We discuss this in detail in Section 9.4.</li><li>If we pre-specify the zero pattern in <strong>Θ</strong> , we can use standard linear regres- sion in place of the lasso, leaving out the predictors that are supposed to</li></ul><h6 id="graph-selection-via-penalized-likelihood-251"><a class="header-anchor" href="#graph-selection-via-penalized-likelihood-251" aria-hidden="true">#</a> GRAPH SELECTION VIA PENALIZED LIKELIHOOD 251</h6><div class="language-"><pre><code>0 1 2 3 4\n</code></pre></div><div class="language-"><pre><code>−0.4 −0.2 0.0 0.2 0.4 0.6\n</code></pre></div><div class="language-"><pre><code>12\n15\n</code></pre></div><div class="language-"><pre><code>23\n</code></pre></div><div class="language-"><pre><code>24\n</code></pre></div><div class="language-"><pre><code>34\n</code></pre></div><div class="language-"><pre><code>35\n</code></pre></div><div class="language-"><pre><code>No Noise\n</code></pre></div><div class="language-"><pre><code>0 1 2 3 4 5\n</code></pre></div><div class="language-"><pre><code>−0.4 −0.2 0.0 0.2 0.4 0.6\n</code></pre></div><div class="language-"><pre><code>12\n</code></pre></div><div class="language-"><pre><code>13\n</code></pre></div><div class="language-"><pre><code>14\n</code></pre></div><div class="language-"><pre><code>15\n</code></pre></div><div class="language-"><pre><code>23\n</code></pre></div><div class="language-"><pre><code>24\n25\n</code></pre></div><div class="language-"><pre><code>34\n</code></pre></div><div class="language-"><pre><code>35\n45\n</code></pre></div><div class="language-"><pre><code>With Noise\n</code></pre></div><div class="language-"><pre><code>ℓ 1 Norm ℓ 1 Norm\n</code></pre></div><div class="language-"><pre><code>Estimated\n</code></pre></div><div class="language-"><pre><code>Θ\n</code></pre></div><div class="language-"><pre><code>Estimated\n</code></pre></div><div class="language-"><pre><code>Θ\n</code></pre></div><p><strong>Figure 9.4</strong> Left: <em>Profiles of estimates from the graphical lasso, for data simulated from the model of Figure 9.3. The actual values of</em> Θ <em>are achieved at the right end of the plot.</em> Right: <em>Same setup, except that standard Gaussian noise has been added to each column of the data. Nowhere along the path is the true edge set recovered.</em></p><div class="language-"><pre><code>have coefficients of zero. This provides a convenient way of computing the\nconstrained maximum likelihood estimate of Θ. Details are in Chapter 17\nof Hastie et al. (2009).\n</code></pre></div><h4 id="_9-3-3-exploiting-block-diagonal-structure"><a class="header-anchor" href="#_9-3-3-exploiting-block-diagonal-structure" aria-hidden="true">#</a> 9.3.3 Exploiting Block-Diagonal Structure</h4><p>If the inverse covariance matrix has a block diagonal structure</p><h6 id="θ-2"><a class="header-anchor" href="#θ-2" aria-hidden="true">#</a> Θ =</h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="θ-11-0-···-0"><a class="header-anchor" href="#θ-11-0-···-0" aria-hidden="true">#</a> Θ 11 0 ··· 0</h6><h6 id="_0-θ-22-···-0"><a class="header-anchor" href="#_0-θ-22-···-0" aria-hidden="true">#</a> 0 Θ 22 ··· 0</h6><h6 id="-358"><a class="header-anchor" href="#-358" aria-hidden="true">#</a> ..</h6><h6 id="-359"><a class="header-anchor" href="#-359" aria-hidden="true">#</a> .</h6><h6 id="-360"><a class="header-anchor" href="#-360" aria-hidden="true">#</a> ..</h6><h6 id="-361"><a class="header-anchor" href="#-361" aria-hidden="true">#</a> .</h6><h6 id="-362"><a class="header-anchor" href="#-362" aria-hidden="true">#</a> ..</h6><h6 id="-363"><a class="header-anchor" href="#-363" aria-hidden="true">#</a> .</h6><h6 id="-364"><a class="header-anchor" href="#-364" aria-hidden="true">#</a> ..</h6><h6 id="-365"><a class="header-anchor" href="#-365" aria-hidden="true">#</a> .</h6><div class="language-"><pre><code>0 0 ··· Θ kk\n</code></pre></div><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id=""><a class="header-anchor" href="#" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a> </h6><h6 id="_9-20"><a class="header-anchor" href="#_9-20" aria-hidden="true">#</a> , (9.20)</h6><p>for some ordering of the variables, then the graphical lasso problem can be solved separately for each block, and the solution is constructed from the individual solutions. This fact follows directly from the subgradient equations (9.14). It turns out that there is a very simple necessary and sufficient condition for a graphical-lasso solution to have such structure (Witten, Friedman and Simon 2011, Mazumder and Hastie 2012). Let <em>C</em> 1 <em>,C</em> 2 <em>,...CK</em> be a partition of the indices 1 <em>,</em> 2 <em>,...,p</em> of <strong>S</strong> into <em>K</em> blocks. Then the corresponding arrangement of <strong>Θ</strong> ˆ has this same block structure if and only if| <em>sii</em> ′| ≤ <em>λ</em> for all pairs ( <em>i,i</em> ′) not belonging to the same block. The proof is easy, by inspection of (9.14) and using the fact that the inverse of a block diagonal matrix has the same block-diagonal structure. This means that the elements of each block <em>Ck</em> are fully disconnected from elements of all other blocks.</p><h6 id="_252-graphs-and-model-selection"><a class="header-anchor" href="#_252-graphs-and-model-selection" aria-hidden="true">#</a> 252 GRAPHS AND MODEL SELECTION</h6><div class="language-"><pre><code>● ● ● ● ● ● ● ● ●\n</code></pre></div><div class="language-"><pre><code>1000 3000 5000 7000\n</code></pre></div><div class="language-"><pre><code>0.15\n</code></pre></div><div class="language-"><pre><code>0.20\n</code></pre></div><div class="language-"><pre><code>0.25\n</code></pre></div><div class="language-"><pre><code>0.30\n</code></pre></div><div class="language-"><pre><code>Sample Size\n</code></pre></div><div class="language-"><pre><code>Operator−Norm Error\n</code></pre></div><div class="language-"><pre><code>● ● ● ● ● ● ●\n</code></pre></div><div class="language-"><pre><code>● ●\n</code></pre></div><div class="language-"><pre><code>● ● ● ● ● ● ●\n</code></pre></div><div class="language-"><pre><code>● ●\n</code></pre></div><div class="language-"><pre><code>p=64\np=100\np=225\n</code></pre></div><div class="language-"><pre><code>● ● ● ● ● ●\n</code></pre></div><div class="language-"><pre><code>● ●\n●\n</code></pre></div><div class="language-"><pre><code>10 15 20 25 30\n</code></pre></div><div class="language-"><pre><code>0.10\n</code></pre></div><div class="language-"><pre><code>0.15\n</code></pre></div><div class="language-"><pre><code>0.20\n</code></pre></div><div class="language-"><pre><code>0.25\n</code></pre></div><div class="language-"><pre><code>0.30\n</code></pre></div><div class="language-"><pre><code>Rescaled Sample Size\n</code></pre></div><div class="language-"><pre><code>● ● ● ● ● ● ●\n</code></pre></div><div class="language-"><pre><code>● ●\n</code></pre></div><div class="language-"><pre><code>● ● ● ● ● ● ● ● ●\n</code></pre></div><p><strong>Figure 9.5</strong> <em>Plots of the operator-norm error</em> ‖ <strong>Θ</strong> ̂− <strong>Θ</strong> ∗‖ 2 <em>between the graphical lasso estimate</em> <strong>Θ</strong> ̂ <em>and the true inverse covariance matrix.</em> Left: <em>plotted versus the raw sample sizeN, for three different graph sizesp</em> ∈{ 64 <em>,</em> 100 <em>,</em> 225 }<em>. Note how the curves shift to the right as the graph sizepincreases, reflecting the fact that larger graphs require more samples for consistent estimation.</em> Right: <em>the same operator-norm error curves plotted versus the</em> rescaled <em>sample sized</em> 2 <em>N</em> log <em>pfor three different graph sizes p</em> ∈{ 64 <em>,</em> 100 <em>,</em> 225 }<em>. As predicted by theory, the curves are now quite wel l-aligned.</em></p><p>This fact can be exploited to provide a substantial speedup in computa- tion, by first identifying the disconnected components and then solving the subproblems in each block. The number of blocks is monotone in <em>λ</em>. This means, for example, that solutions can be found for very large problems (that perhaps could not be solved in general), as long as <em>λ</em> is sufficiently large.</p><h4 id="_9-3-4-theoretical-guarantees-for-the-graphical-lasso"><a class="header-anchor" href="#_9-3-4-theoretical-guarantees-for-the-graphical-lasso" aria-hidden="true">#</a> 9.3.4 Theoretical Guarantees for the Graphical Lasso</h4><p>In order to explore the behavior of graphical lasso or log-determinant method, we carried out a series of simulations. For a given graph type with <em>p</em> nodes and specified covariance matrix, we generated a set of <em>N</em> zero-mean multivariate Gaussian samples, and used them to form the empirical covariance matrix <strong>S</strong>. We solved the graphical lasso program using the regularization parameter</p><p><em>λN</em> = 2</p><h6 id="√-26"><a class="header-anchor" href="#√-26" aria-hidden="true">#</a> √</h6><p>log <em>p N</em> (Ravikumar, Wainwright, Raskutti and Yu 2011, as suggested by theory) and plot the operator norm error‖ <strong>Θ</strong> ̂− <strong>Θ</strong> ∗‖ 2 versus the sample size <em>N</em>. Figure 9.5 (left) shows such plots for three different dimensions <em>p</em> ∈ { 64 <em>,</em> 100 <em>,</em> 225 }of two-dimensional grid graphs, in which each node has degree four. For each graph size, we generated an inverse covariance matrix <strong>Θ</strong> ∗∈</p><h6 id="graph-selection-via-penalized-likelihood-253"><a class="header-anchor" href="#graph-selection-via-penalized-likelihood-253" aria-hidden="true">#</a> GRAPH SELECTION VIA PENALIZED LIKELIHOOD 253</h6><p>R <em>p</em> × <em>p</em> with entries</p><div class="language-"><pre><code>θst ∗=\n</code></pre></div><h6 id="-46"><a class="header-anchor" href="#-46" aria-hidden="true">#</a> </h6><h6 id="-6"><a class="header-anchor" href="#-6" aria-hidden="true">#</a> </h6><h6 id="-6"><a class="header-anchor" href="#-6" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>1 if s = t ,\n0. 2 if| s − t |= 1, and\n0 otherwise\n</code></pre></div><p>The plots in the left panel show that the graphical lasso is a consistent pro- cedure for estimating the inverse covariance <strong>Θ</strong> ∗in operator norm, since the error curves converge to zero as <em>N</em> increases. Comparing the curves for differ- ent graph sizes, we see a rightward-upward shift in the error curves, reflecting the fact that larger graphs require more samples to achieve the same error tolerance. It is known that the solution <strong>Θ</strong> ̂to the graphical lasso satisfies, with high probability, the error bound</p><h6 id="‖-θ-̂−-θ-∗‖-2"><a class="header-anchor" href="#‖-θ-̂−-θ-∗‖-2" aria-hidden="true">#</a> ‖ Θ ̂− Θ ∗‖ 2.</h6><h6 id="√-27"><a class="header-anchor" href="#√-27" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>d^2 log p\nN\n</code></pre></div><h6 id="_9-21"><a class="header-anchor" href="#_9-21" aria-hidden="true">#</a> , (9.21)</h6><p>where <em>d</em> is the maximum degree of any node in the graph, and.denotes inequality up to constant terms. (See the bibliographic notes on page 261 for details). If this theoretical prediction were sharp, one would expect that the same error curves—if replotted versus the <em>rescaled sample sized</em> 2 <em>N</em> log <em>p</em> —should be relatively well-aligned. The right panel in Figure 9.5 provides empirical confirmation of this prediction. We note that there are also theoretical results that guarantee that, as long as <em>N</em> = Ω( <em>d</em>^2 log <em>p</em> ), the support set <strong>Θ</strong> ̂ of the graphical lasso estimate coincides with the support set of <strong>Θ</strong> ∗. Thus, the graphical lasso also succeeds in recovering the true graph structure.</p><h4 id="_9-3-5-global-likelihood-for-discrete-models"><a class="header-anchor" href="#_9-3-5-global-likelihood-for-discrete-models" aria-hidden="true">#</a> 9.3.5 Global Likelihood for Discrete Models</h4><p>In principle, one could imagine adopting the <em>`</em> 1 -regularized version of the global likelihood to the problem of graph selection in models with discrete variables. A major challenge here is that the partition functions <em>A</em> in (9.4)– (9.6)—in sharp contrast to the multivariate Gaussian case—are difficult to compute in general. For instance, in the case of the Ising model (9.4), it takes the form</p><div class="language-"><pre><code>A ( θ ) = log\n</code></pre></div><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="∑-62"><a class="header-anchor" href="#∑-62" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>x ∈{− 1 , +1} p\n</code></pre></div><div class="language-"><pre><code>exp\n</code></pre></div><h6 id="-47"><a class="header-anchor" href="#-47" aria-hidden="true">#</a> </h6><h6 id="-40"><a class="header-anchor" href="#-40" aria-hidden="true">#</a> </h6><h6 id="-40"><a class="header-anchor" href="#-40" aria-hidden="true">#</a> </h6><h6 id="∑-63"><a class="header-anchor" href="#∑-63" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>s ∈ V\n</code></pre></div><div class="language-"><pre><code>θsxs +\n</code></pre></div><h6 id="∑-64"><a class="header-anchor" href="#∑-64" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>( s,t )∈ E\n</code></pre></div><div class="language-"><pre><code>θstxsxt\n</code></pre></div><h6 id="-41"><a class="header-anchor" href="#-41" aria-hidden="true">#</a> </h6><h6 id="-40"><a class="header-anchor" href="#-40" aria-hidden="true">#</a> </h6><h6 id="-40"><a class="header-anchor" href="#-40" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> </h6><h6 id="-2"><a class="header-anchor" href="#-2" aria-hidden="true">#</a> .</h6><p>Thus, a brute force approach is intractable for large <em>p</em> , since it involves a summation over 2 <em>p</em> terms. With the exception of some special cases, computing the value of <em>A</em> ( <em>θ</em> ) is computationally intractable in general. There are various approaches that can be taken to approximate or bound the partition function (see the bibliographic discussion for some references). However, these methods are somewhat off the main course of development</p><h6 id="_254-graphs-and-model-selection"><a class="header-anchor" href="#_254-graphs-and-model-selection" aria-hidden="true">#</a> 254 GRAPHS AND MODEL SELECTION</h6><p>in this chapter. Instead, the next section moves away from global likelihoods toward the idea of conditional or pseudo-likelihoods. These approaches can be used both for Gaussian and discrete variable models, and are computationally tractable, meaning polynomial-time in both sample size and graph size in either case.</p><h3 id="_9-4-graph-selection-via-conditional-inference"><a class="header-anchor" href="#_9-4-graph-selection-via-conditional-inference" aria-hidden="true">#</a> 9.4 Graph Selection via Conditional Inference</h3><p>An alternative approach to graph selection is based on the idea of neighborhood-based likelihood, or products of such quantities that are known as pseudo-likelihoods. Both of these methods focus on conditional distribu- tions, which for many situations are tractable. For a given vertex <em>s</em> ∈ <em>V</em> , we use</p><div class="language-"><pre><code>X \\{ s }={ Xt,t ∈ V \\{ s }}∈R p −^1.\n</code></pre></div><p>to denote the collection of all other random variables in the graph. Now con- sider the distribution of <em>Xs</em> given the random vector <em>X</em> { <em>s</em> }. By the conditional</p><div class="language-"><pre><code>s\n</code></pre></div><div class="language-"><pre><code>t 1\nt 2\n</code></pre></div><div class="language-"><pre><code>t 3\n</code></pre></div><div class="language-"><pre><code>t 4\n</code></pre></div><div class="language-"><pre><code>t 5\n</code></pre></div><p><strong>Figure 9.6</strong> <em>The dark blue vertices form the neighborhood set</em> N( <em>s</em> ) <em>of vertexs(drawn in red); the set</em> N+( <em>s</em> ) <em>is given by the union</em> N( <em>s</em> )∪{ <em>s</em> }<em>. Note that</em> N( <em>s</em> ) <em>is a cut set in the graph that separates</em> { <em>s</em> } <em>fromV</em> \\N+( <em>s</em> )<em>. Consequently, the variableXsis conditionally independent ofXV</em> \\N+( <em>s</em> ) <em>given the variablesX</em> N( <em>s</em> ) <em>in the neighborhood set. This conditional independence implies that the optimal predictor ofXsbased on al l other variables in the graph depends only onX</em> N( <em>s</em> )<em>.</em></p><p>independence properties of any undirected graphical model (see Section 9.2), the only relevant variables to this conditioning are those in the <em>neighborhood set</em></p><div class="language-"><pre><code>N( s ) =\n</code></pre></div><h6 id="-366"><a class="header-anchor" href="#-366" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>t ∈ V |( s,t )∈ E\n</code></pre></div><h6 id="-367"><a class="header-anchor" href="#-367" aria-hidden="true">#</a> }</h6><h6 id="_9-22"><a class="header-anchor" href="#_9-22" aria-hidden="true">#</a> . (9.22)</h6><p>Indeed, as shown in Figure 9.6, the setN( <em>s</em> ) is a cut set that separates{ <em>s</em> } from the remaining vertices <em>V</em> \\N+( <em>s</em> ), where we defineN+( <em>s</em> ) =N( <em>s</em> )∪</p><h6 id="graph-selection-via-conditional-inference-255"><a class="header-anchor" href="#graph-selection-via-conditional-inference-255" aria-hidden="true">#</a> GRAPH SELECTION VIA CONDITIONAL INFERENCE 255</h6><p>{ <em>s</em> }. Consequently, we are guaranteed that the variable <em>Xs</em> is conditionally independent of <em>XV</em> \\N+( <em>s</em> )given the variables <em>X</em> N( <em>s</em> ), or equivalently that</p><div class="language-"><pre><code>( Xs | X \\{ s })\nd\n= ( Xs | X N( s )). (9.23)\n</code></pre></div><p>How can this conditional independence (CI) property be exploited in the context of graph selection? If we consider the problem of predicting the value of <em>Xs</em> based on <em>XV</em> { <em>s</em> }, then by the CI property, the best predictor can be specified as a function of only <em>X</em> N( <em>s</em> ). Consequently, the problem of finding the neighborhood can be tackled by solving a prediction problem, as detailed next.</p><h4 id="_9-4-1-neighborhood-based-likelihood-for-gaussians"><a class="header-anchor" href="#_9-4-1-neighborhood-based-likelihood-for-gaussians" aria-hidden="true">#</a> 9.4.1 Neighborhood-Based Likelihood for Gaussians</h4><p>Let us first develop this approach for the multivariate Gaussian. In this case, the conditional distribution of <em>Xs</em> given <em>X</em> { <em>s</em> }is also Gaussian, so that <em>Xs</em> can be decomposed into the best linear prediction based on <em>X</em> { <em>s</em> }and an error term—namely</p><div class="language-"><pre><code>Xs = X \\{ Ts } βs + W \\{ s }. (9.24)\n</code></pre></div><p>In this decomposition, <em>W</em> { <em>s</em> } is zero-mean Gaussian variable with Var( <em>W</em> { <em>s</em> }) = Var( <em>Xs</em> | <em>X</em> { <em>s</em> }), corresponding to the prediction error, and is independent of <em>X</em> { <em>s</em> }. So the dependence is captured entirely by the linear regression coefficients <em>βs</em> , which are a scalar multiple of <em>θs</em> , the corresponding subvector of <strong>Θ</strong> in (9.8) in Section 9.2.2 (Exercise 9.4). The decomposition (9.24) shows that in the multivariate Gaussian case, the prediction problem reduces to a linear regression of <em>Xs</em> on <em>X</em> { <em>s</em> }. The key property here—as shown in Exercise 9.4—is that the regression vector <em>βs</em> satisfies supp( <em>βs</em> ) =N( <em>s</em> ). If the graph is relatively sparse—meaning that the degree|N( <em>s</em> )|of node <em>s</em> is small relative to <em>p</em> —then it is natural to consider estimating <em>βs</em> via the lasso. This leads to the following neighborhood-based approach to Gaussian graphical model selection, based on a set of samples <strong>X</strong> ={ <em>x</em> 1 <em>,...,xN</em> }. In step 1(a), <em>xi,V</em> { <em>s</em> }represents the ( <em>p</em> −1) dimensional subvector of the <em>p</em> - vector <em>xi</em> , omitting the <em>sth</em> component. To clarify step 2, the AND rule declares that edge ( <em>s,t</em> ) belongs to the estimated edge set <em>E</em> ̂if and only <em>s</em> ∈N̂( <em>t</em> ) <em>andt</em> ∈N̂( <em>s</em> ). On the other hand, the OR rule is less conservative, allowing ( <em>s,t</em> )∈ <em>E</em> ̂if either <em>s</em> ∈N̂( <em>t</em> ) <em>ort</em> ∈N̂( <em>s</em> ). An advantage of neighborhood models is speed. Many efficient implemen- tations of the lasso are available, and the <em>p</em> regression problems can be solved independently of each other, and hence in parallel. The AND/OR rules can be avoided by using a joint estimation approach using the pseudo-likelihood, which is essentially the sum of the log-likelihoods in (9.25). In this case we would enforce the symmetry of <strong>Θ</strong>. While more ele- gant, this does incur a small additional computational cost (Friedman, Hastie</p><h6 id="_256-graphs-and-model-selection"><a class="header-anchor" href="#_256-graphs-and-model-selection" aria-hidden="true">#</a> 256 GRAPHS AND MODEL SELECTION</h6><div class="language-"><pre><code>Algorithm 9.2 Neighborhood-based graph selection for Gaussian\ngraphical models.\n</code></pre></div><ol><li>For each vertex <em>s</em> = 1 <em>,</em> 2 <em>,...,p</em> :</li></ol><div class="language-"><pre><code>(a) Apply the lasso to solve the neighborhood prediction problem:\n</code></pre></div><div class="language-"><pre><code>β ̂ s ∈arg min\nβs ∈R p −^1\n</code></pre></div><h6 id="-368"><a class="header-anchor" href="#-368" aria-hidden="true">#</a> {</h6><h6 id="_1-139"><a class="header-anchor" href="#_1-139" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-17"><a class="header-anchor" href="#_2-n-17" aria-hidden="true">#</a> 2 N</h6><h6 id="∑-n-104"><a class="header-anchor" href="#∑-n-104" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><h6 id="-369"><a class="header-anchor" href="#-369" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>xis − xTi,V \\{ s } βs )^2 + λ ‖ βs ‖ 1\n</code></pre></div><h6 id="-370"><a class="header-anchor" href="#-370" aria-hidden="true">#</a> }</h6><h6 id="_9-25"><a class="header-anchor" href="#_9-25" aria-hidden="true">#</a> . (9.25)</h6><div class="language-"><pre><code>(b) Compute the estimateN̂( s ) = supp( β ̂ s ) of the neighborhood setN( s ).\n</code></pre></div><ol start="2"><li>Combine the neighborhood estimates{N̂( <em>s</em> ) <em>,s</em> ∈ <em>V</em> }via the AND or OR rule to form a graph estimate <em>G</em> ̂= ( <em>V,E</em> ̂).</li></ol><div class="language-"><pre><code>and Tibshirani 2010 a ). It also produces an estimate Θ ̂, and not just its graph.\nWe discuss such approaches in more generality in Section 9.4.3.\n</code></pre></div><h4 id="_9-4-2-neighborhood-based-likelihood-for-discrete-models"><a class="header-anchor" href="#_9-4-2-neighborhood-based-likelihood-for-discrete-models" aria-hidden="true">#</a> 9.4.2 Neighborhood-Based Likelihood for Discrete Models</h4><div class="language-"><pre><code>The idea of a neighborhood-based likelihood is not limited to Gaussian mod-\nels, but can also be applied to other types of graphical models that can be\nwritten in the exponential family form. In fact, given that global likelihood\ncalculations are intractable for discrete graphical models, the neighborhood-\nbased approach is especially attractive in the discrete case, at least from the\ncomputational perspective.\nThe simplest type of discrete graphical model is the Ising model (9.4),\nused to model a collection of variables ( X 1 ,X 2 ,...,Xp )∈ {− 1 , +1} p that\ninteract in a pairwise manner. In this case, as we explore in Exercise 9.5, the\nconditional log-odds for the probability of Xs given XV \\{ s }takes the form^3\n</code></pre></div><div class="language-"><pre><code>ηθs ( X \\{ s }) = log\n</code></pre></div><h6 id="-371"><a class="header-anchor" href="#-371" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>P( Xs = +1| X \\{ s })\nP( Xs =− 1 | X \\{ s })\n</code></pre></div><h6 id="-372"><a class="header-anchor" href="#-372" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>= 2 θs +\n</code></pre></div><h6 id="∑-65"><a class="header-anchor" href="#∑-65" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>t ∈ V \\{ s }\n</code></pre></div><div class="language-"><pre><code>2 θstXt, (9.26)\n</code></pre></div><div class="language-"><pre><code>with θs = [ θs, { θst } t ∈ V \\{ s }]. Consequently, the neighborhood-based approach\nfor the Ising model has the same form as Algorithm 9.2, with the ordinary\nlasso in step 1(a) replaced by the lasso logistic regression problem\n</code></pre></div><div class="language-"><pre><code>θ ̂ s ∈arg min\nθs ∈R p\n</code></pre></div><h6 id="-48"><a class="header-anchor" href="#-48" aria-hidden="true">#</a> </h6><h6 id="-41"><a class="header-anchor" href="#-41" aria-hidden="true">#</a> </h6><h6 id="-41"><a class="header-anchor" href="#-41" aria-hidden="true">#</a> </h6><h6 id="_1-140"><a class="header-anchor" href="#_1-140" aria-hidden="true">#</a> 1</h6><h6 id="n-38"><a class="header-anchor" href="#n-38" aria-hidden="true">#</a> N</h6><h6 id="∑-n-105"><a class="header-anchor" href="#∑-n-105" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>` [ xis,ηθs ( xi, \\{ s })] + λ\n</code></pre></div><h6 id="∑-66"><a class="header-anchor" href="#∑-66" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>t ∈ V \\{ s }\n</code></pre></div><div class="language-"><pre><code>| θst |\n</code></pre></div><h6 id="-42"><a class="header-anchor" href="#-42" aria-hidden="true">#</a> </h6><h6 id="-41"><a class="header-anchor" href="#-41" aria-hidden="true">#</a> </h6><h6 id="-41"><a class="header-anchor" href="#-41" aria-hidden="true">#</a> </h6><h6 id="_9-27"><a class="header-anchor" href="#_9-27" aria-hidden="true">#</a> , (9.27)</h6><p>(^3) the factor of 2 comes from the particular response coding +1/-1 rather than the tradi- tional 0/1.</p><h6 id="graph-selection-via-conditional-inference-257"><a class="header-anchor" href="#graph-selection-via-conditional-inference-257" aria-hidden="true">#</a> GRAPH SELECTION VIA CONDITIONAL INFERENCE 257</h6><div class="language-"><pre><code>Jeffords\n</code></pre></div><div class="language-"><pre><code>Grassley\n</code></pre></div><div class="language-"><pre><code>Hagel\n</code></pre></div><div class="language-"><pre><code>Thomas\nMcCain Sununu Bunning Sessions\nWarner\nKyl\n</code></pre></div><div class="language-"><pre><code>Kennedy\n</code></pre></div><div class="language-"><pre><code>Reid\n</code></pre></div><div class="language-"><pre><code>Kohl\nClinton\nRockefeller\nReed Kerry Lieberman\n</code></pre></div><div class="language-"><pre><code>Biden\n</code></pre></div><div class="language-"><pre><code>Salazar\n</code></pre></div><div class="language-"><pre><code>Cantwell\n</code></pre></div><div class="language-"><pre><code>Jeffords\n</code></pre></div><div class="language-"><pre><code>Grassley\n</code></pre></div><div class="language-"><pre><code>Hagel\n</code></pre></div><div class="language-"><pre><code>Thomas\nMcCain Sununu Bunning Sessions\nWarner\nKyl\n</code></pre></div><div class="language-"><pre><code>Kennedy\n</code></pre></div><div class="language-"><pre><code>Reid\n</code></pre></div><div class="language-"><pre><code>Kohl\nClinton\nRockefeller\nReed Kerry Lieberman\n</code></pre></div><div class="language-"><pre><code>Biden\n</code></pre></div><div class="language-"><pre><code>Salazar\n</code></pre></div><div class="language-"><pre><code>Cantwell\n</code></pre></div><div class="language-"><pre><code>(a) (b)\n</code></pre></div><p><strong>Figure 9.7</strong> <em>Politician networks estimated from voting records of U.S. Senate (2004– 2006). A total ofN</em> = 546 <em>votes were col lected for each ofp</em> = 100 <em>senators, with Xs</em> = +1 <em>(respectivelyXs</em> =− 1 <em>) meaning that senatorsvoted “yes” (respectively “no”). A pairwise graphical model was fit to the dataset using the neighborhood- based logistic regression approach. (a) A subgraph of</em> 55 <em>senators from the fitted graph, with Democratic/Republican/Independent senators coded as blue/red/yel low nodes, respectively. Note that the subgraph shows a strong bipartite tendency with clustering within party lines. A few senators show cross-party connections. (b) A smal ler subgraph of the same social network.</em></p><p>where <em><code>_ is the negative log-likelihood for the binomial distribution. This prob- lem is again a convex program, and any algorithm for _</code></em> 1 -penalized logistic regression can be used, such as the coordinate-descent procedure discussed in Chapter 5. As in the Gaussian case, rules have to be used to enforce symmetry in the edge calls. Hoefling and Tibshirani (2009) present a pseudo-likelihood method for this problem which imposes the symmetry. It can be thought of as intermediate to the exact, but computationally intractable, global likelihood approach and the neighborhood-based method described above. We cover their approach in Section 9.4.3. Figure 9.7 shows the results of fitting an Ising model to represent the politician social network for the U.S. Senate, reconstructed on the basis of voting records. Details are given in the figure caption; overall we see strong clustering within each party. The neighborhood-based approach for graphical model selection can be shown to be consistent under relatively mild conditions on the sample size (see the bibliographic section for references and further discussion). In the case of the Ising model, let <em>G</em> ̂denote the output of the neighborhood-based approach using logistic regression. It is known that <em>N</em> = Ω( <em>d</em>^2 log <em>p</em> ) samples</p><h6 id="_258-graphs-and-model-selection"><a class="header-anchor" href="#_258-graphs-and-model-selection" aria-hidden="true">#</a> 258 GRAPHS AND MODEL SELECTION</h6><div class="language-"><pre><code>● ● ●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>● ●\n● ● ● ●\n</code></pre></div><div class="language-"><pre><code>0 200 400 600 800 1200\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>Sample Size\n</code></pre></div><div class="language-"><pre><code>Probability of Correct Edge Detection\n</code></pre></div><div class="language-"><pre><code>●●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●●\n</code></pre></div><div class="language-"><pre><code>●●●●\n</code></pre></div><div class="language-"><pre><code>●●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●●\n</code></pre></div><div class="language-"><pre><code>●●●●\n</code></pre></div><div class="language-"><pre><code>p=64\np=100\np=225 ● ●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n● ● ● ● ●\n</code></pre></div><div class="language-"><pre><code>2 4 6 8 10\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>Rescaled Sample Size\n</code></pre></div><div class="language-"><pre><code>● ●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>● ●\n</code></pre></div><div class="language-"><pre><code>● ● ● ●\n</code></pre></div><div class="language-"><pre><code>● ● ●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>●\n</code></pre></div><div class="language-"><pre><code>● ●\n● ● ● ●\n</code></pre></div><p><strong>Figure 9.8</strong> <em>Plots of the probability</em> P[ <em>G</em> ̂= <em>G</em> ] <em>of correctly recovering the graph versus the sample size. The neighborhood-based logistic regression procedure was applied to recover a star-shaped graph (central hub with spokes) withpvertices and hub degree d</em> =d 0_._ 1 <em>p</em> e_. (a) Plotted versus the raw sample sizeN. As would be expected, larger graphs require more samples for consistent estimation. (b) The same simulation results plotted versus the rescaled sample sized_ log <em>Np.</em></p><p>are sufficient to ensure that <em>G</em> ̂= <em>G</em> with high probability. Figure 9.8 illustrates the sufficiency of this condition in practice, when the method is applied to a star-shaped graph with <em>p</em> nodes, in which a central hub node is connected to <em>d</em> =d 0_._ 1 <em>p</em> espoke nodes. (The remaining nodes in the graph are discon- nected from the hub-spoke subgraph.) We implemented the neighborhood- based logistic-regression method for graphs with <em>p</em> ∈ { 64 <em>,</em> 100 <em>,</em> 225 }nodes, using the AND rule to combine the neighborhoods so as to form the graph estimate. In panel (a) of Figure 9.8, we plot the probabilityP[ <em>G</em> ̂= <em>G</em> ] of correctly recovering the unknown graph versus the sample size <em>N</em> , with each curve corresponding to a different graph size as labelled. Each of these plots show that the method is model-selection consistent, in that as the sample size <em>N</em> increases, the probabilityP[ <em>G</em> ̂= <em>G</em> ] converges to one. Naturally, the tran- sition from failure to success occurs later (at larger sample sizes) for larger graphs, reflecting that the problem is more difficult. Panel (b) shows the same simulation results plotted versus the rescaled sample size <em>d</em> log <em>Np</em> ; on this new axis, all three curves are now well-aligned. This simulation confirms that the theoretical scaling <em>N</em> = Ω( <em>d</em>^2 log <em>p</em> ) is sufficient to ensure successful graph re- covery, but not necessary for this class of graphs. However, there are other classes of graphs for which this scaling is both sufficient and necessary (see the bibliographic section for details).</p><h6 id="graph-selection-via-conditional-inference-259"><a class="header-anchor" href="#graph-selection-via-conditional-inference-259" aria-hidden="true">#</a> GRAPH SELECTION VIA CONDITIONAL INFERENCE 259</h6><h4 id="_9-4-3-pseudo-likelihood-for-mixed-models"><a class="header-anchor" href="#_9-4-3-pseudo-likelihood-for-mixed-models" aria-hidden="true">#</a> 9.4.3 Pseudo-Likelihood for Mixed Models</h4><p>So far we have covered graphical models for all continuous variables (the Gaussian graphical model), and models for all binary variables (the Ising model). These do not cover other frequently occurring situations:</p><ul><li>discrete variables with more than two states;</li><li>mixed data types: i.e., some continuous and some discrete.</li></ul><p>In this section, we extend the models covered so far to include these cases, and demonstrate an approach for inference based on the pseudo-likelihood. A simple generalization of the Gaussian and Ising models is the pairwise Markov random field model. For convenience in notation, we denote the <em>p</em> continuous variables by <em>X</em> and the <em>q</em> discrete variables by <em>Y</em>. The density P <strong>Ω</strong> ( <em>x,y</em> ) is proportional to</p><div class="language-"><pre><code>exp\n</code></pre></div><h6 id="-49"><a class="header-anchor" href="#-49" aria-hidden="true">#</a> </h6><h6 id="-42"><a class="header-anchor" href="#-42" aria-hidden="true">#</a> </h6><h6 id="-42"><a class="header-anchor" href="#-42" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>s =1\n</code></pre></div><div class="language-"><pre><code>γsxs −\n</code></pre></div><h6 id="_1-141"><a class="header-anchor" href="#_1-141" aria-hidden="true">#</a> 1</h6><h6 id="_2-88"><a class="header-anchor" href="#_2-88" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>s =1\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>t =1\n</code></pre></div><div class="language-"><pre><code>θstxsxt +\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>s =1\n</code></pre></div><div class="language-"><pre><code>∑ q\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>ρsj [ yj ] xs +\n</code></pre></div><div class="language-"><pre><code>∑ q\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>∑ q\n</code></pre></div><div class="language-"><pre><code>r =1\n</code></pre></div><div class="language-"><pre><code>ψ jr [ yj,yr ]\n</code></pre></div><h6 id="-43"><a class="header-anchor" href="#-43" aria-hidden="true">#</a> </h6><h6 id="-42"><a class="header-anchor" href="#-42" aria-hidden="true">#</a> </h6><h6 id="-42"><a class="header-anchor" href="#-42" aria-hidden="true">#</a> </h6><h6 id="-373"><a class="header-anchor" href="#-373" aria-hidden="true">#</a> .</h6><h6 id="_9-28"><a class="header-anchor" href="#_9-28" aria-hidden="true">#</a> (9.28)</h6><p>The first two terms are as in the Gaussian graphical model (9.8). The term <em>ρsj</em> represents an edge between continuous <em>Xs</em> and discrete <em>Yj</em>. If <em>Yj</em> has <em>Lj</em> possible states or levels, then <em>ρsj</em> is a vector of <em>Lj</em> parameters, and <em>ρsj</em> [ <em>yj</em> ] references the <em>yjth</em> value. Likewise <strong><em>ψ</em></strong> <em>jr</em> will be an <em>Lj</em> × <em>Lr</em> matrix representing an edge between discrete <em>Yj</em> and <em>Yr</em> , and <strong><em>ψ</em></strong> <em>jr</em> [ <em>yj,yr</em> ] references the element in row <em>yj</em> and column <em>yr</em>. The terms <strong><em>ψ</em></strong> <em>jj</em> will be diagonal, and represent the <em>node potentials</em> (they correspond to the <em>θs</em> in the Ising model (9.4) on page 244). The matrix <strong>Ω</strong> represents the entire collection of parameters. Needless to say, the partition function is typically intractable, except in very low-dimensional cases. Here the pseudo-likelihood is attractive: it is the product of the <em>p</em> + <em>q</em> conditional likelihoods, and each of these are simple (Exercise 9.8), depending on the type of response:</p><p><em>Continuous:</em> The conditional distribution for each of the <em>p</em> continuous vari- ables is Gaussian, with mean linear in the conditioning variables.</p><div class="language-"><pre><code>P( Xs | X \\{ s } ,Y ; Ω ) =\n</code></pre></div><h6 id="-374"><a class="header-anchor" href="#-374" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>θss\n2 π\n</code></pre></div><h6 id="_12"><a class="header-anchor" href="#_12" aria-hidden="true">#</a> )^12</h6><div class="language-"><pre><code>e\n</code></pre></div><div class="language-"><pre><code>− θss 2\n</code></pre></div><h6 id="-375"><a class="header-anchor" href="#-375" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Xs −\n</code></pre></div><div class="language-"><pre><code>γs +\n</code></pre></div><h6 id="∑-67"><a class="header-anchor" href="#∑-67" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>jρsj [ Yj ]−\n</code></pre></div><h6 id="∑-68"><a class="header-anchor" href="#∑-68" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>t 6 = sθstXt\nθss\n</code></pre></div><h6 id="_2-89"><a class="header-anchor" href="#_2-89" aria-hidden="true">#</a> ) 2</h6><h6 id="_9-29"><a class="header-anchor" href="#_9-29" aria-hidden="true">#</a> (9.29)</h6><div class="language-"><pre><code>The contributions of the discrete conditioning variables on the right-hand\nside are different additive constants, as for qualitative factors in linear re-\ngression models; i.e., a constant for each level, determined by the ρsj.\n</code></pre></div><p><em>Discrete:</em> The conditional distribution for each of the <em>q</em> discrete variables is multinomial, with log-odds linear in the conditioning variables.</p><div class="language-"><pre><code>P( Yj | X,Y \\{ j }; Ω ) =\ne\nψ jj [ Yj,Yj ]+\n</code></pre></div><h6 id="∑-69"><a class="header-anchor" href="#∑-69" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>sρsj [ Yj ] Xs +\n</code></pre></div><h6 id="∑-70"><a class="header-anchor" href="#∑-70" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>r 6 = j ψ [ Yj,Yr ]\n∑ Lj\n` =1 e\n</code></pre></div><div class="language-"><pre><code>ψ jj [ `,` ]+\n</code></pre></div><h6 id="∑-71"><a class="header-anchor" href="#∑-71" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>sρsj [ ` ] Xs +\n</code></pre></div><h6 id="∑-72"><a class="header-anchor" href="#∑-72" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>r 6 = j ψ [ `,Yr ]\n</code></pre></div><h6 id="_9-30"><a class="header-anchor" href="#_9-30" aria-hidden="true">#</a> (9.30)</h6><h6 id="_260-graphs-and-model-selection"><a class="header-anchor" href="#_260-graphs-and-model-selection" aria-hidden="true">#</a> 260 GRAPHS AND MODEL SELECTION</h6><div class="language-"><pre><code>X 1\n</code></pre></div><div class="language-"><pre><code>X 2\n</code></pre></div><div class="language-"><pre><code>X 3\n</code></pre></div><div class="language-"><pre><code>Y 1\n</code></pre></div><div class="language-"><pre><code>Y 2\n</code></pre></div><p><strong>Figure 9.9</strong> <em>Mixed graphical model, with three continuous and two discrete variables. Y</em> 1 <em>has two states, andY</em> 3 <em>has three. The diagram on the right shows the sets of pa- rameters associated with each edge. The group lasso treats these as sets, and includes or excludes them al l together.</em></p><div class="language-"><pre><code>With this notation, the pseudo-log-likelihood is defined to be\n</code></pre></div><div class="language-"><pre><code>`p ( Ω ; X , Y ) =\n</code></pre></div><h6 id="∑-n-106"><a class="header-anchor" href="#∑-n-106" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>s =1\n</code></pre></div><div class="language-"><pre><code>logP( xis | xi \\{ s } ,yi ; Ω )\n</code></pre></div><div class="language-"><pre><code>∑ q\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>logP( yij | xi,yi \\{ j }; Ω )\n</code></pre></div><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> .</h6><h6 id="_9-31"><a class="header-anchor" href="#_9-31" aria-hidden="true">#</a> (9.31)</h6><p>One can show that (9.31) is a concave function of <strong>Ω</strong>. Notice that each of the parameters appears twice: once when one of the indices corresponds to the response, and again when that index refers to a conditioning variable. The edge parameters are now of three kinds: scalars <em>θst</em> , vectors <em>ρsj</em> , and matrices <strong><em>ψ</em></strong> <em>jr</em>. Figure 9.9 illustrates with a small example. Lee and Hastie (2014) use the group-lasso to select these different parameter types; they pro- pose optimizing the penalized pseudo-log-likelihood</p><div class="language-"><pre><code>`p ( Ω ; X , Y )− λ\n</code></pre></div><h6 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> </h6><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>s =1\n</code></pre></div><div class="language-"><pre><code>∑ s −^1\n</code></pre></div><div class="language-"><pre><code>t =1\n</code></pre></div><div class="language-"><pre><code>| θst |+\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>s =1\n</code></pre></div><div class="language-"><pre><code>∑ q\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>‖ ρsj ‖ 2 +\n</code></pre></div><div class="language-"><pre><code>∑ q\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>j ∑− 1\n</code></pre></div><div class="language-"><pre><code>r =1\n</code></pre></div><div class="language-"><pre><code>‖ ψ jr ‖ 2\n</code></pre></div><h6 id="-6"><a class="header-anchor" href="#-6" aria-hidden="true">#</a> </h6><h6 id="-9-32"><a class="header-anchor" href="#-9-32" aria-hidden="true">#</a>  (9.32)</h6><p>w.r.t. the parameters <strong>Ω</strong>. Notice that not all the parameters are penalized. In particular the diagonal of <strong>Θ</strong> is left alone (as in the graphical lasso algorithm), as are each of the node-potentials <strong><em>ψ</em></strong> <em>jj</em>. This imposes interesting constraints on some of the parameter estimates, which are explored in Exercise 9.9. With only continuous variables, this is exactly the penalized pseudo- likelihood for the Gaussian graphical model. For all binary variables, one can show that this is equivalent to the lasso-penalized pseudo-likelihood for the Ising model (see Hoefling and Tibshirani (2009) and Exercise 9.10). Block coordinate descent is attractive here, since each of the components has well-studied solutions. However, the parameters are shared, and so care must be taken to respect this symmetry. Lee and Hastie (2014) use a proximal- Newton algorithm.</p><h6 id="graphical-models-with-hidden-variables-261"><a class="header-anchor" href="#graphical-models-with-hidden-variables-261" aria-hidden="true">#</a> GRAPHICAL MODELS WITH HIDDEN VARIABLES 261</h6><h3 id="_9-5-graphical-models-with-hidden-variables"><a class="header-anchor" href="#_9-5-graphical-models-with-hidden-variables" aria-hidden="true">#</a> 9.5 Graphical Models with Hidden Variables</h3><p>Chandrasekaran et al. (2012) propose a method for undirected graphical mod- els, in which some of the variables are unobserved (or “latent”). Suppose for example that we are modelling stock prices in a certain sector, and they are heavily dependent on the price of energy, the latter being unmeasured in our data. Then the concentration matrix of the stock prices will not look sparse in our data, but may instead be sparse if we could condition on energy price. Let the covariance matrix of all variables—observed and unobserved—be <strong>Σ</strong>. The sub-block of <strong>Σ</strong> corresponding to the observed variables is <strong>Σ</strong> <em>O</em>. Let <strong>K</strong> = <strong>Σ</strong> −^1 be the concentration matrix for the set of observed and hidden variables, with sub-matrices <strong>K</strong> <em>O,</em> <strong>K</strong> <em>O,H,</em> <strong>K</strong> <em>H,O</em> and <strong>K</strong> <em>H</em>. These capture the dependencies among the observed variables, between the observed and hidden variables, and among the hidden variables, respectively. Making use of the partitioned-inverse formulas, we get the following expression for the concen- tration matrix of the observed variables:</p><div class="language-"><pre><code>K ̃ O = Σ −^1\nO = K O − K O,H K\n</code></pre></div><div class="language-"><pre><code>− 1\nH K H,O (9.33)\n</code></pre></div><p>Here <strong>K</strong> <em>O</em> is the concentration matrix of the conditional statistics of the ob- served variables given the hidden variables. Now <strong>K</strong> ̃ <em>O</em> may not be sparse, but if the graphical model for all variables (observed and hidden) has few edges then <strong>K</strong> <em>O</em> will be sparse. Motivated by the form (9.33), letting <strong>K</strong> <em>O</em> = <strong>Θ</strong> the idea is to write</p><div class="language-"><pre><code>K ̃ O = Θ − L , (9.34)\n</code></pre></div><p>where <strong>L</strong> is assumed to be low rank, with the rank at most the number of hidden variables. We then solve the problem</p><div class="language-"><pre><code>minimize\nΘ , L\n</code></pre></div><h6 id="-376"><a class="header-anchor" href="#-376" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>trace[ S ( Θ − L )]−log[det( Θ − L )] + λ ‖ Θ ‖ 1 + trace( L )\n</code></pre></div><h6 id="-377"><a class="header-anchor" href="#-377" aria-hidden="true">#</a> }</h6><h6 id="_9-35"><a class="header-anchor" href="#_9-35" aria-hidden="true">#</a> (9.35)</h6><p>over the set{ <strong>Θ</strong> − <strong>L</strong>  <strong>0</strong> <em>,</em> <strong>L</strong>  <strong>0</strong> }. Like the graphical lasso, this is again a convex problem. This relates to the “sparse plus low rank” idea discussed in Mazumder et al. (2010) and Chandrasekaran et al. (2011). Ma, Xue and Zou (2013) propose first-order alternating direction-of-multipliers (ADMM) techniques for this problem, and compare them to second order methods. Some details are given in Exercise 9.11.</p><h3 id="bibliographic-notes-6"><a class="header-anchor" href="#bibliographic-notes-6" aria-hidden="true">#</a> Bibliographic Notes</h3><p>Detailed discussion of graphical models can be found in Whittaker (1990), Lauritzen (1996), Cox and Wermuth (1996), Edwards (2000), Pearl (2000), Anderson (2003), and Koller and Friedman (2009). The Hammersley–Clifford theorem was first announced in the unpublished note of Hammersley and Clifford (1971). Independent proofs were given by Be- sag (1974) and Grimmett (1973), the latter proof using the M ̈oebius inversion</p><h6 id="_262-graphs-and-model-selection"><a class="header-anchor" href="#_262-graphs-and-model-selection" aria-hidden="true">#</a> 262 GRAPHS AND MODEL SELECTION</h6><p>formula. See Clifford (1990) for some historical discussion and context of the result. Welsh (1993) discusses the computational intractability of evaluating the partition function for general discrete graphical models. For graphs with spe- cial structure, exact computation of the cumulant function is possible in poly- nomial time. Examples include graphs with low tree width, for which the junction tree algorithm can be applied (Lauritzen and Spiegelhalter 1988, Lauritzen 1996), and certain classes of planar models (Kastelyn 1963, Fisher 1966). For other special cases, there are rapidly mixing Markov chains that can be used to obtain good approximations to the cumulant function (Jerrum and Sinclair 1993, Jerrum and Sinclair 1996, for example, and the references therein). A complementary approach is provided by the class of variational methods, which provide approximations to the cumulant generating func- tion (e.g., see the monograph by Wainwright and Jordan (2008) and refer- ences therein). Examples include the mean-field algorithm, the sum-product or belief-propagation algorithm, expectation propagation, as well as various other convex relaxations. For certain graphs, particularly those that are “lo- cally tree-like,” there are various kinds of asymptotic exactness results (e.g., see the book by M ́ezard and Montanari (2008) and references therein). Gaussian graphical models are used for modelling gene expression data (Dobra, Hans, Jones, Nevins, Yao and West 2004), and other genomic and proteomic assays. The Ising model (9.4) was first proposed in the con- text of statistical physics by Ising (1925). In more recent work, it and re- lated models have been used as simple models for binary images (Geman and Geman 1984, Greig, Porteous and Seheuly 1989, Winkler 1995), voting be- havior in politicians (Banerjee et al. 2008), citation network analysis (Zhao, Levina and Zhu 2011). Some of the methods discussed in this chapter for undirected models can be used to aid in the more difficult model search for directed graphical models; see for example Schmidt, Niculescu-Mizil and Murphy (2007). The paper by Vandenberghe et al. (1998) provides an introduction to the problem of deter- minant maximization with constraints; the Gaussian MLE (with or without regularization) is a special case of this class of problems. Yuan and Lin (2007 <em>a</em> ) proposed the use of <em>`</em> 1 -regularization in conjunction with the Gaussian (log- determinant) likelihood for the covariance-selection problem, and used inte- rior point methods (Vandenberghe et al. 1998) to solve it. d’Aspremont et al. (2008) and Friedman et al. (2008) develop faster coordinate descent algo- rithms for solving the graphical lasso (9.13), based on solving a sequence of subproblems. Mazumder and Hastie (2012) offer variants on these algorithms with better convergence properties. Witten et al. (2011) and Mazumder and Hastie (2012) show how to exploit block-diagonal structure in <strong>S</strong> in computing graphical-lasso solutions. Rothman, Bickel, Levina and Zhu (2008) established consistency of the estimator in Frobenius norm, whereas Ravikumar et al. (2011) provide some results on model selection consistency as well as rates</p><h6 id="exercises-263"><a class="header-anchor" href="#exercises-263" aria-hidden="true">#</a> EXERCISES 263</h6><p>for operator norm. In particular, they proved the operator-norm bound (9.21) illustrated in Figure 9.5. The idea of pseudo-likelihood itself is quite old, dating back to the semi- nal work of Besag (1975). Meinshausen and B ̈uhlmann (2006) were the first to propose and develop the lasso-based neighborhood selection for Gaussian graphical models, and to derive consistency results under high-dimensional scaling; see also the papers by Zhao and Yu (2006) and Wainwright (2009) for related results on static graphs. Zhou, Lafferty and Wasserman (2008) con- sider the problem of tracking a time-varying sequence of Gaussian graphical models. Ravikumar, Wainwright and Lafferty (2010) proposed <em><code>_ 1 -regularized lo- gistic regression for model selection in discrete binary graphical models, and showed that it is model-selection consistent under the scaling _N_ = Ω( _d_^3 log _p_ ). Subsequent analysis by Bento and Montanari (2009) improved this scal- ing to _N_ = Ω( _d_^2 log _p_ ) for Ising models below the phase transition. Koh et al. (2007) develop an interior-point algorithm suitable for large-scale _</code></em> 1 - regularized logistic regression. Instead of solving separate a logistic regression problem at each node, Hoefling and Tibshirani (2009) propose minimization of the <em>`</em> 1 -regularized pseudo-likelihood, and derive efficient algorithms for it; see also Friedman et al. (2010 <em>a</em> ). Santhanam and Wainwright (2008) derive information-theoretic lower bounds on Ising model selection, showing that no method can succeed more than half the time if <em>N</em> =O( <em>d</em>^2 log <em>p</em> ). This shows that the neighborhood approach is an optimal procedure up to constant fac- tors. Cheng, Levina and Zhu (2013) and Lee and Hastie (2014) discuss mixed graphical models, involving both continuous and discrete variables. Kalisch and B ̈uhlmann (2007) show that a variant of the PC algorithm can be used for high-dimensional model selection in directed graphs. A different kind of graphical model is the <em>covariance graph</em> or <em>relevance network</em> , in which vertices are connected by bidirectional edges if the covari- ance (rather than the partial covariance) between the corresponding variables is nonzero. These are popular in genomics; see for example Butte, Tamayo, Slonim, Golub and Kohane (2000). The negative log-likelihood from these models is not convex, making the computations more challenging (Chaudhuri, Drton and Richardson 2007). Recent progress on this problem has been made by Bien and Tibshirani (2011) and Wang (2014). The latter paper derives a blockwise coordinate descent algorithm analogous to the to the graphical lasso procedure. Some theoretical study of the estimation of large covariance matrices is given by Bickel and Levina (2008) and El Karoui (2008).</p><p><strong>Exercises</strong></p><p>Ex. 9.1 The most familiar parametrization of the multivariate Gaussian is in terms of its mean vector <em>μ</em> ∈R <em>p</em> and covariance matrix <strong>Σ</strong> ∈R <em>p</em> × <em>p</em>. Assuming that the distribution is nondegenerate (i.e., <strong>Σ</strong> is strictly positive definite),</p><h6 id="_264-graphs-and-model-selection"><a class="header-anchor" href="#_264-graphs-and-model-selection" aria-hidden="true">#</a> 264 GRAPHS AND MODEL SELECTION</h6><p>show that the canonical parameters ( <em>γ,</em> <strong>Θ</strong> )∈R <em>p</em> × S+ <em>p</em> from the factoriza- tion (9.8) are related by</p><div class="language-"><pre><code>μ =− Θ −^1 γ, and Σ = Θ −^1. (9.36)\n</code></pre></div><p>Ex. 9.2 Let{ <em>x</em> 1 <em>,...,xN</em> }be <em>N</em> i.i.d. samples from a Gaussian graphical model,</p><p>and letL( <strong>Θ</strong> ; <strong>X</strong> ) = <em>N</em>^1</p><h6 id="∑-n-107"><a class="header-anchor" href="#∑-n-107" aria-hidden="true">#</a> ∑ N</h6><p><em>i</em> =1logP <strong>Θ</strong> ( <em>xi</em> ) denote the rescaled log-likelihood of the sample.</p><div class="language-"><pre><code>(a) Show that\nL( Θ ; X ) = log det Θ −trace( SΘ ) + C,\n</code></pre></div><div class="language-"><pre><code>where S = N^1\n</code></pre></div><h6 id="∑-n-108"><a class="header-anchor" href="#∑-n-108" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1 xix\n</code></pre></div><div class="language-"><pre><code>T\ni is the empirical covariance matrix, and C is a\nconstant independent of Θ.\n(b) Show that the function f ( Θ ) =−log det Θ is a strictly convex function\non the cone of positive definite matrices. Prove that∇ f ( Θ ) = Θ −^1 for any\nΘ ∈S+ p.\n(c) The (unregularized) Gaussian MLE is given by\n</code></pre></div><div class="language-"><pre><code>Θ ̂∈arg max\nΘ ∈S+ p\n</code></pre></div><h6 id="-378"><a class="header-anchor" href="#-378" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>log det Θ −trace( SΘ )\n</code></pre></div><h6 id="-379"><a class="header-anchor" href="#-379" aria-hidden="true">#</a> }</h6><h6 id="-380"><a class="header-anchor" href="#-380" aria-hidden="true">#</a> ,</h6><div class="language-"><pre><code>when this maximum is attained. Assuming that the maximum is attained,\nshow that Θ ̂= S −^1. Discuss what happens when N &lt; p.\n(d) Now consider the graphical lasso (9.13), based on augmenting the rescaled\nlog-likelihood with an ` 1 -regularizer. Derive the Karush–Kuhn–Tucker equa-\ntions that any primal-optimal pair ( Θ ̂ , W ̂)∈S p +×R p × p must satisfy.\n(e) Derive the dual program associated with the graphical lasso. Can you\ngeneralize your result to regularization with any `q -norm, for q ∈[1 , ∞]?\n</code></pre></div><p>Ex. 9.3 Show that if <strong>S</strong> is positive definite, the graphical lasso algorithm with <em>λ</em> = 0 computes <strong>Θ</strong> ̂= <strong>S</strong> −^1.</p><p>Ex. 9.4 In this exercise, we explore properties of jointly Gaussian random vectors that guarantee Fisher consistency of the neighborhood-based lasso approach to covariance selection. Let ( <em>X</em> 1 <em>,X</em> 2 <em>,...,Xp</em> ) be a zero-mean jointly Gaussian random vector with positive definite covariance matrix <strong>Σ</strong>. Letting <em>T</em> ={ 2 <em>,</em> 3 <em>,...,p</em> }, consider the conditioned random variable <em>Z</em> = ( <em>X</em> 1 | <em>XT</em> ).</p><div class="language-"><pre><code>(a) Show that there is a vector θ ∈R p −^1 such that\n</code></pre></div><div class="language-"><pre><code>Z = θTXT + W,\n</code></pre></div><div class="language-"><pre><code>where W is a zero-mean Gaussian variable independent of XT. Hint: con-\nsider the best linear predictor of X 1 given XT.\n(b) Show that θ = Σ − TT^1 Σ T 1 , where Σ T 1 ∈R p −^1 is the vector of covariances\nbetween X 1 and XT.\n</code></pre></div><h6 id="exercises-265"><a class="header-anchor" href="#exercises-265" aria-hidden="true">#</a> EXERCISES 265</h6><div class="language-"><pre><code>(c) Show that θj = 0 if and only if j / ∈N(1). Hint: The following elementary\nfact could be useful: let A be an invertible matrix, given in the block-\npartitioned form\n</code></pre></div><h6 id="a"><a class="header-anchor" href="#a" aria-hidden="true">#</a> A =</h6><h6 id="-381"><a class="header-anchor" href="#-381" aria-hidden="true">#</a> [</h6><h6 id="a-11-a-12"><a class="header-anchor" href="#a-11-a-12" aria-hidden="true">#</a> A 11 A 12</h6><h6 id="a-21-a-22"><a class="header-anchor" href="#a-21-a-22" aria-hidden="true">#</a> A 21 A 22</h6><h6 id="-382"><a class="header-anchor" href="#-382" aria-hidden="true">#</a> ]</h6><h6 id="-383"><a class="header-anchor" href="#-383" aria-hidden="true">#</a> .</h6><div class="language-"><pre><code>Then letting B = A −^1 , we have B 12 = A − 111 A 12\n</code></pre></div><h6 id="-384"><a class="header-anchor" href="#-384" aria-hidden="true">#</a> [</h6><h6 id="a-21-a-−-111-a-12-−-a-22"><a class="header-anchor" href="#a-21-a-−-111-a-12-−-a-22" aria-hidden="true">#</a> A 21 A − 111 A 12 − A 22</h6><h6 id="−-1-4"><a class="header-anchor" href="#−-1-4" aria-hidden="true">#</a> ]− 1</h6><div class="language-"><pre><code>(Horn and Johnson 1985, for example).\n</code></pre></div><p>Ex. 9.5 Consider the neighborhood-based likelihood approach for selection of Ising models.</p><div class="language-"><pre><code>(a) Derive the conditional distributionP( xs | xV \\{ s }; θ ), and show how the\nneighborhood-prediction reduces to logistic regression.\n(b) Verify that the method is Fisher-consistent, meaning that the true con-\nditional distribution is the population minimizer.\n</code></pre></div><p>Ex. 9.6 Here we show how, in expression (9.14), we can solve for <strong>Θ</strong> and its inverse <strong>W</strong> = <strong>Θ</strong> −^1 one row and column at a time. For simplicity let’s focus on the last row and column. Then the upper right block of Equation (9.14) can be written as <strong>w</strong> 12 − <strong>s</strong> 12 − <em>λ</em> ·sign( <strong><em>θ</em></strong> 12 ) = 0_._ (9.37)</p><p>Here we have partitioned the matrices into two parts: part 1 being the first <em>p</em> −1 rows and columns, and part 2 the <em>pth</em> row and column. With <strong>W</strong> and its inverse <strong>Θ</strong> partitioned in a similar fashion ( <strong>W</strong> 11 <strong>w</strong> 12 <strong>w</strong> <em>T</em> 12 <em>w</em> 22</p><h6 id="-385"><a class="header-anchor" href="#-385" aria-hidden="true">#</a> )(</h6><div class="language-"><pre><code>Θ 11 θ 12\nθ T 12 θ 22\n</code></pre></div><h6 id="-386"><a class="header-anchor" href="#-386" aria-hidden="true">#</a> )</h6><h6 id="-387"><a class="header-anchor" href="#-387" aria-hidden="true">#</a> =</h6><h6 id="-388"><a class="header-anchor" href="#-388" aria-hidden="true">#</a> (</h6><h6 id="i-0"><a class="header-anchor" href="#i-0" aria-hidden="true">#</a> I 0</h6><h6 id="_0-t-1"><a class="header-anchor" href="#_0-t-1" aria-hidden="true">#</a> 0 T 1</h6><h6 id="-389"><a class="header-anchor" href="#-389" aria-hidden="true">#</a> )</h6><h6 id="_9-38"><a class="header-anchor" href="#_9-38" aria-hidden="true">#</a> , (9.38)</h6><p>show that</p><div class="language-"><pre><code>w 12 = − W 11 θ 12 /θ 22 (9.39)\n= W 11 β (9.40)\n</code></pre></div><p>where <strong><em>β</em></strong> =− <strong><em>θ</em></strong> 12 <em>/θ</em> 22. This is obtained from the formula for the inverse of a partitioned inverse of a matrix (Horn and Johnson 1985, for example). Sub- stituting (9.40) into (9.37) gives</p><div class="language-"><pre><code>W 11 β − s 12 + λ ·sign( β ) = 0. (9.41)\n</code></pre></div><p>Ex. 9.7 With the partitioning as in (9.38), write down the expressions for the partitioned inverses of each matrix in terms of the other. Show that since <strong>W</strong> 11 depends on <strong><em>θ</em></strong> 12 , we are not really holding <strong>W</strong> 11 <em>fixed</em> as assumed in the graphical lasso Algorithm 9.1.</p><div class="language-"><pre><code>(a) Show that as an alternative we can write (9.37) as\n</code></pre></div><div class="language-"><pre><code>Θ − 111 θ 12 w 22 + s 12 + λ sign( θ 12 ) = 0. (9.42)\n</code></pre></div><h6 id="_266-graphs-and-model-selection"><a class="header-anchor" href="#_266-graphs-and-model-selection" aria-hidden="true">#</a> 266 GRAPHS AND MODEL SELECTION</h6><div class="language-"><pre><code>(b) Show how to use the solution θ 12 to update the current version of W and\nΘ ̂in O ( p^2 ) operations.\n(c) Likewise, show how to move to a new block of equations in O ( p^2 ) opera-\ntions.\n(d) You have derived a primal graphical lasso algorithm. Write it down in\nalgorithmic form, as in Algorithm 9.1\n</code></pre></div><p>(Mazumder and Hastie 2012)</p><p>Ex. 9.8 Derive the conditional distributions (9.29) and (9.30) for the mixed graphical model.</p><p>Ex. 9.9 Close inspection of the pairwise Markov random field model (9.28) will show that it is overparametrized with respect to the discrete potentials <em>ρsj</em> and <strong><em>ψ</em></strong> <em>jr</em>. This exercise shows that this aliasing is resolved by the quadratic penalties in the penalized pseudo-likelihood, in the form of “sum-to-zero” con- straints familiar in regression and ANOVA modelling. Consider the penalized pseudo log-likelihood (9.32), with <em>λ &gt;</em> 0. (a) Since the <em>γs</em> are not penalized, show that the solution ˆ <em>ρsj</em> for any <em>s</em> and <em>j</em> satisfies ∑ <em>Lj</em></p><div class="language-"><pre><code>` =1\n</code></pre></div><div class="language-"><pre><code>ρ ˆ sj [ ` ] = 0.\n</code></pre></div><div class="language-"><pre><code>(b) Since the (diagonal) matrices ψ jj are not penalized, show that the solu-\ntion ψ ˆ jr for any j 6 = r satisfies\n</code></pre></div><div class="language-"><pre><code>∑ Lj\n</code></pre></div><div class="language-"><pre><code>` =1\n</code></pre></div><div class="language-"><pre><code>ψ ˆ jr [ `,m ] = 0 , m = 1 ,...,Lr ; (9.43)\n</code></pre></div><div class="language-"><pre><code>∑ Lr\n</code></pre></div><div class="language-"><pre><code>m =1\n</code></pre></div><div class="language-"><pre><code>ψ ˆ jr [ `,m ] = 0 , ` = 1 ,...,Lj. (9.44)\n</code></pre></div><p>Ex. 9.10 Consider the pairwise Markov random field model with only binary discrete variables. This appears to be different from the Ising model, since we have four parameters per edge. Use Exercise 9.9 to show that with the quadratic constraints in (9.32), it is exactly equivalent to a lasso-penalized pseudo log-likelihood for the Ising model.</p><p>Ex. 9.11 Consider the objective function (9.35) for the graphical model that al- lows for latent variables. Defining a new variable <strong>R</strong> = <strong>Θ</strong> − <strong>L</strong> , derive the details of the steps of an ADMM algorithm for solving (9.35) using the augmented Lagrangian</p><p><em>Lμ</em> ( <strong>R</strong> <em>,</em> <strong>Θ</strong> 0 <em>,</em> <strong>L</strong> <em>,</em> <strong>Γ</strong> ) = trace( <strong>SR</strong> )−log det <strong>R</strong> + <em>λ</em> ‖ <strong>Θ</strong> ‖ 1 + <em>β</em> ·trace( <strong>L</strong> )</p><div class="language-"><pre><code>+ I ( L  0 )−trace[ Γ ( R − Θ + L )] +\n</code></pre></div><h6 id="_1-142"><a class="header-anchor" href="#_1-142" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>2 μ\n</code></pre></div><h6 id="‖-r-−-θ-l-‖-2-f"><a class="header-anchor" href="#‖-r-−-θ-l-‖-2-f" aria-hidden="true">#</a> ‖ R − Θ + L ‖^2 F</h6><h6 id="exercises-267"><a class="header-anchor" href="#exercises-267" aria-hidden="true">#</a> EXERCISES 267</h6><p>successively over <strong>R</strong> <em>,</em> <strong>Θ</strong> <em>,</em> <strong>L</strong> and <strong>Γ</strong> (Ma et al. 2013).</p><div class="language-"><pre><code>Chapter 10\n</code></pre></div><h2 id="signal-approximation-and-compressed"><a class="header-anchor" href="#signal-approximation-and-compressed" aria-hidden="true">#</a> Signal Approximation and Compressed</h2><h2 id="sensing"><a class="header-anchor" href="#sensing" aria-hidden="true">#</a> Sensing</h2><h3 id="_10-1-introduction"><a class="header-anchor" href="#_10-1-introduction" aria-hidden="true">#</a> 10.1 Introduction</h3><p>In this chapter, we discuss applications of <em><code>_ 1 -based relaxation to problems of signal recovery and approximation. Our focus is the role played by sparsity in signal representation and approximation, and the use of _</code></em> 1 -methods for exploiting this sparsity for solving problems like signal denoising, compression, and approximation. We begin by illustrating that many classes of “natural” signals are sparse when represented in suitable bases, such as those afforded by wavelets and other multiscale transforms. We illustrate how such sparsity can be exploited for compression and denoising in orthogonal bases. Next we discuss the problem of signal approximation in overcomplete bases, and the role of <em>`</em> 1 -relaxation in finding near-optimal approximations. Finally, we discuss the method of compressed sensing for recovering sparse signals. It is a combination of two ideas: taking measurements of signals via random projections, and solving a lasso-type problem for reconstruction.</p><h3 id="_10-2-signals-and-sparse-representations"><a class="header-anchor" href="#_10-2-signals-and-sparse-representations" aria-hidden="true">#</a> 10.2 Signals and Sparse Representations</h3><p>Let us begin by providing some background on the role of sparse representa- tions in signal processing. To be clear, our use of the term “signal” is general, including (among other examples) data such as sea water levels, seismic record- ings, medical time series, audio recordings, photographic images, video data, and financial data. In all cases, we represent the signal by a vector <em>θ</em> ∗∈R <em>p</em>. (For two-dimensional signals such as images, the reader should think about a vectorized form of the image.)</p><h4 id="_10-2-1-orthogonal-bases"><a class="header-anchor" href="#_10-2-1-orthogonal-bases" aria-hidden="true">#</a> 10.2.1 Orthogonal Bases</h4><p>In signal processing, it is frequently useful to represent signals in different types of bases. Examples include Fourier representations, useful for extract- ing periodic structure in time series, and multiscale representations such as wavelets. Such representations are described by a collection of vectors</p><div class="language-"><pre><code>269\n</code></pre></div><h6 id="_270-signal-approximation-and-compressed-sensing"><a class="header-anchor" href="#_270-signal-approximation-and-compressed-sensing" aria-hidden="true">#</a> 270 SIGNAL APPROXIMATION AND COMPRESSED SENSING</h6><p>{ <em>ψj</em> } <em>pj</em> =1that form an orthonormal basis ofR <em>p</em>. If we define the <em>p</em> × <em>p</em> ma- trix <strong>Ψ</strong> : =</p><h6 id="-390"><a class="header-anchor" href="#-390" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>ψ 1 ψ 2 ... ψp\n</code></pre></div><h6 id="-391"><a class="header-anchor" href="#-391" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>, then the orthonormality condition guarantees\n</code></pre></div><p>that <strong>Ψ</strong> <em>T</em> <strong>Ψ</strong> = <em>Ip</em> × <em>p</em>. Given an orthonormal basis, any signal <em>θ</em> ∗∈R <em>p</em> can be expanded in the form</p><div class="language-"><pre><code>θ ∗: =\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>β ∗ jψj, (10.1)\n</code></pre></div><p>where the <em>jthbasis coefficientβj</em> ∗: =〈 <em>θ</em> ∗ <em>, ψj</em> 〉=</p><p>∑ <em>p i</em> =1 <em>θ</em> ∗ <em>iψij</em> is obtained by projecting the signal onto the <em>jth</em> basis vector <em>ψj</em>. Equivalently, we can write the transformation from signal <em>θ</em> ∗∈R <em>p</em> to basis coefficient vector <em>β</em> ∗∈R <em>p</em> as the matrix-vector product <em>β</em> ∗= <strong>Ψ</strong> <em>Tθ</em> ∗.</p><div class="language-"><pre><code>20\n</code></pre></div><div class="language-"><pre><code>60\n</code></pre></div><div class="language-"><pre><code>100\n</code></pre></div><div class="language-"><pre><code>Time\n</code></pre></div><div class="language-"><pre><code>Arterial Pressure\n</code></pre></div><div class="language-"><pre><code>Original\n</code></pre></div><div class="language-"><pre><code>1 31 64 96 128\n</code></pre></div><div class="language-"><pre><code>20\n</code></pre></div><div class="language-"><pre><code>60\n</code></pre></div><div class="language-"><pre><code>100\n</code></pre></div><div class="language-"><pre><code>Time\n</code></pre></div><div class="language-"><pre><code>Arterial Pressure\n</code></pre></div><div class="language-"><pre><code>Reconstructed\n</code></pre></div><div class="language-"><pre><code>1 31 64 96 128\n</code></pre></div><div class="language-"><pre><code>−60\n</code></pre></div><div class="language-"><pre><code>−40\n</code></pre></div><div class="language-"><pre><code>−20\n</code></pre></div><div class="language-"><pre><code>0\n</code></pre></div><div class="language-"><pre><code>20\n</code></pre></div><div class="language-"><pre><code>40\n</code></pre></div><div class="language-"><pre><code>60\n</code></pre></div><div class="language-"><pre><code>Index\n</code></pre></div><div class="language-"><pre><code>Wavelet Coefficient\n</code></pre></div><div class="language-"><pre><code>Haar Representation\n</code></pre></div><div class="language-"><pre><code>1 31 64 96 128\n</code></pre></div><p><strong>Figure 10.1</strong> <em>Il lustration of sparsity in time series data. Left, top panel: Signal θ</em> ∗∈R <em>pof arterial pressure versus time overp</em> = 128 <em>points. Left, bottom panel: Reconstruction</em> ̂ <em>θ</em>^128 <em>based on retaining the largest (in absolute amplitude)k</em> = 64 <em>coefficients from the Haar basis. Right: Haar basis coefficientsβ</em> ∗= <strong>Ψ</strong> <em>Tθ</em> ∗ <em>of the signal.</em></p><div class="language-"><pre><code>To give a simple example, consider the following matrix\n</code></pre></div><h6 id="ψ-1"><a class="header-anchor" href="#ψ-1" aria-hidden="true">#</a> Ψ : =</h6><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><h6 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> </h6><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><div class="language-"><pre><code>1\n2\n</code></pre></div><div class="language-"><pre><code>1\n2\n√^1\n1 2 0\n2\n</code></pre></div><div class="language-"><pre><code>1\n2\n√−^1\n1 2 0\n2 −\n</code></pre></div><div class="language-"><pre><code>1\n2 0\n√^1\n2\n1\n2 −\n</code></pre></div><div class="language-"><pre><code>1\n2 0\n</code></pre></div><div class="language-"><pre><code>−√ 1\n2\n</code></pre></div><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><h6 id="-3"><a class="header-anchor" href="#-3" aria-hidden="true">#</a> </h6><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><h6 id="-5"><a class="header-anchor" href="#-5" aria-hidden="true">#</a> </h6><h6 id="-4"><a class="header-anchor" href="#-4" aria-hidden="true">#</a> </h6><h6 id="_10-2"><a class="header-anchor" href="#_10-2" aria-hidden="true">#</a> . (10.2)</h6><p>It is an orthonormal matrix, satisfying <strong>Ψ</strong> <em>T</em> <strong>Ψ</strong> = <strong>I</strong> 4 × 4 , and corresponds to a two-level Haar transform for signal length <em>p</em> = 4. For any given signal <em>θ</em> ∗∈R^4 , the Haar basis coefficients <em>β</em> ∗= <strong>Ψ</strong> <em>Tθ</em> ∗have a natural interpretation. The first</p><h6 id="signals-and-sparse-representations-271"><a class="header-anchor" href="#signals-and-sparse-representations-271" aria-hidden="true">#</a> SIGNALS AND SPARSE REPRESENTATIONS 271</h6><p>coefficient <em>β</em> 1 ∗ = 〈 <em>ψ</em> 1 <em>, θ</em> ∗〉=^12</p><h6 id="∑-4-1"><a class="header-anchor" href="#∑-4-1" aria-hidden="true">#</a> ∑ 4</h6><div class="language-"><pre><code>j =1 θ\n</code></pre></div><p>∗ <em>j</em> is a rescaled version of the averaged signal. The second column <em>ψ</em> 2 is a differencing operator on the full signal, whereas the third and fourth columns are local differencing operators on each half of the signal. This Haar transform is the simplest example of a wavelet transform. An important fact is that many signal classes, while not sparse in the canonical basis, become sparse when represented in a different orthogonal basis. Figure 10.1 provides an illustration of this phenomenon for some medical time series data. The top-left panel shows <em>p</em> = 128 samples of arterial pressure from a patient, showing that the signal <em>θ</em> ∗itself is not at all sparse. The right panel shows the Haar coefficient representation <em>β</em> ∗= <strong>Ψ</strong> <em>Tθ</em> ∗of the signal; note how in contrast it is relatively sparse. Finally, the bottom-left panel shows a reconstruction̂ <em>θ</em> of the original signal, based on discarding half of the Haar coefficients. Although not a perfect reconstruction, it captures the dominant features of the time series. Figure 10.2 provides a second illustration of this sparsity phenomenon, this time for the class of photographic images and two-dimensional wavelet trans- forms. Panel (a) shows a 512×512 portion of the “Boats” image; in our frame- work, we view this two-dimensional image as a vector in <em>p</em> = 512^2 = 262 <em>,</em> 144 dimensions. Shown in panel (b) is the form of a particular two-dimensional wavelet; as can be discerned from the shape, it is designed to extract diago- nally oriented structure at a particular scale. Taking inner products with this wavelet over all spatial positions of the image (a procedure known as con- volution) yields a collection of wavelet coefficients at all spatial positions of the image. These coefficients are then sub-sampled, depending on the scale of the wavelet. Then we reconstruct the image from these coefficients. Doing so at multiple scales (three in this illustration) and orientations (four in this illustration) yields the multiscale pyramid shown in panel (c). Once again, although the original image is not a sparse signal, its representation in this multiscale basis is very sparse, with many coefficients either zero or very close to zero. As a demonstration of this sparsity, panel (d) shows a histogram of one of the wavelet coefficients, obtained by pooling its values over all spatial positions of the image. This histogram is plotted on the log scale, and the sharp peak around zero reveals the sparsity of the coefficient distribution.</p><h4 id="_10-2-2-approximation-in-orthogonal-bases"><a class="header-anchor" href="#_10-2-2-approximation-in-orthogonal-bases" aria-hidden="true">#</a> 10.2.2 Approximation in Orthogonal Bases</h4><p>The goal of signal compression is to represent the signal <em>θ</em> ∗∈R <em>p</em> , typically in an approximate manner, using some number <em>k</em>  <em>p</em> of coefficients much smaller than the ambient dimension. In the setting of orthogonal bases, one method for doing so is based on using only a sparse subset of the orthogonal vectors { <em>ψj</em> } <em>pj</em> =1. In particular, for an integer <em>k</em> ∈ { 1 <em>,</em> 2 <em>,...,p</em> }that characterizes the</p><h6 id="_272-signal-approximation-and-compressed-sensing"><a class="header-anchor" href="#_272-signal-approximation-and-compressed-sensing" aria-hidden="true">#</a> 272 SIGNAL APPROXIMATION AND COMPRESSED SENSING</h6><div class="language-"><pre><code>(a) (b)\n</code></pre></div><div class="language-"><pre><code>−500 0 500\n</code></pre></div><div class="language-"><pre><code>−10\n</code></pre></div><div class="language-"><pre><code>−8\n</code></pre></div><div class="language-"><pre><code>−6\n</code></pre></div><div class="language-"><pre><code>−4\n</code></pre></div><div class="language-"><pre><code>−2\n</code></pre></div><div class="language-"><pre><code>Log Probability\n</code></pre></div><div class="language-"><pre><code>(c) (d)\n</code></pre></div><p><strong>Figure 10.2</strong> <em>Sparsity in wavelet-based representations of natural images. (a) “Boats” image. (b) Basis vector of a multiscale pyramid transform, drawn here as a 2-dimensional image. (c) Three levels of a multiscale representation of “Boats” image with four different orientations at each scale. (d) Log histogram of the am- plitudes of a wavelet coefficient from a fixed scale and orientation, pooled over all pixels within the image. Note that the majority of coefficients are close to zero, with relatively few large in absolute value.</em></p><p>approximation accuracy, let us consider reconstructions of the form</p><div class="language-"><pre><code>Ψ β =\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>βjψj, such that‖ β ‖ 0 : =\n</code></pre></div><div class="language-"><pre><code>∑ p\nj =1I[ βj^6 = 0] ≤ k. (10.3)\n</code></pre></div><p>Here we have introduced the <em>`</em> 0 -“norm,” which simply counts the number of nonzero elements in the vector <em>β</em> ∈R <em>p</em>. We then consider the problem of</p><h6 id="signals-and-sparse-representations-273"><a class="header-anchor" href="#signals-and-sparse-representations-273" aria-hidden="true">#</a> SIGNALS AND SPARSE REPRESENTATIONS 273</h6><div class="language-"><pre><code>(a) (b)\n</code></pre></div><p><strong>Figure 10.3</strong> <em>Il lustration of image compression based on wavelet thresholding. (a) Zoomed portion of the original “Boats” image from Figure 10.2(a). (b) Reconstruc- tion based on retaining 5% of the wavelet coefficients largest in absolute magnitude. Note that the distortion is quite smal l, and concentrated mainly on the fine-scale features of the image.</em></p><p>optimal <em>k</em> -sparse approximation—namely, to compute</p><div class="language-"><pre><code>β ̂ k ∈arg min\nβ ∈R p\n</code></pre></div><div class="language-"><pre><code>‖ θ ∗− Ψ β ‖^22 such that‖ β ‖ 0 ≤ k. (10.4)\n</code></pre></div><p>Given the optimal solution <em>β</em> ̂ <em>k</em> of this problem, the reconstruction</p><div class="language-"><pre><code>θk : =\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>β ̂ k\njψj (10.5)\n</code></pre></div><p>defines the best least-squares approximation to <em>θ</em> ∗based on <em>k</em> terms. Fig- ure 10.3 illustrates the idea. Note that the problem (10.4) is nonconvex and combinatorial, due to the <em>`</em> 0 -norm constraint. Despite this fact, it is actually very easy to solve in this particular case, essentially due to the structure afforded by orthonormal trans- forms. In particular, suppose that we order the vector <em>β</em> ∗∈R <em>p</em> of basis coef- ficients in terms of their absolute values, thereby defining the order statistics</p><div class="language-"><pre><code>| β ∗(1)|≥| β ∗(2)|≥ ... ≥| β ∗( p )|. (10.6)\n</code></pre></div><p>Then for any given integer <em>k</em> ∈{ 1 <em>,</em> 2 <em>,...,p</em> }, it can be shown that the optimal <em>k</em> -term approximation is given by</p><div class="language-"><pre><code>θ ̂ k : =\n</code></pre></div><div class="language-"><pre><code>∑ k\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>β (∗ j ) ψσ ( j ) , (10.7)\n</code></pre></div><h6 id="_274-signal-approximation-and-compressed-sensing"><a class="header-anchor" href="#_274-signal-approximation-and-compressed-sensing" aria-hidden="true">#</a> 274 SIGNAL APPROXIMATION AND COMPRESSED SENSING</h6><div class="language-"><pre><code>where σ ( j ) denotes the basis vector associated with the jth order statistic. In\nwords, we retain only the basis vectors associated with the largest k coefficients\nin absolute value.\nIn summary, then, we have the following simple algorithm for computing\noptimal k -term approximations in an orthogonal basis:\n</code></pre></div><ol><li>Compute the basis coefficients <em>βj</em> ∗=〈 <em>θ</em> ∗ <em>, ψj</em> 〉for <em>j</em> = 1 <em>,</em> 2 <em>,...,p</em>. In matrix-</li></ol><div class="language-"><pre><code>vector notation, compute the vector β ∗= Ψ Tθ ∗.\n</code></pre></div><ol start="2"><li>Sort the coefficients in terms of absolute values as in (10.6), and extract the top <em>k</em> coefficients.</li><li>Compute the best <em>k</em> -term approximation̂ <em>θk</em> as in (10.7).</li></ol><div class="language-"><pre><code>For any orthogonal basis, the computational complexity of this procedure is at\nmostO( p^2 ), with theO( p log p ) complexity of sorting in step 2 dominated by\nthe complexity of computing the basis coefficients in step 1. An attractive fea-\nture of many orthogonal representations, including Fourier bases and discrete\nwavelets, is that the basis coefficients can be computed in timeO( p log p ).\nAs discussed previously, Figure 10.1 provides one illustration of signal ap-\nproximation within the Haar wavelet basis. In particular, the bottom-left panel\nshows the approximated signal̂ θ^64 , based on retaining only half of the Haar\nwavelet coefficients ( k/p = 64 / 128 = 0. 5).\n</code></pre></div><h4 id="_10-2-3-reconstruction-in-overcomplete-bases"><a class="header-anchor" href="#_10-2-3-reconstruction-in-overcomplete-bases" aria-hidden="true">#</a> 10.2.3 Reconstruction in Overcomplete Bases</h4><div class="language-"><pre><code>Orthonormal bases, though useful in many ways, have a number of short-\ncomings. In particular, there is a limited class of signals that have sparse\nrepresentations in any given orthonormal basis. For instance, Fourier bases\nare particularly well-suited to reconstructing signals with a globally periodic\nstructure; in contrast, the Haar basis with its localized basis vectors is rather\npoor at capturing this kind of structure. On the other hand, the Haar basis ex-\ncels at capturing step discontinuities, whereas such jumps have very nonsparse\nrepresentations in the Fourier basis.\nBased on this intuition, it is relatively straightforward to construct signals\nthat are in some sense “simple,” but fail to have sparse representations in a\nclassical orthonormal basis. As an illustration, panel (a) of Figure 10.4 shows\na signal θ ∗∈R^128 that contains a mixture of both some globally periodic\ncomponents, and some rapid (nearly discontinuous) transitions. As shown in\npanel (b), its Haar coefficients β ∗= Ψ Tθ ∗are relatively dense, because many\nbasis vectors are required to reconstruct the globally periodic portion of the\nsignal. Similarly, as shown in panel (c), its representation α ∗= Φ Tθ ∗in the\ndiscrete cosine basis (a type of Fourier representation) is also relatively dense.\nDue to this lack of sparsity, neither basis alone will provide a good sparse\napproximation to the original signal.\nHowever, suppose that we allow the reconstruction to use subsets of vectors\nfrom both bases simultaneously; in this case, it might be possible to obtain a\nsignificantly more accurate, or even exact, sparse approximation. To set up\n</code></pre></div><h6 id="signals-and-sparse-representations-275"><a class="header-anchor" href="#signals-and-sparse-representations-275" aria-hidden="true">#</a> SIGNALS AND SPARSE REPRESENTATIONS 275</h6><div class="language-"><pre><code>Index\n</code></pre></div><div class="language-"><pre><code>Signal\n</code></pre></div><div class="language-"><pre><code>1 16 32 48 64 80 96 112 128\n−0.05\n</code></pre></div><div class="language-"><pre><code>0.05\n</code></pre></div><div class="language-"><pre><code>0.15\n</code></pre></div><div class="language-"><pre><code>0.25\n</code></pre></div><div class="language-"><pre><code>(a) Mixed Signal\n</code></pre></div><div class="language-"><pre><code>0.0 0.1 0.2 0.3 0.4 0.5\n</code></pre></div><div class="language-"><pre><code>Index\n</code></pre></div><div class="language-"><pre><code>Wavelet Coefficient\n</code></pre></div><div class="language-"><pre><code>1 16 32 48 64 80 96 112 128\n</code></pre></div><div class="language-"><pre><code>(b) Representation in Haar Basis\n</code></pre></div><div class="language-"><pre><code>−0.2\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>Index\n</code></pre></div><div class="language-"><pre><code>Wavelet Coefficient\n</code></pre></div><div class="language-"><pre><code>1 16 32 48 64 80 96 112 128\n</code></pre></div><div class="language-"><pre><code>(c) Representation in Cosine Basis\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.1\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.3\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.5\n</code></pre></div><div class="language-"><pre><code>Index\n</code></pre></div><div class="language-"><pre><code>Coefficient\n</code></pre></div><div class="language-"><pre><code>1 32 64 96 128 160 192 224 256\n</code></pre></div><div class="language-"><pre><code>Haar\nCosine\n</code></pre></div><div class="language-"><pre><code>(d) Optimal Joint Representation\n</code></pre></div><p><strong>Figure 10.4</strong> <em>(a) Original signalθ</em> ∗∈R <em>pwithp</em> = 128_. (b) Representation_ <strong>Ψ</strong> <em>Tθ</em> ∗ <em>in the Haar basis. (c) Representation</em> <strong>Φ</strong> <em>Tθ</em> ∗ <em>in the discrete cosine basis. (d) Coefficients</em> ( <em>α,</em> ̂ <em>β</em> ̂)∈R <em>p</em> ×R <em>pof the optimal ly sparse joint representation obtained by solving basis pursuit linear program</em> (10.11)<em>.</em></p><p>the problem more precisely, given a pair of orthonormal bases{ <em>ψj</em> } <em>pj</em> =1and { <em>φj</em> } <em>pj</em> =1, let us consider reconstructions of the form</p><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>αjφj\n︸ ︷︷ ︸\nΦ α\n</code></pre></div><h6 id="-392"><a class="header-anchor" href="#-392" aria-hidden="true">#</a> +</h6><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>βjψj\n︸ ︷︷ ︸\nΨ β\n</code></pre></div><div class="language-"><pre><code>such that‖ α ‖ 0 +‖ β ‖ 0 ≤ k , (10.8)\n</code></pre></div><p>and the associated optimization problem</p><div class="language-"><pre><code>minimize\n( α,β )∈R p ×R p\n</code></pre></div><div class="language-"><pre><code>‖ θ ∗− Φ α − Ψ β ‖^22 such that‖ α ‖ 0 +‖ β ‖ 0 ≤ k. (10.9)\n</code></pre></div><p>Despite its superficial similarity to our earlier <em>k</em> -term approximation prob- lem (10.5), the optimization problem (10.9) is actually very difficult to solve. Unlike the earlier case, we are now working in an <em>overcomplete basis</em> described by the union of the two bases <strong>Φ</strong> and <strong>Ψ</strong>.</p><h6 id="_276-signal-approximation-and-compressed-sensing"><a class="header-anchor" href="#_276-signal-approximation-and-compressed-sensing" aria-hidden="true">#</a> 276 SIGNAL APPROXIMATION AND COMPRESSED SENSING</h6><p>Nonetheless, we can resort to our usual relaxation of the <em>`</em> 0 -“norm,” and consider the following convex program</p><div class="language-"><pre><code>minimize\n( α,β )∈R p ×R p\n‖ θ ∗− Φ α − Ψ β ‖^22 such that‖ α ‖ 1 +‖ β ‖ 1 ≤ R , (10.10)\n</code></pre></div><p>where <em>R &gt;</em> 0 is a user-defined radius. This program is a constrained version of the lasso program, also referred to as the relaxed basis-pursuit program. When seeking a perfect reconstruction, we can also consider the even simpler problem</p><div class="language-"><pre><code>minimize\n( α,β )∈R p ×R p\n</code></pre></div><div class="language-"><pre><code>‖ α ‖ 1 +‖ β ‖ 1 such that θ ∗=\n</code></pre></div><h6 id="-393"><a class="header-anchor" href="#-393" aria-hidden="true">#</a> [</h6><h6 id="φ-ψ"><a class="header-anchor" href="#φ-ψ" aria-hidden="true">#</a> Φ Ψ</h6><h6 id="-394"><a class="header-anchor" href="#-394" aria-hidden="true">#</a> ]</h6><h6 id="-395"><a class="header-anchor" href="#-395" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>α\nβ\n</code></pre></div><h6 id="-396"><a class="header-anchor" href="#-396" aria-hidden="true">#</a> ]</h6><h6 id="_10-11"><a class="header-anchor" href="#_10-11" aria-hidden="true">#</a> . (10.11)</h6><p>This problem is a linear program (LP), often referred to as the basis-pursuit linear program. Returning to the example discussed in Figure 10.4, panel (d) shows the</p><p>optimal coefficients (̂ <em>α,β</em> ̂)∈R <em>p</em> ×R <em>p</em> obtained by solving the basis pursuit LP (10.11). We thus find that the original signal in panel (a) can be generated by an extremely sparse combination, with only six nonzero coefficients, in the overcomplete basis formed by combining the Haar and discrete cosine representations. In fact, this is the sparsest possible representation of the signal, so that in this case, solving the basis pursuit LP (10.11) is equivalent to solving the <em><code>_ 0 -constrained problem (10.9). Naturally, the reader might wonder about the generality of this phenomenon—namely, when does the solution to the basis pursuit LP co- incide with the computationally difficult _</code></em> 0 -problem (10.9)? As it turns out, the answer to this question depends on the degree of incoherence between the two bases <strong>Φ</strong> and <strong>Ψ</strong> , as we explore at more length in Section 10.4.</p><h3 id="_10-3-random-projection-and-approximation"><a class="header-anchor" href="#_10-3-random-projection-and-approximation" aria-hidden="true">#</a> 10.3 Random Projection and Approximation</h3><p>In the previous sections, we discussed approximating a signal by computing its projection onto each of a fixed set of basis functions. We now turn to the use of random projections in signal approximation. This allows one to use a smaller number of (random) basis functions than is required under a fixed basis. We will combine this with an <em>`</em> 1 -penalty on the coefficient of each projection, leading to the idea of <em>compressed sensing</em>. A random projection of a signal <em>θ</em> ∗is a measurement of the form</p><div class="language-"><pre><code>yi =〈 zi, θ ∗〉=\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>zijθ ∗ j, (10.12)\n</code></pre></div><p>where <em>zi</em> ∈R <em>p</em> is a random vector. The idea of using random projections for dimensionality reduction and approximation is an old one, dating back (at least) to classical work on metric embedding and spherical sections of convex</p><h6 id="random-projection-and-approximation-277"><a class="header-anchor" href="#random-projection-and-approximation-277" aria-hidden="true">#</a> RANDOM PROJECTION AND APPROXIMATION 277</h6><p>bodies (see the bibliographic section for more details). We begin by describ- ing a classical use of random projection, namely for embedding data while preserving distances between points, and then move on to discuss compressed sensing, which combines random projections with <em>`</em> 1 -relaxation.</p><h4 id="_10-3-1-johnson–lindenstrauss-approximation"><a class="header-anchor" href="#_10-3-1-johnson–lindenstrauss-approximation" aria-hidden="true">#</a> 10.3.1 Johnson–Lindenstrauss Approximation</h4><p>As one application of random projection, let us consider how they can be used to approximate a finite collection of vectors, say representing some dataset. The technique that we describe is often known as Johnson–Lindenstrauss em- bedding, based on the authors who pioneered its use in studying the more general problem of metric embedding (see the bibliographic section for more details). Suppose that we are given <em>M</em> data points{ <em>u</em> 1 <em>,...,uM</em> }lying inR <em>p</em>. If the data dimension <em>p</em> is large, then it might be too expensive to store the dataset. In this setting, one approach is to design a dimension-reducing map- ping <em>F</em> :R <em>p</em> →R <em>N</em> with <em>N</em>  <em>p</em> that preserves some “essential” features of the dataset, and then store only the projected dataset{ <em>F</em> ( <em>u</em> 1 ) <em>,...,F</em> ( <em>uM</em> )}. For example, since many algorithms operate on datasets by computing pairwise distances, we might be interested in a mapping <em>F</em> with the guarantee that for some tolerance <em>δ</em> ∈(0 <em>,</em> 1), we have</p><p>(1− <em>δ</em> )‖ <em>ui</em> − <em>ui</em> ′‖^22 ≤‖ <em>F</em> ( <em>ui</em> )− <em>F</em> ( <em>ui</em> ′)‖^22 ≤(1+ <em>δ</em> )‖ <em>ui</em> − <em>ui</em> ′‖^22 for all pairs <em>i</em> 6 = <em>i</em> ′. (10.13) Of course, this is always possible if the projected dimension <em>N</em> is large enough, but the goal is to do it with relatively small <em>N</em>. As shown in the seminal work of Johnson and Lindenstrauss, random projections provide one method for designing such approximate distance- preserving embeddings. The construction is straightforward:</p><p>(a) Form a random matrix <strong>Z</strong> ∈R <em>N</em> × <em>p</em> with each <em>Zij</em> ∼ <em>N</em> (0 <em>,</em> 1), i.i.d., and define the linear mapping <em>F</em> :R <em>p</em> →R <em>N</em> via</p><div class="language-"><pre><code>F ( u ) : =\n</code></pre></div><h6 id="_1-143"><a class="header-anchor" href="#_1-143" aria-hidden="true">#</a> 1</h6><h6 id="√-28"><a class="header-anchor" href="#√-28" aria-hidden="true">#</a> √</h6><h6 id="n-39"><a class="header-anchor" href="#n-39" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>Z u. (10.14)\n</code></pre></div><p>(b) Compute the projected dataset{ <em>F</em> ( <em>u</em> 1 ) <em>,F</em> ( <em>u</em> 2 ) <em>,...,F</em> ( <em>uM</em> )}.</p><p>An interesting question is the following: for a given tolerance <em>δ</em> ∈(0 <em>,</em> 1) and number of data points <em>M</em> , how large should we choose the projected dimension <em>N</em> to ensure that approximate distance-preserving property (10.13) holds with high probability? In Exercises 10.1 and 10.2, we show that this property holds with high probability as long as <em>N &gt;δc</em> 2 log <em>M</em> for some universal constant <em>c</em>. Thus, the dependence on the number <em>M</em> of data points scales logarithmically, and hence is very mild.</p><p>As a particular example, suppose that our goal is to obtain a compressed representation of all Boolean vectors <em>u</em> ∈ {− 1 <em>,</em> 0 <em>,</em> 1 } <em>p</em> that are <em>k</em> -sparse.^1 By</p><p>(^1) A vector <em>u</em> ∈R <em>p</em> is <em>k</em> -sparse if only <em>k</em> ≤ <em>p</em> elements are nonzero.</p><h6 id="_278-signal-approximation-and-compressed-sensing"><a class="header-anchor" href="#_278-signal-approximation-and-compressed-sensing" aria-hidden="true">#</a> 278 SIGNAL APPROXIMATION AND COMPRESSED SENSING</h6><p>a simple counting argument, there are <em>M</em> = 2 <em>k</em></p><div class="language-"><pre><code>( p\nk\n</code></pre></div><h6 id="-397"><a class="header-anchor" href="#-397" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>such vectors. Noting that\n</code></pre></div><p>log <em>M</em> ≤ <em>k</em> log</p><p>( <em>e</em> (^2) <em>p k</em></p><h6 id="-398"><a class="header-anchor" href="#-398" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>, we see that a projection dimension N &gt;δc 2 k log\n</code></pre></div><p>( <em>e</em> (^2) <em>p k</em></p><h6 id="-399"><a class="header-anchor" href="#-399" aria-hidden="true">#</a> )</h6><p>suffices to preserve pairwise distances up to <em>δ</em> -accuracy between all <em>k</em> -sparse Boolean vectors. This example provides a natural segue to the method of compressed sensing, which combines random projections with <em>`</em> 1 -relaxation.</p><h4 id="_10-3-2-compressed-sensing"><a class="header-anchor" href="#_10-3-2-compressed-sensing" aria-hidden="true">#</a> 10.3.2 Compressed Sensing</h4><p>Compressed sensing is a combination of random projection and <em><code>_ 1 -regular- ization that was introduced in independent work by Candes and Tao (2005) and Donoho (2006); since this pioneering work, an extensive literature on the topic has developed, with numerous applications including medical imaging and single-pixel cameras, among others. In this section, we provide a brief introduction to the basic ideas. The motivation for compressed sensing is the inherent wastefulness of the standard method for compressing signals in an orthogonal basis. As described in Section 10.2.2, this approach involves first computing the full vector _β_ ∗∈R _p_ of basis coefficients (step 1 on page 274), and then discarding a large fraction of them in order to obtain the _k_ -sparse approximation _θ_ ̂ _k_ of the underlying signal _θ_ ∗(step 2). Given that we end up discarding most of the basis coefficients, is it really necessary to compute all of them? Of course, if one knew _a priori_ which subset of _k_ coefficients were to be retained for the sparse approximation _θ_ ̂ _k_ , then one could simply compute this subset of basis coefficients. We refer to this approach as the oracle technique. Of course, it is unimplementable in practice, since we don’t know _a priori_ which coefficients are the most relevant for a given signal. The power of compressed sensing is that it enables one to mimic the behav- ior of the oracle with very little computational overhead. It combines random projection with _</code></em> 1 -minimization in the following way. Instead of pre-computing all of the basis coefficients <em>β</em> ∗= <strong>Ψ</strong> <em>Tθ</em> ∗, suppose that we compute some number <em>N</em> of random projections, say of the form <em>yi</em> =〈 <em>zi, θ</em> ∗〉, for <em>i</em> = 1 <em>,</em> 2 <em>,...,N</em>. We are free to choose the form of the random projection vectors <em>zi</em> ∈R <em>p</em> , and we discuss a number of reasonable choices shortly. Thus, the setup of our problem is as follows: we are given an <em>N</em> -vector <strong>y</strong> of random projections of the signal <em>θ</em> ∗. Also known to us is the <em>N</em> × <em>p</em> random matrix <strong>Z</strong> with <em>ith</em> row <em>zi</em> , used to compute the random projections; we refer to <strong>Z</strong> as the <em>design matrix</em> or measurement matrix. The observation vector <strong>y</strong> and design matrix <strong>Z</strong> are linked to the unknown signal <em>θ</em> ∗∈R <em>N</em> by the matrix- vector equation <strong>y</strong> = <strong>Z</strong> <em>θ</em> ∗, and our goal is to recover (exactly or approximately) the signal <em>θ</em> ∗∈R <em>p</em>. See Figure 10.5(a) for an illustration of this setup. At first sight, the problem seems very simple, since determining <em>θ</em> ∗amounts to solving a linear system. However, for this method to be cheaper than the standard approach (and therefore of practical interest), it is essential that the number of projections (or sample size) <em>N</em> be much smaller than the ambient dimension <em>p</em>. For this reason, the linear system <strong>y</strong> = <strong>Z</strong> <em>θ</em> ∗is highly under-</p><h6 id="random-projection-and-approximation-279"><a class="header-anchor" href="#random-projection-and-approximation-279" aria-hidden="true">#</a> RANDOM PROJECTION AND APPROXIMATION 279</h6><p>=</p><div class="language-"><pre><code>y\n</code></pre></div><div class="language-"><pre><code>N×p\n</code></pre></div><div class="language-"><pre><code>Z θ∗\n</code></pre></div><div class="language-"><pre><code>(a)\n</code></pre></div><p>=</p><div class="language-"><pre><code>y\n</code></pre></div><div class="language-"><pre><code>N×p\n</code></pre></div><div class="language-"><pre><code>Z Ψ β∗\n</code></pre></div><div class="language-"><pre><code>p−k\n</code></pre></div><div class="language-"><pre><code>k\n</code></pre></div><div class="language-"><pre><code>(b)\n</code></pre></div><p><strong>Figure 10.5</strong> <em>(a) An under-determined linear system</em> <strong>y</strong> = <strong>Z</strong> <em>θ</em> ∗ <em>: Each rowziof the N</em> × <em>pmeasurement matrix</em> <strong>Z</strong> <em>defines the random projectionyi</em> =〈 <em>zi, θ</em> ∗〉<em>. The signal θ</em> ∗∈R <em>pneed not be sparse in the canonical basis. (b) Equivalent representation of the linear system: Basis coefficientsβ</em> ∗= <strong>Ψ</strong> <em>Tθ</em> ∗ <em>are assumed to bek-sparse. This transformation defines an equivalent linear system</em> <strong>y</strong> = <strong>Z</strong> ̃ <em>β</em> ∗ <em>with sparsity that can be exploited.</em></p><p>determined: there are many signals <em>θ</em> that are consistent with the observed random projections. However, if we also have the additional side-information that <strong>Ψ</strong> <em>Tθ</em> ∗is sparse, then it could be possible to recover <em>θ</em> ∗exactly, even though the linear system on its own is under-determined. In an ideal world, we would like to exploit this sparsity by solving the <em>`</em> 0 -based problem</p><div class="language-"><pre><code>minimize\nθ ∈R p\n‖ Ψ Tθ ‖ 0 such that y = Z θ. (10.15)\n</code></pre></div><p>The <em><code>_ 0 -problem is combinatorial, and known to be computationally intractable (NP-hard) in general; thus, we are led to consider the _</code></em> 1 -relaxation</p><div class="language-"><pre><code>minimize\nθ ∈R p\n</code></pre></div><div class="language-"><pre><code>‖ Ψ Tθ ‖ 1 such that y = Z θ. (10.16)\n</code></pre></div><p>Equivalently, we can write this problem in terms of the transform coefficient vector <em>β</em> ∈R <em>p</em> , namely as</p><div class="language-"><pre><code>minimize\nβ ∈R p\n‖ β ‖ 1 such that y = Z ̃ β , (10.17)\n</code></pre></div><h6 id="_280-signal-approximation-and-compressed-sensing"><a class="header-anchor" href="#_280-signal-approximation-and-compressed-sensing" aria-hidden="true">#</a> 280 SIGNAL APPROXIMATION AND COMPRESSED SENSING</h6><div class="language-"><pre><code>where we have defined the transformed matrix ̃ Z : = ZΨ ∈R N × p. See Fig-\nure 10.5(b) for an illustration of this transformed linear system.\nIn summary, then, the method of compressed sensing operates as follows:\n</code></pre></div><ol><li>For a given sample size <em>N</em> , compute the random projections <em>yi</em> =〈 <em>zi, θ</em> ∗〉 for <em>i</em> = 1 <em>,</em> 2 <em>,...,N</em>.</li><li>Estimate the signal <em>θ</em> ∗ by solving the linear program (10.16) to obtain</li></ol><div class="language-"><pre><code>θ ̂. (Equivalently, solve the linear program (10.17) to obtain β ̂, and set\nθ ̂= Ψ β ̂.)\nTo be clear, we have actually described a family of procedures, depending\non our choice of the random projection vectors{ zi } Ni =1, or equivalently the\ntransformed design matrix Z. A variety of different design matrices Z have\nbeen studied for the purposes of compressed sensing. Perhaps the simplest\nchoice is to choose its entries zij ∼ N (0 , 1) in an i.i.d. manner, leading to a\nstandard Gaussian random matrix. Other choices of matrices for compressed\nsensing include random Bernoulli matrices formed with i.i.d. entries drawn\nas zij ∈ {− 1 , +1}with equal probability, as well as random submatrices of\nFourier matrices.\nWhen can compressed sensing succeed using a number of projections N less\nthan the signal dimension p? As we discuss in Section 10.4.2, it is sufficient that\nthe columns of the transformed design matrix Z ̃be “incoherent”, and there are\ndifferent measures of such incoherence. The simplest measure of incoherence\nis pairwise, based on the inner products between the columns of Z ̃. A more\nsophisticated notion of incoherence is the restricted isometry property (RIP),\nbased on looking on the conditioning of submatrices of ̃ Z consisting of up to\nk columns. An important fact is that the random design matrices discussed\nabove satisfy RIP with high probability using a relatively small number of\nprojections N. For instance, for the standard Gaussian or Bernoulli cases,\nit can be shown that RIP holds with high probability with as few as N =\nΩ\n</code></pre></div><h6 id="-400"><a class="header-anchor" href="#-400" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>k log pk\n</code></pre></div><h6 id="-401"><a class="header-anchor" href="#-401" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>samples, where k &lt; p is the sparsity of the basis coefficient vector\nβ ∗. Note that any method—even the unimplementable oracle that already\nknew the support of β ∗—would require at least N = k random projections for\nexact recovery. Thus, compressed sensing incurs a multiplicative overhead of\nonlyO\n</code></pre></div><h6 id="-402"><a class="header-anchor" href="#-402" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>log( p/k )\n</code></pre></div><h6 id="-403"><a class="header-anchor" href="#-403" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>relative to oracle performance.\n</code></pre></div><h3 id="_10-4-equivalence-between-0-and-1-recovery"><a class="header-anchor" href="#_10-4-equivalence-between-0-and-1-recovery" aria-hidden="true">#</a> 10.4 Equivalence between <code>0 and</code> 1 Recovery</h3><div class="language-"><pre><code>Thus far, we have discussed a number of applications of ` 1 -norm regular-\nization in signal processing, including sparse approximation in overcomplete\nbases (Section 10.2.3), and compressed sensing (Section 10.3.2). In both cases,\nthe ` 1 -norm is introduced as a computationally tractable surrogate to opti-\nmization problems involving the intractable ` 0 -“norm.” Up to this point, we\nhave not addressed in any depth an important question: when is solving the\n` 1 -relaxation equivalent to solving the original ` 0 -problem?\nMore precisely, given an observation vector y ∈R p and a design matrix\n</code></pre></div><h6 id="equivalence-between-0-and-1-recovery-281"><a class="header-anchor" href="#equivalence-between-0-and-1-recovery-281" aria-hidden="true">#</a> EQUIVALENCE BETWEEN <code>0 AND</code> 1 RECOVERY 281</h6><p><strong>X</strong> ∈R <em>N</em> × <em>p</em> , let us consider the two problems</p><div class="language-"><pre><code>minimize\nβ ∈R p\n‖ β ‖ 0 such that X β = y , (10.18)\n</code></pre></div><p>and minimize <em>β</em> ∈R <em>p</em></p><div class="language-"><pre><code>‖ β ‖ 1 such that X β = y. (10.19)\n</code></pre></div><p>This setup includes as a special case the problem of sparse approximation in an overcomplete basis, as discussed in Section 10.2.3; in this case, the obser- vation <strong>y</strong> is equal to the signal <em>θ</em> ∗to be approximated, and the design matrix <strong>X</strong> =</p><h6 id="-404"><a class="header-anchor" href="#-404" aria-hidden="true">#</a> [</h6><h6 id="φ-ψ-1"><a class="header-anchor" href="#φ-ψ-1" aria-hidden="true">#</a> Φ Ψ</h6><h6 id="-405"><a class="header-anchor" href="#-405" aria-hidden="true">#</a> ]</h6><p>. It also includes the case of compressed sensing, where <strong>X</strong> is the</p><p>transformed version of the random projection matrix (namely, <strong>Z</strong> ̃in our earlier notation).</p><h4 id="_10-4-1-restricted-nullspace-property"><a class="header-anchor" href="#_10-4-1-restricted-nullspace-property" aria-hidden="true">#</a> 10.4.1 Restricted Nullspace Property</h4><p>Suppose that the <em><code>_ 0 -based problem (10.18) has a unique optimal solution, say _β_ ∗∈R _p_. Our interest is in understanding when _β_ ∗is also the unique optimal solution of the _</code></em> 1 -based problem (10.19), in which case we say that the basis pursuit LP is <em>equivalent</em> to <em>`</em> 0 -recovery. Remarkably, there exists a very simple necessary and sufficient condition on the design matrix <strong>X</strong> for this equivalence to hold. For a given subset <em>S</em> ⊆{ 1 <em>,</em> 2 <em>,...,p</em> }, it is stated in terms of the set</p><div class="language-"><pre><code>C( S ) : =\n</code></pre></div><h6 id="-406"><a class="header-anchor" href="#-406" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>β ∈R p | ‖ βSc ‖ 1 ≤‖ βS ‖ 1\n</code></pre></div><h6 id="-407"><a class="header-anchor" href="#-407" aria-hidden="true">#</a> }</h6><h6 id="_10-20"><a class="header-anchor" href="#_10-20" aria-hidden="true">#</a> . (10.20)</h6><p>The setC( <em>S</em> ) is a cone, containing all vectors that are supported on <em>S</em> , and other vectors as well. Roughly, it corresponds to the cone of vectors that have most of their mass allocated to <em>S</em>. Given a matrix <strong>X</strong> ∈R <em>N</em> × <em>p</em> , its nullspace is given by null( <strong>X</strong> ) =</p><h6 id="-408"><a class="header-anchor" href="#-408" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>β ∈R p | X β = 0\n</code></pre></div><h6 id="-409"><a class="header-anchor" href="#-409" aria-hidden="true">#</a> }</h6><h6 id="-410"><a class="header-anchor" href="#-410" aria-hidden="true">#</a> .</h6><p><em>Definition 10.1. Restricted nul lspace property.</em> For a given subset <em>S</em> ⊆ { 1 <em>,</em> 2 <em>,...,p</em> }, we say that the design matrix <strong>X</strong> ∈R <em>N</em> × <em>p</em> satisfies the <em>restricted nul lspace property</em> over <em>S</em> , denoted by RN( <em>S</em> ), if</p><div class="language-"><pre><code>null( X )∩C( S ) ={ 0 }. (10.21)\n</code></pre></div><p>In words, the RN( <em>S</em> ) property holds when the only element of the coneC( <em>S</em> ) that lies within the nullspace of <strong>X</strong> is the all-zeroes vector. The following theorem highlights the significance of this property:</p><p><em>Theorem 10.1.<code>_ 0 _and</code></em> 1 <em>equivalence.</em> Suppose that <em>β</em> ∗ ∈R <em>p</em> is the unique solution to the <em>`</em> 0 problem (10.18), and has support <em>S</em>. Then the basis pursuit relaxation (10.19) has a unique solution equal to <em>β</em> ∗if and only if <strong>X</strong> satisfies the RN( <em>S</em> ) property.</p><p>The proof of Theorem 10.1 is relatively short, and is provided in Sec- tion 10.4.3. Since the subset <em>S</em> is not known in advance—indeed, it is usually what we</p><h6 id="_282-signal-approximation-and-compressed-sensing"><a class="header-anchor" href="#_282-signal-approximation-and-compressed-sensing" aria-hidden="true">#</a> 282 SIGNAL APPROXIMATION AND COMPRESSED SENSING</h6><p>are trying to determine—it is natural to seek matrices that satisfy a uniform version of the restricted nullspace property. For instance, we say that the uniform RN property of order <em>k</em> holds if RN( <em>S</em> ) holds for all subsets of size at most <em>k</em>. In this case, we are guaranteed that the <em>`</em> 1 -relaxation succeeds for any vector supported on any subset of size at most <em>k</em>.</p><h4 id="_10-4-2-sufficient-conditions-for-restricted-nullspace"><a class="header-anchor" href="#_10-4-2-sufficient-conditions-for-restricted-nullspace" aria-hidden="true">#</a> 10.4.2 Sufficient Conditions for Restricted Nullspace</h4><p>Of course, in order for Theorem 10.1 to be useful in practice, we need to verify the restricted nullspace property. A line of work has developed various con- ditions for certifying the uniform RN property. The simplest and historically earliest condition is based on the <em>pairwise incoherence</em></p><div class="language-"><pre><code>ν ( X ) : = max\nj,j ′=1 , 2 ,...,p\nj 6 = j ′\n</code></pre></div><div class="language-"><pre><code>|〈 x j, x j ′〉|\n‖ x j ‖ 2 ‖ x j ′‖ 2\n</code></pre></div><h6 id="_10-22"><a class="header-anchor" href="#_10-22" aria-hidden="true">#</a> . (10.22)</h6><p>For centered <strong>x</strong> <em>j</em> this is the maximal absolute pairwise correlation. When <strong>X</strong> is rescaled to have unit-norm columns, an equivalent representation is given by <em>ν</em> ( <strong>X</strong> ) = max <em>j</em> 6 = <em>j</em> ′|〈 <strong>x</strong> <em>j,</em> <strong>x</strong> <em>j</em> ′〉|, which illustrates that the pairwise incoherence measures how close the Gram matrix <strong>X</strong> <em>T</em> <strong>X</strong> is to the <em>p</em> -dimensional identity matrix in an element-wise sense. The following result shows that having a low pairwise incoherence is suf- ficient to guarantee exactness of the basis pursuit LP:</p><p><em>Proposition 10.1. Pairwise incoherence implies</em> RN_._ Suppose that for some in- teger <em>k</em> ∈{ 1 <em>,</em> 2 <em>,...,p</em> }, the pairwise incoherence satisfies the bound <em>ν</em> ( <strong>X</strong> ) <em>&lt;</em> 31 <em>k</em>. Then <strong>X</strong> satisfies the uniform RN property of order <em>k</em> , and hence, the basis pursuit LP is exact for all vectors with support at most <em>k</em>.</p><p>See Section 10.4.3 for the proof of this claim. An attractive feature of the pairwise incoherence is that it is easily com- puted; in particular, inO( <em>Np</em>^2 ) time. A disadvantage is that it provides very conservative bounds that do not always capture the actual performance of <em>`</em> 1 -relaxation in practice. For instance, consider the matrix <strong>X</strong> =</p><h6 id="-411"><a class="header-anchor" href="#-411" aria-hidden="true">#</a> [</h6><h6 id="φ-ψ-2"><a class="header-anchor" href="#φ-ψ-2" aria-hidden="true">#</a> Φ Ψ</h6><h6 id="-412"><a class="header-anchor" href="#-412" aria-hidden="true">#</a> ]</h6><p>, as arises in the overcomplete basis problem (10.11). We can numerically com- pute the incoherence, say for the discrete cosine and Haar bases in dimension <em>p</em> = 128, as illustrated in Figure 10.4. We find that Proposition 10.1 guaran- tees exact recovery of all signals with sparsity <em>k</em> = 1, whereas in practice, the <em>`</em> 1 -relaxation works for much larger values of <em>k</em>. For random design matrices, such as those that arise in compressed sens- ing, one can use probabilistic methods to bound the incoherence. For instance, consider a random matrix <strong>X</strong> ∈R <em>N</em> × <em>p</em> with i.i.d. <em>N</em> (0 <em>,</em> 1 <em>/N</em> ) entries. Here we have rescaled the variance so that the columns of <strong>X</strong> have expected norm equal</p><p>to one. For such a matrix, one can show that <em>ν</em> ( <strong>X</strong> )-</p><h6 id="√-29"><a class="header-anchor" href="#√-29" aria-hidden="true">#</a> √</h6><p>log <em>p N</em> with high prob- ability as ( <em>N,p</em> ) tend to infinity (see Exercise 10.5). Combined with Proposi- tion 10.1, we conclude that the <em>`</em> 1 -relaxation (10.16) will exactly recover all</p><h6 id="equivalence-between-0-and-1-recovery-283"><a class="header-anchor" href="#equivalence-between-0-and-1-recovery-283" aria-hidden="true">#</a> EQUIVALENCE BETWEEN <code>0 AND</code> 1 RECOVERY 283</h6><p>signals with sparsity at most <em>k</em> as long as the number of projections scales as <em>N</em> % <em>k</em>^2 log <em>p</em>. In fact, for random designs and compressed sensing, this scaling can be sharpened using the <em>restricted isometry property</em> (RIP). Recall that the in- coherence condition (10.22) is a measure of the orthonormality of pairs of columns of the design matrix <strong>X</strong>. The notion of restricted isometry is to con- strain much larger submatrices of <strong>X</strong> to have nearly orthogonal columns.</p><p><em>Definition 10.2. Restricted isometry property.</em> For a tolerance <em>δ</em> ∈(0 <em>,</em> 1) and integer <em>k</em> ∈{ 1 <em>,</em> 2 <em>,...,p</em> }, we say that RIP( <em>k,δ</em> ) holds if</p><div class="language-"><pre><code>‖ X TS X S − I k × k ‖op≤ δ (10.23)\n</code></pre></div><p>for all subsets <em>S</em> ⊂{ 1 <em>,</em> 2 <em>,...,p</em> }of cardinality <em>k</em>.</p><p>We recall here that‖·‖opdenotes the operator norm, or maximal singular value of a matrix. Due to the symmetry of <strong>X</strong> <em>TS</em> <strong>X</strong> <em>S</em> , we have the equivalent representation</p><div class="language-"><pre><code>‖ X TS X S − I k × k ‖op= sup\n‖ u ‖ 2 =1\n</code></pre></div><h6 id="∣-49"><a class="header-anchor" href="#∣-49" aria-hidden="true">#</a> ∣</h6><div class="language-"><pre><code>∣∣ uT ( X T\nS X S − I k × k\n</code></pre></div><h6 id="-413"><a class="header-anchor" href="#-413" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>u\n</code></pre></div><h6 id="∣-50"><a class="header-anchor" href="#∣-50" aria-hidden="true">#</a> ∣</h6><div class="language-"><pre><code>∣∣ = sup\n‖ u ‖ 2 =1\n</code></pre></div><h6 id="∣-51"><a class="header-anchor" href="#∣-51" aria-hidden="true">#</a> ∣</h6><h6 id="∣∣‖-x"><a class="header-anchor" href="#∣∣‖-x" aria-hidden="true">#</a> ∣∣‖ X</h6><div class="language-"><pre><code>Su ‖^22 −^1\n</code></pre></div><h6 id="∣-52"><a class="header-anchor" href="#∣-52" aria-hidden="true">#</a> ∣</h6><h6 id="∣∣"><a class="header-anchor" href="#∣∣" aria-hidden="true">#</a> ∣∣.</h6><p>Thus, we see that RIP( <em>k,δ</em> ) holds if and only if for all subsets <em>S</em> of cardinality <em>k</em> , we have</p><div class="language-"><pre><code>‖ X Su ‖^22\n‖ u ‖^22\n</code></pre></div><div class="language-"><pre><code>∈[1− δ, 1 + δ ] for all u ∈R k \\{ 0 },\n</code></pre></div><p>hence the terminology of restricted isometry. The following result shows that RIP is a sufficient condition for the re- stricted nullspace to hold:</p><p><em>Proposition 10.2. RIP implies restricted nul lspace.</em> If RIP(2 <em>k,δ</em> ) holds with <em>δ &lt;</em> 1 <em>/</em> 3, then the uniform RN property of order <em>k</em> holds, and hence the <em>`</em> 1 - relaxation is exact for all vectors supported on at most <em>k</em> elements.</p><p>We work through the proof of a slightly weaker version of this claim in Ex- ercise 10.8. Observe that the RIP(2 <em>k,δ</em> ) condition imposes constraints on a huge number of submatrices, namely</p><div class="language-"><pre><code>( p\n2 k\n</code></pre></div><h6 id="-414"><a class="header-anchor" href="#-414" aria-hidden="true">#</a> )</h6><p>in total. On the other hand, as op- posed to the pairwise incoherence condition, the actual RIP constant <em>δ</em> has no dependence on <em>k</em>. From known results in random matrix theory, various choices of ran- dom projection matrices <strong>X</strong> satisfy RIP with high probability as long as <em>N</em> % <em>k</em> log <em>epk</em>. Among other matrix ensembles, this statement applies to a stan- dard Gaussian random matrix <strong>X</strong> with i.i.d. <em>N</em> (0 <em>,N</em>^1 ) entries; see Exercise 10.6 for details. Thus, we see that the RIP-based approach provides a certificate for exact recovery based on far fewer samples than pairwise incoherence, which as previously discussed, provides guarantees when <em>N</em> % <em>k</em>^2 log <em>p</em>. On the other hand, a major drawback of RIP is that—in sharp contrast to the pairwise incoherence—it is very difficult to verify in practice due to the number</p><div class="language-"><pre><code>( p\n2 k\n</code></pre></div><h6 id="-415"><a class="header-anchor" href="#-415" aria-hidden="true">#</a> )</h6><p>of submatrices.</p><h6 id="_284-signal-approximation-and-compressed-sensing"><a class="header-anchor" href="#_284-signal-approximation-and-compressed-sensing" aria-hidden="true">#</a> 284 SIGNAL APPROXIMATION AND COMPRESSED SENSING</h6><h4 id="_10-4-3-proofs"><a class="header-anchor" href="#_10-4-3-proofs" aria-hidden="true">#</a> 10.4.3 Proofs</h4><p>We conclude the chapter by providing proofs of the claims given in the pre- ceding section.</p><h5 id="_10-4-3-1-proof-of-theorem-10-1"><a class="header-anchor" href="#_10-4-3-1-proof-of-theorem-10-1" aria-hidden="true">#</a> 10.4.3.1 Proof of Theorem 10.1</h5><p>First, suppose that <strong>X</strong> satisfies the RN( <em>S</em> ) property. Let <em>β</em> ̂∈R <em>p</em> be any op- timal solution to the basis pursuit LP (10.19), and define the error vector</p><p>∆ : = <em>β</em> ̂− <em>β</em> ∗. Our goal is to show that ∆ = 0, and in order to do so, it suffices to show that ∆∈null( <strong>X</strong> )∩C( <em>S</em> ). On the one hand, since <em>β</em> ∗and <em>β</em> ̂are optimal (and hence feasible) solutions to the <em><code>_ 0 and _</code></em> 1 problems, respectively, we are guaranteed that <strong>X</strong> <em>β</em> ∗= <strong>y</strong> = <strong>X</strong> <em>β</em> ̂, showing that <strong>X</strong> ∆ = 0. On the other hand, since <em>β</em> ∗is also feasible for the <em>`</em> 1 -based problem (10.19), the optimality of <em>β</em> ̂ implies that‖ <em>β</em> ̂‖ 1 ≤‖ <em>β</em> ∗‖ 1 =‖ <em>βS</em> ∗‖ 1. Writing <em>β</em> ̂= <em>β</em> ∗+ ∆, we have</p><div class="language-"><pre><code>‖ βS ∗‖ 1 ≥ ‖ β ̂‖ 1 =‖ β ∗ S + ∆ S ‖ 1 +‖∆ Sc ‖ 1\n≥‖ β ∗ S ‖ 1 −‖∆ S ‖ 1 +‖∆ Sc ‖ 1 ,\n</code></pre></div><p>where the final bound follows by triangle inequality. Rearranging terms, we find that ∆∈C( <em>S</em> ); since <strong>X</strong> satisfies the RN( <em>S</em> ) condition by assumption, we conclude that ∆ = 0 as required. We lead the reader through a proof of the converse in Exercise 10.4.</p><h5 id="_10-4-3-2-proof-of-proposition-10-1"><a class="header-anchor" href="#_10-4-3-2-proof-of-proposition-10-1" aria-hidden="true">#</a> 10.4.3.2 Proof of Proposition 10.1</h5><p>We may assume without loss of generality (rescaling as needed) that‖ <strong>x</strong> <em>j</em> ‖ 2 = 1 for all <em>j</em> = 1 <em>,</em> 2 <em>,...,p</em>. To simplify notation, let us assume an incoherence condition of the form <em>ν</em> ( <strong>X</strong> ) <em>&lt;δk</em> for some <em>δ &gt;</em> 0, and verify the sufficiency of <em>δ</em> = 1 <em>/</em> 3 in the course of the argument. For an arbitrary subset <em>S</em> of cardinality <em>k</em> , suppose that <em>β</em> ∈C( <em>S</em> ){ 0 }. It suffices to show that‖ <strong>X</strong> <em>β</em> ‖^22 <em>&gt;</em> 0, and so we begin with the lower bound</p><div class="language-"><pre><code>‖ X β ‖^22 ≥‖ X SβS ‖^22 + 2 βTS X TS X ScβSc. (10.24)\n</code></pre></div><p>On one hand, we have</p><div class="language-"><pre><code>2\n</code></pre></div><h6 id="∣-53"><a class="header-anchor" href="#∣-53" aria-hidden="true">#</a> ∣</h6><div class="language-"><pre><code>∣ βST X TS X ScβSc\n</code></pre></div><h6 id="∣-54"><a class="header-anchor" href="#∣-54" aria-hidden="true">#</a> ∣</h6><h6 id="∣≤-2"><a class="header-anchor" href="#∣≤-2" aria-hidden="true">#</a> ∣≤ 2</h6><h6 id="∑-73"><a class="header-anchor" href="#∑-73" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>i ∈ S\n</code></pre></div><h6 id="∑-74"><a class="header-anchor" href="#∑-74" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>j ∈ Sc\n</code></pre></div><div class="language-"><pre><code>| βi |·| βj |·|〈 x i, x j 〉|\n</code></pre></div><div class="language-"><pre><code>( i )\n≤ 2 ‖ βS ‖ 1 ‖ βSc ‖ 1 ν ( X )\n( ii )\n≤\n</code></pre></div><div class="language-"><pre><code>2 δ ‖ βS ‖^21\nk\n( iii )\n≤ 2 δ ‖ βS ‖^22 ,\n</code></pre></div><p>where inequality (i) uses the definition (10.22) of the pairwise incoherence;</p><h6 id="bibliographic-notes-285"><a class="header-anchor" href="#bibliographic-notes-285" aria-hidden="true">#</a> BIBLIOGRAPHIC NOTES 285</h6><p>inequality (ii) exploits the assumed bound on <em>ν</em> ( <strong>X</strong> ) combined with the fact that <em>β</em> ∈C( <em>S</em> ); and inequality (iii) uses the fact that‖ <em>βS</em> ‖^21 ≤ <em>k</em> ‖ <em>βS</em> ‖^22 , by Cauchy–Schwarz, since the cardinality of <em>S</em> is at most <em>k</em>. Consequently, we have established that</p><div class="language-"><pre><code>‖ X β ‖^22 ≥‖ X SβS ‖^22 − 2 δ ‖ βS ‖^22. (10.25)\n</code></pre></div><p>In order to complete the proof, it remains to lower bound‖ <strong>X</strong> <em>SβS</em> ‖^22. Letting ‖·‖opdenote the operator norm (maximum singular value) of a matrix, we have</p><div class="language-"><pre><code>‖ X TS X S − I k × k ‖op≤max\ni ∈ S\n</code></pre></div><h6 id="∑-75"><a class="header-anchor" href="#∑-75" aria-hidden="true">#</a> ∑</h6><div class="language-"><pre><code>j ∈ S \\{ i }\n</code></pre></div><div class="language-"><pre><code>|〈 xi, xj 〉| ≤ k\n</code></pre></div><div class="language-"><pre><code>δ\nk\n= δ.\n</code></pre></div><p>Consequently,‖ <strong>X</strong> <em>SβS</em> ‖^22 ≥(1− <em>δ</em> )‖ <em>βS</em> ‖^22 , and combined with the bound (10.25), we conclude that‖ <strong>X</strong> <em>β</em> ‖^22 <em>&gt;</em> (1− 3 <em>δ</em> )‖ <em>βS</em> ‖^22 , so that <em>δ</em> = 1 <em>/</em> 3 is sufficient as claimed.</p><h3 id="bibliographic-notes-7"><a class="header-anchor" href="#bibliographic-notes-7" aria-hidden="true">#</a> Bibliographic Notes</h3><p>There is an extensive literature on the sparsity of images and other signal classes when represented in wavelet and other multiscale bases (Field 1987, Ruderman 1994, Wainwright, Simoncelli and Willsky 2001, Simoncelli 2005). Sparse approximation in overcomplete bases is discussed in various pa- pers (Donoho and Stark 1989, Chen et al. 1998, Donoho and Huo 2001, Elad and Bruckstein 2002, Feuer and Nemirovski 2003). The multiscale basis il- lustrated in Figure 10.2 is known as the steerable pyramid (Simoncelli and Freeman 1995). Random projection is a widely used technique in computer science and numerical linear algebra (Vempala 2004, Mahoney 2011, Pilanci and Wainwright 2014, e.g.). Johnson and Lindenstrauss (1984) proved the lemma that now bears their name in the context of establishing the existence of metric embeddings, using random projection as a proof technique. Com- pressed sensing was introduced independently by Cand<code>es, Romberg and Tao (2006) and Donoho (2006). Lustig, Donoho, Santos and Pauly (2008) discuss the applications of compressed sensing to medical imaging, whereas Cand</code>es and Wakin (2008) discuss various applications in signal processing. The restricted nullspace property is discussed in Donoho and Huo (2001), Feuer and Nemirovski (2003), and Cohen, Dahmen and DeVore (2009). Var- ious authors (Donoho and Huo 2001, Elad and Bruckstein 2002, Feuer and Nemirovski 2003) have studied the pairwise incoherence of overcomplete bases and other design matrices, as a sufficient condition for the restricted nullspace property. Cand`es and Tao (2005) introduced the restricted isometry prop- erty as a milder sufficient condition for the restricted nullspace property. For random matrices with i.i.d. sub-Gaussian rows, it follows from a combination of union bound and standard results in random matrix theory (Davidson and Szarek 2001, Vershynin 2012) that a sample size <em>N &gt; ck</em> log</p><div class="language-"><pre><code>( ep\nk\n</code></pre></div><h6 id="-416"><a class="header-anchor" href="#-416" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>suffices to\n</code></pre></div><h6 id="_286-signal-approximation-and-compressed-sensing"><a class="header-anchor" href="#_286-signal-approximation-and-compressed-sensing" aria-hidden="true">#</a> 286 SIGNAL APPROXIMATION AND COMPRESSED SENSING</h6><p>ensure that the RIP is satisfied with high probability. Baraniuk, Davenport, DeVore and Wakin (2008) point out connections between the RIP and the Johnson–Lindenstrauss lemma; see also Exercise 10.6 for some related cal- culations. Krahmer and Ward (2011) establish a partial converse, showing that restricted isometry can be used to establish Johnson–Lindenstrauss type guarantees.</p><h3 id="exercises-7"><a class="header-anchor" href="#exercises-7" aria-hidden="true">#</a> Exercises</h3><p>Ex. 10.1 <em>Chi-squared concentration.</em> If <em>Y</em> 1 <em>,...,YN</em> are i.i.dN(0 <em>,</em> 1) variates, then the variable <em>Z</em> : =</p><h6 id="∑-n-109"><a class="header-anchor" href="#∑-n-109" aria-hidden="true">#</a> ∑ N</h6><p><em>i</em> =1 <em>Y</em> 2 <em>i</em> has a chi-squared distribution with <em>N</em> degrees of freedom. (In short, we write <em>Z</em> ∼ <em>χ</em>^2 <em>N</em> .)</p><div class="language-"><pre><code>(a) Show that for all λ ∈[−∞ , 1 / 2), we have\n</code></pre></div><div class="language-"><pre><code>E[exp( λ ( Z − d ))] =\n</code></pre></div><div class="language-"><pre><code>[ e − λ\n√\n1 − 2 λ\n</code></pre></div><h6 id="n-40"><a class="header-anchor" href="#n-40" aria-hidden="true">#</a> ] N</h6><h6 id="_10-26"><a class="header-anchor" href="#_10-26" aria-hidden="true">#</a> . (10.26)</h6><div class="language-"><pre><code>(b) Use the bound (10.26) to show that\n</code></pre></div><h6 id="p"><a class="header-anchor" href="#p" aria-hidden="true">#</a> P</h6><h6 id="-417"><a class="header-anchor" href="#-417" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>| Z − N |≥ tN\n</code></pre></div><h6 id="-418"><a class="header-anchor" href="#-418" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>≤ 2 e −\n</code></pre></div><div class="language-"><pre><code>Nt 322\nfor all t ∈(0 , 1 / 2). (10.27)\n</code></pre></div><div class="language-"><pre><code>(The constants in this tail bound are not sharp, and can be improved.)\n</code></pre></div><p>Ex. 10.2 <em>Johnson–Lindenstrauss approximation.</em> Recall from Section 10.3.1 the problem of distance-preserving embedding.</p><div class="language-"><pre><code>(a) Show that for any vector u with unit Euclidean norm, the random variable\nN ‖ F ( u )‖^22 follows a χ^2 -squared distribution with N degrees of freedom.\n(b) For any δ ∈(0 , 1), define the event\n</code></pre></div><div class="language-"><pre><code>E( δ ) : =\n</code></pre></div><h6 id="-419"><a class="header-anchor" href="#-419" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>‖ F ( ui )− F ( uj )‖^22\n‖ ui − uj ‖^22\n</code></pre></div><div class="language-"><pre><code>∈[1− δ, 1 + δ ] for all pairs i 6 = j.\n</code></pre></div><h6 id="-420"><a class="header-anchor" href="#-420" aria-hidden="true">#</a> }</h6><h6 id="-421"><a class="header-anchor" href="#-421" aria-hidden="true">#</a> .</h6><div class="language-"><pre><code>Use the results of Exercise 10.1 and the union bound to show that\n</code></pre></div><div class="language-"><pre><code>P\n</code></pre></div><h6 id="-422"><a class="header-anchor" href="#-422" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>E( δ )]≥ 1 − 2 e − N.\n</code></pre></div><div class="language-"><pre><code>as long as N &gt;^64 δ 2 log M.\n</code></pre></div><p>Ex. 10.3 For a given compact setA ⊂R <em>p</em> , an <em></em> -covering set is a subset { <em>u</em> 1 <em>,...,uM</em> }of elements ofAwith the property for any <em>u</em> ∈A, there is some index <em>j</em> ∈{ 1 <em>,...,M</em> }such that‖ <em>u</em> − <em>uj</em> ‖ 2 ≤ <em></em>. A <em></em> -packing set is a subset { <em>v</em>^1 <em>,...,vM</em></p><p>′ }of elements ofAsuch that such that‖ <em>vi</em> − <em>vj</em> ‖ 2 <em>&gt; </em> for all pairs <em>i</em> 6 = <em>j</em> in{ 1 <em>,...,M</em> ′}. We use <em>M</em> ( <em></em> ) to denote the size of the largest <em></em> -packing, and <em>N</em> ( <em></em> ) to denote the size of the smallest <em></em> -covering.</p><div class="language-"><pre><code>(a) Show that M (2  )≤ N (  ).\n</code></pre></div><h6 id="exercises-287"><a class="header-anchor" href="#exercises-287" aria-hidden="true">#</a> EXERCISES 287</h6><div class="language-"><pre><code>(b) Show that N (  )≤ M (  ).\n(c) Consider the Euclidean ballB 2 (1) ={ u ∈R p | ‖ u ‖ 2 = 1}. For each\n ∈(0 , 1), show that there exists an  -covering set with at most M = ( c/ ) p\nelements, for some universal constant c &gt; 0. ( Hint: Use part (b) and consider\nthe volumes of Euclidean balls in p -dimensions.)\n</code></pre></div><p>Ex. 10.4 In this exercise, we work through the proof of the converse of The- orem 10.1, in particular showing that if the <em><code>_ 1 -relaxation has a unique opti- mal solution, equal to the _</code></em> 0 -solution, for all <em>S</em> -sparse vectors, then the set null( <strong>X</strong> ){ 0 }has no intersection withC( <em>S</em> ).</p><div class="language-"><pre><code>(a) For a given vector β ∗∈null( X )\\{ 0 }, consider the basis-pursuit problem\n</code></pre></div><div class="language-"><pre><code>minimize\nβ ∈R p\n‖ β ‖ 1 such that X β = X\n</code></pre></div><h6 id="-423"><a class="header-anchor" href="#-423" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>β ∗ S\n0\n</code></pre></div><h6 id="-424"><a class="header-anchor" href="#-424" aria-hidden="true">#</a> ]</h6><h6 id="-425"><a class="header-anchor" href="#-425" aria-hidden="true">#</a> .</h6><div class="language-"><pre><code>What is the link between its unique optimal solution β ̂and the vector\n[\n0\n− β ∗ Sc\n</code></pre></div><h6 id="-426"><a class="header-anchor" href="#-426" aria-hidden="true">#</a> ]</h6><h6 id="-427"><a class="header-anchor" href="#-427" aria-hidden="true">#</a> ?</h6><div class="language-"><pre><code>(b) Use part (a) to show that β ∗∈ / C( S ).\n</code></pre></div><p>Ex. 10.5 Let <strong>X</strong> ∈R <em>N</em> × <em>p</em> be a random matrix with i.i.d.N</p><h6 id="-428"><a class="header-anchor" href="#-428" aria-hidden="true">#</a> (</h6><h6 id="_0-1-n"><a class="header-anchor" href="#_0-1-n" aria-hidden="true">#</a> 0 , 1 /N</h6><h6 id="-429"><a class="header-anchor" href="#-429" aria-hidden="true">#</a> )</h6><p>entries. Show that it satisfies the pairwise incoherence condition (10.22) as long as <em>N &gt; ck</em>^2 log <em>p</em> for a universal constant <em>c</em>. ( <em>Hint:</em> The result of Exercise 10.1 may be useful.)</p><p>Ex. 10.6 Let <strong>X</strong> ∈R <em>N</em> × <em>p</em> be a random matrix with i.i.d.N</p><h6 id="-430"><a class="header-anchor" href="#-430" aria-hidden="true">#</a> (</h6><h6 id="_0-1-n-1"><a class="header-anchor" href="#_0-1-n-1" aria-hidden="true">#</a> 0 , 1 /N</h6><h6 id="-431"><a class="header-anchor" href="#-431" aria-hidden="true">#</a> )</h6><p>entries. In this exercise, we show that the restricted isometry property (RIP) holds with high probability as long as <em>N &gt; ck</em> log( <em>ep/k</em> ) for a sufficiently large constant <em>c &gt;</em> 0.</p><div class="language-"><pre><code>(a) Explain why it is sufficient to prove that there are constants c 1 ,c 2 such\nthat\n‖ X TS X S − I 2 k × 2 k ‖op≤ t (10.28)\n</code></pre></div><div class="language-"><pre><code>with probability at least 1− c 1 e − c^2 Nt\n</code></pre></div><div class="language-"><pre><code>2\n, for any fixed subset S of cardinality\n2 k , and any t ∈(0 , 1).\n(b) LetB 2 (1; S ) ={ u ∈R p | ‖ u ‖ 2 = 1 and uSc = 0}denote the inter-\nsection of the Euclidean ball with the subspace of vectors supported on a\ngiven subset S. Let{ u 1 ,...,uM }be an  -covering of the setB 2 (1; S ), as\npreviously defined in Exercise 10.3. Show that the bound (10.28) is implied\nby a bound of the form\n</code></pre></div><div class="language-"><pre><code>max\nj =1 ,...,M\n</code></pre></div><h6 id="∣-55"><a class="header-anchor" href="#∣-55" aria-hidden="true">#</a> ∣</h6><div class="language-"><pre><code>∣‖ X uj ‖^22 − 1\n</code></pre></div><h6 id="∣-56"><a class="header-anchor" href="#∣-56" aria-hidden="true">#</a> ∣</h6><h6 id="∣≤"><a class="header-anchor" href="#∣≤" aria-hidden="true">#</a> ∣≤ ,</h6><div class="language-"><pre><code>with probability at least 1− c 3 ec^4 N\n2\n, for any  ∈(0 , 1).\n(c) Use part (b) and Exercise 10.3 to complete the proof.\n</code></pre></div><h6 id="_288-signal-approximation-and-compressed-sensing"><a class="header-anchor" href="#_288-signal-approximation-and-compressed-sensing" aria-hidden="true">#</a> 288 SIGNAL APPROXIMATION AND COMPRESSED SENSING</h6><p>Ex. 10.7 <em><code>_ 0 _and</code></em> 1 <em>-bal ls.</em> In this exercise, we consider the relationship between <em><code>_ 0 and _</code></em> 1 -balls, and prove a containment property related to the success of <em>`</em> 1 -relaxation. For an integer <em>r</em> ∈{ 1 <em>,...,p</em> }, consider the following two subsets:</p><div class="language-"><pre><code>L 0 ( r ) : =B 2 (1)∩B 0 ( r ) =\n</code></pre></div><h6 id="-432"><a class="header-anchor" href="#-432" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>θ ∈R p | ‖ θ ‖ 2 ≤ 1 , and‖ θ ‖ 0 ≤ r\n</code></pre></div><h6 id="-433"><a class="header-anchor" href="#-433" aria-hidden="true">#</a> }</h6><h6 id="-434"><a class="header-anchor" href="#-434" aria-hidden="true">#</a> ,</h6><div class="language-"><pre><code>L 1 ( r ) : =B 2 (1)∩B 1 (\n</code></pre></div><h6 id="√-30"><a class="header-anchor" href="#√-30" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>r ) =\n</code></pre></div><h6 id="-435"><a class="header-anchor" href="#-435" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>θ ∈R p | ‖ θ ‖ 2 ≤ 1 , and‖ θ ‖ 1 ≤\n</code></pre></div><h6 id="√-31"><a class="header-anchor" href="#√-31" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>r\n</code></pre></div><h6 id="-436"><a class="header-anchor" href="#-436" aria-hidden="true">#</a> }</h6><h6 id="-437"><a class="header-anchor" href="#-437" aria-hidden="true">#</a> .</h6><p>Letconv denote the closure of the convex hull (when applied to a set).</p><div class="language-"><pre><code>(a) Prove thatconv\n</code></pre></div><h6 id="-438"><a class="header-anchor" href="#-438" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>L 0 ( r )\n</code></pre></div><h6 id="-439"><a class="header-anchor" href="#-439" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>⊆L 1 ( r ).\n(b) Prove thatL 1 ( r )⊆ 2 conv\n</code></pre></div><h6 id="-440"><a class="header-anchor" href="#-440" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>L 0 ( r )\n</code></pre></div><h6 id="-441"><a class="header-anchor" href="#-441" aria-hidden="true">#</a> )</h6><h6 id="-442"><a class="header-anchor" href="#-442" aria-hidden="true">#</a> .</h6><p>( <em>Hint:</em> Part (b) is a more challenging problem: you may find it useful to con- sider the support functions of the two sets. )</p><p>Ex. 10.8 In this exercise, we work through a proof of (a slightly weaker version of) Proposition 10.2.</p><div class="language-"><pre><code>(a) For any subset S of cardinality k , the setC( S )∩B 2 (1) is contained within\nthe setL 1 ( r ) with r = 4 k.\n(b) Now show that if RIP(8 k,δ ) holds with δ &lt; 1 / 4, then the restricted\nnullspace property holds. ( Hint: Part (b) of Exercise 10.7 could be useful.)\n</code></pre></div><div class="language-"><pre><code>Chapter 11\n</code></pre></div><h2 id="theoretical-results-for-the-lasso"><a class="header-anchor" href="#theoretical-results-for-the-lasso" aria-hidden="true">#</a> Theoretical Results for the Lasso</h2><p>In this chapter, we turn our attention to some theoretical results concern- ing the behavior of the lasso. We provide non-asymptotic bounds for the <em>`</em> 2 and prediction error of the lasso, as well as its performance in recovering the support set of the unknown regression vector.</p><h3 id="_11-1-introduction"><a class="header-anchor" href="#_11-1-introduction" aria-hidden="true">#</a> 11.1 Introduction</h3><p>Consider the standard linear regression model in matrix-vector form</p><div class="language-"><pre><code>y = X β ∗+ w , (11.1)\n</code></pre></div><p>where <strong>X</strong> ∈R <em>N</em> × <em>p</em> is the model (design) matrix, <strong>w</strong> ∈R <em>N</em> is a vector of noise variables, and <em>β</em> ∗∈R <em>p</em> is the unknown coefficient vector. In this chapter, we develop some theoretical guarantees for both the constrained form of the lasso</p><div class="language-"><pre><code>minimize\n‖ β ‖ 1 ≤ R\n</code></pre></div><div class="language-"><pre><code>‖ y − X β ‖^22 , (11.2)\n</code></pre></div><div class="language-"><pre><code>as well as for its Lagrangian version\n</code></pre></div><div class="language-"><pre><code>minimize\nβ ∈R p\n</code></pre></div><h6 id="_1-144"><a class="header-anchor" href="#_1-144" aria-hidden="true">#</a> { 1</h6><h6 id="_2-n-18"><a class="header-anchor" href="#_2-n-18" aria-hidden="true">#</a> 2 N</h6><div class="language-"><pre><code>‖ y − X β ‖^22 + λN ‖ β ‖ 1\n</code></pre></div><h6 id="-443"><a class="header-anchor" href="#-443" aria-hidden="true">#</a> }</h6><h6 id="_11-3"><a class="header-anchor" href="#_11-3" aria-hidden="true">#</a> . (11.3)</h6><p>As we have discussed previously, by Lagrangian duality, there is a correspon- dence between these two families of quadratic programs, where <em>λN</em> can be in- terpreted as the Lagrange multiplier associated with the constraint‖ <em>β</em> ‖ 1 ≤ <em>R</em>.</p><h4 id="_11-1-1-types-of-loss-functions"><a class="header-anchor" href="#_11-1-1-types-of-loss-functions" aria-hidden="true">#</a> 11.1.1 Types of Loss Functions</h4><p>Given a lasso estimate <em>β</em> ̂∈R <em>p</em> , we can assess its quality in various ways. In some settings, we are interested in the predictive performance of <em>β</em> ̂, so that we might compute a <em>prediction loss function</em> of the form</p><div class="language-"><pre><code>Lpred( β ̂; β ∗) =\n</code></pre></div><h6 id="_1-145"><a class="header-anchor" href="#_1-145" aria-hidden="true">#</a> 1</h6><h6 id="n-41"><a class="header-anchor" href="#n-41" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>‖ X β ̂− X β ∗‖^22 , (11.4)\n</code></pre></div><div class="language-"><pre><code>289\n</code></pre></div><h6 id="_290-theoretical-results-for-the-lasso"><a class="header-anchor" href="#_290-theoretical-results-for-the-lasso" aria-hidden="true">#</a> 290 THEORETICAL RESULTS FOR THE LASSO</h6><p>corresponding to the mean-squared error of <em>β</em> ̂over the given samples of <strong>X</strong>. In other applications—among them medical imaging, remote sensing, and com- pressed sensing—the unknown vector <em>β</em> ∗is of primary interest, so that it is most appropriate to consider loss functions such as the <em>`</em> 2 -error</p><div class="language-"><pre><code>L 2 ( β ̂; β ∗) =‖ β ̂− β ∗‖^22 , (11.5)\n</code></pre></div><p>which we refer to as a <em>parameter estimation loss</em>. Finally, we might actually be interested in variable selection or <em>support recovery</em> , and so use the loss function</p><div class="language-"><pre><code>Lvs( β ̂; β ∗) =\n</code></pre></div><h6 id="-444"><a class="header-anchor" href="#-444" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>0 if sign( β ̂ i ) = sign( β ∗ i ) for all i = 1 ,...,p,\n1 otherwise.\n</code></pre></div><h6 id="_11-6"><a class="header-anchor" href="#_11-6" aria-hidden="true">#</a> (11.6)</h6><p>This assesses whether or not the estimate <em>β</em> ̂shares the same signed support as <em>β</em> ∗. In this chapter, we provide theoretical results for all three types of losses.</p><h4 id="_11-1-2-types-of-sparsity-models"><a class="header-anchor" href="#_11-1-2-types-of-sparsity-models" aria-hidden="true">#</a> 11.1.2 Types of Sparsity Models</h4><p>A classical analysis of a method such as the lasso would fix the number of covariates <em>p</em> , and then take the sample size <em>N</em> to infinity. Although this type of analysis is certainly useful in some regimes, there are many settings in which the number of covariates <em>p</em> may be of the same order, or substantially larger than the sample size <em>N</em>. Examples include microarray gene expression analysis, which might involve <em>N</em> = 100 observations of <em>p</em> = 10 <em>,</em> 000 genes, or social networks, in which one makes relatively few observations of a large number of individuals. In such settings, it is doubtful whether theoretical results based on “fixed <em>p</em> , large <em>N</em> ” scaling would provide useful guidance to practitioners. Accordingly, our aim in this chapter is to develop theory that is applicable to the high-dimensional regime, meaning that it allows for the scaling <em>p</em>  <em>N</em>. Of course, if the model lacks any additional structure, then there is no hope of recovering useful information about a <em>p</em> -dimensional vector with limited samples. Indeed, whenever <em>N &lt; p</em> , the linear model (11.1) is unidentifiable; for instance, it is impossible to distinguish between the models <em>β</em> ∗= 0 and <em>β</em> ∗= ∆, where ∆∈R <em>p</em> is any element of the <em>p</em> − <em>N</em> -dimensional nullspace of <strong>X</strong>. For this reason, it is necessary to impose additional constraints on the unknown regression vector <em>β</em> ∗∈R <em>p</em> , and here we focus on various types of sparsity constraints. The first setting is that of <em>hard sparsity</em> , in which we as- sume that <em>β</em> ∗has at most <em>k</em> ≤ <em>p</em> nonzero entries. For such hard-sparse models, it makes sense to consider the prediction and <em>`</em> 2 -norm losses as well as the variable selection loss (11.6). Assuming that the model is exactly supported on <em>k</em> coefficients may be overly restrictive, so that we also consider the case of <em>weakly sparse</em> models, meaning that <em>β</em> ∗can be closely approximated by vectors with few nonzero entries. For instance, one way of formalizing this</p><h6 id="bounds-on-lasso-2-error-291"><a class="header-anchor" href="#bounds-on-lasso-2-error-291" aria-hidden="true">#</a> BOUNDS ON LASSO ` 2 -ERROR 291</h6><div class="language-"><pre><code>β 2\nβ 1\n</code></pre></div><div class="language-"><pre><code>β 3\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><div class="language-"><pre><code>β 2\nβ 1\n</code></pre></div><div class="language-"><pre><code>β 3\n</code></pre></div><div class="language-"><pre><code>1\n</code></pre></div><p><strong>Figure 11.1</strong> <em>Left: Forq</em> = 1 <em>, the set</em> B( <em>Rq</em> ) <em>corresponds to the`</em> 1 <em>-ball, which is a convex set. Right: Settingq</em> = 0_._ 75 <em>yields a nonconvex set, with spikes along the coordinate axes.</em></p><p>notion is by defining, for a parameter <em>q</em> ∈[0 <em>,</em> 1] and radius <em>Rq&gt;</em> 0, the set</p><div class="language-"><pre><code>B( Rq ) =\n</code></pre></div><h6 id="-445"><a class="header-anchor" href="#-445" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>β ∈R p |\n</code></pre></div><div class="language-"><pre><code>∑ p\n</code></pre></div><div class="language-"><pre><code>j =1\n</code></pre></div><div class="language-"><pre><code>| βi | q ≤ Rq\n</code></pre></div><h6 id="-446"><a class="header-anchor" href="#-446" aria-hidden="true">#</a> }</h6><h6 id="_11-7"><a class="header-anchor" href="#_11-7" aria-hidden="true">#</a> . (11.7)</h6><p>This set is known as the <em>`q</em> -“ball” of radius^1 <em>Rq</em> ; as illustrated in Figure 11.1, for <em>q</em> ∈[0 <em>,</em> 1), it is not a ball in the strict sense of the word, since it is a nonconvex set. In the special case <em>q</em> = 0, imposing the constraint <em>β</em> ∗∈B( <em>R</em> 0 ) is equivalent to requiring that <em>β</em> ∗has at most <em>k</em> = <em>R</em> 0 nonzero entries.</p><h3 id="_11-2-bounds-on-lasso-2-error"><a class="header-anchor" href="#_11-2-bounds-on-lasso-2-error" aria-hidden="true">#</a> 11.2 Bounds on Lasso ` 2 -Error</h3><p>We begin by developing some results on the <em>`</em> 2 -norm loss (11.5) between a lasso</p><p>solution <em>β</em> ̂and the true regression vector <em>β</em> ∗. We focus on the case when <em>β</em> ∗is <em>k</em> -sparse, meaning that its entries are nonzero on a subset <em>S</em> ( <em>β</em> ∗)⊂{ 1 <em>,</em> 2 <em>,...,p</em> } of cardinality <em>k</em> =| <em>S</em> ( <em>β</em> ∗)|. In the exercises, we work through some extensions to the case of weakly-sparse coefficient vectors.</p><h4 id="_11-2-1-strong-convexity-in-the-classical-setting"><a class="header-anchor" href="#_11-2-1-strong-convexity-in-the-classical-setting" aria-hidden="true">#</a> 11.2.1 Strong Convexity in the Classical Setting</h4><p>We begin by developing some conditions on the model matrix <strong>X</strong> that are needed to establish bounds on <em><code>_ 2 -error. In order to provide some intuition for these conditions, we begin by considering one route for proving _</code></em> 2 -consistency in the classical setting (i.e., <em>p</em> fixed, <em>N</em> tending to infinity). Suppose that we estimate some parameter vector <em>β</em> ∗by minimizing a data-dependent objective</p><p>(^1) Strictly speaking, the radius would be <em>R</em> (^1) <em>q q</em> , but we take this liberty so as to simplify notation.</p><h6 id="_292-theoretical-results-for-the-lasso"><a class="header-anchor" href="#_292-theoretical-results-for-the-lasso" aria-hidden="true">#</a> 292 THEORETICAL RESULTS FOR THE LASSO</h6><p>function <em>fN</em> ( <em>β</em> ) over some constraint set. (For instance, the lasso minimizes the least-squares loss <em>fN</em> ( <em>β</em> ) = <em>N</em>^1 ‖ <strong>y</strong> − <strong>X</strong> <em>β</em> ‖^22 subject to an <em>`</em> 1 -constraint.) Let</p><p>us suppose that the difference in function values ∆ <em>fN</em> =| <em>fN</em> ( <em>β</em> ̂)− <em>fN</em> ( <em>β</em> ∗)| converges to zero as the sample size <em>N</em> increases. The key question is the following: what additional conditions are needed to ensure that the <em>`</em> 2 -norm of the parameter vector difference ∆ <em>β</em> =‖ <em>β</em> ̂− <em>β</em> ∗‖ 2 also converges to zero?</p><div class="language-"><pre><code>β∗ β̂\n</code></pre></div><div class="language-"><pre><code>∆fN\n</code></pre></div><div class="language-"><pre><code>∆β\n</code></pre></div><div class="language-"><pre><code>β∗ β̂\n</code></pre></div><div class="language-"><pre><code>∆fN\n</code></pre></div><div class="language-"><pre><code>∆β\n</code></pre></div><p><strong>Figure 11.2</strong> <em>Relation between differences in objective function values and differ- ences in parameter values. Left: the functionfNis relatively “flat” around its opti- mumβ</em> ̂ <em>, so that a smal l function difference</em> ∆ <em>fN</em> =| <em>fN</em> (̂ <em>β</em> )− <em>fN</em> ( <em>β</em> ∗)| <em>does not imply that</em> ∆ <em>β</em> =‖ <em>β</em> ̂− <em>β</em> ∗‖ 2 <em>is smal l. Right: the functionfNis strongly curved around its optimum, so that a smal l difference</em> ∆ <em>fNin function values translates into a small difference in parameter values.</em></p><p>To understand the issues involved, suppose that for some <em>N</em> , the objec- tive function <em>fN</em> takes the form shown in Figure 11.2(a). Due to the relative “flatness” of the objective function around its minimum <em>β</em> ̂, we see that the difference ∆ <em>fN</em> =| <em>fN</em> ( <em>β</em> ̂)− <em>fN</em> ( <em>β</em> ∗)|in function values is quite small while at the same time the difference ∆ <em>β</em> =‖ <em>β</em> ̂− <em>β</em> ∗‖ 2 in parameter values is relatively large. In contrast, Figure 11.2(b) shows a more desirable situation, in which the objective function has a high degree of curvature around its minimum <em>β</em> ̂. In this case, a bound on the function difference ∆ <em>fN</em> =| <em>fN</em> ( <em>β</em> ̂)− <em>fN</em> ( <em>β</em> ∗)| translates directly into a bound on ∆ <em>β</em> =‖ <em>β</em> ̂− <em>β</em> ∗‖ 2. How do we formalize the intuition captured by Figure 11.2? A natural way to specify that a function is suitably “curved” is via the notion of strong convexity. More specifically, given a differentiable function <em>f</em> :R <em>p</em> →R, we say that it is <em>strongly convex</em> with parameter <em>γ &gt;</em> 0 at <em>θ</em> ∈R <em>p</em> if the inequality</p><div class="language-"><pre><code>f ( θ ′)− f ( θ )≥∇ f ( θ ) T ( θ ′− θ ) +\n</code></pre></div><div class="language-"><pre><code>γ\n2\n</code></pre></div><div class="language-"><pre><code>‖ θ ′− θ ‖^22 (11.8)\n</code></pre></div><p>hold for all <em>θ</em> ′ ∈R <em>p</em>. Note that this notion is a strengthening of ordinary convexity, which corresponds to the case <em>γ</em> = 0. When the function <em>f</em> is twice continuously differentiable, an alternative characterization of strong convexity</p><h6 id="bounds-on-lasso-2-error-293"><a class="header-anchor" href="#bounds-on-lasso-2-error-293" aria-hidden="true">#</a> BOUNDS ON LASSO ` 2 -ERROR 293</h6><p>is in terms of the Hessian∇^2 <em>f</em> : in particular, the function <em>f</em> is strongly convex with parameter <em>γ</em> around <em>β</em> ∗∈R <em>p</em> if and only if the minimum eigenvalue of the Hessian matrix∇^2 <em>f</em> ( <em>β</em> ) is at least <em>γ</em> for all vectors <em>β</em> in a neighborhood of <em>β</em> ∗. If <em>f</em> is the negative log-likelihood under a parametric model, then∇^2 <em>f</em> ( <em>β</em> ∗) is the observed <em>Fisher information</em> matrix, so that strong convexity corresponds to a uniform lower bound on the Fisher information in all directions.</p><h4 id="_11-2-2-restricted-eigenvalues-for-regression"><a class="header-anchor" href="#_11-2-2-restricted-eigenvalues-for-regression" aria-hidden="true">#</a> 11.2.2 Restricted Eigenvalues for Regression</h4><p>Let us now return to the high-dimensional setting, in which the number of parameters <em>p</em> might be larger than <em>N</em>. It is clear that the least-squares objective function <em>fN</em> ( <em>β</em> ) = 21 <em>N</em> ‖ <strong>y</strong> − <strong>X</strong> <em>β</em> ‖^22 is always convex; under what additional conditions is it also strongly convex? A straightforward calculation yields that ∇^2 <em>f</em> ( <em>β</em> ) = <strong>X</strong> <em>T</em> <strong>X</strong> <em>/N</em> for all <em>β</em> ∈R <em>p</em>. Thus, the least-squares loss is strongly convex if and only if the eigenvalues of the <em>p</em> × <em>p</em> positive semidefinite matrix <strong>X</strong> <em>T</em> <strong>X</strong> are uniformly bounded away from zero. However, it is easy to see that any matrix of the form <strong>X</strong> <em>T</em> <strong>X</strong> has rank at most min{ <em>N,p</em> }, so it is always rank-deficient—and hence <em>not</em> strongly convex—whenever <em>N &lt; p</em>. Figure 11.3 illustrates the situation.</p><div class="language-"><pre><code>C\n</code></pre></div><div class="language-"><pre><code>νbad\n</code></pre></div><div class="language-"><pre><code>νgood\n</code></pre></div><p><strong>Figure 11.3</strong> <em>A convex loss function in high-dimensional settings (withp</em>  <em>N) can- not be strongly convex; rather, it wil l be curved in some directions but flat in others. As shown in Lemma 11.1, the lasso error</em> ̂ <em>ν</em> = <em>β</em> ̂− <em>β</em> ∗ <em>must lie in a restricted subset</em> C <em>of</em> R <em>p. For this reason, it is only necessary that the loss function be curved in certain directions of space.</em></p><p>For this reason, we need to relax our notion of strong convexity. It turns out, as will be clarified by the analysis below, that it is only necessary to impose a type of strong convexity condition for some subsetC ⊂R <em>p</em> of possible</p><h6 id="_294-theoretical-results-for-the-lasso"><a class="header-anchor" href="#_294-theoretical-results-for-the-lasso" aria-hidden="true">#</a> 294 THEORETICAL RESULTS FOR THE LASSO</h6><p>perturbation vectors <em>ν</em> ∈R <em>p</em>. In particular, we say that a function <em>f</em> satisfies <em>restricted strong convexity</em> at <em>β</em> ∗with respect toCif there is a constant <em>γ &gt;</em> 0 such that</p><div class="language-"><pre><code>νT ∇^2 f ( β ) ν\n‖ ν ‖^22\n</code></pre></div><div class="language-"><pre><code>≥ γ for all nonzero ν ∈C, (11.9)\n</code></pre></div><p>and for all <em>β</em> ∈R <em>p</em> in a neighborhood of <em>β</em> ∗. In the specific case of linear re- gression, this notion is equivalent to lower bounding the <em>restricted eigenvalues</em> of the model matrix—in particular, requiring that</p><div class="language-"><pre><code>1\nNν\n</code></pre></div><div class="language-"><pre><code>T X T X ν\n‖ ν ‖^22\n≥ γ for all nonzero ν ∈C. (11.10)\n</code></pre></div><p>What constraint setsCare relevant? Suppose that the parameter vector <em>β</em> ∗ is sparse—say supported on the subset <em>S</em> = <em>S</em> ( <em>β</em> ∗). Defining the lasso error ̂ <em>ν</em> = <em>β</em> ̂− <em>β</em> ∗, let̂ <em>νS</em> ∈R| <em>S</em> |denote the subvector indexed by elements of <em>S</em> , with <em>ν</em> ̂ <em>Sc</em> defined in an analogous manner. For appropriate choices of the <em>`</em> 1 - ball radius—or equivalently, of the regularization parameter <em>λN</em> —it turns out that the lasso error satisfies a <em>cone constraint</em> of the form</p><div class="language-"><pre><code>‖̂ νSc ‖ 1 ≤ α ‖̂ νS ‖ 1 , (11.11)\n</code></pre></div><p>for some constant <em>α</em> ≥1. This fact is easiest to see for the lasso in its con- strained version. Indeed, assuming that we solve the constrained lasso (11.2) with ball radius <em>R</em> =‖ <em>β</em> ∗‖ 1 , then since <em>β</em> ̂is feasible for the program, we have</p><div class="language-"><pre><code>R =‖ β ∗ S ‖ 1 ≥‖ β ∗+ ν ̂‖ 1\n=‖ β ∗ S +̂ νS ‖ 1 +‖̂ νSc ‖ 1\n≥‖ β ∗ S ‖ 1 −‖ ν ̂ S ‖ 1 +‖̂ νSc ‖ 1.\n</code></pre></div><p>Rearranging this inequality, we see that the bound (11.11) holds with <em>α</em> = 1. If we instead solve the regularized version (11.3) of the lasso with a “suitable” choice of <em>λN</em> , then it turns out that the error satisfies the constraint</p><div class="language-"><pre><code>‖ ν ̂ Sc ‖ 1 ≤ 3 ‖ ν ̂ S ‖ 1. (11.12)\n</code></pre></div><p>(We establish this fact during the proof of Theorem 11.1 to follow.) Thus, in either its constrained or regularized form, the lasso error is restricted to a set of the form</p><div class="language-"><pre><code>C( S ; α ) : =\n</code></pre></div><h6 id="-447"><a class="header-anchor" href="#-447" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>ν ∈R p |‖ νSc ‖ 1 ≤ α ‖ νS ‖ 1\n</code></pre></div><h6 id="-448"><a class="header-anchor" href="#-448" aria-hidden="true">#</a> }</h6><h6 id="_11-13"><a class="header-anchor" href="#_11-13" aria-hidden="true">#</a> , (11.13)</h6><p>for some parameter <em>α</em> ≥1; see Figure 11.3 for an illustration.</p><h4 id="_11-2-3-a-basic-consistency-result"><a class="header-anchor" href="#_11-2-3-a-basic-consistency-result" aria-hidden="true">#</a> 11.2.3 A Basic Consistency Result</h4><p>With this intuition in place, we now state a result that provides a bound on the lasso error‖ <em>β</em> ̂− <em>β</em> ∗‖ 2 , based on the linear observation model <strong>y</strong> = <strong>X</strong> <em>β</em> ∗+ <strong>w</strong> , where <em>β</em> ∗is <em>k</em> -sparse, supported on the subset <em>S</em>.</p><h6 id="bounds-on-lasso-2-error-295"><a class="header-anchor" href="#bounds-on-lasso-2-error-295" aria-hidden="true">#</a> BOUNDS ON LASSO ` 2 -ERROR 295</h6><p><em>Theorem 11.1.</em> Suppose that the model matrix <strong>X</strong> satisfies the restricted eigen- value bound (11.10) with parameter <em>γ &gt;</em> 0 overC( <em>S</em> ; 3).</p><div class="language-"><pre><code>(a) Then any estimate β ̂based on the constrained lasso (11.2) with R =\n‖ β ∗‖ 1 satisfies the bound\n</code></pre></div><div class="language-"><pre><code>‖ β ̂− β ∗‖ 2 ≤\n</code></pre></div><h6 id="_4"><a class="header-anchor" href="#_4" aria-hidden="true">#</a> 4</h6><div class="language-"><pre><code>γ\n</code></pre></div><h6 id="√-32"><a class="header-anchor" href="#√-32" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>k\nN\n</code></pre></div><h6 id="∥-11"><a class="header-anchor" href="#∥-11" aria-hidden="true">#</a> ∥</h6><h6 id="∥-x"><a class="header-anchor" href="#∥-x" aria-hidden="true">#</a> ∥ X</h6><div class="language-"><pre><code>T w\n√\nN\n</code></pre></div><h6 id="∥-12"><a class="header-anchor" href="#∥-12" aria-hidden="true">#</a> ∥</h6><h6 id="∥-13"><a class="header-anchor" href="#∥-13" aria-hidden="true">#</a> ∥</h6><div class="language-"><pre><code>∞. (11.14a)\n</code></pre></div><div class="language-"><pre><code>(b) Given a regularization parameter λN ≥ 2 ‖ X T w ‖∞ /N &gt; 0, any estimate\nβ ̂from the regularized lasso (11.3) satisfies the bound\n</code></pre></div><div class="language-"><pre><code>‖ β ̂− β ∗‖ 2 ≤\n</code></pre></div><h6 id="_3"><a class="header-anchor" href="#_3" aria-hidden="true">#</a> 3</h6><div class="language-"><pre><code>γ\n</code></pre></div><h6 id="√-33"><a class="header-anchor" href="#√-33" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>k\nN\n</code></pre></div><h6 id="√-34"><a class="header-anchor" href="#√-34" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>N λN. (11.14b)\n</code></pre></div><p>Before proving these results, let us discuss the different factors in the bounds (11.14a) and (11.14b), and then illustrate them with some examples. First, it is important to note that these results are deterministic, and apply to any set of linear regression equations with a given observed noise vector <strong>w</strong>. Below we obtain results for specific statistical models, as determined by assumptions on the noise vector <strong>w</strong> and/or the model matrix. These assump- tions will affect the rate through the restricted eigenvalue constant <em>γ</em> , and the terms‖ <strong>X</strong> <em>T</em> <strong>w</strong> ‖∞and <em>λN</em> in the two bounds. Based on our earlier discussion of the role of strong convexity, it is natural that lasso <em>`</em> 2 -error is inversely pro- portional to the restricted eigenvalue constant <em>γ &gt;</em> 0. The second term</p><h6 id="√-35"><a class="header-anchor" href="#√-35" aria-hidden="true">#</a> √</h6><p><em>k/N</em> is also to be expected, since we are trying to estimate unknown regression vector with <em>k</em> unknown entries based on <em>N</em> samples. As we have discussed, the final term in both bounds, involving either‖ <strong>X</strong> <em>T</em> <strong>w</strong> ‖∞or <em>λN</em> , reflects the interaction of the observation noise <strong>w</strong> with the model matrix <strong>X</strong>. It is instructive to consider the consequences of Theorem 11.1 for some linear regression models that are commonly used and studied.</p><p><em>Example 11.1. Classical linear Gaussian model.</em> We begin with the classical linear Gaussian model, for which the observation noise <strong>w</strong> ∈R <em>N</em> is Gaussian, with i.i.d. <em>N</em> (0 <em>,σ</em>^2 ) entries. Let us view the design matrix <strong>X</strong> as fixed, with columns{ <strong>x</strong> 1 <em>,...,</em> <strong>x</strong> <em>p</em> }. For any given column <em>j</em> ∈ { 1 <em>,...,p</em> }, a simple calcula-</p><p>tion shows that the random variable <strong>x</strong> <em>Tj</em> <strong>w</strong> <em>/N</em> is distributed as <em>N</em> (0 <em>,σ</em></p><div class="language-"><pre><code>2\nN ·\n</code></pre></div><p>‖ <strong>x</strong> <em>j</em> ‖^22 <em>N</em> ). Consequently, if the columns of the design matrix <strong>X</strong> are normalized (mean- ing‖ <strong>x</strong> <em>j</em> ‖ 2 <em>/</em></p><h6 id="√-36"><a class="header-anchor" href="#√-36" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>N ≤1 for all j = 1 ,...,p ), then this variable is stochastically\n</code></pre></div><p>dominated by a <em>N</em> (0 <em>,σ</em></p><div class="language-"><pre><code>2\nN ) variable, so that we have the Gaussian tail bound\n</code></pre></div><div class="language-"><pre><code>P\n</code></pre></div><h6 id="-449"><a class="header-anchor" href="#-449" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>| x Tj w |\nN\n</code></pre></div><div class="language-"><pre><code>≥ t\n</code></pre></div><h6 id="-450"><a class="header-anchor" href="#-450" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>≤ 2 e −\n</code></pre></div><div class="language-"><pre><code>Nt^2\n2 σ^2.\n</code></pre></div><p>Since‖ <strong>X</strong></p><p><em>T</em> <strong>w</strong> ‖∞ <em>N</em> corresponds to the maximum over <em>p</em> such variables, the union bound yields</p><div class="language-"><pre><code>P\n</code></pre></div><div class="language-"><pre><code>[‖ X T w ‖\n∞\nN\n</code></pre></div><div class="language-"><pre><code>≥ t\n</code></pre></div><h6 id="-451"><a class="header-anchor" href="#-451" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>≤ 2 e −\n</code></pre></div><div class="language-"><pre><code>Nt^2\n2 σ^2 +log p = 2 e −\n</code></pre></div><p>(^12) ( <em>τ</em> −2) log <em>p ,</em></p><h6 id="_296-theoretical-results-for-the-lasso"><a class="header-anchor" href="#_296-theoretical-results-for-the-lasso" aria-hidden="true">#</a> 296 THEORETICAL RESULTS FOR THE LASSO</h6><p>where the second equality follows by setting <em>t</em> = <em>σ</em></p><h6 id="√-37"><a class="header-anchor" href="#√-37" aria-hidden="true">#</a> √</h6><p><em>τ</em> log <em>p N</em> for some <em>τ &gt;</em> 2. Consequently, we conclude that the lasso error satisfies the bound</p><div class="language-"><pre><code>‖ β ̂− β ∗‖ 2 ≤\n</code></pre></div><div class="language-"><pre><code>cσ\nγ\n</code></pre></div><h6 id="√-38"><a class="header-anchor" href="#√-38" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>τk log p\nN\n</code></pre></div><h6 id="_11-15"><a class="header-anchor" href="#_11-15" aria-hidden="true">#</a> . (11.15)</h6><p>probability at least 1− 2 <em>e</em> −</p><p>(^12) ( <em>τ</em> −2) log <em>p</em></p><p>. This calculation has also given us a choice of the regularization parameter <em>λN</em> that is valid for the La- grangian lasso in Theorem 11.1(b). In particular, from our calculations, set-</p><p>ting <em>λN</em> = 2 <em>σ</em></p><h6 id="√-39"><a class="header-anchor" href="#√-39" aria-hidden="true">#</a> √</h6><p><em>τ</em> log <em>Np</em> for some <em>τ &gt;</em> 2 will be a valid choice with the same high probability. It should also be noted that the rate (11.15) is intuitively reasonable. Indeed, if support set <em>S</em> ( <em>β</em> ∗) were known, then estimation of <em>β</em> ∗would re- quire approximating a total of <em>k</em> parameters—namely, the elements <em>βi</em> ∗for all <em>i</em> ∈ <em>S</em> ( <em>β</em> ∗). Even with knowledge of the support set, since the model has <em>k</em> free parameters, no method can achieve squared <em>`</em> 2 -error that decays more quickly than <em>Nk</em>. Thus, apart from the logarithmic factor, the lasso rate matches the best possible that one could achieve, even if the subset <em>S</em> ( <em>β</em> ∗) were known <em>a priori</em>. In fact, the rate (11.15)—including the logarithmic factor—is known to be minimax optimal, meaning that it cannot be substantially improved upon by any estimator. See the bibliographic section for further discussion.</p><p><em>Example 11.2. Compressed sensing.</em> In the domain of compressed sensing (Chapter 10), the design matrix <strong>X</strong> can be chosen by the user, and one standard choice is to form a random matrix with i.i.d. <em>N</em> (0 <em>,</em> 1) entries, and model the noise vector <strong>w</strong> ∈R <em>N</em> as deterministic, say with bounded entries (‖ <strong>w</strong> ‖∞≤ <em>σ</em> .) Under these assumptions, each variable <em>N</em>^1 <strong>x</strong> <em>Tj</em> <strong>w</strong> is a zero-mean Gaussian with</p><p>variance at most <em>σ</em></p><p>2 <em>N</em>. Thus, by following the same argument as in the preceding example, we conclude that the lasso error will again obey the bound (11.15) with high probability for this set-up. By a more refined argument, one can derive a strengthening of the error bound (11.15), namely:</p><div class="language-"><pre><code>‖ β ̂− β ∗‖ 2 ≤ cσ\n</code></pre></div><h6 id="√-40"><a class="header-anchor" href="#√-40" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>k log( ep/k )\nN\n</code></pre></div><h6 id="_11-16"><a class="header-anchor" href="#_11-16" aria-hidden="true">#</a> . (11.16)</h6><p>where <em>e</em> ≈ 2_._ 71828, and <em>c</em> is a universal constant. This bound suggests that the sample size <em>N</em> should satisfy the lower bound</p><div class="language-"><pre><code>N ≥ k log( ep/k ) (11.17)\n</code></pre></div><p>in order for the lasso to have small error. Following Donoho and Tanner (2009), let us consider the ratios <em>ρ</em> = <em>k/N</em> and <em>α</em> = <em>N/p</em> , in which form the bound (11.17) can be rewritten as</p><div class="language-"><pre><code>ρ (1−log( ρα ))≤ 1. (11.18)\n</code></pre></div><h6 id="bounds-on-lasso-2-error-297"><a class="header-anchor" href="#bounds-on-lasso-2-error-297" aria-hidden="true">#</a> BOUNDS ON LASSO ` 2 -ERROR 297</h6><p>In order to study the accuracy of this prediction, we generated random ensem- bles of the linear regression problem in dimension <em>p</em> = 200 and sample sizes <em>N</em> ranging from 10 and 200, where each feature <em>xij</em> ∼ <em>N</em> (0 <em>,</em> 1) was generated independently. Given this random design, we then generated outcomes from a linear model <em>yi</em> = <em>ν</em> 〈 <em>xi, β</em> ∗〉+ <em>σwi</em> where <em>wi</em> ∼ <em>N</em> (0 <em>,</em> 1) and <em>σ</em> = 4. For a given sparsity level <em>k</em> , we chose a random subset <em>S</em> of size <em>k</em> , and for each <em>j</em> ∈ <em>S</em> , we generated <em>βj</em> ∗∼ <em>N</em> (0 <em>,</em> 1) independently at random. In all cases, the pre-factor <em>ν</em> was chosen for each <em>N</em> and <em>k</em> , so that the signal-to-noise ratio was approximately equal to 10. We then solved the Lagrangian lasso using</p><p>the regularization parameter <em>λN</em> = 2 <em>σ</em></p><h6 id="√-41"><a class="header-anchor" href="#√-41" aria-hidden="true">#</a> √</h6><h6 id="_3-1"><a class="header-anchor" href="#_3-1" aria-hidden="true">#</a> 3</h6><p>log <em>epk N</em>. Figure 11.4 is a heatmap of the median of the Euclidean error‖ <em>β</em> ̂− <em>β</em> ∗‖ 2 over 10 realizations, with the boundary (11.18) super-imposed. We see that there is a fairly sharp change at the theoretical boundary, indicating that more samples are needed when the underlying model is more dense.</p><div class="language-"><pre><code>0.0 0.2 0.4 0.6 0.8 1.0\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>2\n</code></pre></div><div class="language-"><pre><code>4\n</code></pre></div><div class="language-"><pre><code>6\n</code></pre></div><div class="language-"><pre><code>8\n</code></pre></div><div class="language-"><pre><code>10\n</code></pre></div><div class="language-"><pre><code>12\n</code></pre></div><div class="language-"><pre><code>14\n</code></pre></div><div class="language-"><pre><code>α=N/p\n</code></pre></div><div class="language-"><pre><code>ρ\n</code></pre></div><div class="language-"><pre><code>=\n</code></pre></div><div class="language-"><pre><code>k/N\n</code></pre></div><p><strong>Figure 11.4</strong> <em>Simulation experiment: median of the error</em> ‖ <em>β</em> ̂− <em>β</em> ∗‖ 2 <em>over</em> 10 <em>realiza- tions, with the boundary</em> (11.18) <em>super-imposed.</em></p><p>Equipped with this intuition, we now turn to the proof of Theorem 11.1.</p><p><em>Proof of Theorem 11.1:</em> The proof is very straightforward for the constrained lasso bound (11.14a), and requires a bit more work for the regularized lasso bound (11.14b).</p><p><em>Constrained Lasso.</em> In this case, since <em>β</em> ∗ is feasible and <em>β</em> ̂is optimal, we have the inequality‖ <strong>y</strong> − <strong>X</strong> <em>β</em> ̂‖^22 ≤‖ <strong>y</strong> − <strong>X</strong> <em>β</em> ∗‖^22. Defining the error vector ̂ <em>ν</em> : = <em>β</em> ̂− <em>β</em> ∗, substituting in the relation <strong>y</strong> = <strong>X</strong> <em>β</em> ∗+ <strong>w</strong> , and performing some</p><h6 id="_298-theoretical-results-for-the-lasso"><a class="header-anchor" href="#_298-theoretical-results-for-the-lasso" aria-hidden="true">#</a> 298 THEORETICAL RESULTS FOR THE LASSO</h6><p>algebra yields the <em>basic inequality</em></p><div class="language-"><pre><code>‖ X ν ̂‖^22\n2 N\n</code></pre></div><h6 id="≤"><a class="header-anchor" href="#≤" aria-hidden="true">#</a> ≤</h6><div class="language-"><pre><code>w T X ν ̂\nN\n</code></pre></div><h6 id="_11-19"><a class="header-anchor" href="#_11-19" aria-hidden="true">#</a> . (11.19)</h6><p>Applying a version of H ̈older’s inequality to the right-hand side yields the upper bound <em>N</em>^1 | <strong>w</strong> <em>T</em> <strong>X</strong> ̂ <em>ν</em> |≤ <em>N</em>^1 ‖ <strong>X</strong> <em>T</em> <strong>w</strong> ‖∞‖̂ <em>ν</em> ‖ 1. As shown in Chapter 10, the in-</p><p>equality‖ <em>β</em> ̂‖ 1 ≤ <em>R</em> =‖ <em>β</em> ∗‖ 1 implies that̂ <em>ν</em> ∈C( <em>S</em> ; 1), whence we have</p><div class="language-"><pre><code>‖ ν ̂‖ 1 =‖̂ νS ‖ 1 +‖̂ νSc ‖ 1 ≤ 2 ‖̂ νS ‖ 1 ≤ 2\n</code></pre></div><h6 id="√-42"><a class="header-anchor" href="#√-42" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>k ‖ ν ̂‖ 2.\n</code></pre></div><p>On the other hand, applying the restricted eigenvalue condition (11.10) to the left-hand side of the inequality (11.19) yields <em>N</em>^1 ‖ <strong>X</strong> ̂ <em>ν</em> ‖^22 ≥ <em>γ</em> ‖̂ <em>ν</em> ‖^22. Putting together the pieces yields the claimed bound (11.14a).</p><p><em>Lagrangian Lasso.</em> Define the function</p><div class="language-"><pre><code>G ( ν ) : =\n</code></pre></div><h6 id="_1-146"><a class="header-anchor" href="#_1-146" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-19"><a class="header-anchor" href="#_2-n-19" aria-hidden="true">#</a> 2 N</h6><div class="language-"><pre><code>‖ y − X ( β ∗+ ν )‖^22 + λN ‖ β ∗+ ν ‖ 1. (11.20)\n</code></pre></div><p>Noting that <em>ν</em> ̂: = <em>β</em> ̂− <em>β</em> ∗minimizes <em>G</em> by construction, we have <em>G</em> ( <em>ν</em> ̂)≤ <em>G</em> (0). Some algebra yields the <em>modified basic inequality</em></p><div class="language-"><pre><code>‖ X ̂ ν ‖^22\n2 N\n</code></pre></div><h6 id="≤-1"><a class="header-anchor" href="#≤-1" aria-hidden="true">#</a> ≤</h6><div class="language-"><pre><code>w T X ̂ ν\nN\n</code></pre></div><div class="language-"><pre><code>+ λN\n</code></pre></div><h6 id="-452"><a class="header-anchor" href="#-452" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>‖ β ∗‖ 1 −‖ β ∗+̂ ν ‖ 1\n</code></pre></div><h6 id="-453"><a class="header-anchor" href="#-453" aria-hidden="true">#</a> }</h6><h6 id="_11-21"><a class="header-anchor" href="#_11-21" aria-hidden="true">#</a> . (11.21)</h6><p>Now since <em>β</em> ∗ <em>Sc</em> = 0, we have‖ <em>β</em> ∗‖ 1 =‖ <em>βS</em> ∗‖ 1 , and</p><div class="language-"><pre><code>‖ β ∗+̂ ν ‖ 1 =‖ β ∗ S +̂ νS ‖ 1 +‖̂ νSc ‖ 1 ≥ ‖ βS ∗‖ 1 −‖̂ νS ‖ 1 +‖̂ νSc ‖ 1.\n</code></pre></div><p>Substituting these relations into inequality (11.21) yields</p><div class="language-"><pre><code>‖ X ̂ ν ‖^22\n2 N\n</code></pre></div><h6 id="≤-2"><a class="header-anchor" href="#≤-2" aria-hidden="true">#</a> ≤</h6><div class="language-"><pre><code>w T X ̂ ν\nN\n</code></pre></div><div class="language-"><pre><code>+ λN\n</code></pre></div><h6 id="-454"><a class="header-anchor" href="#-454" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>‖̂ νS ‖ 1 −‖̂ νSc ‖ 1\n</code></pre></div><h6 id="-455"><a class="header-anchor" href="#-455" aria-hidden="true">#</a> }</h6><h6 id="≤-3"><a class="header-anchor" href="#≤-3" aria-hidden="true">#</a> ≤</h6><div class="language-"><pre><code>‖ X T w ‖∞\nN\n</code></pre></div><div class="language-"><pre><code>‖̂ ν ‖ 1 + λN\n</code></pre></div><h6 id="-456"><a class="header-anchor" href="#-456" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>‖ ν ̂ S ‖ 1 −‖̂ νSc ‖ 1\n</code></pre></div><h6 id="-457"><a class="header-anchor" href="#-457" aria-hidden="true">#</a> }</h6><h6 id="_11-22"><a class="header-anchor" href="#_11-22" aria-hidden="true">#</a> , (11.22)</h6><p>where the second step follows by applying H ̈older’s inequality with <em><code>_ 1 and _</code></em> ∞ norms. Since <em>N</em>^1 ‖ <strong>X</strong> <em>T</em> <strong>w</strong> ‖∞≤ <em>λ</em> 2 <em>N</em> by assumption, we have</p><div class="language-"><pre><code>‖ X ̂ ν ‖^22\n2 N\n</code></pre></div><h6 id="≤-4"><a class="header-anchor" href="#≤-4" aria-hidden="true">#</a> ≤</h6><div class="language-"><pre><code>λN\n2\n</code></pre></div><h6 id="-458"><a class="header-anchor" href="#-458" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>‖̂ νS ‖ 1 +‖̂ νSc ‖ 1\n</code></pre></div><h6 id="-459"><a class="header-anchor" href="#-459" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>+ λN\n</code></pre></div><h6 id="-460"><a class="header-anchor" href="#-460" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>‖̂ νS ‖ 1 −‖̂ νSc ‖ 1\n</code></pre></div><h6 id="-461"><a class="header-anchor" href="#-461" aria-hidden="true">#</a> }</h6><h6 id="≤-5"><a class="header-anchor" href="#≤-5" aria-hidden="true">#</a> ≤</h6><h6 id="_3-3"><a class="header-anchor" href="#_3-3" aria-hidden="true">#</a> 3</h6><h6 id="_2-90"><a class="header-anchor" href="#_2-90" aria-hidden="true">#</a> 2</h6><h6 id="√-43"><a class="header-anchor" href="#√-43" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>kλN ‖̂ ν ‖ 2 ,\n(11.23)\n</code></pre></div><p>where the final step uses the fact that‖̂ <em>νS</em> ‖ 1 ≤</p><h6 id="√-44"><a class="header-anchor" href="#√-44" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>k ‖̂ ν ‖ 2.\nIn order to complete the proof, we require the following auxiliary result:\n</code></pre></div><p><em>Lemma 11.1.</em> Suppose that <em>λN</em> ≥ 2 ‖ <strong>X</strong></p><div class="language-"><pre><code>T w\nN ‖∞ &gt; 0. Then the error̂ ν : =\nβ ̂− β ∗\n</code></pre></div><p>associated with any lasso solution <em>β</em> ̂belongs to the cone setC( <em>S</em> ; 3).</p><h6 id="bounds-on-prediction-error-299"><a class="header-anchor" href="#bounds-on-prediction-error-299" aria-hidden="true">#</a> BOUNDS ON PREDICTION ERROR 299</h6><p>Taking this claim as given for the moment, let us complete the proof of the bound (11.14b). Lemma 11.1 allows us to apply the <em>γ</em> -RE condition (11.10) tô <em>ν</em> , which ensures that <em>N</em>^1 ‖ <strong>X</strong> <em>ν</em> ̂‖^22 ≥ <em>γ</em> ‖̂ <em>ν</em> ‖^22. Combining this lower bound with our earlier inequality (11.23) yields</p><div class="language-"><pre><code>γ\n2\n</code></pre></div><div class="language-"><pre><code>‖ ν ̂‖^22 ≤\n</code></pre></div><h6 id="_3-5"><a class="header-anchor" href="#_3-5" aria-hidden="true">#</a> 3</h6><h6 id="_2-91"><a class="header-anchor" href="#_2-91" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>λN\n</code></pre></div><h6 id="√-45"><a class="header-anchor" href="#√-45" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>k ‖̂ ν ‖ 2 ,\n</code></pre></div><p>and rearranging yields the bound (11.14b).</p><div class="language-"><pre><code>It remains to prove Lemma 11.1. Since‖ X\n</code></pre></div><div class="language-"><pre><code>T w ‖∞\nN ≤\n</code></pre></div><p><em>λN</em> 2 , inequality (11.22) implies that</p><div class="language-"><pre><code>0 ≤\n</code></pre></div><div class="language-"><pre><code>λN\n2\n‖̂ ν ‖ 1 + λN\n</code></pre></div><h6 id="-462"><a class="header-anchor" href="#-462" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>‖̂ νS ‖ 1 −‖ ν ̂ Sc ‖ 1\n</code></pre></div><h6 id="-463"><a class="header-anchor" href="#-463" aria-hidden="true">#</a> }</h6><h6 id="-464"><a class="header-anchor" href="#-464" aria-hidden="true">#</a> ,</h6><p>Rearranging and then dividing out by <em>λN&gt;</em> 0 yields that‖̂ <em>νSc</em> ‖ 1 ≤ 3 ‖̂ <em>νS</em> ‖ 1 as claimed.</p><p><em>Some extensions.</em> As stated, Theorem 11.1 applies to regression models in which <em>β</em> ∗has at most <em>k</em> nonzero entries, an assumption that we referred to as hard sparsity. However, a similar type of analysis can be performed for weakly sparse models, say with <em>β</em> ∗belonging to the <em>`q</em> -ballB <em>q</em> ( <em>Rq</em> ) previously defined in Equation (11.7). Under a similar set of assumptions, it can be shown that the lasso error will satisfy the bound</p><div class="language-"><pre><code>‖ β ̂− β ∗‖^22 ≤ c Rq\n</code></pre></div><h6 id="-465"><a class="header-anchor" href="#-465" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>σ^2 log p\nN\n</code></pre></div><div class="language-"><pre><code>) 1 − q/ 2\n(11.24)\n</code></pre></div><p>with high probability. We work through portions of this derivation in Exer- cise 11.3. In the special case <em>q</em> = 0, assuming that <em>β</em> ∗belongs to the <em><code>_ 0 ball is equivalent to the assumption of hard sparsity (with radius _R_ 0 = _k_ ), so that this rate (11.24) is equivalent to our previous result (11.16) derived as a con- sequence of Theorem 11.1. Otherwise, note that the rate slows down as the weak sparsity parameter _q_ increases away from zero toward one, reflecting the fact that we are imposing weaker conditions on the true regression vector _β_ ∗. The rate (11.24) is known to be minimax-optimal over the _</code>q</em> -ball, meaning that no other estimator can achieve a substantially smaller <em>`</em> 2 -error; see the bibliographic section for further discussion.</p><h3 id="_11-3-bounds-on-prediction-error"><a class="header-anchor" href="#_11-3-bounds-on-prediction-error" aria-hidden="true">#</a> 11.3 Bounds on Prediction Error</h3><p>Thus far, we have studied the performance of the lasso in recovering the true regression vector, as assessed by the Euclidean error‖ <em>β</em> ̂− <em>β</em> ∗‖ 2. In other</p><p>settings, it may suffice to obtain an estimate <em>β</em> ̂that has a relatively low (in- sample) prediction errorLpred( <em>β,β</em> ̂ ∗) = <em>N</em>^1 ‖ <strong>X</strong> ( <em>β</em> ̂− <em>β</em> ∗)‖^22. In this section, we develop some theoretical guarantees on this form of loss. For concreteness, we focus on the Lagrangian lasso (11.3), although analogous results can be derived for other forms of the lasso.</p><h6 id="_300-theoretical-results-for-the-lasso"><a class="header-anchor" href="#_300-theoretical-results-for-the-lasso" aria-hidden="true">#</a> 300 THEORETICAL RESULTS FOR THE LASSO</h6><p><em>Theorem 11.2.</em> Consider the Lagrangian lasso with a regularization parameter <em>λN</em> ≥ <em>N</em>^2 ‖ <strong>X</strong> <em>T</em> <strong>w</strong> ‖∞.</p><div class="language-"><pre><code>(a) If‖ β ∗‖ 1 ≤ R 1 , then any optimal solution β ̂satisfies\n</code></pre></div><div class="language-"><pre><code>‖ X ( β ̂− β ∗)‖^22\nN\n</code></pre></div><div class="language-"><pre><code>≤ 12 R 1 λN. (11.25a)\n</code></pre></div><div class="language-"><pre><code>(b) If β ∗is supported on a subset S , and the design matrix X satisfies the\nγ -RE condition (11.10) overC( S ; 3), then any optimal solution β ̂satisfies\n</code></pre></div><div class="language-"><pre><code>‖ X ( β ̂− β ∗)‖^22\nN\n</code></pre></div><h6 id="≤-6"><a class="header-anchor" href="#≤-6" aria-hidden="true">#</a> ≤</h6><h6 id="_9"><a class="header-anchor" href="#_9" aria-hidden="true">#</a> 9</h6><div class="language-"><pre><code>γ\n</code></pre></div><div class="language-"><pre><code>| S | λ^2 N. (11.25b)\n</code></pre></div><p>As we have discussed, for various statistical models, the choice <em>λN</em> = <em>cσ</em></p><h6 id="√-46"><a class="header-anchor" href="#√-46" aria-hidden="true">#</a> √</h6><p>log <em>p N</em> is valid for Theorem 11.2 with high probability, so the two bounds take the form</p><div class="language-"><pre><code>‖ X ( β ̂− β ∗)‖^22\nN\n</code></pre></div><div class="language-"><pre><code>≤ c 1 σ R 1\n</code></pre></div><h6 id="√-47"><a class="header-anchor" href="#√-47" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>log p\nN\n</code></pre></div><div class="language-"><pre><code>, and (11.26a)\n</code></pre></div><div class="language-"><pre><code>‖ X ( β ̂− β ∗)‖^22\nN\n</code></pre></div><div class="language-"><pre><code>≤ c 2\n</code></pre></div><div class="language-"><pre><code>σ^2\nγ\n</code></pre></div><div class="language-"><pre><code>| S |log p\nN\n</code></pre></div><p><em>.</em> (11.26b)</p><p>The bound (11.26a), which depends on the <em>`</em> 1 -ball radius <em>R</em> 1 , is known as the “slow rate” for the lasso, since the squared prediction error decays as 1 <em>/</em></p><h6 id="√-48"><a class="header-anchor" href="#√-48" aria-hidden="true">#</a> √</h6><h6 id="n-42"><a class="header-anchor" href="#n-42" aria-hidden="true">#</a> N.</h6><p>On the other hand, the bound (11.26b) is known as the “fast rate,” since it decays as 1 <em>/N</em>. Note that the latter is based on much stronger assumptions: namely, the hard sparsity condition that <em>β</em> ∗is supported on a small subset <em>S</em> , and more disconcertingly, the <em>γ</em> -RE condition on the design matrix <strong>X</strong>. In principle, prediction performance should not require an RE condition, so that one might suspect that this requirement is an artifact of our proof technique. Remarkably, as we discuss in the bibliographic section, this dependence turns out to be unavoidable for any polynomial-time method.</p><p><em>Proof of Theorem 11.2:</em> The proofs of both claims are relatively straightfor- ward given our development thus far.</p><p><em>Proof of bound (11.25a):</em> Beginning with the modified basic inequal- ity (11.21), we have</p><h6 id="_0-≤"><a class="header-anchor" href="#_0-≤" aria-hidden="true">#</a> 0 ≤</h6><div class="language-"><pre><code>‖ X T w ‖∞\nN\n</code></pre></div><div class="language-"><pre><code>‖̂ ν ‖ 1 + λN\n</code></pre></div><h6 id="-466"><a class="header-anchor" href="#-466" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>‖ β ∗‖ 1 −‖ β ∗+̂ ν ‖ 1\n</code></pre></div><h6 id="-467"><a class="header-anchor" href="#-467" aria-hidden="true">#</a> }</h6><h6 id="≤-7"><a class="header-anchor" href="#≤-7" aria-hidden="true">#</a> ≤</h6><div class="language-"><pre><code>{‖ X T w ‖∞\nN\n− λN\n</code></pre></div><h6 id="-468"><a class="header-anchor" href="#-468" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>‖̂ ν ‖ 1 + 2 λN ‖ β ∗‖ 1\n( i )\n≤\n</code></pre></div><h6 id="_1-147"><a class="header-anchor" href="#_1-147" aria-hidden="true">#</a> 1</h6><h6 id="_2-92"><a class="header-anchor" href="#_2-92" aria-hidden="true">#</a> 2</h6><div class="language-"><pre><code>λN\n</code></pre></div><h6 id="-469"><a class="header-anchor" href="#-469" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>−‖̂ ν ‖ 1 + 4‖ β ∗‖ 1\n</code></pre></div><h6 id="-470"><a class="header-anchor" href="#-470" aria-hidden="true">#</a> }</h6><h6 id="-471"><a class="header-anchor" href="#-471" aria-hidden="true">#</a> ,</h6><h6 id="support-recovery-in-linear-regression-301"><a class="header-anchor" href="#support-recovery-in-linear-regression-301" aria-hidden="true">#</a> SUPPORT RECOVERY IN LINEAR REGRESSION 301</h6><p>where step (i) uses the fact that <em>N</em>^1 ‖ <strong>X</strong> <em>T</em> <strong>w</strong> ‖∞≤ <em>λ</em> 2 <em>N</em> by assumption. Putting together the pieces, we conclude that‖̂ <em>ν</em> ‖ 1 ≤ 4 ‖ <em>β</em> ∗‖ 1 ≤ 4 <em>R</em> 1. Returning again to the modified basic inequality (11.21), we have</p><div class="language-"><pre><code>‖ X ̂ ν ‖^22\n2 N\n</code></pre></div><h6 id="≤-8"><a class="header-anchor" href="#≤-8" aria-hidden="true">#</a> ≤</h6><div class="language-"><pre><code>{‖ X T w ‖\n∞\nN\n</code></pre></div><div class="language-"><pre><code>+ λN\n</code></pre></div><h6 id="-472"><a class="header-anchor" href="#-472" aria-hidden="true">#</a> }</h6><div class="language-"><pre><code>‖̂ ν ‖ 1 ≤ 6 λNR 1 ,\n</code></pre></div><p>which establishes the claim (11.25a).</p><p><em>Proof of bound (11.25b):</em> Given the stated choice of <em>λN</em> , inequality (11.23) holds, whence</p><div class="language-"><pre><code>‖ X ̂ ν ‖^22\n2 N\n</code></pre></div><h6 id="≤-3-1"><a class="header-anchor" href="#≤-3-1" aria-hidden="true">#</a> ≤ 3</h6><h6 id="√-49"><a class="header-anchor" href="#√-49" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>kλN ‖̂ ν ‖^22.\n</code></pre></div><p>By Lemma 11.1, the error vector̂ <em>ν</em> belongs to the coneC( <em>S</em> ; 3), so that the <em>γ</em> -RE condition guarantees that‖̂ <em>ν</em> ‖^22 ≤ <em>N γ</em>^1 ‖ <strong>X</strong> <em>ν</em> ̂‖^22. Putting together the pieces</p><p>yields the claim (11.25b).</p><h3 id="_11-4-support-recovery-in-linear-regression"><a class="header-anchor" href="#_11-4-support-recovery-in-linear-regression" aria-hidden="true">#</a> 11.4 Support Recovery in Linear Regression</h3><p>Thus far, we have focused on bounds on either the <em><code>_ 2 -error or the prediction error associated with a lasso solution. In other settings, we are interested in a somewhat more refined question, namely whether or not a lasso estimate _β_ ̂has nonzero entries in the same positions as the true regression vector _β_ ∗. More precisely, suppose that the true regression vector _β_ ∗is _k_ -sparse, meaning that it is supported on a subset _S_ = _S_ ( _β_ ∗) of cardinality _k_ =| _S_ |. In such a setting, a natural goal is to correctly identify the subset _S_ of relevant variables. In terms of the lasso, we ask the following question: given an optimal lasso solution _β_ ̂, when is its support set—denoted by _S_ ̂= _S_ ( _β_ ̂)—exactly equal to the true support _S_? We refer to this property as _variable selection consistency_ or _sparsistency_. Note that it is possible for the _</code></em> 2 error‖ <em>β</em> ̂− <em>β</em> ∗‖ 2 to be quite small even if <em>β</em> ̂ and <em>β</em> ∗have different supports, as long as <em>β</em> ̂is nonzero for all “suitably large” entries of <em>β</em> ∗, and not “too large” in positions where <em>β</em> ∗is zero. Similarly, it is possible for the prediction error‖ <strong>X</strong> ( <em>β</em> ̂− <em>β</em> ∗)‖ 2 <em>/</em></p><h6 id="√-50"><a class="header-anchor" href="#√-50" aria-hidden="true">#</a> √</h6><p><em>N</em> to be small even when <em>β</em> ̂and <em>β</em> ∗have very different supports. On the other hand, as we discuss in the sequel, given an estimate <em>β</em> ̂that correctly recovers the support of <em>β</em> ∗, we can estimate <em>β</em> ∗very well—both in <em>`</em> 2 -norm and the prediction semi-norm— simply by performing an ordinary least-squares regression restricted to this subset.</p><h4 id="_11-4-1-variable-selection-consistency-for-the-lasso"><a class="header-anchor" href="#_11-4-1-variable-selection-consistency-for-the-lasso" aria-hidden="true">#</a> 11.4.1 Variable-Selection Consistency for the Lasso</h4><p>We begin by addressing the issue of variable selection in the context of deter- ministic design matrices <strong>X</strong>. It turns out that variable selection requires a con- dition related to but distinct from the restricted eigenvalue condition (11.10).</p><h6 id="_302-theoretical-results-for-the-lasso"><a class="header-anchor" href="#_302-theoretical-results-for-the-lasso" aria-hidden="true">#</a> 302 THEORETICAL RESULTS FOR THE LASSO</h6><p>In particular, we assume a condition known either as <em>mutual incoherence</em> or <em>irrepresentability</em> : there must exist some <em>γ &gt;</em> 0 such that</p><div class="language-"><pre><code>max\nj ∈ Sc\n‖( X TS X S )−^1 X TS x j ‖ 1 ≤ 1 − γ. (11.27)\n</code></pre></div><p>To interpret this condition, note that the submatrix <strong>X</strong> <em>S</em> ∈R <em>N</em> × <em>k</em> corresponds to the subset of covariates that are in the support set. For each index <em>j</em> in the complementary set <em>Sc</em> , the <em>k</em> -vector ( <strong>X</strong> <em>TS</em> <strong>X</strong> <em>S</em> )−^1 <strong>X</strong> <em>TS</em> <strong>x</strong> <em>j</em> is the regression coefficient of <strong>x</strong> <em>j</em> on <strong>X</strong> <em>S</em> ; this vector is a measure of how well the column <strong>x</strong> <em>j</em> aligns with the columns of the submatrix <strong>X</strong> <em>S</em>. In the most desirable case, the columns{ <strong>x</strong> <em>j,j</em> ∈ <em>Sc</em> }would all be orthogonal to the columns of <strong>X</strong> <em>S</em> , and we would be guaranteed that <em>γ</em> = 1. Of course, in the high-dimensional setting ( <em>p</em>  <em>N</em> ), this complete orthogonality is not possible, but we can still hope for a type of “near orthogonality” to hold. In addition to this incoherence condition, we also assume that the design matrix has normalized columns</p><div class="language-"><pre><code>max\nj =1 ,...,p\n</code></pre></div><div class="language-"><pre><code>‖ x j ‖ 2 /\n</code></pre></div><h6 id="√-51"><a class="header-anchor" href="#√-51" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>N ≤ K clm. (11.28)\n</code></pre></div><p>For example, we can take‖ <strong>x</strong> <em>j</em> ‖ 2 =</p><h6 id="√-52"><a class="header-anchor" href="#√-52" aria-hidden="true">#</a> √</h6><p><em>N</em> and <em>K</em> clm= 1. Further we assume that the submatrix <strong>X</strong> <em>S</em> is well-behaved in the sense that</p><div class="language-"><pre><code>λ min( X TS X S/N )≥ C min. (11.29)\n</code></pre></div><p>Note that if this condition were violated, then the columns of <strong>X</strong> <em>S</em> would be linearly dependent, and it would be impossible to estimate <em>β</em> ∗even in the “oracle case” when the support set <em>S</em> were known. The following result applies to the regularized lasso (11.3) when applied to an instance the linear observation model (11.1) such that the true parameter <em>β</em> ∗has support size <em>k</em>.</p><p><em>Theorem 11.3.</em> Suppose that the design matrix <strong>X</strong> satisfies the mutual incoher- ence condition (11.27) with parameter <em>γ &gt;</em> 0, and the column normalization condition (11.28) and the eigenvalue condition (11.29) both hold. For a noise vector <strong>w</strong> ∈R <em>N</em> with i.i.d. <em>N</em> (0 <em>,σ</em>^2 ) entries, consider the regularized lasso pro- gram (11.3) with</p><div class="language-"><pre><code>λN ≥\n</code></pre></div><div class="language-"><pre><code>8 K clm σ\nγ\n</code></pre></div><h6 id="√-53"><a class="header-anchor" href="#√-53" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>log p\nN\n</code></pre></div><h6 id="_11-30"><a class="header-anchor" href="#_11-30" aria-hidden="true">#</a> . (11.30)</h6><p>Then with probability greater than 1− <em>c</em> 1 <em>e</em> − <em>c</em>^2 <em>Nλ</em></p><p>(^2) <em>N</em> , the lasso has the following properties: (a) <em>Uniqueness:</em> The optimal solution <em>β</em> ̂is unique. (b) <em>No false inclusion:</em> The unique optimal solution has its support <em>S</em> ( <em>β</em> ̂) contained within the true support <em>S</em> ( <em>β</em> ∗).</p><h6 id="support-recovery-in-linear-regression-303"><a class="header-anchor" href="#support-recovery-in-linear-regression-303" aria-hidden="true">#</a> SUPPORT RECOVERY IN LINEAR REGRESSION 303</h6><div class="language-"><pre><code>(c) ` ∞ -bounds: The error β ̂− β ∗satisfies the ` ∞bound\n</code></pre></div><div class="language-"><pre><code>‖ β ̂ S − βS ∗‖∞≤ λN\n</code></pre></div><div class="language-"><pre><code>[ 4 σ\n√\nC min\n</code></pre></div><h6 id="-473"><a class="header-anchor" href="#-473" aria-hidden="true">#</a> +</h6><h6 id="∥-14"><a class="header-anchor" href="#∥-14" aria-hidden="true">#</a> ∥</h6><h6 id="∥-x-ts-x-s-n-−-1"><a class="header-anchor" href="#∥-x-ts-x-s-n-−-1" aria-hidden="true">#</a> ∥( X TS X S/N )−^1</h6><h6 id="∥-15"><a class="header-anchor" href="#∥-15" aria-hidden="true">#</a> ∥</h6><h6 id="∥-16"><a class="header-anchor" href="#∥-16" aria-hidden="true">#</a> ∥</h6><div class="language-"><pre><code>∞\n</code></pre></div><h6 id="-474"><a class="header-anchor" href="#-474" aria-hidden="true">#</a> ]</h6><h6 id="︸-︷︷-︸"><a class="header-anchor" href="#︸-︷︷-︸" aria-hidden="true">#</a> ︸ ︷︷ ︸</h6><div class="language-"><pre><code>B ( λN,σ ; X )\n</code></pre></div><h6 id="_11-31"><a class="header-anchor" href="#_11-31" aria-hidden="true">#</a> , (11.31)</h6><div class="language-"><pre><code>where for a matrix A , its∞-norm is given by‖ A ‖∞= max‖ u ‖∞=1‖ A u ‖∞.\n</code></pre></div><div class="language-"><pre><code>(d) No false exclusion: The lasso solution includes all indices j ∈ S ( β ∗) such\nthat| β ∗ j | &gt; B ( λN,σ ; X ), and hence is variable selection consistent as long\nas min\nj ∈ S\n</code></pre></div><div class="language-"><pre><code>| β ∗ j | &gt; B ( λN,σ ; X ).\n</code></pre></div><p>Before proving this result, let us try to interpret its main claims. First, the uniqueness claim in part (a) is not trivial in the high-dimensional setting, because as discussed previously, although the lasso objective is convex, it can never be strictly convex when <em>p &gt; N</em>. The uniqueness claim is important, because it allows us to talk unambiguously about the support of the lasso estimate <em>β</em> ̂. Part (b) guarantees that the lasso does not falsely include variables that are not in the support of <em>β</em> ∗, or equivalently that <em>β</em> ̂ <em>Sc</em> = 0, whereas part (c) guarantees that <em>β</em> ̂ <em>S</em> is uniformly close to <em>βS</em> ∗in the <em>`</em> ∞-norm. Finally, part (d) is a consequence of this uniform norm bound: as long as the minimum value of| <em>β</em> ∗ <em>j</em> |over indices <em>j</em> ∈ <em>S</em> is not too small, then the lasso is variable-selection consistent in the full sense.</p><h5 id="_11-4-1-1-some-numerical-studies"><a class="header-anchor" href="#_11-4-1-1-some-numerical-studies" aria-hidden="true">#</a> 11.4.1.1 Some Numerical Studies</h5><p>In order to learn more about the impact of these results in practice, we ran a few small simulation studies. We first explore the impact of the irrepresentabil- ity condition (11.27). We fixed the sample size to <em>N</em> = 1000, and for a range of problem dimensions <em>p</em> , we generated <em>p</em> i.i.d standard Gaussian variates, with a fraction <em>f</em> = <em>k/p</em> of them being in the support set <em>S</em>. For correlations <em>ρ</em> ranging over the interval [0 <em>,</em> 0_._ 6], for each <em>j</em> ∈ <em>S</em> we randomly chose a predic- tor <em><code>_ ∈ _Sc_ , and set **x** _</code></em> ← <strong>x</strong> <em><code>_ + _c_ · **x** _j_ with _c_ chosen so that corr( **x** _j,_ **x** _</code></em> ) = <em>ρ</em>. Figure 11.5 shows the average value of 1− <em>γ</em> , the value of the irrepresentability condition (11.27), over five realizations. We see for example with <em>ρ</em> = 0, we fall into the “good” region 1− <em>γ &lt;</em> 1 when <em>p</em> ≤1000 and there is <em>f</em> ≤2% sparsity or <em>p</em> ≤500 with <em>f</em> ≤5% sparsity. However the maximum size of <em>p</em> and sparsity level <em>f</em> decrease as the correlation <em>ρ</em> increases. We also ran a small simulation study to examine the false discovery and false exclusion rates for a lasso regression. We set <em>N</em> = 1000 and <em>p</em> = 500 with <em>k</em> = 15 predictors in <em>S</em> having nonzero coefficients. The data matrices <strong>X</strong> <em>S</em> and <strong>X</strong> <em>Sc</em> were generated as above, with different values for the correlations <em>ρ</em>. We then generated a response <strong>y</strong> according to <strong>y</strong> = <strong>X</strong> <em>SβS</em> + <strong>w</strong> , with the elements of <strong>w</strong> i.i.d. <em>N</em> (0 <em>,</em> 1). We tried two different values for the nonzero regression coefficients in <em>βS</em> : all 0_._ 25 or all 0_._ 15, with randomly selected signs. These result in “effect sizes”</p><h6 id="_304-theoretical-results-for-the-lasso"><a class="header-anchor" href="#_304-theoretical-results-for-the-lasso" aria-hidden="true">#</a> 304 THEORETICAL RESULTS FOR THE LASSO</h6><div class="language-"><pre><code>50 100 200 500 1000 2000\n</code></pre></div><div class="language-"><pre><code>0.05\n</code></pre></div><div class="language-"><pre><code>0.20 0.50\n</code></pre></div><div class="language-"><pre><code>2.00 5.00\n</code></pre></div><div class="language-"><pre><code>Number of Variables p\n</code></pre></div><div class="language-"><pre><code>Maximum Inner Product 1\n</code></pre></div><div class="language-"><pre><code>− γ\n</code></pre></div><div class="language-"><pre><code>ρ = 0\n</code></pre></div><div class="language-"><pre><code>f = 0.02\nf = 0.05\nf = 0.1\nf = 0.2\n50 100 200 500 1000 2000\n</code></pre></div><div class="language-"><pre><code>0.05\n</code></pre></div><div class="language-"><pre><code>0.20 0.50\n</code></pre></div><div class="language-"><pre><code>2.00 5.00\n</code></pre></div><div class="language-"><pre><code>Number of Variables p\n</code></pre></div><div class="language-"><pre><code>Maximum Inner Product 1\n</code></pre></div><div class="language-"><pre><code>− γ\n</code></pre></div><div class="language-"><pre><code>ρ =0.2\n</code></pre></div><div class="language-"><pre><code>50 100 200 500 1000 2000\n</code></pre></div><div class="language-"><pre><code>0.05\n</code></pre></div><div class="language-"><pre><code>0.20 0.50\n</code></pre></div><div class="language-"><pre><code>2.00 5.00\n</code></pre></div><div class="language-"><pre><code>Number of Variables p\n</code></pre></div><div class="language-"><pre><code>Maximum Inner Product 1\n</code></pre></div><div class="language-"><pre><code>− γ\n</code></pre></div><div class="language-"><pre><code>ρ =0.4\n</code></pre></div><div class="language-"><pre><code>50 100 200 500 1000 2000\n</code></pre></div><div class="language-"><pre><code>0.05\n</code></pre></div><div class="language-"><pre><code>0.20 0.50\n</code></pre></div><div class="language-"><pre><code>2.00 5.00\n</code></pre></div><div class="language-"><pre><code>Number of Variables p\n</code></pre></div><div class="language-"><pre><code>Maximum Inner Product 1\n</code></pre></div><div class="language-"><pre><code>− γ\n</code></pre></div><div class="language-"><pre><code>ρ =0.6\n</code></pre></div><p><strong>Figure 11.5</strong> <em>Irrepresentability condition in practice. Each plot shows values for</em> 1 − <em>γin (11.27) for simulated Gaussian data. Values less than one are good, and the smal ler the better. The sample sizeN</em> = 1000 <em>is fixed, and the number of predictors pvaries along the horizontal axis. The fractionf</em> = <em>k/pof true nonzero coefficients (the sparsity level) varies within each panel, and final ly, the correlation between each true predictor and its null predictor partner (as described in the text) varies across the four panels. A horizontal broken line is drawn at</em> 1 − <em>γ</em> = 1 <em>, below which the irrepresentability condition holds. Each point is a mean of</em> 1 − <em>γover five simulations; the standard errors of the means are smal l, averaging about 0.03.</em></p><h6 id="support-recovery-in-linear-regression-305"><a class="header-anchor" href="#support-recovery-in-linear-regression-305" aria-hidden="true">#</a> SUPPORT RECOVERY IN LINEAR REGRESSION 305</h6><p>(absolute standardized regression coefficients) for the 15 true predictors of 7_._ 9 and 4_._ 7, respectively. Finally, we chose <em>λN</em> in an “optimal” way in each run: we used the value yielding the correct number of nonzero coefficients (15). The top row of Figure 11.6 shows the results. In the top left panel (the best case), the average false discovery and false exclusion probabilities are zero until <em>ρ</em> is greater than about 0.6. After that point, the lasso starts to include false variables and exclude good ones, due to the high correlation between signal and noise variables. The value <em>γ</em> from the irrepresentability condition is also shown, and drops below zero at around the value <em>ρ</em> = 0_._ 6. (Hence the condition holds below correlation 0.6.) In the top right panel, we see error rates increase overall, even for small <em>ρ</em>. Here the effect size is modestly reduced from 7_._ 9 to 4_._ 7, which is the cause of the increase. The lower panel of Figure 11.6 shows the results when the sample size <em>N</em> is reduced to 200 ( <em>p &lt; N</em> ) and the size <em>k</em> of the support set is increased to 25. The values used for the nonzero regression coefficients were 5_._ 0 and 0_._ 5, yielding effect sizes of about 71 and 7, respectively. The irrepresentability condition and other assumptions of the theorem do not hold. Now the error rates are 15% or more irrespective of <em>ρ</em> , and recovery of the true support set seems unrealistic in this scenario.</p><h4 id="_11-4-2-proof-of-theorem-11-3"><a class="header-anchor" href="#_11-4-2-proof-of-theorem-11-3" aria-hidden="true">#</a> 11.4.2 Proof of Theorem 11.3</h4><p>We begin by developing the necessary and sufficient conditions for optimality in the lasso. A minor complication arises because the <em><code>_ 1 -norm is not differen- tiable, due to its sharp point at the origin. Instead, we need to work in terms of the subdifferential of the _</code></em> 1 -norm. Here we provide a very brief introduc- tion; see Chapter 5 for further details. Given a convex function <em>f</em> :R <em>p</em> →R, we say that <em>z</em> ∈R <em>p</em> is a subgradient at <em>β</em> , denoted by <em>z</em> ∈ <em>∂f</em> ( <em>β</em> ), if we have</p><div class="language-"><pre><code>f ( β + ∆)≥ f ( β ) +〈 z, ∆〉 for all ∆∈R p.\n</code></pre></div><p>When <em>f</em> ( <em>β</em> ) =‖ <em>β</em> ‖ 1 , it can be seen that <em>z</em> ∈ <em>∂</em> ‖ <em>β</em> ‖ 1 if and only if <em>zj</em> = sign( <em>βj</em> ) for all <em>j</em> = 1 <em>,</em> 2 <em>,...,p</em> , where we allow sign(0) to be any number in the interval</p><p>[− 1 <em>,</em> 1]. In application to the lasso program, we say that a pair ( <em>β,</em> ̂̂ <em>z</em> )∈R <em>p</em> ×R <em>p</em> is primal-dual optimal if <em>β</em> ̂is a minimizer and̂ <em>z</em> ∈ <em>∂</em> ‖ <em>β</em> ̂‖ 1. Any such pair must satisfy the zero-subgradient condition</p><h6 id="−-19"><a class="header-anchor" href="#−-19" aria-hidden="true">#</a> −</h6><h6 id="_1-148"><a class="header-anchor" href="#_1-148" aria-hidden="true">#</a> 1</h6><h6 id="n-43"><a class="header-anchor" href="#n-43" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>X T ( y − X β ̂) + λN ̂ z = 0 , (11.32)\n</code></pre></div><p>which is the analogue of a zero gradient condition in this nondifferentiable setting. Our proof of Theorem 11.3 is based on a constructive procedure, known as a <em>primal-dual witness method</em> (PDW). When this procedure succeeds, it</p><p>constructs a pair ( <em>β,</em> ̂ <em>z</em> ̂)∈R <em>p</em> ×R <em>p</em> that are primal-dual optimal, and act as a witness for the fact that the lasso has a unique optimal solution with the</p><h6 id="_306-theoretical-results-for-the-lasso"><a class="header-anchor" href="#_306-theoretical-results-for-the-lasso" aria-hidden="true">#</a> 306 THEORETICAL RESULTS FOR THE LASSO</h6><div class="language-"><pre><code>0.0 0.2 0.4 0.6 0.8 1.0\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>Proportion\n</code></pre></div><div class="language-"><pre><code>False Discovery Rate\nFalse Exclusion Rateγ\n</code></pre></div><div class="language-"><pre><code>0.0 0.2 0.4 0.6 0.8 1.0\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>Proportion N=1000\n</code></pre></div><div class="language-"><pre><code>0.0 0.2 0.4 0.6 0.8 1.0\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>Proportion\n</code></pre></div><div class="language-"><pre><code>0.0 0.2 0.4 0.6 0.8 1.0\n</code></pre></div><div class="language-"><pre><code>0.0\n</code></pre></div><div class="language-"><pre><code>0.2\n</code></pre></div><div class="language-"><pre><code>0.4\n</code></pre></div><div class="language-"><pre><code>0.6\n</code></pre></div><div class="language-"><pre><code>0.8\n</code></pre></div><div class="language-"><pre><code>1.0\n</code></pre></div><div class="language-"><pre><code>Proportion N=200\n</code></pre></div><div class="language-"><pre><code>ρ ρ\n</code></pre></div><div class="language-"><pre><code>ρ ρ\n</code></pre></div><div class="language-"><pre><code>Effect Size=7.9,k= 15 Effect Size=4.7,k= 15\n</code></pre></div><div class="language-"><pre><code>Effect Size=70.7,k= 25 Effect Size=7.1,k= 25\n</code></pre></div><p><strong>Figure 11.6</strong> <em>Average false discovery and exclusion rates (with</em> ± <em>one standard er- ror) from simulation experiments withp</em> = 500 <em>variables. In the top rowN</em> = 1000 <em>, and the size ofSisk</em> = 15_. In the second rowN_ = 200 <em>and the subset size is k</em> = 25_. The effect size is the strength of real coefficients, as measured by an absolute Zstatistic. Overal l conclusion: whenγis favorable, and the signal is strong, recovery is good (top left). Al l other situations are problematic._</p><h6 id="support-recovery-in-linear-regression-307"><a class="header-anchor" href="#support-recovery-in-linear-regression-307" aria-hidden="true">#</a> SUPPORT RECOVERY IN LINEAR REGRESSION 307</h6><div class="language-"><pre><code>correct signed support. Using S = supp( β ∗) to denote the support set of β ∗,\nthis procedure consists of the following steps:\n</code></pre></div><div class="language-"><pre><code>Primal-dual witness (PDW) construction.\n</code></pre></div><ol><li>Set <em>β</em> ̂ <em>Sc</em> = 0.</li><li>Determine ( <em>β</em> ̂ <em>S,</em> ̂ <em>zS</em> ) by solving the <em>k</em> -dimensional oracle subproblem</li></ol><div class="language-"><pre><code>β ̂ S ∈arg min\nβS ∈R k\n</code></pre></div><h6 id="-475"><a class="header-anchor" href="#-475" aria-hidden="true">#</a> {</h6><h6 id="_1-149"><a class="header-anchor" href="#_1-149" aria-hidden="true">#</a> 1</h6><h6 id="_2-n-20"><a class="header-anchor" href="#_2-n-20" aria-hidden="true">#</a> 2 N</h6><div class="language-"><pre><code>‖ y − X SβS ‖^22 + λN ‖ βS ‖ 1\n</code></pre></div><h6 id="-476"><a class="header-anchor" href="#-476" aria-hidden="true">#</a> }</h6><h6 id="_11-33"><a class="header-anchor" href="#_11-33" aria-hidden="true">#</a> . (11.33)</h6><div class="language-"><pre><code>Thuŝ zS is an element of subdifferential ∂ ‖ β ̂ S ‖ 1 satisfying the relation\n1\nN X\n</code></pre></div><div class="language-"><pre><code>T\nS ( y − X Sβ ̂ S ) + λNz ̂ S = 0.\n</code></pre></div><ol start="3"><li>Solve for̂ <em>zSc</em> via the zero-subgradient Equation (11.32), and check whether or not the <em>strict dual feasibility</em> condition‖̂ <em>zSc</em> ‖∞ <em>&lt;</em> 1 holds. To be clear, this procedure is <em>not</em> an implementable method for actu- ally solving the lasso program (since it pre-supposes knowledge of the true support); rather, it is a proof technique for certifying variable-selection con- sistency of the lasso. Note that the subvector <em>β</em> ̂ <em>Sc</em> is determined in step 1, whereas the remaining three subvectors are determined in steps 2 and 3. By construction, the subvectors <em>β</em> ̂ <em>S</em> ,̂ <em>zS</em> and̂ <em>zSc</em> satisfy the zero-subgradient con- dition (11.32). We say that the PDW construction succeeds if the vector̂ <em>zSc</em> constructed in step 3 satisfies the strict dual feasibility condition. The follow- ing result shows that this success acts as a witness for the lasso: <em>Lemma 11.2.</em> If the PDW construction succeeds, then under the lower eigen- value condition (11.29), the vector ( <em>β</em> ̂ <em>S,</em> 0)∈R <em>p</em> is the unique optimal solution of the regularized lasso (11.3).</li></ol><div class="language-"><pre><code>Proof: When the PDW construction succeeds, then β ̂= ( β ̂ S, 0) is an optimal\nsolution with associated subgradient vector z ̂∈R p satisfying‖ z ̂ Sc ‖∞ &lt; 1,\nand〈̂ z,β ̂〉=‖ β ̂‖ 1. Now let β ̃∈R p be any other optimal solution of the lasso.\nIf we introduce the shorthand notation F ( β ) = 21 N ‖ y − X β ‖^22 , then we are\nguaranteed that F ( β ̂) + λN 〈̂ z,β ̂〉= F ( β ̃) + λN ‖ β ̃‖ 1 , and hence\n</code></pre></div><div class="language-"><pre><code>F ( β ̂)− λN 〈̂ z,β ̃− β ̂〉= F ( β ̃) + λN\n</code></pre></div><h6 id="-477"><a class="header-anchor" href="#-477" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>‖ β ̃‖ 1 −〈 z, ̂ β ̃〉\n</code></pre></div><h6 id="-478"><a class="header-anchor" href="#-478" aria-hidden="true">#</a> )</h6><h6 id="-479"><a class="header-anchor" href="#-479" aria-hidden="true">#</a> .</h6><div class="language-"><pre><code>But by the zero-subgradient conditions for optimality (11.32), we have\nλN ̂ z =−∇ F ( β ̂), which implies that\n</code></pre></div><div class="language-"><pre><code>F ( β ̂) +〈∇ F ( β ̂) ,β ̃− β ̂〉− F ( β ̃) = λN\n</code></pre></div><h6 id="-480"><a class="header-anchor" href="#-480" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>‖ β ̃‖ 1 −〈̂ z,β ̃〉\n</code></pre></div><h6 id="-481"><a class="header-anchor" href="#-481" aria-hidden="true">#</a> )</h6><h6 id="-482"><a class="header-anchor" href="#-482" aria-hidden="true">#</a> .</h6><div class="language-"><pre><code>By convexity of F , the left-hand side is negative, so that we must have\n‖ β ̃‖ 1 ≤〈 z, ̂ β ̃〉. Applying H ̈older’s inequality with the ` 1 and ` ∞norms yields\nthe upper bound〈 z, ̂ β ̃〉≤‖̂ z ‖∞‖ β ̃‖ 1. These two inequalities together imply\n</code></pre></div><h6 id="_308-theoretical-results-for-the-lasso"><a class="header-anchor" href="#_308-theoretical-results-for-the-lasso" aria-hidden="true">#</a> 308 THEORETICAL RESULTS FOR THE LASSO</h6><p>that‖ <em>β</em> ̃‖ 1 =〈 <em>z,</em> ̂ <em>β</em> ̃〉. Since‖̂ <em>zSc</em> ‖∞ <em>&lt;</em> 1, this equality can occur only if <em>β</em> ̃ <em>j</em> = 0 for all <em>j</em> ∈ <em>Sc</em>. Thus, all optimal solutions are supported only on <em>S</em> , and hence can be obtained by solving the oracle subproblem (11.33). Given the lower eigen- value bound (11.29), this subproblem is strictly convex, and so has a unique minimizer.</p><p>Based on Lemma 11.2, in order to prove parts (a) and (b) of Theorem 11.3, it suffices to show that the subvector̂ <em>zSc</em> constructed in step 3 satisfies the <em>strict dual feasibility</em> condition‖ <em>z</em> ̂ <em>Sc</em> ‖∞ <em>&lt;</em> 1.</p><p><em>Establishing strict dual feasibility.</em> Let us delve into the form of the subvector ̂ <em>zSc</em> constructed in step 3. By using the fact that <em>β</em> ̂ <em>Sc</em> = <em>β</em> ∗ <em>Sc</em> = 0 and writing out the zero-subgradient condition (11.32) in block matrix form, we obtain</p><div class="language-"><pre><code>1\nN\n</code></pre></div><h6 id="-483"><a class="header-anchor" href="#-483" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>X TS X S X TS X Sc\nX TSc X S X TSc X Sc\n</code></pre></div><h6 id="-484"><a class="header-anchor" href="#-484" aria-hidden="true">#</a> ][</h6><div class="language-"><pre><code>β ̂ S − β ∗ S\n0\n</code></pre></div><h6 id="-485"><a class="header-anchor" href="#-485" aria-hidden="true">#</a> ]</h6><h6 id="−-20"><a class="header-anchor" href="#−-20" aria-hidden="true">#</a> −</h6><h6 id="_1-150"><a class="header-anchor" href="#_1-150" aria-hidden="true">#</a> 1</h6><h6 id="n-44"><a class="header-anchor" href="#n-44" aria-hidden="true">#</a> N</h6><h6 id="-486"><a class="header-anchor" href="#-486" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>X TS w\nX TSc w\n</code></pre></div><h6 id="-487"><a class="header-anchor" href="#-487" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>+ λN\n</code></pre></div><h6 id="-488"><a class="header-anchor" href="#-488" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>̂ zS\n̂ zSc\n</code></pre></div><h6 id="-489"><a class="header-anchor" href="#-489" aria-hidden="true">#</a> ]</h6><h6 id="-490"><a class="header-anchor" href="#-490" aria-hidden="true">#</a> =</h6><h6 id="-491"><a class="header-anchor" href="#-491" aria-hidden="true">#</a> [</h6><h6 id="_0"><a class="header-anchor" href="#_0" aria-hidden="true">#</a> 0</h6><h6 id="_0-1"><a class="header-anchor" href="#_0-1" aria-hidden="true">#</a> 0</h6><h6 id="-492"><a class="header-anchor" href="#-492" aria-hidden="true">#</a> ]</h6><h6 id="-493"><a class="header-anchor" href="#-493" aria-hidden="true">#</a> .</h6><h6 id="_11-34"><a class="header-anchor" href="#_11-34" aria-hidden="true">#</a> (11.34)</h6><p>Solving for the vector̂ <em>zSc</em> ∈R <em>p</em> − <em>k</em> yields</p><div class="language-"><pre><code>̂ zSc =\n</code></pre></div><h6 id="_1-151"><a class="header-anchor" href="#_1-151" aria-hidden="true">#</a> 1</h6><div class="language-"><pre><code>λN\n</code></pre></div><h6 id="-494"><a class="header-anchor" href="#-494" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>X TSc w\nN\n</code></pre></div><h6 id="−-21"><a class="header-anchor" href="#−-21" aria-hidden="true">#</a> −</h6><div class="language-"><pre><code>X TSc X S\nN\n</code></pre></div><h6 id="̂-1"><a class="header-anchor" href="#̂-1" aria-hidden="true">#</a> (̂</h6><div class="language-"><pre><code>βS − β ∗ S\n</code></pre></div><h6 id="-495"><a class="header-anchor" href="#-495" aria-hidden="true">#</a> )</h6><h6 id="-496"><a class="header-anchor" href="#-496" aria-hidden="true">#</a> }</h6><h6 id="_11-35"><a class="header-anchor" href="#_11-35" aria-hidden="true">#</a> . (11.35)</h6><p>Similarly, using the assumed invertibility of <strong>X</strong> <em>TS</em> <strong>X</strong> <em>S</em> in order to solve for the</p><p>difference <em>β</em> ̂ <em>S</em> − <em>β</em> ∗ <em>S</em> yields</p><div class="language-"><pre><code>β ̂ S − βS ∗=\n</code></pre></div><h6 id="-497"><a class="header-anchor" href="#-497" aria-hidden="true">#</a> (</h6><h6 id="x-ts-x-s"><a class="header-anchor" href="#x-ts-x-s" aria-hidden="true">#</a> X TS X S</h6><h6 id="n-45"><a class="header-anchor" href="#n-45" aria-hidden="true">#</a> N</h6><h6 id="−-1-5"><a class="header-anchor" href="#−-1-5" aria-hidden="true">#</a> )− 1</h6><div class="language-"><pre><code>X TS w\nN\n</code></pre></div><div class="language-"><pre><code>− λN\n</code></pre></div><h6 id="-498"><a class="header-anchor" href="#-498" aria-hidden="true">#</a> (</h6><h6 id="x-ts-x-s-1"><a class="header-anchor" href="#x-ts-x-s-1" aria-hidden="true">#</a> X TS X S</h6><h6 id="n-46"><a class="header-anchor" href="#n-46" aria-hidden="true">#</a> N</h6><h6 id="−-1-6"><a class="header-anchor" href="#−-1-6" aria-hidden="true">#</a> )− 1</h6><div class="language-"><pre><code>sign( βS ∗)\n︸ ︷︷ ︸\nUS\n</code></pre></div><h6 id="_11-36"><a class="header-anchor" href="#_11-36" aria-hidden="true">#</a> . (11.36)</h6><p>Substituting this expression back into Equation (11.35) and simplifying yields</p><div class="language-"><pre><code>̂ zSc = X TSc X S ( X TS X S )−^1 sign( βS ∗)\n︸ ︷︷ ︸\nμ\n</code></pre></div><div class="language-"><pre><code>+ X TSc\n</code></pre></div><h6 id="-499"><a class="header-anchor" href="#-499" aria-hidden="true">#</a> [</h6><h6 id="i-−-x-s-x-ts-x-s-−-1-x-ts"><a class="header-anchor" href="#i-−-x-s-x-ts-x-s-−-1-x-ts" aria-hidden="true">#</a> I − X S ( X TS X S )−^1 X TS</h6><h6 id="-500"><a class="header-anchor" href="#-500" aria-hidden="true">#</a> ]</h6><h6 id="-501"><a class="header-anchor" href="#-501" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>w\nλNN\n</code></pre></div><h6 id="-502"><a class="header-anchor" href="#-502" aria-hidden="true">#</a> )</h6><h6 id="︸-︷︷-︸-1"><a class="header-anchor" href="#︸-︷︷-︸-1" aria-hidden="true">#</a> ︸ ︷︷ ︸</h6><div class="language-"><pre><code>VSc\n</code></pre></div><h6 id="-503"><a class="header-anchor" href="#-503" aria-hidden="true">#</a> .</h6><p>By triangle inequality, we have</p><div class="language-"><pre><code>‖ z ̂ Sc ‖∞≤‖ μ ‖∞+‖ VSc ‖∞.\n</code></pre></div><p>Note that the vector <em>μ</em> ∈R <em>p</em> − <em>k</em> is a deterministic quantity, and moreover, by the mutual incoherence condition (11.27), we have‖ <em>μ</em> ‖∞≤ 1 − <em>γ</em>. The remaining quantity <em>VSc</em> ∈R <em>p</em> − <em>k</em> is a zero-mean Gaussian random vector, and we need to show that‖ <em>VSc</em> ‖∞ <em>&lt; γ</em> with high probability.</p><h6 id="support-recovery-in-linear-regression-309"><a class="header-anchor" href="#support-recovery-in-linear-regression-309" aria-hidden="true">#</a> SUPPORT RECOVERY IN LINEAR REGRESSION 309</h6><div class="language-"><pre><code>For an arbitrary j ∈ Sc , consider the random variable\n</code></pre></div><div class="language-"><pre><code>Vj : = X Tj\n</code></pre></div><h6 id="-504"><a class="header-anchor" href="#-504" aria-hidden="true">#</a> [</h6><h6 id="i-−-x-s-x-ts-x-s-−-1-x-ts-1"><a class="header-anchor" href="#i-−-x-s-x-ts-x-s-−-1-x-ts-1" aria-hidden="true">#</a> I − X S ( X TS X S )−^1 X TS</h6><h6 id="-505"><a class="header-anchor" href="#-505" aria-hidden="true">#</a> ]</h6><h6 id="︸-︷︷-︸-2"><a class="header-anchor" href="#︸-︷︷-︸-2" aria-hidden="true">#</a> ︸ ︷︷ ︸</h6><div class="language-"><pre><code>Π S ⊥( X )\n</code></pre></div><h6 id="-506"><a class="header-anchor" href="#-506" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>w\nλNN\n</code></pre></div><h6 id="-507"><a class="header-anchor" href="#-507" aria-hidden="true">#</a> )</h6><h6 id="-508"><a class="header-anchor" href="#-508" aria-hidden="true">#</a> .</h6><p>Noting that the matrix Π <em>S</em> ⊥( <strong>X</strong> ) is an orthogonal projection matrix and using the column normalization condition (11.28), we see that each <em>Vj</em> is zero-mean with variance at most <em>σ</em>^2 <em>K</em> clm^2 <em>/</em> ( <em>λ</em>^2 <em>NN</em> ). Therefore, combining Gaussian tail bounds with the union bound, we find that</p><h6 id="p-1"><a class="header-anchor" href="#p-1" aria-hidden="true">#</a> P</h6><h6 id="-509"><a class="header-anchor" href="#-509" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>‖ VSc ‖∞≥ γ/ 2\n</code></pre></div><h6 id="-510"><a class="header-anchor" href="#-510" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>≤2 ( p − k ) e\n</code></pre></div><div class="language-"><pre><code>−\nλ^2 NN ( γ/ 2)^2\n2 σ^2 K^2 clm.\n</code></pre></div><p>This probability vanishes at rate 2 <em>e</em> −^2 <em>λ</em></p><p>(^2) <em>NN</em> for the choice of <em>λN</em> given in the theorem statement. <em>Establishing<code>_ ∞ _bounds._ Next we establish a bound on the _</code></em> ∞-norm of the difference vector <em>US</em> = <em>β</em> ̂ <em>S</em> − <em>βS</em> ∗from Equation (11.36). By the triangle in- equality, we have</p><h6 id="‖-us-‖∞≤"><a class="header-anchor" href="#‖-us-‖∞≤" aria-hidden="true">#</a> ‖ US ‖∞≤</h6><h6 id="∥-17"><a class="header-anchor" href="#∥-17" aria-hidden="true">#</a> ∥</h6><h6 id="∥-18"><a class="header-anchor" href="#∥-18" aria-hidden="true">#</a> ∥</h6><h6 id="∥-19"><a class="header-anchor" href="#∥-19" aria-hidden="true">#</a> ∥</h6><h6 id="∥-20"><a class="header-anchor" href="#∥-20" aria-hidden="true">#</a> ∥</h6><h6 id="∥-21"><a class="header-anchor" href="#∥-21" aria-hidden="true">#</a> ∥</h6><h6 id="-511"><a class="header-anchor" href="#-511" aria-hidden="true">#</a> (</h6><h6 id="x-ts-x-s-2"><a class="header-anchor" href="#x-ts-x-s-2" aria-hidden="true">#</a> X TS X S</h6><h6 id="n-47"><a class="header-anchor" href="#n-47" aria-hidden="true">#</a> N</h6><h6 id="−-1-7"><a class="header-anchor" href="#−-1-7" aria-hidden="true">#</a> )− 1</h6><div class="language-"><pre><code>X TS w\nN\n</code></pre></div><h6 id="∥-∥-∥-∥-∥-∞"><a class="header-anchor" href="#∥-∥-∥-∥-∥-∞" aria-hidden="true">#</a> ∥ ∥ ∥ ∥ ∥ ∞</h6><h6 id="-512"><a class="header-anchor" href="#-512" aria-hidden="true">#</a> +</h6><h6 id="∥-22"><a class="header-anchor" href="#∥-22" aria-hidden="true">#</a> ∥</h6><h6 id="∥-23"><a class="header-anchor" href="#∥-23" aria-hidden="true">#</a> ∥</h6><h6 id="∥-24"><a class="header-anchor" href="#∥-24" aria-hidden="true">#</a> ∥</h6><h6 id="∥-25"><a class="header-anchor" href="#∥-25" aria-hidden="true">#</a> ∥</h6><h6 id="∥-26"><a class="header-anchor" href="#∥-26" aria-hidden="true">#</a> ∥</h6><h6 id="-513"><a class="header-anchor" href="#-513" aria-hidden="true">#</a> (</h6><h6 id="x-ts-x-s-3"><a class="header-anchor" href="#x-ts-x-s-3" aria-hidden="true">#</a> X TS X S</h6><h6 id="n-48"><a class="header-anchor" href="#n-48" aria-hidden="true">#</a> N</h6><h6 id="−-1-∥∥"><a class="header-anchor" href="#−-1-∥∥" aria-hidden="true">#</a> )− 1 ∥∥</h6><h6 id="∥-27"><a class="header-anchor" href="#∥-27" aria-hidden="true">#</a> ∥</h6><h6 id="∥-28"><a class="header-anchor" href="#∥-28" aria-hidden="true">#</a> ∥</h6><h6 id="∥-29"><a class="header-anchor" href="#∥-29" aria-hidden="true">#</a> ∥</h6><div class="language-"><pre><code>∞\n</code></pre></div><div class="language-"><pre><code>λN, (11.37)\n</code></pre></div><p>where we have multiplied and divided different terms by <em>N</em> for later conve- nience. The second term is a deterministic quantity, so that it remains to bound the first term. For each <em>i</em> = 1 <em>,...,k</em> , consider the random variable</p><div class="language-"><pre><code>Zi : = eTi\n</code></pre></div><h6 id="-514"><a class="header-anchor" href="#-514" aria-hidden="true">#</a> (</h6><h6 id="_1-152"><a class="header-anchor" href="#_1-152" aria-hidden="true">#</a> 1</h6><h6 id="n-49"><a class="header-anchor" href="#n-49" aria-hidden="true">#</a> N</h6><h6 id="x-ts-x-s-4"><a class="header-anchor" href="#x-ts-x-s-4" aria-hidden="true">#</a> X TS X S</h6><h6 id="−-1-8"><a class="header-anchor" href="#−-1-8" aria-hidden="true">#</a> )− 1</h6><h6 id="_1-153"><a class="header-anchor" href="#_1-153" aria-hidden="true">#</a> 1</h6><h6 id="n-50"><a class="header-anchor" href="#n-50" aria-hidden="true">#</a> N</h6><div class="language-"><pre><code>X TS w.\n</code></pre></div><p>Since the elements of <strong>w</strong> are i.i.d. <em>N</em> (0 <em>,σ</em>^2 ) variables, the variable <em>Zi</em> is zero- mean Gaussian with variance at most</p><div class="language-"><pre><code>σ^2\nN\n</code></pre></div><h6 id="∥-30"><a class="header-anchor" href="#∥-30" aria-hidden="true">#</a> ∥</h6><h6 id="∥-31"><a class="header-anchor" href="#∥-31" aria-hidden="true">#</a> ∥</h6><h6 id="∥-32"><a class="header-anchor" href="#∥-32" aria-hidden="true">#</a> ∥</h6><h6 id="∥-33"><a class="header-anchor" href="#∥-33" aria-hidden="true">#</a> ∥</h6><h6 id="∥-34"><a class="header-anchor" href="#∥-34" aria-hidden="true">#</a> ∥</h6><h6 id="-515"><a class="header-anchor" href="#-515" aria-hidden="true">#</a> (</h6><h6 id="_1-154"><a class="header-anchor" href="#_1-154" aria-hidden="true">#</a> 1</h6><h6 id="n-51"><a class="header-anchor" href="#n-51" aria-hidden="true">#</a> N</h6><h6 id="x-ts-x-s-5"><a class="header-anchor" href="#x-ts-x-s-5" aria-hidden="true">#</a> X TS X S</h6><h6 id="−-1-∥∥-1"><a class="header-anchor" href="#−-1-∥∥-1" aria-hidden="true">#</a> )− 1 ∥∥</h6><h6 id="∥-35"><a class="header-anchor" href="#∥-35" aria-hidden="true">#</a> ∥</h6><h6 id="∥-36"><a class="header-anchor" href="#∥-36" aria-hidden="true">#</a> ∥</h6><h6 id="∥-37"><a class="header-anchor" href="#∥-37" aria-hidden="true">#</a> ∥</h6><div class="language-"><pre><code>2\n</code></pre></div><h6 id="≤-9"><a class="header-anchor" href="#≤-9" aria-hidden="true">#</a> ≤</h6><div class="language-"><pre><code>σ^2\nC min N\n</code></pre></div><h6 id="-516"><a class="header-anchor" href="#-516" aria-hidden="true">#</a> ,</h6><p>where we have used the eigenvalue condition (11.29). Therefore, again com- bining Gaussian tail bounds with the union bound, we find that</p><div class="language-"><pre><code>P[‖ US ‖∞ &gt; t ]≤ 2 e −\n</code></pre></div><div class="language-"><pre><code>t^2 C min N\n2 σ^2 +log k.\n</code></pre></div><p>Let us set <em>t</em> = 4 <em>σλN/</em></p><h6 id="√-54"><a class="header-anchor" href="#√-54" aria-hidden="true">#</a> √</h6><p><em>C</em> minand then observe that our choice of <em>λN</em> guarantees that 8 <em>Nλ</em>^2 <em>N&gt;</em> log <em>p</em> ≥log <em>k</em>. Putting together these pieces, we conclude that</p><p>‖ <em>US</em> ‖∞≤ 4 <em>σλN/</em></p><h6 id="√-55"><a class="header-anchor" href="#√-55" aria-hidden="true">#</a> √</h6><div class="language-"><pre><code>C minwith probability at least 1− 2 e − c^2 λ\n</code></pre></div><p>(^2) <em>NN</em></p><p>. Overall, we conclude that</p><div class="language-"><pre><code>‖ β ̂ S − β ∗ S ‖∞≤ λN\n</code></pre></div><h6 id="-517"><a class="header-anchor" href="#-517" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>4 σ\n√\nC min\n</code></pre></div><h6 id="-518"><a class="header-anchor" href="#-518" aria-hidden="true">#</a> +</h6><h6 id="∥-38"><a class="header-anchor" href="#∥-38" aria-hidden="true">#</a> ∥</h6><h6 id="∥-x-ts-x-s-n-−-1-1"><a class="header-anchor" href="#∥-x-ts-x-s-n-−-1-1" aria-hidden="true">#</a> ∥( X TS X S/N )−^1</h6><h6 id="∥-39"><a class="header-anchor" href="#∥-39" aria-hidden="true">#</a> ∥</h6><h6 id="∥-40"><a class="header-anchor" href="#∥-40" aria-hidden="true">#</a> ∥</h6><div class="language-"><pre><code>∞\n</code></pre></div><h6 id="-519"><a class="header-anchor" href="#-519" aria-hidden="true">#</a> ]</h6><h6 id="-520"><a class="header-anchor" href="#-520" aria-hidden="true">#</a> ,</h6><p>with probability greater than 1− 2 <em>e</em> − <em>c</em>^2 <em>λ</em></p><p>(^2) <em>NN</em> , as claimed.</p><h6 id="_310-theoretical-results-for-the-lasso"><a class="header-anchor" href="#_310-theoretical-results-for-the-lasso" aria-hidden="true">#</a> 310 THEORETICAL RESULTS FOR THE LASSO</h6><h3 id="_11-5-beyond-the-basic-lasso"><a class="header-anchor" href="#_11-5-beyond-the-basic-lasso" aria-hidden="true">#</a> 11.5 Beyond the Basic Lasso</h3><p>In this chapter, we have limited ourselves to discussion of the basic lasso, which involves the least-squares loss function combined with the <em>`</em> 1 -norm as a regularizer. However, many of the ingredients have straightforward extensions to more general cost functions, including logistic regression and other types of generalized linear models, as well as to more exotic forms of regulariza- tion, including the group lasso, nuclear norm, and other types of structured regularizers. Here we sketch out the basic picture, referring the reader to the bibliographic section for links to further details. Consider an objective function of the form</p><div class="language-"><pre><code>F ( β ) =\n</code></pre></div><h6 id="_1-155"><a class="header-anchor" href="#_1-155" aria-hidden="true">#</a> 1</h6><h6 id="n-52"><a class="header-anchor" href="#n-52" aria-hidden="true">#</a> N</h6><h6 id="∑-n-110"><a class="header-anchor" href="#∑-n-110" aria-hidden="true">#</a> ∑ N</h6><div class="language-"><pre><code>i =1\n</code></pre></div><div class="language-"><pre><code>f ( β ; zi ) , (11.38)\n</code></pre></div><p>where the function <em>β</em> 7→ <em>g</em> ( <em>β</em> ; <em>zi</em> ) measures the fit of the parameter vector <em>β</em> ∈R <em>p</em> to the sample <em>zi</em>. In the context of regression problems, each sample takes the form <em>zi</em> = ( <em>xi,yi</em> )∈R <em>p</em> ×R, whereas in problems such as the graphical lasso, each sample corresponds to a vector <em>zi</em> = <em>xi</em> ∈R <em>p</em>. Letting Φ :R <em>p</em> →R denote a regularizer, we then consider an estimator of the form</p><div class="language-"><pre><code>β ̂∈arg min\nβ ∈Ω\n</code></pre></div><div class="language-"><pre><code>{ F ( β ) + λN Φ( β )}. (11.39)\n</code></pre></div><p>We can view <em>β</em> ̂as an estimate of the deterministic vector <em>β</em> ∗that minimizes the population objective function <em>F</em> ̄( <em>β</em> ) : =E[ <em>f</em> ( <em>β</em> ; <em>Z</em> )]. To put our previous discussion in context, the familiar lasso is a special case of this general <em>M</em> -estimator, based on the choices</p><div class="language-"><pre><code>f ( β ; xi,yi ) =\n</code></pre></div><h6 id="_1-156"><a class="header-anchor" href="#_1-156" aria-hidden="true">#</a> 1</h6><h6 id="_2-93"><a class="header-anchor" href="#_2-93" aria-hidden="true">#</a> 2</h6><h6 id="-521"><a class="header-anchor" href="#-521" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>yi −〈 xi, β 〉\n</code></pre></div><h6 id="_2-94"><a class="header-anchor" href="#_2-94" aria-hidden="true">#</a> ) 2</h6><div class="language-"><pre><code>, and Φ( β ) =‖ β ‖ 1 ,\n</code></pre></div><p>and with the optimization taking place over Ω =R <em>p</em>. In the case of random design, say with covariates <em>xi</em> ∼ <em>N</em> (0 <em>,</em> <strong>Σ</strong> ), the population objective function for linear regression takes the form <em>F</em> ̄( <em>β</em> ) =^12 ( <em>β</em> − <em>β</em> ∗) <em>T</em> <strong>Σ</strong> ( <em>β</em> − <em>β</em> ∗) +^12 <em>σ</em>^2. Considering the general <em>M</em> -estimator (11.39), our goal here is to provide some intuition on how to analyze the error‖ <em>β</em> ̂− <em>β</em> ∗‖ 2. When <em>N &lt; p</em> , then the objective function (11.38) can never be strongly convex: indeed, assuming that it is twice differentiable, the Hessian is a sum of <em>N</em> matrices in <em>p</em> dimensions, and so must be rank degenerate. As noted previously, the restricted eigenvalue condition is a special case of a more general property of cost functions and regularizers, known as restricted strong convexity. In particular, given a set C ⊂R <em>p</em> , a differentiable function <em>F</em> satisfies <em>restricted strong convexity</em> overC at <em>β</em> ∗if there exists a parameter <em>γ &gt;</em> 0 such that</p><div class="language-"><pre><code>F ( β ∗+ ν )− F ( β ∗)−〈∇ F ( β ∗) , ν 〉≥ γ ‖ ν ‖^22 for all ν ∈C. (11.40)\n</code></pre></div><p>When <em>F</em> is twice differentiable, then this lower bound is equivalent to con- trolling the Hessian in a neighborhood of <em>β</em> ∗, as in the definition (11.9)—see</p><h6 id="bibliographic-notes-311"><a class="header-anchor" href="#bibliographic-notes-311" aria-hidden="true">#</a> BIBLIOGRAPHIC NOTES 311</h6><p>Exercise 11.6 for details. Thus, in the special case of a least-squares problem, restricted strong convexity is equivalent to a restricted eigenvalue condition. For what type of setsCcan a condition of this form be expected to hold? Since our ultimate goal is to control the error vector <em>ν</em> ̂= <em>β</em> ̂− <em>β</em> ∗, we need only ensure that strong convexity hold over a subsetCthat is guaranteed— typically with high probability over the data—to contain the error vector. Such sets exist for regularizers that satisfy a property known as <em>decompos- ability</em> , which generalizes a basic property of the <em>`</em> 1 -norm to a broader family of regularizers. Decomposability is defined in terms of a subspaceMof the parameter set Ω, meant to describe the structure expected in the optimum <em>β</em> ∗, and its orthogonal complementM⊥, corresponding to undesirable pertur- bations away from the model structure. With this notation, a regularizer Φ is said to be decomposable with respect toMif</p><div class="language-"><pre><code>Φ( β + θ ) = Φ( β ) + Φ( θ ) for all pairs ( β,θ )∈M×M⊥. (11.41)\n</code></pre></div><p>In the case of the <em><code>_ 1 -norm, the model subspace is simply the set of all vectors with support on some fixed set _S_ , whereas the orthogonal complementM⊥ consists of vectors supported on the complementary set _Sc_. The decomposabil- ity relation (11.41) follows from the coordinate-wise nature of the _</code></em> 1 -norm. With appropriate choices of subspaces, many other regularizers are decom- posable, including weighted forms of the lasso, the group lasso and overlap group lasso penalties, and (with a minor generalization) the nuclear norm for low-rank matrices. See the bibliographic section for further details.</p><h3 id="bibliographic-notes-8"><a class="header-anchor" href="#bibliographic-notes-8" aria-hidden="true">#</a> Bibliographic Notes</h3><p>Knight and Fu (2000) derive asymptotic theory for the lasso and related es- timators when the dimension <em>p</em> is fixed; the irrepresentable condition (11.27) appears implicitly in their analysis. Greenshtein and Ritov (2004) were the first authors to provide a high-dimensional analysis of the lasso, in particu- lar providing bounds on the prediction error allowing for the <em>p</em>  <em>N</em> setting. The irrepresentable or mutual incoherence condition (11.27) was developed independently by Fuchs (2004) and Tropp (2006) in signal processing, and Meinshausen and B ̈uhlmann (2006) as well as Zhao and Yu (2006) in statis- tics. The notion of restricted eigenvalues was introduced by Bickel, Ritov and Tsybakov (2009); it is a less restrictive condition than the restricted isome- try property from Chapter 10. van de Geer and B ̈uhlmann (2009) provide a comparison between these and other related conditions for proving estimation error bounds on the lasso. Cand`es and Tao (2007) defined and developed the- ory for the “Dantzig selector”, a problem closely related to the lasso. Raskutti, Wainwright and Yu (2010) show that the RE condition holds with high prob- ability for various types of random Gaussian design matrices; see Rudelson and Zhou (2013) for extensions to sub-Gaussian designs. The proof of Theorem 11.1 is based on the work of Bickel et al. (2009),</p><h6 id="_312-theoretical-results-for-the-lasso"><a class="header-anchor" href="#_312-theoretical-results-for-the-lasso" aria-hidden="true">#</a> 312 THEORETICAL RESULTS FOR THE LASSO</h6><p>whereas Negahban et al. (2012) derive the lasso error bound (11.24) for <em><code>q_ - sparse vectors. The basic inequality technique used in these proofs is standard in the analysis of _M_ -estimators (van de Geer 2000). Raskutti, Wainwright and Yu (2011) analyze the minimax rates of regression over _</code>q</em> -balls, obtaining rates for both <em>`</em> 2 -error and prediction error. Theorem 11.2(a) was proved by Bunea, Tsybakov and Wegkamp (2007), whereas part (b) is due to Bickel et al. (2009). The restricted eigenvalue condition is actually required by any polynomial- time method in order to achieve the “fast rates” given in Theorem 11.2(b), as follows from the results of Zhang, Wainwright and Jordan (2014). Under a standard assumption in complexity theory, they prove that no polynomial- time algorithm can achieve the fast rate without imposing an RE condition.</p><p>Theorem 11.3 and the primal-dual witness (PDW) proof is due to Wain- wright (2009). In the same paper, sharp threshold results are established for Gaussian ensembles of design matrices, in particular concrete upper and lower bounds on the sample size that govern the transition from success to failure in support recovery. The proof of Lemma 11.2 was suggested by Carama- nis (2010). The PDW method has been applied to a range of other prob- lems, including analysis of group lasso (Obozinski et al. 2011, Wang, Liang and Xing 2013) and related relaxations (Jalali, Ravikumar, Sanghavi and Ruan 2010, Negahban and Wainwright 2011 <em>b</em> ), graphical lasso (Ravikumar et al. 2011), and methods for Gaussian graph selection with hidden vari- ables (Chandrasekaran et al. 2012). Lee, Sun and Taylor (2013) provide a general formulation of the PDW method for a broader class of <em>M</em> -estimators. As noted in Section 11.5, the analysis in this chapter can be extended to a much broader class of <em>M</em> -estimators, namely those based on decomposable regularizers. Negahban et al. (2012) provide a general framework for analyzing the estimation error‖ <em>β</em> ̂− <em>β</em> ∗‖ 2 for such <em>M</em> -estimators. As alluded to here, the two key ingredients are restricted strong convexity of the cost function, and decomposability of the regularizer.</p><h3 id="exercises-8"><a class="header-anchor" href="#exercises-8" aria-hidden="true">#</a> Exercises</h3><p>Ex. 11.1 For a given <em>q</em> ∈(0 <em>,</em> 1], recall the setB <em>q</em> ( <em>Rq</em> ) defined in Equation (11.7) as a model of soft sparsity.</p><div class="language-"><pre><code>(a) A related object is the weak `q -ball with parameters ( C,α ), given by\n</code></pre></div><div class="language-"><pre><code>B w ( α )( C ) : =\n</code></pre></div><h6 id="-522"><a class="header-anchor" href="#-522" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>θ ∈R p | | θ |( j )≤ Cj − α for j = 1 ,...,p\n</code></pre></div><h6 id="-523"><a class="header-anchor" href="#-523" aria-hidden="true">#</a> }</h6><p><em>.</em> (11.42a)</p><div class="language-"><pre><code>Here| θ |( j )denote the order statistics of θ in absolute value, ordered from\nlargest to smallest (so that| θ |(1)= max\nj =1 , 2 ,...,p\n</code></pre></div><div class="language-"><pre><code>| θj |and| θ |( p )= min\nj =1 , 2 ,...,p\n</code></pre></div><div class="language-"><pre><code>| θj |.)\nFor any α &gt; 1 /q , show that there is a radius Rq depending on ( C,α ) such\nthatB w ( α )( C )⊆B q ( Rq ).\n(b) For a given integer k ∈{ 1 , 2 ,...,p }, the best k -term approximation to a\n</code></pre></div><h6 id="exercises-313"><a class="header-anchor" href="#exercises-313" aria-hidden="true">#</a> EXERCISES 313</h6><div class="language-"><pre><code>vector θ ∗∈R p is given by\n</code></pre></div><div class="language-"><pre><code>Π k ( θ ∗) : = arg min\n‖ θ ‖ 0 ≤ k\n</code></pre></div><div class="language-"><pre><code>‖ θ − θ ∗‖^22. (11.42b)\n</code></pre></div><div class="language-"><pre><code>Give a closed form expression for Π k ( θ ∗).\n(c) When θ ∗∈B q ( Rq ) for some q ∈(0 , 1], show that the best k -term approx-\nimation satisfies\n</code></pre></div><div class="language-"><pre><code>‖Π k ( θ ∗)− θ ∗‖^22 ≤\n</code></pre></div><h6 id="-524"><a class="header-anchor" href="#-524" aria-hidden="true">#</a> (</h6><div class="language-"><pre><code>Rq\n</code></pre></div><div class="language-"><pre><code>) 2 /q ( 1\nk\n</code></pre></div><div class="language-"><pre><code>)^2 q − 1\n</code></pre></div><p><em>.</em> (11.42c)</p><p>Ex. 11.2 In this exercise, we analyze an alternative version of the lasso, namely the estimator</p><div class="language-"><pre><code>β ̂= arg min\nβ ∈R p\n‖ β ‖ 1 such that N^1 ‖ y − X β ‖^22 ≤ C , (11.43)\n</code></pre></div><p>where the constant <em>C &gt;</em> 0 is a parameter to be chosen by the user. (This form of the lasso is often referred to as <em>relaxed basis pursuit</em> .)</p><div class="language-"><pre><code>(a) Suppose that C is chosen such that β ∗is feasible for the convex program.\nShow that the error vector ν ̂= β ̂− β ∗must satisfy the cone constraint\n‖̂ νSc ‖ 1 ≤‖̂ νS ‖ 1.\n(b) Assuming the linear observation model y = X β ∗+ w , show that ν ̂satisfies\nthe basic inequality\n‖ X ̂ ν ‖^22\nN\n</code></pre></div><h6 id="≤-2-1"><a class="header-anchor" href="#≤-2-1" aria-hidden="true">#</a> ≤ 2</h6><div class="language-"><pre><code>‖ X T w ‖∞\nN\n</code></pre></div><div class="language-"><pre><code>‖̂ ν ‖ 1 +\n</code></pre></div><h6 id="-525"><a class="header-anchor" href="#-525" aria-hidden="true">#</a> {</h6><h6 id="c-−"><a class="header-anchor" href="#c-−" aria-hidden="true">#</a> C −</h6><div class="language-"><pre><code>‖ w ‖^22\nN\n</code></pre></div><h6 id="-526"><a class="header-anchor" href="#-526" aria-hidden="true">#</a> }</h6><h6 id="-527"><a class="header-anchor" href="#-527" aria-hidden="true">#</a> .</h6><div class="language-"><pre><code>(c) Assuming a γ -RE condition on X , use part (b) to establish a bound on\nthe ` 2 -error‖ β ̂− β ∗‖ 2.\n</code></pre></div><p>Ex. 11.3 In this exercise, we sketch out the proof of the bound (11.24).</p><p>In particular, we show that if <em>λN</em> ≥ ‖ <strong>X</strong></p><div class="language-"><pre><code>T w ‖∞\nN and β\n</code></pre></div><div class="language-"><pre><code>∗∈B q ( Rq ), then the\n</code></pre></div><p>Lagrangian lasso error satisfies a bound of the form</p><div class="language-"><pre><code>‖ β ̂− β ∗‖^22 ≤ c Rqλ^1 N − q/^2. (11.44a)\n</code></pre></div><div class="language-"><pre><code>(a) Generalize Lemma 11.1 by showing that the error vector̂ ν satisfies the\n“cone-like” constraint\n</code></pre></div><div class="language-"><pre><code>‖̂ νSc ‖ 1 ≤ 3 ‖̂ νS ‖ 1 +‖ βS ∗ c ‖ 1 , (11.44b)\n</code></pre></div><div class="language-"><pre><code>valid for any subset S ⊆{ 1 , 2 ,...,p }and its complement.\n(b) Suppose that X satisfies a γ -RE condition over all vectors satisfying the\ncone-like condition (11.44b). Prove that\n</code></pre></div><div class="language-"><pre><code>‖̂ ν ‖^22 ≤ λN\n</code></pre></div><h6 id="-528"><a class="header-anchor" href="#-528" aria-hidden="true">#</a> {</h6><div class="language-"><pre><code>4 ‖ ν ̂ S ‖ 1 +‖ βS ∗ c ‖ 1\n</code></pre></div><h6 id="-529"><a class="header-anchor" href="#-529" aria-hidden="true">#</a> }</h6><h6 id="-530"><a class="header-anchor" href="#-530" aria-hidden="true">#</a> .</h6><div class="language-"><pre><code>again valid for any subset S of indices.\n(c) Optimize the choice of S so as to obtain the claimed bound (11.44a).\n</code></pre></div><h6 id="_314-theoretical-results-for-the-lasso"><a class="header-anchor" href="#_314-theoretical-results-for-the-lasso" aria-hidden="true">#</a> 314 THEORETICAL RESULTS FOR THE LASSO</h6><p>Ex. 11.4 Consider a random design matrix <strong>X</strong> ∈R <em>N</em> × <em>p</em> with each row <em>xi</em> ∈R <em>p</em> drawn i.i.d. from aN(0 <em>,</em> <strong>Σ</strong> ) distribution, where the covariance matrix <strong>Σ</strong> is strictly positive definite. Show that a <em>γ</em> -RE condition holds over the setC( <em>S</em> ; <em>α</em> ) with high probability whenever the sample size is lower bounded as <em>N &gt; c</em> | <em>S</em> |^2 log <em>p</em> for a sufficiently large constant <em>c</em>. ( <em>Remark:</em> This scaling of the sample size is not optimal; a more refined argument can be used to reduce | <em>S</em> |^2 to| <em>S</em> |.)</p><p>Ex. 11.5 Consider a random design matrix <strong>X</strong> ∈R <em>N</em> × <em>p</em> with i.i.d. <em>N</em> (0 <em>,</em> 1) en- tries. In this exercise, we show that the mutual incoherence condition (11.27) holds with high probability as long as <em>N &gt; ck</em> log <em>p</em> for a sufficiently large numerical constant <em>c</em>. ( <em>Hint:</em> For <em>N &gt;</em> 4 <em>k</em> , it is known that the event</p><p>E={ <em>λ</em> min</p><h6 id="x-ts-x-s-6"><a class="header-anchor" href="#x-ts-x-s-6" aria-hidden="true">#</a> ( X TS X S</h6><div class="language-"><pre><code>N\n</code></pre></div><h6 id="-531"><a class="header-anchor" href="#-531" aria-hidden="true">#</a> )</h6><div class="language-"><pre><code>≥^14 }holds with high probability.)\n(a) Show that\n</code></pre></div><div class="language-"><pre><code>γ = 1−max\nj ∈ Sc\nmax\nz ∈{− 1 , +1} k\n</code></pre></div><div class="language-"><pre><code>x Tj X S ( X TS X S )−^1 z\n︸ ︷︷ ︸\nVj,z\n</code></pre></div><h6 id="-532"><a class="header-anchor" href="#-532" aria-hidden="true">#</a> .</h6><div class="language-"><pre><code>(b) Recalling the eventE, show that there is a numerical constant c 0 &gt; 0\nsuch that\n</code></pre></div><div class="language-"><pre><code>P\n</code></pre></div><h6 id="-533"><a class="header-anchor" href="#-533" aria-hidden="true">#</a> [</h6><div class="language-"><pre><code>Vj,z ≥ t\n</code></pre></div><h6 id="-534"><a class="header-anchor" href="#-534" aria-hidden="true">#</a> ]</h6><div class="language-"><pre><code>≤ e − c^0\n</code></pre></div><div class="language-"><pre><code>Ntk^2\n+P[E c ] for any t &gt; 0 ,\n</code></pre></div><div class="language-"><pre><code>valid for each fixed index j ∈ Sc and vector z ∈{− 1 , +1} k.\n(c) Use part (b) to complete the proof.\n</code></pre></div><p>Ex. 11.6 Consider a twice differentiable function <em>F</em> :R <em>p</em> →Rand a setC ⊂R <em>p</em> such that</p><div class="language-"><pre><code>∇^2 F ( β )\n‖ ν ‖^22\n</code></pre></div><div class="language-"><pre><code>≥ γ ‖ ν ‖^22 for all ν ∈C,\n</code></pre></div><p>uniformly for all <em>β</em> in a neighborhood of some fixed parameter <em>β</em> ∗. Show that the RSC condition (11.40) holds.</p><h2 id="bibliography"><a class="header-anchor" href="#bibliography" aria-hidden="true">#</a> Bibliography</h2><p>Agarwal, A., Anandkumar, A., Jain, P., Netrapalli, P. and Tandon, R. (2014), Learning sparsely used overcomplete dictionaries via alternating mini- mization, <em>Journal of Machine Learning Research Workshop</em> <strong>35</strong> , 123–137.</p><p>Agarwal, A., Negahban, S. and Wainwright, M. J. (2012 <em>a</em> ), Fast global con- vergence of gradient methods for high-dimensional statistical recovery, <em>Annals of Statistics</em> <strong>40</strong> (5), 2452–2482.</p><p>Agarwal, A., Negahban, S. and Wainwright, M. J. (2012 <em>b</em> ), Noisy matrix de- composition via convex relaxation: Optimal rates in high dimensions, <em>Annals of Statistics</em> <strong>40</strong> (2), 1171–1197.</p><p>Alizadeh, A., Eisen, M., Davis, R. E., Ma, C., Lossos, I., Rosenwal, A., Boldrick, J., Sabet, H., Tran, T., Yu, X., Pwellm, J., Marti, G., Moore, T., Hudsom, J., Lu, L., Lewis, D., Tibshirani, R., Sherlock, G., Chan, W., Greiner, T., Weisenburger, D., Armitage, K., Levy, R., Wilson, W., Greve, M., Byrd, J., Botstein, D., Brown, P. and Staudt, L. (2000), Iden- tification of molecularly and clinically distinct subtypes of diffuse large b cell lymphoma by gene expression profiling, <em>Nature</em> <strong>403</strong> , 503–511.</p><p>Alliney, S. and Ruzinsky, S. (1994), An algorithm for the minimization of mixed L1 and L2 norms with application to Bayesian estimation, <em>Trans- actions on Signal Processing</em> <strong>42</strong> (3), 618–627.</p><p>Amini, A. A. and Wainwright, M. J. (2009), High-dimensional analysis of semdefinite relaxations for sparse principal component analysis, <em>Annals of Statistics</em> <strong>5B</strong> , 2877–2921.</p><p>Anderson, T. (2003), <em>An Introduction to Multivariate Statistical Analysis, 3rd ed.</em> , Wiley, New York.</p><p>Antoniadis, A. (2007), Wavelet methods in statistics: Some recent develop- ments and their applications, <em>Statistics Surveys</em> <strong>1</strong> , 16–55.</p><p>Bach, F. (2008), Consistency of trace norm minimization, <em>Journal of Machine Learning Research</em> <strong>9</strong> , 1019–1048.</p><p>Bach, F., Jenatton, R., Mairal, J. and Obozinski, G. (2012), Optimization with sparsity-inducing penalties, <em>Foundations and Trends in Machine Learn- ing</em> <strong>4</strong> (1), 1–106.</p><p>Banerjee, O., El Ghaoui, L. and d’Aspremont, A. (2008), Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data, <em>Journal of Machine Learning Research</em> <strong>9</strong> , 485–516.</p><div class="language-"><pre><code>315\n</code></pre></div><h6 id="_316-bibliography"><a class="header-anchor" href="#_316-bibliography" aria-hidden="true">#</a> 316 BIBLIOGRAPHY</h6><p>Baraniuk, R. G., Davenport, M. A., DeVore, R. A. and Wakin, M. B. (2008), A simple proof of the restricted isometry property for random matrices, <em>Constructive Approximation</em> <strong>28</strong> (3), 253–263.</p><p>Barlow, R. E., Bartholomew, D., Bremner, J. M. and Brunk, H. D. (1972), <em>Statistical Inference under Order Restrictions: The Theory and Applica- tion of Isotonic Regression</em> , Wiley, New York.</p><p>Beck, A. and Teboulle, M. (2009), A fast iterative shrinkage-thresholding al- gorithm for linear inverse problems, <em>SIAM Journal on Imaging Sciences</em><strong>2</strong> , 183–202.</p><p>Benjamini, Y. and Hochberg, Y. (1995), Controlling the false discovery rate: a practical and powerful approach to multiple testing, <em>Journal of the Royal Statistical Society Series B.</em> <strong>85</strong> , 289–300.</p><p>Bennett, J. and Lanning, S. (2007), The netflix prize, in <em>Proceedings of KDD Cup and Workshop in conjunction with KDD</em>.</p><p>Bento, J. and Montanari, A. (2009), Which graphical models are difficulty to learn?, in <em>Advances in Neural Information Processing Systems (NIPS Conference Proceedings)</em>.</p><p>Berk, R., Brown, L., Buja, A., Zhang, K. and Zhao, L. (2013), Valid post- selection inference, <em>Annals of Statistics</em> <strong>41</strong> (2), 802–837.</p><p>Berthet, Q. and Rigollet, P. (2013), Computational lower bounds for sparse PCA, Technical report, Princeton University. arxiv1304.0828.</p><p>Bertsekas, D. (1999), <em>Nonlinear Programming</em> , Athena Scientific, Belmont MA.</p><p>Bertsekas, D. (2003), <em>Convex Analysis and Optimization</em> , Athena Scientific, Belmont MA.</p><p>Besag, J. (1974), Spatial interaction and the statistical analysis of lattice sys- tems, <em>Journal of the Royal Statistical Society Series B</em> <strong>36</strong> , 192–236.</p><p>Besag, J. (1975), Statistical analysis of non-lattice data, <em>The Statistician</em><strong>24</strong> (3), 179–195.</p><p>Bickel, P. J. and Levina, E. (2008), Covariance regularization by thresholding, <em>Annals of Statistics</em> <strong>36</strong> (6), 2577–2604.</p><p>Bickel, P. J., Ritov, Y. and Tsybakov, A. (2009), Simultaneous analysis of Lasso and Dantzig selector, <em>Annals of Statistics</em> <strong>37</strong> (4), 1705–1732.</p><p>Bien, J., Taylor, J. and Tibshirani, R. (2013), A Lasso for hierarchical inter- actions, <em>Annals of Statistics</em> <strong>42</strong> (3), 1111–1141.</p><p>Bien, J. and Tibshirani, R. (2011), Sparse estimation of a covariance matrix, <em>Biometrika</em> <strong>98</strong> (4), 807–820.</p><p>Birnbaum, A., Johnstone, I., Nadler, B. and Paul, D. (2013), Minimax bounds for sparse PCA with noisy high-dimensional data, <em>Annals of Statistics</em><strong>41</strong> (3), 1055–1084.</p><p>Boser, B., Guyon, I. and Vapnik, V. (1992), A training algorithm for optimal</p><h6 id="_317"><a class="header-anchor" href="#_317" aria-hidden="true">#</a> 317</h6><div class="language-"><pre><code>margin classifiers, in Proceedings of the Annual Conference on Learning\nTheory (COLT) , Philadelphia, Pa.\n</code></pre></div><p>Boyd, S., Parikh, N., Chu, E., Peleato, B. and Eckstein, J. (2011), Distributed optimization and statistical learning via the alternating direction method of multipliers, <em>Foundations and Trends in Machine Learning</em> <strong>3</strong> (1), 1–124.</p><p>Boyd, S. and Vandenberghe, L. (2004), <em>Convex Optimization</em> , Cambridge Uni- versity Press, Cambridge, UK.</p><p>Breiman, L. (1995), Better subset selection using the nonnegative garrote, <em>Technometrics</em> <strong>37</strong> , 738–754.</p><p>Breiman, L. and Ihaka, R. (1984), Nonlinear discriminant analysis via scaling and ACE, Technical report, University of California, Berkeley.</p><p>B ̈uhlmann, P. (2013), Statistical significance in high-dimensional linear mod- els, <em>Bernoul li</em> <strong>19</strong> (4), 1212–1242.</p><p>B ̈uhlmann, P. and van de Geer, S. (2011), <em>Statistics for High-Dimensional Data: Methods, Theory and Applications</em> , Springer, New York.</p><p>Bunea, F., She, Y. and Wegkamp, M. (2011), Optimal selection of reduced rank estimators of high-dimensional matrices, <strong>39</strong> (2), 1282–1309.</p><p>Bunea, F., Tsybakov, A. and Wegkamp, M. (2007), Sparsity oracle inequalities for the Lasso, <em>Electronic Journal of Statistics</em> pp. 169–194.</p><p>Burge, C. and Karlin, S. (1977), Prediction of complete gene structures in human genomic DNA, <em>Journal of Molecular Biology</em> <strong>268</strong> , 78–94.</p><p>Butte, A., Tamayo, P., Slonim, D., Golub, T. and Kohane, I. (2000), Discov- ering functional relationships between RNA expression and chemother- apeutic susceptibility using relevance networks, <em>Proceedings of the Na- tional Academy of Sciences</em> pp. 12182–12186.</p><p>Cand`es, E., Li, X., Ma, Y. and Wright, J. (2011), Robust Principal Component Analysis?, <em>Journal of the Association for Computing Machinery</em> <strong>58</strong> , 11:1– 11:37.</p><p>Cand`es, E. and Plan, Y. (2010), Matrix completion with noise, <em>Proceedings of the IEEE</em> <strong>98</strong> (6), 925–936.</p><p>Cand`es, E. and Recht, B. (2009), Exact matrix completion via convex opti- mization, <em>Foundation for Computational Mathematics</em> <strong>9</strong> (6), 717–772.</p><p>Cand`es, E., Romberg, J. K. and Tao, T. (2006), Stable signal recovery from incomplete and inaccurate measurements, <em>Communications on Pure and Applied Mathematics</em> <strong>59</strong> (8), 1207–1223.</p><p>Cand`es, E. and Tao, T. (2005), Decoding by linear programming, <em>IEEE Trans- actions on Information Theory</em> <strong>51</strong> (12), 4203–4215.</p><p>Cand`es, E. and Tao, T. (2007), The Dantzig selector: Statistical estimation when p is much larger than n, <em>Annals of Statistics</em> <strong>35</strong> (6), 2313–2351.</p><p>Cand`es, E. and Wakin, M. (2008), An introduction to compressive sampling, <em>Signal Processing Magazine, IEEE</em> <strong>25</strong> (2), 21–30.</p><h6 id="_318-bibliography"><a class="header-anchor" href="#_318-bibliography" aria-hidden="true">#</a> 318 BIBLIOGRAPHY</h6><p>Caramanis, C. (2010), ‘Personal communication’.</p><p>Chandrasekaran, V., Parrilo, P. A. and Willsky, A. S. (2012), Latent variable graphical model selection via convex optimization, <em>Annals of Statistics</em><strong>40</strong> (4), 1935–1967.</p><p>Chandrasekaran, V., Sanghavi, S., Parrilo, P. A. and Willsky, A. S. (2011), Rank-sparsity incoherence for matrix decomposition, <em>SIAM Journal on Optimization</em> <strong>21</strong> , 572–596.</p><p>Chaudhuri, S., Drton, M. and Richardson, T. S. (2007), Estimation of a co- variance matrix with zeros, <em>Biometrika</em> pp. 1–18.</p><p>Chen, S., Donoho, D. and Saunders, M. (1998), Atomic decomposition by basis pursuit, <em>SIAM Journal of Scientific Computing</em> <strong>20</strong> (1), 33–61.</p><p>Cheng, J., Levina, E. and Zhu, J. (2013), High-dimensional Mixed Graphical Models, <em>arXiv:1304.2810</em>.</p><p>Chi, E. C. and Lange, K. (2014), Splitting methods for convex clustering, <em>Journal of Computational and Graphical Statistics (online access)</em>.</p><p>Choi, Y., Taylor, J. and Tibshirani, R. (2014), Selecting the number of principal components: estimation of the true rank of a noisy matrix. arXiv:1410.8260.</p><p>Clemmensen, L. (2012), <em>sparseLDA: Sparse Discriminant Analysis</em>. R package version 0.1-6. <strong>URL:</strong> <em><a href="http://CRAN.R-project.org/package=sparseLDA" target="_blank" rel="noopener noreferrer">http://CRAN.R-project.org/package=sparseLDA</a></em></p><p>Clemmensen, L., Hastie, T., Witten, D. and Ersboll, B. (2011), Sparse dis- criminant analysis, <em>Technometrics</em> <strong>53</strong> , 406–413.</p><p>Clifford, P. (1990), Markov random fields in statistics, in G. Grimmett and D. J. A. Welsh, eds, <em>Disorder in physical systems</em> , Oxford Science Publi- cations.</p><p>Cohen, A., Dahmen, W. and DeVore, R. A. (2009), Compressed sensing and best k-term approximation, <em>Journal of the American Mathematical Soci- ety</em> <strong>22</strong> (1), 211–231.</p><p>Cox, D. and Wermuth, N. (1996), <em>Multivariate Dependencies: Models, Analysis and Interpretation</em> , Chapman &amp; Hall, London.</p><p>d’Aspremont, A., Banerjee, O. and El Ghaoui, L. (2008), First order methods for sparse covariance selection, <em>SIAM Journal on Matrix Analysis and its Applications</em> <strong>30</strong> (1), 55–66.</p><p>d’Aspremont, A., El Ghaoui, L., Jordan, M. I. and Lanckriet, G. R. G. (2007), A direct formulation for sparse PCA using semidefinite programming, <em>SIAM Review</em> <strong>49</strong> (3), 434–448.</p><p>Davidson, K. R. and Szarek, S. J. (2001), Local operator theory, random ma- trices, and Banach spaces, in <em>Handbook of Banach Spaces</em> , Vol. 1, Elsevier, Amsterdam, NL, pp. 317–336.</p><p>De Leeuw, J. (1994), Block-relaxation algorithms in statistics, in H. Bock,</p><h6 id="_319"><a class="header-anchor" href="#_319" aria-hidden="true">#</a> 319</h6><div class="language-"><pre><code>W. Lenski and M. M. Richter, eds, Information Systems and Data Anal-\nysis , Springer-Verlag, Berlin.\n</code></pre></div><p>Dobra, A., Hans, C., Jones, B., Nevins, J. R., Yao, G. and West, M. (2004), Sparse graphical models for exploring gene expression data, <em>Journal of Multivariate Analysis</em> <strong>90</strong> (1), 196 – 212.</p><p>Donoho, D. (2006), Compressed sensing, <em>IEEE Transactions on Information Theory</em> <strong>52</strong> (4), 1289–1306.</p><p>Donoho, D. and Huo, X. (2001), Uncertainty principles and ideal atomic de- composition, <em>IEEE Trans. Info Theory</em> <strong>47</strong> (7), 2845–2862.</p><p>Donoho, D. and Johnstone, I. (1994), Ideal spatial adaptation by wavelet shrinkage, <em>Biometrika</em> <strong>81</strong> , 425–455.</p><p>Donoho, D. and Stark, P. (1989), Uncertainty principles and signal recovery, <em>SIAM Journal of Applied Mathematics</em> <strong>49</strong> , 906–931.</p><p>Donoho, D. and Tanner, J. (2009), Counting faces of randomly-projected polytopes when the projection radically lowers dimension, <em>Journal of the American Mathematical Society</em> <strong>22</strong> (1), 1–53.</p><p>Dudoit, S., Fridlyand, J. and Speed, T. (2002), Comparison of discrimina- tion methods for the classification of tumors using gene expression data, <em>Journal of the American Statistical Association</em> <strong>97</strong> (457), 77–87.</p><p>Edwards, D. (2000), <em>Introduction to Graphical Modelling, 2nd Edition</em> , Springer, New York.</p><p>Efron, B. (1979), Bootstrap methods: another look at the jackknife, <em>Annals of Statistics</em> <strong>7</strong> , 1–26.</p><p>Efron, B. (1982), <em>The Jackknife, the Bootstrap and Other Resampling plans</em> , Vol. 38, SIAM- CBMS-NSF Regional Conference Series in Applied Math- ematics.</p><p>Efron, B. (2011), The bootstrap and Markov Chain Monte Carlo, <em>Journal of Biopharmaceutical Statistics</em> <strong>21</strong> (6), 1052–1062.</p><p>Efron, B. and Tibshirani, R. (1993), <em>An Introduction to the Bootstrap</em> , Chap- man &amp; Hall, London.</p><p>El Ghaoui, L., Viallon, V. and Rabbani, T. (2010), Safe feature elimination in sparse supervised learning, <em>Pacific journal of optimization</em> <strong>6</strong> (4), 667–698.</p><p>El Karoui, N. (2008), Operator norm consistent estimation of large- dimensional sparse covariance matrices, <em>Annals of Statistics</em> <strong>36</strong> (6), 2717– 2756.</p><p>Elad, M. and Bruckstein, A. M. (2002), A generalized uncertainty principle and sparse representation in pairs of bases, <em>IEEE Transactions on Infor- mation Theory</em> <strong>48</strong> (9), 2558–2567.</p><p>Erdos, P. and Renyi, A. (1961), On a classical problem of probability theory, <em>Magyar Tud. Akad. Mat. Kutat Int. Kzl.</em> <strong>6</strong> , 215–220. (English and Russian summary).</p><h6 id="_320-bibliography"><a class="header-anchor" href="#_320-bibliography" aria-hidden="true">#</a> 320 BIBLIOGRAPHY</h6><p>Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P. and Ben- gio, S. (2010), Why does unsupervised pre-training help deep learning?, <em>Journal of Machine Learning Research</em> <strong>11</strong> , 625–660.</p><p>Fan, J. and Li, R. (2001), Variable selection via nonconcave penalized likeli- hood and its oracle properties, <em>Journal of the American Statistical Asso- ciation</em> <strong>96</strong> (456), 1348–1360.</p><p>Fazel, M. (2002), Matrix Rank Minimization with Applications, PhD thesis, Stanford. Available online: <a href="http://faculty.washington.edu/mfazel/thesis-" target="_blank" rel="noopener noreferrer">http://faculty.washington.edu/mfazel/thesis-</a> final.pdf.</p><p>Feuer, A. and Nemirovski, A. (2003), On sparse representation in pairs of bases, <em>IEEE Transactions on Information Theory</em> <strong>49</strong> (6), 1579–1581.</p><p>Field, D. (1987), Relations between the statistics of natural images and the response properties of cortical cells, <em>Journal of the Optical Society of America</em> <strong>A4</strong> , 2379–2394.</p><p>Fisher, M. E. (1966), On the Dimer solution of planar Ising models, <em>Journal of Mathematical Physics</em> <strong>7</strong> , 1776–1781.</p><p>Fithian, W., Sun, D. and Taylor, J. (2014), Optimal inference after model selection, <em>ArXiv e-prints</em>.</p><p>Friedman, J., Hastie, T., Hoefling, H. and Tibshirani, R. (2007), Pathwise coordinate optimization, <em>Annals of Applied Statistics</em> <strong>1</strong> (2), 302–332.</p><p>Friedman, J., Hastie, T., Simon, N. and Tibshirani, R. (2015), <em>glmnet: Lasso and elastic-net regularized generalized linear models</em>. R package version 2.0.</p><p>Friedman, J., Hastie, T. and Tibshirani, R. (2008), Sparse inverse covariance estimation with the graphical Lasso, <em>Biostatistics</em> <strong>9</strong> , 432–441.</p><p>Friedman, J., Hastie, T. and Tibshirani, R. (2010 <em>a</em> ), Applications of the Lasso and grouped Lasso to the estimation of sparse graphical models, Technical report, Stanford University, Statistics Department.</p><p>Friedman, J., Hastie, T. and Tibshirani, R. (2010 <em>b</em> ), Regularization paths for generalized linear models via coordinate descent, <em>Journal of Statistical Software</em> <strong>33</strong> (1), 1–22.</p><p>Fu, W. (1998), Penalized regressions: the bridge versus the lasso, <em>Journal of Computational and Graphical Statistics</em> <strong>7</strong> (3), 397–416.</p><p>Fuchs, J. (2000), On the application of the global matched filter to doa es- timation with uniform circular arrays, in <em>Proceedings of the Acoustics, Speech, and Signal Processing, 2000. on IEEE International Conference</em></p><p><em>- Volume 05</em> , ICASSP ’00, IEEE Computer Society, Washington, DC, USA, pp. 3089–3092.</p><p>Fuchs, J. (2004), Recovery of exact sparse representations in the presence of noise, in <em>International Conference on Acoustics, Speech, and Signal Processing</em> , Vol. 2, pp. 533–536.</p><h6 id="_321"><a class="header-anchor" href="#_321" aria-hidden="true">#</a> 321</h6><p>Gannaz, I. (2007), Robust estimation and wavelet thresholding in partially linear models, <em>Statistics and Computing</em> <strong>17</strong> (4), 293–310.</p><p>Gao, H. and Bruce, A. (1997), Waveshrink with firm shrinkage, <em>Statistica Sinica</em> <strong>7</strong> , 855–874.</p><p>Geman, S. and Geman, D. (1984), Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images, <em>IEEE Transactions on Pattern Analysis and Machine Intel ligence</em> <strong>6</strong> , 721–741.</p><p>Golub, G. and Loan, C. V. (1996), <em>Matrix Computations</em> , Johns Hopkins Uni- versity Press, Baltimore.</p><p>Gorski, J., Pfeuffer, F. and Klamroth, K. (2007), Biconvex sets and opti- mization with biconvex functions: a survey and extensions, <em>Mathematical Methods of Operations Research</em> <strong>66</strong> (3), 373–407.</p><p>Gramacy, R. (2011), ‘The monomvn package: Estimation for multivariate nor- mal and student-t data with monotone missingness’, CRAN. R package version 1.8.</p><p>Grazier G’Sell, M., Taylor, J. and Tibshirani, R. (2013), Adaptive testing for the graphical Lasso. arXiv: 1307.4765.</p><p>Grazier G’Sell, M., Wager, S., Chouldechova, A. and Tibshirani, R. (2015), Sequential selection procedures and false discovery rate control. arXiv: 1309.5352: To appear, Journal of the Royal Statistical Society Series B.</p><p>Greenshtein, E. and Ritov, Y. (2004), Persistency in high dimensional lin- ear predictor-selection and the virtue of over-parametrization, <em>Bernoul li</em><strong>10</strong> , 971–988.</p><p>Greig, D. M., Porteous, B. T. and Seheuly, A. H. (1989), Exact maximum a posteriori estimation for binary images, <em>Journal of the Royal Statistical Society Series B</em> <strong>51</strong> , 271–279.</p><p>Grimmett, G. R. (1973), A theorem about random fields, <em>Bul letin of the Lon- don Mathematical Society</em> <strong>5</strong> , 81–84.</p><p>Gross, D. (2011), Recovering low-rank matrices from few coefficients in any basis, <em>IEEE Transactions on Information Theory</em> <strong>57</strong> (3), 1548–1566.</p><p>Gu, C. (2002), <em>Smoothing Spline ANOVA Models</em> , Springer Series in Statistics, Springer, New York, NY.</p><p>Hammersley, J. M. and Clifford, P. (1971), Markov fields on finite graphs and lattices. Unpublished.</p><p>Hardoon, D. and Shawe-Taylor, J. (2011), Sparse canonical correlation anal- ysis, <em>Machine Learning</em> <strong>83</strong> (3), 331–353.</p><p>Hastie, T., Buja, A. and Tibshirani, R. (1995), Penalized discriminant analy- sis, <em>Annals of Statistics</em> <strong>23</strong> , 73–102.</p><p>Hastie, T. and Mazumder, R. (2013), <em>softImpute: matrix completion via iter- ative soft-thresholded SVD</em>. R package version 1.0. <strong>URL:</strong> <em><a href="http://CRAN.R-project.org/package=softImpute" target="_blank" rel="noopener noreferrer">http://CRAN.R-project.org/package=softImpute</a></em></p><h6 id="_322-bibliography"><a class="header-anchor" href="#_322-bibliography" aria-hidden="true">#</a> 322 BIBLIOGRAPHY</h6><p>Hastie, T. and Tibshirani, R. (1990), <em>Generalized Additive Models</em> , Chapman &amp; Hall, London.</p><p>Hastie, T. and Tibshirani, R. (2004), Efficient quadratic regularization for expression arrays, <em>Biostatistics,</em> <strong>5</strong> , 329–340.</p><p>Hastie, T., Tibshirani, R. and Buja, A. (1994), Flexible discriminant analy- sis by optimal scoring, <em>Journal of the American Statistical Association</em><strong>89</strong> , 1255–1270.</p><p>Hastie, T., Tibshirani, R. and Friedman, J. (2009), <em>The Elements of Statistical Learning: Data Mining, Inference and Prediction</em> , second edn, Springer Verlag, New York.</p><p>Hastie, T., Tibshirani, R., Narasimhan, B. and Chu, G. (2003), <em>pamr: Predic- tion Analysis for Microarrays in R</em>. R package version 1.54.1. <strong>URL:</strong> <em><a href="http://CRAN.R-project.org/package=pamr" target="_blank" rel="noopener noreferrer">http://CRAN.R-project.org/package=pamr</a></em></p><p>Hocking, T., Vert, J.-P., Bach, F. and Joulin, A. (2011), Clusterpath: an al- gorithm for clustering using convex fusion penalties., in L. Getoor and T. Scheffer, eds, <em>Proceedings of the Twenty-Eighth International Confer- ence on Machine Learning (ICML)</em> , Omnipress, pp. 745–752.</p><p>Hoefling, H. (2010), A path algorithm for the fused Lasso signal approximator, <em>Journal of Computational and Graphical Statistics</em> <strong>19</strong> (4), 984–1006.</p><p>Hoefling, H. and Tibshirani, R. (2009), Estimation of sparse binary pairwise Markov networks using pseudo-likelihoods, <em>Journal of Machine Learning Research</em> <strong>19</strong> , 883–906.</p><p>Horn, R. A. and Johnson, C. R. (1985), <em>Matrix Analysis</em> , Cambridge University Press, Cambridge.</p><p>Hsu, D., Kakade, S. M. and Zhang, T. (2011), Robust matrix decomposi- tion with sparse corruptions, <em>IEEE Transactions on Information Theory</em><strong>57</strong> (11), 7221–7234.</p><p>Huang, J., Ma, S. and Zhang, C.-H. (2008), Adaptive Lasso for sparse high- dimensional regression models, <em>Statistica Sinica</em> <strong>18</strong> , 1603–1618.</p><p>Huang, J. and Zhang, T. (2010), The benefit of group sparsity, <em>The Annals of Statistics</em> <strong>38</strong> (4), 1978–2004.</p><p>Hunter, D. R. and Lange, K. (2004), A tutorial on MM algorithms, <em>The Amer- ican Statistician</em> <strong>58</strong> (1), 30–37.</p><p>Ising, E. (1925), Beitrag zur theorie der ferromagnetismus, <em>Zeitschrift f ̈ur Physik</em> <strong>31</strong> (1), 253–258.</p><p>Jacob, L., Obozinski, G. and Vert, J.-P. (2009), Group Lasso with overlap and graph Lasso, in <em>Proceeding of the 26th International Conference on Machine Learning, Montreal, Canada</em>.</p><p>Jalali, A., Ravikumar, P., Sanghavi, S. and Ruan, C. (2010), A dirty model for multi-task learning, in <em>Advances in Neural Information Processing Systems 23</em> , pp. 964–972.</p><h6 id="_323"><a class="header-anchor" href="#_323" aria-hidden="true">#</a> 323</h6><p>Javanmard, A. and Montanari, A. (2013), Hypothesis testing in high- dimensional regression under the Gaussian random design model: Asymp- totic theory. arXiv: 1301.4240.</p><p>Javanmard, A. and Montanari, A. (2014), Confidence intervals and hypothe- sis testing for high-dimensional regression, <em>Journal of Machine Learning Research</em> <strong>15</strong> , 2869–2909.</p><p>Jerrum, M. and Sinclair, A. (1993), Polynomial-time approximation algo- rithms for the Ising model, <em>SIAM Journal of Computing</em> <strong>22</strong> , 1087–1116.</p><p>Jerrum, M. and Sinclair, A. (1996), The Markov chain Monte Carlo method: An approach to approximate counting and integration, in D. Hochbaum, ed., <em>Approximation algorithms for NP-hard problems</em> , PWS Publishing, Boston.</p><p>Johnson, N. (2013), A dynamic programming algorithm for the fused Lasso and <em>`</em> 0 -segmentation, <em>Journal of Computational and Graphical Statistics</em><strong>22</strong> (2), 246–260.</p><p>Johnson, W. B. and Lindenstrauss, J. (1984), Extensions of Lipschitz map- pings into a Hilbert space, <em>Contemporary Mathematics</em> <strong>26</strong> , 189–206.</p><p>Johnstone, I. (2001), On the distribution of the largest eigenvalue in principal components analysis, <em>Annals of Statistics</em> <strong>29</strong> (2), 295–327.</p><p>Johnstone, I. and Lu, A. (2009), On consistency and sparsity for principal components analysis in high dimensions, <em>Journal of the American Statis- tical Association</em> <strong>104</strong> , 682–693.</p><p>Jolliffe, I. T., Trendafilov, N. T. and Uddin, M. (2003), A modified principal component technique based on the Lasso, <em>Journal of Computational and Graphical Statistics</em> <strong>12</strong> , 531–547.</p><p>Kaiser, H. (1958), The varimax criterion for analytic rotation in factor anal- ysis, <em>Psychometrika</em> <strong>23</strong> , 187–200.</p><p>Kalisch, M. and B ̈uhlmann, P. (2007), Estimating high-dimensional directed acyclic graphs with the PC algorithm, <em>Journal of Machine Learning Re- search</em> <strong>8</strong> , 613–636.</p><p>Kastelyn, P. W. (1963), Dimer statistics and phase transitions, <em>Journal of Mathematical Physics</em> <strong>4</strong> , 287–293.</p><p>Keshavan, R. H., Montanari, A. and Oh, S. (2010), Matrix completion from noisy entries, <em>Journal of Machine Learning Research</em> <strong>11</strong> , 2057–2078.</p><p>Keshavan, R. H., Oh, S. and Montanari, A. (2009), Matrix completion from a few entries, <em>IEEE Transactions on Information Theory</em> <strong>56(6)</strong> , 2980– 2998.</p><p>Kim, S., Koh, K., Boyd, S. and Gorinevsky, D. (2009), L1 trend filtering, <em>SIAM Review, problems and techniques section</em> <strong>51</strong> (2), 339–360.</p><p>Knight, K. and Fu, W. J. (2000), Asymptotics for Lasso-type estimators, <em>An- nals of Statistics</em> <strong>28</strong> , 1356–1378.</p><h6 id="_324-bibliography"><a class="header-anchor" href="#_324-bibliography" aria-hidden="true">#</a> 324 BIBLIOGRAPHY</h6><p>Koh, K., Kim, S. and Boyd, S. (2007), An interior-point method for large-scale <em>`</em> 1 -regularized logistic regression, <em>Journal of Machine Learning Research</em><strong>8</strong> , 1519–1555.</p><p>Koller, D. and Friedman, N. (2009), <em>Probabilistic Graphical Models</em> , The MIT Press, Cambridge MA.</p><p>Koltchinskii, V., Lounici, K. and Tsybakov, A. (2011), Nuclear-norm penal- ization and optimal rates for noisy low-rank matrix completion, <em>Annals of Statistics</em> <strong>39</strong> , 2302–2329.</p><p>Koltchinskii, V. and Yuan, M. (2008), Sparse recovery in large ensembles of kernel machines, in <em>Proceedings of the Annual Conference on Learning Theory (COLT)</em>.</p><p>Koltchinskii, V. and Yuan, M. (2010), Sparsity in multiple kernel learning, <em>Annals of Statistics</em> <strong>38</strong> , 3660–3695.</p><p>Krahmer, F. and Ward, R. (2011), New and improved Johnson-Lindenstrauss embeddings via the restricted isometry property, <em>SIAM Journal on Math- ematical Analysis</em> <strong>43</strong> (3), 1269–1281.</p><p>Lang, K. (1995), Newsweeder: Learning to filter netnews., in <em>Proceedings of the Twelfth International Conference on Machine Learning (ICML)</em> , pp. 331– 339.</p><p>Lange, K. (2004), <em>Optimization</em> , Springer, New York.</p><p>Lange, K., Hunter, D. R. and Yang, I. (2000), Optimization transfer using sur- rogate objective functions (with discussion), <em>Computational and Graphi- cal Statistics</em> <strong>9</strong> , 1–59.</p><p>Laurent, M. (2001), Matrix completion problems, in <em>The Encyclopedia of Op- timization</em> , Kluwer Academic, pp. 221–229.</p><p>Lauritzen, S. L. (1996), <em>Graphical Models</em> , Oxford University Press.</p><p>Lauritzen, S. L. and Spiegelhalter, D. J. (1988), Local computations with probabilities on graphical structures and their application to expert sys- tems (with discussion), <em>Journal of the Royal Statistical Society Series B</em><strong>50</strong> , 155–224.</p><p>Le Cun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W. and Jackel, L. (1990), Handwritten digit recognition with a back- propogation network, in D. Touretzky, ed., <em>Advances in Neural Informa- tion Processing Systems</em> , Vol. 2, Morgan Kaufman, Denver, CO, pp. 386– 404.</p><p>Le, Q., Ranzato, M., Monga, R., Devin, M., Chen, K., Corrado, G., Dean, J. and Ng, A. (2012), Building high-level features using large scale unsu- pervised learning, in <em>Proceedings of the 29th International Conference on Machine Learning</em> , Edinburgh, Scotland.</p><p>Lee, J. and Hastie, T. (2014), Learning the structure of mixed graphical mod- els, <em>Journal of Computational and Graphical Statistics</em>. advanced online</p><h6 id="_325"><a class="header-anchor" href="#_325" aria-hidden="true">#</a> 325</h6><div class="language-"><pre><code>access.\n</code></pre></div><p>Lee, J., Sun, D., Sun, Y. and Taylor, J. (2016), Exact post-selection inference, with application to the Lasso, <em>Annals of Statistics</em> <strong>44</strong> (4), 907–927.</p><p>Lee, J., Sun, Y. and Saunders, M. (2014), Proximal newton-type meth- ods for minimizing composite functions, <em>SIAM Journal on Optimization</em><strong>24</strong> (3), 1420–1443.</p><p>Lee, J., Sun, Y. and Taylor, J. (2013), On model selection consistency of <em>m</em> - estimators with geometrically decomposable penalties, Technical report, Stanford University. arxiv1305.7477v4.</p><p>Lee, M., Shen, H., Huang, J. and Marron, J. (2010), Biclustering via sparse singular value decomposition, <em>Biometrics</em> pp. 1086–1095.</p><p>Lee, S., Lee, H., Abneel, P. and Ng, A. (2006), Efficient L1 logistic regression, in <em>Proceedings of the Twenty-First National Conference on Artificial In- tel ligence (AAAI-06)</em>.</p><p>Lei, J. and Vu, V. Q. (2015), Sparsistency and Agnostic Inference in Sparse PCA, <em>Ann. Statist.</em> <strong>43</strong> (1), 299–322.</p><p>Leng, C. (2008), Sparse optimal scoring for multiclass cancer diagnosis and biomarker detection using microarray data, <em>Computational Biology and Chemistry</em> <strong>32</strong> , 417–425.</p><p>Li, L., Huang, W., Gu, I. Y. and Tian, Q. (2004), Statistical modeling of complex backgrounds for foreground object detection, <em>IEEE Transactions on Image Processing</em> <strong>13</strong> (11), 1459–1472.</p><p>Lim, M. and Hastie, T. (2014), Learning interactions via hierarchical group- Lasso regularization, <em>Journal of Computational and Graphical Statistics (online access)</em>.</p><p>Lin, Y. and Zhang, H. H. (2003), Component selection and smoothing in smoothing spline analysis of variance models, Technical report, Depart- ment of Statistics, University of Wisconsin, Madison.</p><p>Lockhart, R., Taylor, J., Tibshirani 2 , R. and Tibshirani, R. (2014), A signifi- cance test for the Lasso, <em>Annals of Statistics (with discussion)</em> <strong>42</strong> (2), 413– 468.</p><p>Loftus, J. and Taylor, J. (2014), A significance test for forward stepwise model selection. arXiv:1405.3920.</p><p>Lounici, K., Pontil, M., Tsybakov, A. and van de Geer, S. (2009), Taking ad- vantage of sparsity in multi-task learning, Technical report, ETH Zurich.</p><p>Lustig, M., Donoho, D., Santos, J. and Pauly, J. (2008), Compressed sensing MRI, <em>IEEE Signal Processing Magazine</em> <strong>27</strong> , 72–82.</p><p>Lykou, A. and Whittaker, J. (2010), Sparse CCA using a Lasso with positiv- ity constraints, <em>Computational Statistics &amp; Data Analysis</em> <strong>54</strong> (12), 3144– 3157.</p><p>Ma, S., Xue, L. and Zou, H. (2013), Alternating direction methods for la-</p><h6 id="_326-bibliography"><a class="header-anchor" href="#_326-bibliography" aria-hidden="true">#</a> 326 BIBLIOGRAPHY</h6><div class="language-"><pre><code>tent variable Gaussian graphical model selection, Neural Computation\n25 , 2172–2198.\n</code></pre></div><p>Ma, Z. (2010), Contributions to high-dimensional principal component anal- ysis, PhD thesis, Department of Statistics, Stanford University.</p><p>Ma, Z. (2013), Sparse principal component analysis and iterative thresholding, <em>Annals of Statistics</em> <strong>41</strong> (2), 772–801.</p><p>Mahoney, M. W. (2011), Randomized algorithms for matrices and data, <em>Foun- dations and Trends in Machine Learning in Machine Learning</em> <strong>3</strong> (2).</p><p>Mangasarian, O. (1999), Arbitrary-norm separating plane., <em>Operations Re- search Letters</em> <strong>24</strong> (1-2), 15–23.</p><p>Mazumder, R., Friedman, J. and Hastie, T. (2011), <em>Sparsenet</em> : Coordinate descent with non-convex penalties, <em>Journal of the American Statistical Association</em> <strong>106</strong> (495), 1125–1138.</p><p>Mazumder, R. and Hastie, T. (2012), The Graphical Lasso: New insights and alternatives, <em>Electronic Journal of Statistics</em> <strong>6</strong> , 2125–2149.</p><p>Mazumder, R., Hastie, T. and Friedman, J. (2012), <em>sparsenet: Fit sparse linear regression models via nonconvex optimization</em>. R package version 1.0. <strong>URL:</strong> <em><a href="http://CRAN.R-project.org/package=sparsenet" target="_blank" rel="noopener noreferrer">http://CRAN.R-project.org/package=sparsenet</a></em></p><p>Mazumder, R., Hastie, T. and Tibshirani, R. (2010), Spectral regularization algorithms for learning large incomplete matrices, <em>Journal of Machine Learning Research</em> <strong>11</strong> , 2287–2322.</p><p>McCullagh, P. and Nelder, J. (1989), <em>Generalized Linear Models</em> , Chapman &amp; Hall, London.</p><p>Meier, L., van de Geer, S. and B ̈uhlmann, P. (2008), The group Lasso for logistic regression, <em>Journal of the Royal Statistical Society B</em> <strong>70</strong> (1), 53– 71.</p><p>Meier, L., van de Geer, S. and B ̈uhlmann, P. (2009), High-dimensional additive modeling, <em>Annals of Statistics</em> <strong>37</strong> , 3779–3821.</p><p>Meinshausen, N. (2007), Relaxed Lasso, <em>Computational Statistics and Data Analysis</em> pp. 374–393.</p><p>Meinshausen, N. and B ̈uhlmann, P. (2006), High-dimensional graphs and vari- able selection with the Lasso, <em>Annals of Statistics</em> <strong>34</strong> , 1436–1462.</p><p>Meinshausen, N. and B ̈uhlmann, P. (2010), Stability selection, <em>Journal of the Royal Statistical Society Series B</em> <strong>72</strong> (4), 417–473.</p><p>M ́ezard, M. and Montanari, A. (2008), <em>Information, Physics and Computa- tion</em> , Oxford University Press, New York, NY.</p><p>Negahban, S., Ravikumar, P., Wainwright, M. J. and Yu, B. (2012), A unified framework for high-dimensional analysis of <em>M</em> -estimators with decom- posable regularizers, <em>Statistical Science</em> <strong>27</strong> (4), 538–557.</p><p>Negahban, S. and Wainwright, M. J. (2011 <em>a</em> ), Estimation of (near) low-rank matrices with noise and high-dimensional scaling, <em>Annals of Statistics</em></p><h6 id="_327"><a class="header-anchor" href="#_327" aria-hidden="true">#</a> 327</h6><h6 id="_39-2-1069–1097"><a class="header-anchor" href="#_39-2-1069–1097" aria-hidden="true">#</a> 39 (2), 1069–1097.</h6><p>Negahban, S. and Wainwright, M. J. (2011 <em>b</em> ), Simultaneous support recovery in high-dimensional regression: Benefits and perils of <em>`</em> 1 <em>,</em> ∞-regularization, <em>IEEE Transactions on Information Theory</em> <strong>57</strong> (6), 3481–3863.</p><p>Negahban, S. and Wainwright, M. J. (2012), Restricted strong convexity and (weighted) matrix completion: Optimal bounds with noise, <em>Journal of Machine Learning Research</em> <strong>13</strong> , 1665–1697.</p><p>Nelder, J. and Wedderburn, R. (1972), Generalized linear models, <em>J. Royal Statist. Soc. B.</em> <strong>135</strong> (3), 370–384.</p><p>Nemirovski, A. and Yudin, D. B. (1983), <em>Problem Complexity and Method Efficiency in Optimization</em> , John Wiley and Sons, New York.</p><p>Nesterov, Y. (2004), <em>Introductory Lectures on Convex Optimization</em> , Kluwer Academic Publishers, New York.</p><p>Nesterov, Y. (2007), Gradient methods for minimizing composite objective function, Technical Report 76, Center for Operations Research and Econometrics (CORE), Catholic University of Louvain (UCL).</p><p>Netrapalli, P., Jain, P. and Sanghavi, S. (2013), Phase retrieval using alternat- ing minimization, in <em>Advances in Neural Information Processing Systems (NIPS Conference Proceedings)</em> , pp. 2796–2804.</p><p>Obozinski, G., Wainwright, M. J. and Jordan, M. I. (2011), Union support recovery in high-dimensional multivariate regression, <em>Annals of Statistics</em><strong>39</strong> (1), 1–47.</p><p>Oldenburg, D. W., Scheuer, T. and Levy, S. (1983), Recovery of the acoustic impedance from reflection seismograms, <em>Geophysics</em> <strong>48</strong> (10), 1318–1337.</p><p>Olsen, S. (2002), ‘Amazon blushes over sex link gaffe’, CNET News. <a href="http://news.cnet.com/2100-1023-976435.html." target="_blank" rel="noopener noreferrer">http://news.cnet.com/2100-1023-976435.html.</a></p><p>Olshausen, B. and Field, D. (1996), Emergence of simple-cell receptive field properties by learning a sparse code for natural images, <em>Nature</em> <strong>381</strong>.</p><p>Park, T. and Casella, G. (2008), The Bayesian Lasso, <em>Journal of the American Statistical Association</em> <strong>103</strong> (482), 681–686.</p><p>Parkhomenko, E., Tritchler, D. and Beyene, J. (2009), Sparse canonical cor- relation analysis with application to genomic data integration, <em>Statistical Applications in Genetics and Molecular Biology</em> <strong>8</strong> , 1–34.</p><p>Paul, D. and Johnstone, I. (2008), Augmented sparse principal component analysis for high-dimensional data, Technical report, UC Davis.</p><p>Pearl, J. (2000), <em>Causality: Models, Reasoning and Inference</em> , Cambridge Uni- versity Press.</p><p>Pelckmans, K., De Moor, B. and Suykens, J. (2005), Convex clustering shrink- age, in <em>Workshop on Statistics and Optimization of Clustering (PAS- CAL), London, UK</em>.</p><p>Pilanci, M. and Wainwright, M. J. (2014), Randomized sketches of convex</p><h6 id="_328-bibliography"><a class="header-anchor" href="#_328-bibliography" aria-hidden="true">#</a> 328 BIBLIOGRAPHY</h6><div class="language-"><pre><code>programs with sharp guarantees, Technical report, UC Berkeley. Full\nlength version at arXiv:1404.7203; Presented in part at ISIT 2014.\n</code></pre></div><p>Puig, A., Wiesel, A. and Hero, A. (2009), A multidimensional shrinkage thresholding operator, in <em>Proceedings of the 15th workshop on Statistical Signal Processing, SSP’09</em> , IEEE, pp. 113–116.</p><p>Raskutti, G., Wainwright, M. J. and Yu, B. (2009), Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness, in <em>Advances in Neural Information Processing Systems 22</em> , MIT Press, Cambridge MA., pp. 1563–1570.</p><p>Raskutti, G., Wainwright, M. J. and Yu, B. (2010), Restricted eigenvalue conditions for correlated Gaussian designs, <em>Journal of Machine Learning Research</em> <strong>11</strong> , 2241–2259.</p><p>Raskutti, G., Wainwright, M. J. and Yu, B. (2011), Minimax rates of estima- tion for high-dimensional linear regression over <em>`q</em> -balls, <em>IEEE Transac- tions on Information Theory</em> <strong>57</strong> (10), 6976–6994.</p><p>Raskutti, G., Wainwright, M. J. and Yu, B. (2012), Minimax-optimal rates for sparse additive models over kernel classes via convex programming, <em>Journal of Machine Learning Research</em> <strong>12</strong> , 389–427.</p><p>Ravikumar, P., Liu, H., Lafferty, J. and Wasserman, L. (2009), Sparse additive models, <em>Journal of the Royal Statistical Society Series B.</em> <strong>71</strong> (5), 1009– 1030.</p><p>Ravikumar, P., Wainwright, M. J. and Lafferty, J. (2010), High-dimensional ising model selection using <em>`</em> 1 -regularized logistic regression, <em>Annals of Statistics</em> <strong>38</strong> (3), 1287–1319.</p><p>Ravikumar, P., Wainwright, M. J., Raskutti, G. and Yu, B. (2011), High-dimensional covariance estimation by minimizing <em>`</em> 1 -penalized log- determinant divergence, <em>Electronic Journal of Statistics</em> <strong>5</strong> , 935–980.</p><p>Recht, B. (2011), A simpler approach to matrix completion, <em>Journal of Ma- chine Learning Research</em> <strong>12</strong> , 3413–3430.</p><p>Recht, B., Fazel, M. and Parrilo, P. A. (2010), Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization, <em>SIAM Review</em> <strong>52</strong> (3), 471–501.</p><p>Rennie, J. and Srebro, N. (2005), Fast maximum margin matrix factoriza- tion for collaborative prediction, in <em>Proceedings of the 22nd International Conference on Machine Learning</em> , Association for Computing Machinery, pp. 713–719.</p><p>Rish, I. and Grabarnik, G. (2014), <em>Sparse Modeling: Theory, Algorithms, and Applications</em> , Chapman and Hall/CRC.</p><p>Rockafellar, R. T. (1996), <em>Convex Analysis</em> , Princeton University Press.</p><p>Rohde, A. and Tsybakov, A. (2011), Estimation of high-dimensional low-rank matrices, <em>Annals of Statistics</em> <strong>39</strong> (2), 887–930.</p><h6 id="_329"><a class="header-anchor" href="#_329" aria-hidden="true">#</a> 329</h6><p>Rosset, S. and Zhu, J. (2007), Adaptable, efficient and robust methods for regression and classification via piecewise linear regularized coefficient paths, <em>Annals of Statistics</em> <strong>35</strong> (3).</p><p>Rosset, S., Zhu, J. and Hastie, T. (2004), Boosting as a regularized path to a maximum margin classifier, <em>Journal of Machine Learning Research</em><strong>5</strong> , 941–973.</p><p>Rothman, A. J., Bickel, P. J., Levina, E. and Zhu, J. (2008), Sparse permu- tation invariant covariance estimation, <em>Electronic Journal of Statistics</em><strong>2</strong> , 494–515.</p><p>Rubin, D. (1981), The Bayesian Bootstrap, <em>Annals of Statistics</em> <strong>9</strong> , 130–134.</p><p>Rudelson, M. and Zhou, S. (2013), Reconstruction from anisotropic random measurements, <em>IEEE Transactions on Information Theory</em> <strong>59</strong> (6), 3434– 3447.</p><p>Ruderman, D. (1994), The statistics of natural images, <em>Network: Computation in Neural Systems</em> <strong>5</strong> , 517–548.</p><p>Santhanam, N. P. and Wainwright, M. J. (2008), Information-theoretic lim- its of high-dimensional model selection, in <em>International Symposium on Information Theory</em> , Toronto, Canada.</p><p>Santosa, F. and Symes, W. W. (1986), Linear inversion of band-limited reflec- tion seismograms, <em>SIAM Journal of Scientific and Statistical Computing</em><strong>7</strong> (4), 1307–1330.</p><p>Scheff ́e, H. (1953), A method for judging all contrasts in the analysis of vari- ance, <em>Biometrika</em> <strong>40</strong> , 87–104.</p><p>Schmidt, M., Niculescu-Mizil, A. and Murphy, K. (2007), Learning graphical model structure using l1-regularization paths, in <em>AAAI proceedings</em>. <strong>URL:</strong> <em><a href="http://www.cs.ubc.ca/" target="_blank" rel="noopener noreferrer">http://www.cs.ubc.ca/</a> murphyk/Papers/aaai07.pdf</em></p><p>Shalev-Shwartz, S., Singer, Y. and Srebro, N. (2007), Pegasos: Primal esti- mated sub-gradient solver for SVM, in <em>Proceedings of the 24th interna- tional conference on Machine learning</em> , pp. 807–814.</p><p>She, Y. and Owen, A. B. (2011), Outlier detection using nonconvex pe- nalized regression, <em>Journal of the American Statistical Association</em><strong>106</strong> (494), 626–639.</p><p>Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2011), Regulariza- tion paths for Cox’s proportional hazards model via coordinate descent, <em>Journal of Statistical Software</em> <strong>39</strong> (5), 1–13.</p><p>Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2013), A sparse-group Lasso, <em>Journal of Computational and Graphical Statistics</em> <strong>22</strong> (2), 231–245.</p><p>Simon, N. and Tibshirani, R. (2012), Standardization and the group Lasso penalty, <em>Statistica Sinica</em> <strong>22</strong> , 983–1001.</p><p>Simoncelli, E. P. (2005), Statistical modeling of photographic images, in <em>Handbook of Video and Image Processing, 2nd Edition</em> , Academic Press,</p><h6 id="_330-bibliography"><a class="header-anchor" href="#_330-bibliography" aria-hidden="true">#</a> 330 BIBLIOGRAPHY</h6><div class="language-"><pre><code>Waltham MA, pp. 431–441.\n</code></pre></div><p>Simoncelli, E. P. and Freeman, W. T. (1995), The steerable pyramid: A flexible architecture for multi-scale derivative computation, in <em>Int’l Conference on Image Processing</em> , Vol. III, IEEE Sig Proc Soc., Washington, DC, pp. 444–447.</p><p>Singer, Y. and Dubiner, M. (2011), Entire relaxation path for maximum en- tropy models, in <em>Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNPL 2011)</em> , pp. 941–948.</p><p>Srebro, N., Alon, N. and Jaakkola, T. (2005), Generalization error bounds for collaborative prediction with low-rank matrices, <em>Advances in Neural Information Processing Systems</em>.</p><p>Srebro, N. and Jaakkola, T. (2003), Weighted low-rank approximations, in <em>Twentieth International Conference on Machine Learning</em> , AAAI Press, pp. 720–727.</p><p>Srebro, N., Rennie, J. and Jaakkola, T. (2005), Maximum margin matrix fac- torization, <em>Advances in Neural Information Processing Systems</em> <strong>17</strong> , 1329– 1336.</p><p>Stein, C. (1981), Estimation of the mean of a multivariate normal distribution, <em>Annals of Statistics</em> <strong>9</strong> , 1131–1151.</p><p>Stone, C. J. (1985), Additive regression and other non-parametric models, <em>Annals of Statistics</em> <strong>13</strong> (2), 689–705.</p><p>Taylor, J., Lockhart, R., Tibshirani 2 , R. and Tibshirani, R. (2014), Post- selection adaptive inference for least angle regression and the Lasso. arXiv: 1401.3889; submitted.</p><p>Taylor, J., Loftus, J. and Tibshirani 2 , R. (2016), Inference in adaptive regres- sion via the Kac-Rice formula, <em>Annals of Statistics</em> <strong>44</strong> (2), 743–770.</p><p>Thodberg, H. H. and Olafsdottir, H. (2003), Adding curvature to minimum description length shape models, in <em>British Machine Vision Conference (BMVC)</em> , pp. 251–260.</p><p>Thomas, G. S. (1990), <em>The Rating Guide to Life in America’s Smal l Cities</em> , Prometheus books. <a href="http://college.cengage.com/mathematics/" target="_blank" rel="noopener noreferrer">http://college.cengage.com/mathematics/</a> brase/understandable_statistics/7e/students/datasets/mlr/ frames/frame.html.</p><p>Tibshirani, R. (1996), Regression shrinkage and selection via the Lasso, <em>Jour- nal of the Royal Statistical Society, Series B</em> <strong>58</strong> , 267–288.</p><p>Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon, N., Taylor, J. and Tibshirani 2 , R. (2012), Strong rules for discarding predictors in Lasso- type problems, <em>Journal of the Royal Statistical Society Series B.</em> pp. 245– 266.</p><p>Tibshirani, R. and Efron, B. (2002), Pre-validation and inference in microar- rays, <em>Statistical Applications in Genetics and Molecular Biology</em> pp. 1–15.</p><h6 id="_331"><a class="header-anchor" href="#_331" aria-hidden="true">#</a> 331</h6><p>Tibshirani, R., Hastie, T., Narasimhan, B. and Chu, G. (2001), Diagnosis of multiple cancer types by shrunken centroids of gene expression, <em>Proceed- ings of the National Academy of Sciences</em> <strong>99</strong> , 6567–6572.</p><p>Tibshirani, R., Hastie, T., Narasimhan, B. and Chu, G. (2003), Class predic- tion by nearest shrunken centroids, with applications to DNA microar- rays, <em>Statistical Science</em> pp. 104–117.</p><p>Tibshirani, R., Saunders, M., Rosset, S., Zhu, J. and Knight, K. (2005), Spar- sity and smoothness via the fused Lasso, <em>Journal of the Royal Statistical Society, Series B</em> <strong>67</strong> , 91–108.</p><p>Tibshirani 2 , R. (2013), The Lasso problem and uniqueness, <em>Electronic Journal of Statistics</em> <strong>7</strong> , 1456–1490.</p><p>Tibshirani 2 , R. (2014), Adaptive piecewise polynomial estimation via trend filtering, <em>Annals of Statistics</em> <strong>42</strong> (1), 285–323.</p><p>Tibshirani 2 , R., Hoefling, H. and Tibshirani, R. (2011), Nearly-isotonic re- gression, <em>Technometrics</em> <strong>53</strong> (1), 54–61.</p><p>Tibshirani 2 , R. and Taylor, J. (2011), The solution path of the generalized Lasso, <em>Annals of Statistics</em> <strong>39</strong> (3), 1335–1371.</p><p>Tibshirani 2 , R. and Taylor, J. (2012), Degrees of freedom in Lasso problems, <em>Annals of Statistics</em> <strong>40</strong> (2), 1198–1232.</p><p>Trendafilov, N. T. and Jolliffe, I. T. (2007), DALASS: Variable selection in discriminant analysis via the LASSO, <em>Computational Statistics and Data Analysis</em> <strong>51</strong> , 3718–3736.</p><p>Tropp, J. A. (2006), Just relax: Convex programming methods for identify- ing sparse signals in noise, <em>IEEE Transactions on Information Theory</em><strong>52</strong> (3), 1030–1051.</p><p>Tseng, P. (1988), Coordinate ascent for maximizing nondifferentiable concave functions, Technical Report LIDS-P ; 1840, Massachusetts Institute of Technology. Laboratory for Information and Decision Systems.</p><p>Tseng, P. (1993), Dual coordinate ascent methods for non-strictly convex min- imization, <em>Mathematical Programming</em> <strong>59</strong> , 231–247.</p><p>Tseng, P. (2001), Convergence of block coordinate descent method for nondif- ferentiable maximization, <em>Journal of Optimization Theory and Applica- tions</em> <strong>109</strong> (3), 474–494.</p><p>van de Geer, S. (2000), <em>Empirical Processes in M-Estimation</em> , Cambridge Uni- versity Press.</p><p>van de Geer, S. and B ̈uhlmann, P. (2009), On the conditions used to prove oracle results for the Lasso, <em>Electronic Journal of Statistics</em> <strong>3</strong> , 1360–1392.</p><p>van de Geer, S., B ̈uhlmann, P., Ritov, Y. and Dezeure, R. (2013), On asymp- totically optimal confidence regions and tests for high-dimensional mod- els. arXiv: 1303.0518v2.</p><p>van Houwelingen, H. C., Bruinsma, T., Hart, A. A. M., van’t Veer, L. J. and</p><h6 id="_332-bibliography"><a class="header-anchor" href="#_332-bibliography" aria-hidden="true">#</a> 332 BIBLIOGRAPHY</h6><div class="language-"><pre><code>Wessels, L. F. A. (2006), Cross-validated Cox regression on microarray\ngene expression data, Statistics in Medicine 45 , 3201–3216.\n</code></pre></div><p>Vandenberghe, L., Boyd, S. and Wu, S. (1998), Determinant maximization with linear matrix inequality constraints, <em>SIAM Journal on Matrix Anal- ysis and Applications</em> <strong>19</strong> , 499–533.</p><p>Vapnik, V. (1996), <em>The Nature of Statistical Learning Theory</em> , Springer, New York.</p><p>Vempala, S. (2004), <em>The Random Projection Method</em> , Discrete Mathemat- ics and Theoretical Computer Science, American Mathematical Society, Providence, RI.</p><p>Vershynin, R. (2012), Introduction to the non-asymptotic analysis of random matrices, in <em>Compressed Sensing: Theory and Applications</em> , Cambridge University Press.</p><p>Vu, V. Q., Cho, J., Lei, J. and Rohe, K. (2013), Fantope projection and selection: A near-optimal convex relaxation of sparse PCA, in <em>Advances in Neural Information Processing Systems (NIPS Conference Proceedings)</em> , pp. 2670–2678.</p><p>Vu, V. Q. and Lei, J. (2012), Minimax rates of estimation for sparse PCA in high dimensions, in <em>15th Annual Conference on Artificial Intelligence and Statistics</em> , La Palma, Canary Islands.</p><p>Waaijenborg, S., Vers ́elewel de Witt Hamer, P. and Zwinderman, A. (2008), Quantifying the association between gene expressions and DNA-markers by penalized canonical correlation analysis, <em>Statistical Applications in Genetics and Molecular Biology</em> <strong>7</strong> , Article 3.</p><p>Wahba, G. (1990), <em>Spline Models for Observational Data</em> , SIAM, Philadelphia, PA.</p><p>Wainwright, M. J. (2009), Sharp thresholds for noisy and high-dimensional recovery of sparsity using <em>`</em> 1 -constrained quadratic programming (Lasso), <em>IEEE Transactions on Information Theory</em> pp. 2183–2202.</p><p>Wainwright, M. J. and Jordan, M. I. (2008), Graphical models, exponential families and variational inference, <em>Foundations and Trends in Machine Learning</em> <strong>1</strong> (1–2), 1–305.</p><p>Wainwright, M. J., Simoncelli, E. P. and Willsky, A. S. (2001), Random cas- cades on wavelet trees and their use in modeling and analyzing natural images, <em>Applied Computational and Harmonic Analysis</em> <strong>11</strong> , 89–123.</p><p>Wang, H. (2014), Coordinate descent algorithm for covariance graphical Lasso, <em>Statistics and Computing</em> <strong>24</strong> (4), 521–529.</p><p>Wang, J., Lin, B., Gong, P., Wonka, P. and Ye, J. (2013), Lasso screening rules via dual polytope projection, in <em>Advances in Neural Information Processing Systems (NIPS Conference Proceedings)</em> , pp. 1070–1078.</p><p>Wang, L., Zhu, J. and Zou, H. (2006), The doubly regularized support vector</p><h6 id="_333"><a class="header-anchor" href="#_333" aria-hidden="true">#</a> 333</h6><div class="language-"><pre><code>machine, Statistica Sinica 16 (2), 589.\n</code></pre></div><p>Wang, W., Liang, Y. and Xing, E. P. (2013), Block regularized Lasso for multivariate multiresponse linear regression, in <em>Proceedings of the 16th International Conference on Artifical Intel ligence and Statistics</em> , Scotts- dale, AZ.</p><p>Welsh, D. J. A. (1993), <em>Complexity: Knots, Colourings, and Counting</em> , LMS Lecture Note Series, Cambridge University Press, Cambridge.</p><p>Whittaker, J. (1990), <em>Graphical Models in Applied Multivariate Statistics</em> , Wi- ley, Chichester.</p><p>Winkler, G. (1995), <em>Image Analysis, Random Fields, and Dynamic Monte Carlo methods</em> , Springer-Verlag, New York, NY.</p><p>Witten, D. (2011), <em>penalizedLDA: Penalized classification using Fisher’s linear discriminant</em>. R package version 1.0. <strong>URL:</strong> <em><a href="http://CRAN.R-project.org/package=penalizedLDA" target="_blank" rel="noopener noreferrer">http://CRAN.R-project.org/package=penalizedLDA</a></em></p><p>Witten, D., Friedman, J. and Simon, N. (2011), New insights and faster com- putations for the graphical Lasso, <em>Journal of Computational and Graph- ical Statistics</em> <strong>20</strong> , 892–200.</p><p>Witten, D. and Tibshirani, R. (2009), Extensions of sparse canonical correla- tion analysis, with application to genomic data, <em>Statistical Applications in Genetics and Molecular Biology</em> <strong>8(1)</strong> , Article 28.</p><p>Witten, D. and Tibshirani, R. (2010), A framework for feature selec- tion in clustering, <em>Journal of the American Statistical Association</em><strong>105(490)</strong> , 713–726.</p><p>Witten, D. and Tibshirani, R. (2011), Penalized classification using Fisher’s linear discriminant, <em>Journal of the Royal Statistical Society Series B</em><strong>73</strong> (5), 753–772.</p><p>Witten, D., Tibshirani, R. and Hastie, T. (2009), A penalized matrix decom- position, with applications to sparse principal components and canonical correlation analysis, <em>Biometrika</em> <strong>10</strong> , 515–534.</p><p>Wu, T., Chen, Y. F., Hastie, T., Sobel, E. and Lange, K. (2009), Genomewide association analysis by Lasso penalized logistic regression, <em>Bioinformatics</em><strong>25</strong> (6), 714–721.</p><p>Wu, T. and Lange, K. (2008), Coordinate descent procedures for Lasso penal- ized regression, <em>Annals of Applied Statistics</em> <strong>2</strong> (1), 224–244.</p><p>Wu, T. and Lange, K. (2010), The MM alternative to EM, <em>Statistical Science</em><strong>25</strong> (4), 492–505.</p><p>Xu, H., Caramanis, C. and Mannor, S. (2010), Robust regression and Lasso, <em>IEEE Transactions on Information Theory</em> <strong>56</strong> (7), 3561–3574.</p><p>Xu, H., Caramanis, C. and Sanghavi, S. (2012), Robust PCA via outlier pur- suit, <em>IEEE Transactions on Information Theory</em> <strong>58</strong> (5), 3047–3064.</p><p>Yi, X., Caramanis, C. and Sanghavi, S. (2014), Alternating minimization for</p><h6 id="_334-bibliography"><a class="header-anchor" href="#_334-bibliography" aria-hidden="true">#</a> 334 BIBLIOGRAPHY</h6><div class="language-"><pre><code>mixed linear regression, in Proceedings of The 31st International Confer-\nence on Machine Learning , pp. 613–621.\n</code></pre></div><p>Yuan, M., Ekici, A., Lu, Z. and Monteiro, R. (2007), Dimension reduction and coefficient estimation in multivariate linear regression, <em>Journal of the Royal Statistical Society Series B</em> <strong>69</strong> (3), 329–346.</p><p>Yuan, M. and Lin, Y. (2006), Model selection and estimation in regression with grouped variables, <em>Journal of the Royal Statistical Society, Series B</em><strong>68</strong> (1), 49–67.</p><p>Yuan, M. and Lin, Y. (2007 <em>a</em> ), Model selection and estimation in the Gaussian graphical model, <em>Biometrika</em> <strong>94</strong> (1), 19–35.</p><p>Yuan, M. and Lin, Y. (2007 <em>b</em> ), On the non-negative garrotte estimator, <em>Journal of the Royal Statistical Society, Series B</em> <strong>69</strong> (2), 143–161.</p><p>Yuan, X. T. and Zhang, T. (2013), Truncated power method for sparse eigen- value problems, <em>Journal of Machine Learning Research</em> <strong>14</strong> , 899–925.</p><p>Zhang, C.-H. (2010), Nearly unbiased variable selection under minimax con- cave penalty, <em>Annals of Statistics</em> <strong>38</strong> (2), 894–942.</p><p>Zhang, C.-H. and Zhang, S. (2014), Confidence intervals for low-dimensional parameters with high-dimensional data, <em>Journal of the Royal Statistical Society Series B</em> <strong>76</strong> (1), 217–242.</p><p>Zhang, Y., Wainwright, M. J. and Jordan, M. I. (2014), Lower bounds on the performance of polynomial-time algorithms for sparse linear regression, in <em>Proceedings of the Annual Conference on Learning Theory (COLT)</em> , Barcelona, Spain. Full length version at <a href="http://arxiv.org/abs/1402.1918." target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1402.1918.</a></p><p>Zhao, P., Rocha, G. and Yu, B. (2009), Grouped and hierarchical model selection through composite absolute penalties, <em>Annals of Statistics</em><strong>37</strong> (6A), 3468–3497.</p><p>Zhao, P. and Yu, B. (2006), On model selection consistency of Lasso, <em>Journal of Machine Learning Research</em> <strong>7</strong> , 2541–2567.</p><p>Zhao, Y., Levina, E. and Zhu, J. (2011), Community extraction for social net- works, <em>Proceedings of the National Academy of Sciences</em> <strong>108</strong> (18), 7321– 7326.</p><p>Zhou, S., Lafferty, J. and Wasserman, L. (2008), Time-varying undirected graphs, in <em>Proceedings of the Annual Conference on Learning Theory (COLT)</em> , Helsinki, Finland.</p><p>Zhu, J., Rosset, S., Hastie, T. and Tibshirani, R. (2004), 1-norm support vector machines, in <em>Advances in Neural Information Processing Systems</em> , Vol. 16, pp. 49–56.</p><p>Zou, H. (2006), The adaptive Lasso and its oracle properties, <em>Journal of the American Statistical Association</em> <strong>101</strong> , 1418–1429.</p><p>Zou, H. and Hastie, T. (2005), Regularization and variable selection via the elastic net, <em>Journal of the Royal Statistical Society Series B.</em> <strong>67</strong> (2), 301–</p><h6 id="_335"><a class="header-anchor" href="#_335" aria-hidden="true">#</a> 335</h6><h6 id="_320"><a class="header-anchor" href="#_320" aria-hidden="true">#</a> 320.</h6><p>Zou, H., Hastie, T. and Tibshirani, R. (2006), Sparse principal component analysis, <em>Journal of Computational and Graphical Statistics</em> <strong>15</strong> (2), 265– 286.</p><p>Zou, H., Hastie, T. and Tibshirani, R. (2007), On the degrees of freedom of the Lasso, <em>Annals of Statistics</em> <strong>35</strong> (5), 2173–2192.</p><p>Zou, H. and Li, R. (2008), One-step sparse estimates in nonconcave penalized likelihood models, <em>The Annals of Statistics</em> <strong>36</strong> (4), 1509–1533.</p><h2 id="author-index"><a class="header-anchor" href="#author-index" aria-hidden="true">#</a> Author Index</h2><p>Abneel, P. 116 Agarwal, A. 176, 195 Alizadeh, A. 42 Alliney, S. 23 Alon, N. 195 Amini, A. A. 213, 232 Anandkumar, A. 195 Anderson, T. 261 Antoniadis, A. 27 Armitage, K. 42</p><p>Bach, F. 87, 195, 233 Banerjee, O. 248, 249, 262 Baraniuk, R. G. 285 Barlow, R. E. 83 Bartholomew, D. 83 Beck, A. 108 Bengio, S. 212 Bengio, Y. 212 Benjamini, Y. 162 Bennett, J. 170 Bento, J. 263 Berk, R. 160, 162 Berthet, Q. 232 Bertsekas, D. 131, 133 Besag, J. 261, 263 Beyene, J. 233 Bickel, P. J. 262, 263, 311 Bien, J. 68, 128, 130, 263 Birnbaum, A. 213, 232 Boldrick, J. 42 Boser, B. 37, 52 Botstein, D. 42 Boyd, S. 32, 35, 81, 121, 131, 206, 248, 262, 263</p><div class="language-"><pre><code>Breiman, L. 20, 23, 225\nBremner, J. M. 83\nBrown, L. 160, 162\nBrown, P. 42\nBruce, A. 87\nBruckstein, A. M. 285\nBruinsma, T. 43\nBrunk, H. D. 83\nB ̈uhlmann, P. 142, 250, 263, 311\nBuja, A. 160, 162, 218, 222, 225\nBunea, F. 186, 311\nBurge, C. 60\nButte, A. 263\nByrd, J. 42\n</code></pre></div><div class="language-"><pre><code>Cand`es, E. 311\nCaramanis, C. 26, 195, 312\nCasella, G. 139, 161\nChan, W. 42\nChandrasekaran, V. 195, 260, 261,\n312\nChaudhuri, S. 263\nChen, K. 211, 233\nChen, S. 23, 285\nChen, Y. F. 52\nCheng, J. 263\nChi, E. C. 232\nCho, J. 210\nChoi, Y. 161\nChouldechova, A. 161, 163\nChu, E. 121\nChu, G. 219, 233\nClemmensen, L. 225, 226, 233, 239\nClifford, P. 261\nCohen, A. 285\n</code></pre></div><div class="language-"><pre><code>337\n</code></pre></div><h6 id="_338-author-index"><a class="header-anchor" href="#_338-author-index" aria-hidden="true">#</a> 338 AUTHOR INDEX</h6><p>Corrado, G. 211, 233 Courville, A. 212 Cox, D. 261</p><p>Dahmen, W. 285 d’Aspremont, A. 205, 206, 210, 232, 248, 249, 262 Davenport, M. A. 285 Davidson, K. R. 285 Davis, R. E. 42 De Leeuw, J. 127 De Moor, B. 233 Dean, J. 211, 233 Denker, J. 37 Devin, M. 211, 233 DeVore, R. A. 285 Dezeure, R. 158, 159, 162 Dobra, A. 262 Donoho, D. 23, 278, 285, 296 Drton, M. 263 Dubiner, M. 40 Dudoit, S. 233</p><p>Eckstein, J. 121 Edwards, D. 261 Efron, B. 45, 52, 142, 146, 147, 161 Eisen, M. 42 Ekici, A. 186 El Ghaoui, L. 131, 205, 206, 210, 232, 248, 249, 262 El Karoui, N. 263 Elad, M. 285 Erdos, P. 177 Erhan, D. 212 Ersboll, B. 225, 226, 233, 239</p><p>Fan, J. 84 Fazel, M. 174, 195 Feuer, A. 285 Field, D. 211, 233, 285 Fisher, M. E. 261 Fithian, W. 161 Freeman, W. T. 285 Fridlyand, J. 233 Friedman, J. 24, 36, 37, 43, 46, 48, 50, 52, 58, 77, 79, 86, 87, 113, 128,</p><h6 id="_130-184-217-221-225-227-230"><a class="header-anchor" href="#_130-184-217-221-225-227-230" aria-hidden="true">#</a> 130, 184, 217, 221, 225, 227, 230,</h6><h6 id="_248–251-255-262-263"><a class="header-anchor" href="#_248–251-255-262-263" aria-hidden="true">#</a> 248–251, 255, 262, 263</h6><div class="language-"><pre><code>Friedman, N. 261\nFu, W. 131\nFu, W. J. 311\nFuchs, J. 23, 311\n</code></pre></div><div class="language-"><pre><code>Gannaz, I. 27\nGao, H. 87\nGeman, D. 262\nGeman, S. 262\nGolub, G. 127, 169\nGolub, T. 263\nGong, P. 127, 131, 132\nGorinevsky, D. 81\nGorski, J. 131\nGrabarnik, G. 23\nGramacy, R. 141\nGrazier G’Sell, M. 161, 163\nGreenshtein, E. 20, 311\nGreig, D. M. 262\nGreiner, T. 42\nGreve, M. 42\nGrimmett, G. R. 261\nGross, D. 178, 195\nGu, C. 87\nGu, I. Y. 192\nGuyon, I. 52\n</code></pre></div><div class="language-"><pre><code>Hammersley, J. M. 261\nHans, C. 262\nHart, A. A. M. 43\nHastie, T. 18, 19, 24, 34, 36, 37, 43,\n46–50, 52, 56, 58, 67, 69, 71, 77,\n79, 86, 87, 113, 128, 130, 170, 175,\n176, 182, 184, 195, 206, 208,\n217–219, 221, 222, 225–228, 230,\n232, 233, 239, 248–251, 255,\n260–263, 266\nHenderson, D. 37\nHero, A. 87\nHochberg, Y. 162\nHocking, T. 233\nHoefling, H. 52, 77, 79, 83, 87, 257,\n260, 263\n</code></pre></div><h6 id="_339"><a class="header-anchor" href="#_339" aria-hidden="true">#</a> 339</h6><p>Horn, R. A. 265 Howard, R. 37 Hsu, D. 195 Huang, J. 86, 190 Huang, W. 192 Hubbard, W. 37 Hudsom, J. 42 Hunter, D. R. 124, 131, 233 Huo, X. 285</p><p>Ihaka, R. 225 Ising, E. 244, 262</p><p>Jaakkola, T. 195 Jackel, L. 37 Jacob, L. 66, 67, 87 Jain, P. 195 Jalali, A. 312 Javanmard, A. 158, 159, 162 Jenatton, R. 87 Jerrum, M. 261 Johnson, C. R. 265 Johnson, N. 81, 87 Johnson, W. B. 285 Johnstone, I. 23, 204, 212, 213, 232 Jolliffe, I. T. 204, 232, 233 Jones, B. 262 Jordan, M. I. 86, 205, 206, 210, 232, 261, 311, 312 Joulin, A. 233</p><p>Kaiser, H. 236 Kakade, S. M. 195 Kalisch, M. 263 Karlin, S. 60 Kastelyn, P. W. 261 Keshavan, R. H. 178, 180, 182, 183, 195 Kim, S. 32, 35, 81, 263 Klamroth, K. 131 Knight, K. 87, 311 Koh, K. 32, 35, 81, 263 Kohane, I. 263 Koller, D. 261 Koltchinskii, V. 87, 180 Krahmer, F. 285</p><div class="language-"><pre><code>Lafferty, J. 70, 87, 90, 263\nLanckriet, G. R. G. 205, 206, 210,\n232\nLang, K. 32\nLange, K. 52, 124, 131, 232, 233\nLanning, S. 170\nLaurent, M. 169\nLauritzen, S. L. 261\nLe Cun, Y. 37\nLe, Q. 211, 233\nLee, H. 116\nLee, J. 35, 116, 151, 154, 161, 260,\n263, 312\nLee, M. 190\nLee, S. 116\nLei, J. 210, 213, 232\nLeng, C. 225, 233\nLevina, E. 262, 263\nLevy, R. 42\nLevy, S. 23\nLewis, D. 42\nLi, L. 192\nLi, R. 84, 87\nLi, X. 195\nLiang, Y. 312\nLim, M. 67\nLin, B. 127, 131, 132\nLin, Y. 20, 21, 59, 63, 72–74, 86, 87,\n262\nLindenstrauss, J. 285\nLiu, H. 70, 87, 90\nLoan, C. V. 127, 169\nLockhart, R. 151, 156, 157, 161\nLoftus, J. 161\nLossos, I. 42\nLounici, K. 86, 180\nLu, A. 212, 213, 232\nLu, L. 42\nLu, Z. 186\nLustig, M. 285\nLykou, A. 233\n</code></pre></div><div class="language-"><pre><code>Ma, C. 42\nMa, S. 86, 261, 266\nMa, Y. 195\n</code></pre></div><h6 id="_340-author-index"><a class="header-anchor" href="#_340-author-index" aria-hidden="true">#</a> 340 AUTHOR INDEX</h6><p>Ma, Z. 195, 213, 232 Mahoney, M. W. 285 Mairal, J. 87 Mangasarian, O. 50 Mannor, S. 26 Manzagol, P.-A. 212 Marron, J. 190 Marti, G. 42 Mazumder, R. 86, 87, 170, 175, 176, 182, 195, 249, 251, 261, 262, 266 McCullagh, P. 30, 52 Meier, L. 60, 86, 87 Meinshausen, N. 12, 142, 250, 263, 311 M ́ezard, M. 261 Monga, R. 211, 233 Montanari, A. 158, 159, 162, 178, 180, 182, 183, 195, 261, 263 Monteiro, R. 186 Moore, T. 42 Murphy, K. 262</p><p>Nadler, B. 213, 232 Narasimhan, B. 219, 233 Negahban, S. 86, 176, 179, 186, 195, 311, 312 Nelder, J. 30, 52 Nemirovski, A. 176, 285 Nesterov, Y. 105, 107, 108, 131 Netrapalli, P. 195 Nevins, J. R. 262 Ng, A. 116, 211, 233 Niculescu-Mizil, A. 262</p><p>Obozinski, G. 66, 67, 86, 87, 312 Oh, S. 178, 180, 182, 183, 195 Olafsdottir, H. 226 Oldenburg, D. W. 23 Olsen, S. 193 Olshausen, B. 211, 233 Owen, A. B. 27</p><p>Parikh, N. 121 Park, T. 139, 161 Parkhomenko, E. 233 Parrilo, P. A. 195, 260, 261, 312</p><div class="language-"><pre><code>Paul, D. 213, 232\nPauly, J. 285\nPearl, J. 261\nPelckmans, K. 233\nPeleato, B. 121\nPfeuffer, F. 131\nPhardoon, D. 233\nPilanci, M. 285\nPlan, Y. 195\nPontil, M. 86\nPorteous, B. T. 262\nPuig, A. 87\nPwellm, J. 42\n</code></pre></div><div class="language-"><pre><code>Rabbani, T. 131\nRanzato, M. 211, 233\nRaskutti, G. 75, 87, 252, 262, 311,\n312\nRavikumar, P. 70, 86, 87, 90, 252,\n262, 263, 311, 312\nRecht, B. 178, 195\nRennie, J. 181, 182, 195\nRenyi, A. 177\nRichardson, T. S. 263\nRigollet, P. 232\nRish, I. 23\nRitov, Y. 20, 158, 159, 162, 311\nRocha, G. 86\nRockafellar, R. T. 131\nRohde, A. 186, 195\nRohe, K. 210\nRomberg, J. K. 285\nRosenwal, A. 42\nRosset, S. 19, 47, 49, 87, 121\nRothman, A. J. 262\nRuan, C. 312\nRubin, D. 146, 161\nRudelson, M. 311\nRuderman, D. 285\nRuzinsky, S. 23\n</code></pre></div><div class="language-"><pre><code>Sabet, H. 42\nSanghavi, S. 195, 261, 312\nSanthanam, N. P. 263\nSantos, J. 285\n</code></pre></div><h6 id="_341"><a class="header-anchor" href="#_341" aria-hidden="true">#</a> 341</h6><p>Santosa, F. 23 Saunders, M. 23, 35, 87, 116, 285 Scheff ́e, H. 160 Scheuer, T. 23 Schmidt, M. 262 Seheuly, A. H. 262 Shalev-Shwartz, S. 47 Shawe-Taylor, J. 233 She, Y. 27, 186 Shen, H. 190 Sherlock, G. 42 Simon, N. 37, 43, 50, 58, 63, 86, 87, 128, 130, 251, 262 Simoncelli, E. P. 285 Sinclair, A. 261 Singer, Y. 40, 47 Slonim, D. 263 Sobel, E. 52 Speed, T. 233 Spiegelhalter, D. J. 261 Srebro, N. 47, 181, 182, 195 Stark, P. 23, 285 Staudt, L. 42 Stein, C. 25 Stone, C. J. 87 Sun, D. 151, 154, 161 Sun, Y. 35, 116, 151, 154, 161, 312 Suykens, J. 233 Symes, W. W. 23 Szarek, S. J. 285</p><p>Tamayo, P. 263 Tandon, R. 195 Tanner, J. 296 Tao, T. 278, 285, 311 Taylor, J. 18, 68, 79–81, 87, 128, 130, 151, 154, 156, 157, 161, 312 Teboulle, M. 108 Thodberg, H. H. 226 Thomas, G. S. 9 Tian, Q. 192 Tibshirani, R. 7, 18, 23, 24, 34, 36, 37, 42, 43, 45–48, 50, 52, 58, 63, 68, 69, 71, 77, 79, 83, 86, 87, 113, 128, 130, 142, 151, 156, 157, 161,</p><h6 id="_163-170-175-176-182-184-195"><a class="header-anchor" href="#_163-170-175-176-182-184-195" aria-hidden="true">#</a> 163, 170, 175, 176, 182, 184, 195,</h6><h6 id="_206-208-217–219-221-222-225"><a class="header-anchor" href="#_206-208-217–219-221-222-225" aria-hidden="true">#</a> 206, 208, 217–219, 221, 222, 225,</h6><h6 id="_227-228-230-232-233-239"><a class="header-anchor" href="#_227-228-230-232-233-239" aria-hidden="true">#</a> 227, 228, 230, 232, 233, 239,</h6><h6 id="_248–250-255-257-260–263"><a class="header-anchor" href="#_248–250-255-257-260–263" aria-hidden="true">#</a> 248–250, 255, 257, 260–263</h6><div class="language-"><pre><code>Tibshirani 2 , R. 18, 19, 25, 79–83, 87,\n128, 130, 151, 156, 157, 161\nTran, T. 42\nTrendafilov, N. T. 204, 232, 233\nTritchler, D. 233\nTropp, J. A. 311\nTseng, P. 62, 110, 111\nTsybakov, A. 86, 180, 186, 195, 311\n</code></pre></div><div class="language-"><pre><code>Uddin, M. 204, 232\n</code></pre></div><div class="language-"><pre><code>van de Geer, S. 20, 60, 86, 87, 158,\n159, 162, 311\nvan Houwelingen, H. C. 43\nVandenberghe, L. 131, 206, 248, 262\nvan’t Veer, L. J. 43\nVapnik, V. 52\nVempala, S. 285\nVers ́elewel de Witt Hamer, P. 233\nVershynin, R. 285\nVert, J.-P. 66, 67, 87, 233\nViallon, V. 131\nVincent, P. 212\nVu, V. Q. 210, 213, 232\n</code></pre></div><div class="language-"><pre><code>Waaijenborg, S. 233\nWager, S. 161, 163\nWahba, G. 87\nWainwright, M. J. 75, 86, 87, 176,\n179, 186, 195, 213, 232, 252,\n261–263, 285, 311, 312\nWakin, M. 4, 285\nWakin, M. B. 285\nWang, H. 263\nWang, J. 127, 131, 132\nWang, L. 47\nWang, W. 312\nWard, R. 285\nWasserman, L. 70, 87, 90, 263\nWedderburn, R. 52\nWegkamp, M. 186, 311\nWeisenburger, D. 42\n</code></pre></div><h6 id="_342-author-index"><a class="header-anchor" href="#_342-author-index" aria-hidden="true">#</a> 342 AUTHOR INDEX</h6><p>Welsh, D. J. A. 261 Wermuth, N. 261 Wessels, L. F. A. 43 West, M. 262 Whittaker, J. 233, 261 Wiesel, A. 87 Willsky, A. S. 195, 260, 261, 285, 312 Wilson, W. 42 Winkler, G. 262 Witten, D. 195, 222, 225, 226, 228, 230, 232, 233, 239, 251, 262 Wonka, P. 127, 131, 132 Wright, J. 195 Wu, S. 248, 262 Wu, T. 52, 124</p><p>Xing, E. P. 312 Xu, H. 26, 195 Xue, L. 261, 266</p><p>Yang, I. 233 Yao, G. 262 Ye, J. 127, 131, 132</p><div class="language-"><pre><code>Yi, X. 195\nYu, B. 75, 86, 87, 252, 262, 263, 311,\n312\nYu, X. 42\nYuan, M. 20, 21, 59, 63, 86, 87, 186,\n262\nYuan, X. T. 195, 213, 232\nYudin, D. B. 176\n</code></pre></div><div class="language-"><pre><code>Zhang, C.-H. 84, 86, 158, 162\nZhang, H. H. 72–74, 87\nZhang, K. 160, 162\nZhang, S. 158, 162\nZhang, T. 86, 195, 213, 232\nZhang, Y. 311\nZhao, L. 160, 162\nZhao, P. 86, 263, 311\nZhao, Y. 262\nZhou, S. 263, 311\nZhu, J. 263\nZou, H. 18, 20, 21, 47, 56, 86, 87,\n206, 208, 232, 261, 266\nZwinderman, A. 233\n</code></pre></div><h2 id="index"><a class="header-anchor" href="#index" aria-hidden="true">#</a> Index</h2><p>ACS, <em>see</em> alternate convex search Adaptive hypothesis test, 157 Adaptive lasso, 86 Additive matrix decomposition, 190–194 model, 69–76 ADMM, 121 applied to lasso, 122 Aliased, 60 Alternate convex search, 126 Alternating algorithm, 205 direction method of multipliers see ADMM, 121 minimization, 124 partial optimum, 126 regression, 237 subspace algorithm, 126 Analysis of deviance, 33 ANOVA, 68 Applications 20-newsgroups corpus, 32 air pollution, 71 arterial pressure, 271 comparative genomic hybridiza- tion (CGH), 76 crime, 10 diabetes data, 140, 149, 159 face silhouettes, 226 handwritten digits, 37, 209 helicopter data, 184, 193 image processing, 271 lymphoma, 42, 219 mammal dentition, 232 natural images, 271 Netflix challenge, 170, 187, 215 splice-site detection, 60</p><div class="language-"><pre><code>video denoising, 184\nvoting, 244, 257\nAugmented SPCA algorithm, 213\nAutoencoder, 236\nsparse, 210, 236\nAuxiliary variables, 79\nAverage linkage, 227\n</code></pre></div><div class="language-"><pre><code>Backfitting, 69–72\nBase class, 36\nBaseline hazard, 43\nBasic inequality, 298, 313\nBasis functions, 71\nHaar, 270\nmultiscale, 271\northogonal, 269\novercomplete, 274\nBasis pursuit, 23, 276\nBayes\ndecision boundary, 217\nrule, 217\nBayesian, 23\nlasso, 139, 144\nmethods, 22, 139\nBellkor’s Pragmatic Chaos, 172\nBenjamini–Hochberg (BH) procedure,\n163\nBest-additive approximation, 69\nBest-subset selection, 22\nBet-on-sparsity principle, 24\nBias term (intercept), 7\nBias-variance tradeoff, 7\nBiclustering , 190\nBiconvex\nfunction, 124, 189, 207\nset, 125\nBiconvexity, 124\n</code></pre></div><div class="language-"><pre><code>343\n</code></pre></div><h6 id="_344-index"><a class="header-anchor" href="#_344-index" aria-hidden="true">#</a> 344 INDEX</h6><p>Binomial, 29 Binomial log-likelihood, 29 Biological pathway, 60, 64 Block separable, 63 Block-wise coordinate descent, 63, 65 Bonferroni adjustment, 160 Bootstrap, 12 methods, 142–147 nonparametric, 143 parametric, 146 Bottleneck, 211</p><p>Canonical correlation analysis, 214, 237 low-rank, 238 sparse, 213–215, 238 via optimal scoring, 237 Canonical variates sparse, 201 Cardinality constraint, 192 Categorical predictor, 19, 68 Cauchy-Schwarz inequality, 235 CCA, <em>see</em> Canonical correlation anal- ysis Chi-squared statistic, 148 Chronic lymphocytic lymphoma, 219 Cinematch score, 173 Clique-based factorization, 243 Clustering, 227 convex, 231 hierarchical , 227 sparse, 201, 227–232 Coefficient paths, 33 Coherence of a matrix, 177 Collaborative filtering, 169 Combinatorial optimization, 22 Compatibility function, 242 Complementary slackness, 98 Complete linkage, 227 Composite gradient, 63 Compressed sensing, 4, 278–288 <em>`</em> 2 -error bound, 296 noisy case, 296 Concentration matrix, 246, 261 Conditional</p><div class="language-"><pre><code>independence, 243\ninference, 254\nlikelihood, 254\nCone constraint\nlasso analysis, 294\nConstrained lasso, 276, 289\n` 2 -bound, 295\nConstraint region, 12\nContrasts, 60\nConvergence rate, 76\nConvex\nrelaxation, 23\nclustering, 231\nconstrained program, 95\nfunction, 95\nstrongly, 106\nmatrix approximation, 168\nmatrix completion\nnoisy setting, 178\nrelaxation, 248\nrelaxation of matrix rank, 174\nset, 95\nspectral regularization, 173\nConvexity, 14\nCoordinate descent, 14–17, 35, 40, 109\nblockwise, 63\nconvergence guarantee, 111\nfailure of, 110\nregularity condition, 112\nCorrelated\nfeatures, 55\ngenes, 60\nCorrupted matrix entries, 192\nCOSSO, 72\nCoupon collector problem, 177, 198\nCovariance graph, 263\nCovariance model\nspiked, 212\nCovariance test, 147–150\nstatistic, 149\nCovering set, 286\nCox proportional-hazards model, 31,\n42–43\nCross-validation, 13–14, 34, 43, 142,\n144\n</code></pre></div><h6 id="index-345"><a class="header-anchor" href="#index-345" aria-hidden="true">#</a> INDEX 345</h6><p>curve, 144 tenfold, 34 Cubic smoothing spline, 72–74 Cumulative distribution function, 152 Curse of dimensionality, 69 Cut set, 243 Cyclical coordinate descent, 15</p><p>Debiased Lasso, 158–160 Debiasing, 12 Decomposable regularizer, 311 Deep learning, 210 Degrees of freedom, 17–19 Deviance, 33, 51 Diffuse large B-cell lymphoma, 219 Directed acyclic graphs, 241 Discriminant analysis Fisher’s, 221 flexible, 222 penalized, 222 Document classification, 31, 32 Double exponential distribution, 140 Dual-path algorithm, 79–80 Dummy variables, 58, 60 Dynamic programming, 80–81</p><p>Effective degrees of freedom, <em>see</em> de- grees of freedom Effective number of parameters, <em>see</em> degrees of freedom Eigenvector computation, 127 Elastic net, 51, 55–58 ball, 57 coefficient path, 57 <em><code>_ 1 ball, 57 _</code></em> 1 exactness, 280–283, 287 equivalent to restricted nullspace, 281 sufficiency of pairwise incoher- ence, 282 sufficiency of RIP, 283 <em><code>_ 1 penalty, 30 _</code></em> 1 -regularized linear SVM, 31, 47 <em><code>_ 1 -regularized logistic regression, 50 _</code>q</em> penalty, 22</p><div class="language-"><pre><code>`q “ball”, 290, 313\nbest k -term approximation, 313\nweak and strong, 312\n` 2 ball, 57\nEM algorithm, 124\nEquivariant, 36\nExpectation-maximization algorithm,\nsee EM algorithm\nExponential\nfamily, 31, 246\nlimiting distribution, 149, 150\n</code></pre></div><div class="language-"><pre><code>Factor analysis, 191\nFalse discovery rate, 149\nFantope projection, 210\nFDR, see False discovery rate\nFeature vector, 7\nFirst-order optimality conditions, 96\nFisher’s\nbetween-to-within variance crite-\nrion, 221\nlinear discriminant analysis, 221\nFlexible discriminant analysis, 222\nFollicular lymphoma, 219\nForward stepwise\nmethods, 86\nregression, 118, 147, 158\nForwardStop rule, 163\nFraction of deviance explained, 33, 34\nFrobenius norm, 167\nFused lasso, 55, 76–81, 189\ndual path algorithm, 79–80\ndynamic programming, 80–81\nsignal approximator, 76\n</code></pre></div><div class="language-"><pre><code>Gamma distribution, 141\nGarrote, see nonnegative garrote\nGene-expression arrays, 60\nGeneral matrix regression framework,\n185\nGeneral position, 19\nGeneralization, 13\nGeneralized linear models, 29–54, 115\nGeneralized penalties, 55–93\nGenome-wide association studies, 32\n</code></pre></div><h6 id="_346-index"><a class="header-anchor" href="#_346-index" aria-hidden="true">#</a> 346 INDEX</h6><p>Geometric convergence, 107, 177 Gibbs sampler, 244 glmnet, 33, 35, 50–52 Gradient descent, 100 accelerated, 107 momentum, 108 projected, 102 proximal method, 103, 108 steepest, 101 unconstrained, 101 Gram matrix, 73 Graph clique, 241 maximal, 242 Graphical lasso, 248, 250 asymptotics, 252 Graphical model, 241–267 selection, 241 block-diagonal structure, 251 factorization property, 242–243 Gaussian, 245–246 graph selection, 254–260 hidden variables, 261 Markov property, 243 maximum likelihood, 247 mixed (continuous and discrete), 259 neighborhood-based likelihood, 255–258 pseudo-likelihood, 259–260 Group lasso, 55, 58–68, 260 ball, 64 overlap, 55, 65–68 sparse, 64–65 Grouped response, 37 Groups of variables, 55</p><p>Hammersley–Clifford theorem, 261 Hard sparsity, 290 Hard thresholding, 22 Hard-impute, 173 algorithm, 176 Hazard function, 31, 43 Hierarchical clustering, 227 sparse, 228 Hierarchy, 67, 68</p><div class="language-"><pre><code>Hilbert-space norm, 72\nHinge loss, 31\nHomotopy methods, 17\nHuber loss function, 194\nHuman DNA, 61\nHyperparameters, 141\n</code></pre></div><div class="language-"><pre><code>Implicit penalty, 66\nIncoherence, 178\nmaximal, 179\nIndicator response, 37\nInference for lasso, 154\nInner products, 151\nInteraction models, 67–68\nIRLS, see iteratively reweighted least-\nsquares\nIrrepresentability, 302\nIsing model, 244\nIsotonic regression, 83\nIterative Lanczos methods, 176\nIteratively reweighted least-squares,\n40\n</code></pre></div><div class="language-"><pre><code>Jensen’s algorithm, 124\nJohnson–Lindenstrauss approximation,\n277, 286\nsparse Boolean vectors, 277\n</code></pre></div><div class="language-"><pre><code>K-means clustering\nsparse, 230\nKarush–Kuhn–Tucker conditions, 9,\n97, 165\nKernel Gram matrix, 75\nKernel trick, 34, 46\nKKT conditions, see Karush–Kuhn–\nTucker conditions\nKnots, 71, 72, 82\nKullback–Leibler divergence, 41\n</code></pre></div><div class="language-"><pre><code>Lagrange\nfunction, 97\nmultipliers, 97\noptimality conditions, 97\nLagrange dual, 41\nLagrangian, 70\nduality, 9\n</code></pre></div><h6 id="index-347"><a class="header-anchor" href="#index-347" aria-hidden="true">#</a> INDEX 347</h6><p>form, 9 Lagrangian lasso, 289 <em><code>_ 2 -bound, 295 _</code></em> 2 -bound for weak sparsity, 299, 313 <em>`</em> ∞-bounds, 303 fast rate for prediction error, 300 slow rate for prediction error, 300 variable selection guarantee, 302 Lanczos iteration, 176 Laplacian distribution, 23, 140 prior, 139 Lasso, 7–12 fixed- <em>λ</em> inference, 154 necessary and sufficient condi- tions for solution, 9 uniqueness, 19 Least angle regression, 118–121, 147 Lifted problem, 79 Line search Armijo rule, 102 limited minimization, 102 Linear logistic regression, 29 discriminant analysis sparse, 201, 217–227 via optimal scoring, 225 model, 7–8 Linear convergence, 107 Link function, 29 Linkage measure for clustering, 227 Loading vectors, 204 Local minima, 16 Log-determinant program, 248 Log-linear model, 30, 40–42 Log-odds ratio, <em>see</em> logistic regression Logistic regression, 29, 31–36, 115, 217 coefficient path, 49 logit, 29 multiclass, 36 with separable data, 49–50 Loss parameter estimation, 290</p><div class="language-"><pre><code>prediction error, 289\nvariable selection, 290\nLow-rank CCA, 238\nLower bounds, 51\n</code></pre></div><div class="language-"><pre><code>Majorization, 123\nMajorization-minimization algorithm,\nsee MM algorithm\nMajorizing function, 123\nMargin, 31, 32, 46–48\nMarkov chain Monte Carlo, 140\nMarkov property, 241, 243\nMatrix Completion\ntheory, 177\nMatrix completion, 167, 169–183\nnuclear norm, 174\nrobust, 193\nMatrix decomposition\nadditive, 190\nMatrix decompositions, 167–199\nMatrix lasso, 186\nMatrix trace, 205\nMaximal variance, 202\nsparsity, 204\nMaximal-margin classifier, 48–49\nMaximum entropy, 53\nMaximum likelihood, 30\nMaximum Margin Matrix Factoriza-\ntion, 181\nMaximum margin matrix factoriza-\ntion, 168\nMCMC, see Markov chain Monte\nCarlo\nMDL, see minimum description length\nMean-squared-error consistency, 20\nMetric entropy, 286\nMill’s ratio, 164\nMinimax-optimal, 76\nMinimum description length, 226\nMinorization-majorization algorithm,\nsee MM algorithm\nMinorization-maximization algorithm,\nsee MM algorithm, see MM\nalgorithm\nMissing data, 169–183\n</code></pre></div><h6 id="_348-index"><a class="header-anchor" href="#_348-index" aria-hidden="true">#</a> 348 INDEX</h6><p>Mixed models, 259 MM algorithm, 123 EM as example, 124 proximal gradient as example, 124 MMMF, 181 relationship to spectral regular- ization, 182 Model selection, 8–14 Monotone, 83 fusion, 79 Movie ratings, 170 Multiclass logistic regression, 36–40 Multilevel factors, 60 Multinomial, 30 distribution, 36 grouped lasso, 39–40 regression, 54 Multitask learning, 51, 61, 184 Multivariate methods, 201–239 regression, 61, 184 Multivariate regression, 194 Mutual incoherence, 302 random designs, 314</p><p>Naive Bayes classifier, 218, 239 Nearest shrunken centroids, 218, 239 Nearly-isotonic regression, 83–84 Neighborhood based likelihood, 254 penalty, 77 set, 254 Nesterov’s method, 107–109, 176, 197 Netflix data, 176 Newton’s method, 101, 116 Newton–Raphson algorithm, 101 Node potentials, 259 Noisy subspace’ model, 191 Nonconvex penalties, 84–86 Nonnegative garrote, 20, 86 lasso, 74 Nonparametric bootstrap, 146</p><div class="language-"><pre><code>regression, 69\nNuclear norm, 174\nas an SDP, 197\nsubgradient, 197\nNull deviance, 51\nNull hypothesis\ncomplete, 157\nincremental, 157\n</code></pre></div><div class="language-"><pre><code>Offset, 40, 51\nOne versus all, 36\nOne versus one, 36\nOne-standard-error rule, 13, 144\nOptimal scoring, 225, 237\nOptimal separating hyperplane, 49\nOptimization, 95\nOrder statistics, 273\nOrthogonal\nbases, 17\nfeatures, 63\nOvA, see one versus all\nOvO, see one versus one\n</code></pre></div><div class="language-"><pre><code>Pairwise incoherence, 287\nPairwise plots, 144\nPairwise-Markov model, 245\nPAM package, 219\nParameter estimation loss, 290\nclassical linear model, 296\nParametric bootstrap, 146\nPartial likelihood, 43\nPartial optimum, 126\nPartial regression coefficient, 156\nPartial residual, 65, 69\nPath algorithm, 77, 118–121\nPathwise coordinate descent, 17, 249\nPCA, 169\nrobust, 192\nPDW method\nsee primal dual witness method,\n305\nPenalized discriminant analysis, 222\nPenalized Fisher’s discriminant, 239\nPenalized matrix decomposition, 187–\n190, 201\n</code></pre></div><h6 id="index-349"><a class="header-anchor" href="#index-349" aria-hidden="true">#</a> INDEX 349</h6><p>multifactor, 190 Penalized optimal scoring, 239 Poisson log-likelihood, 30 model, 40–42 Polyhedral constraint region, 188 Polyhedral lemma, 151, 152 Pool adjacent violators algorithm, 83 PoSI method, 160 Post-selection inference, 147–158 Posterior distribution, 22, 139, 140 mode, 140 Power method, 127, 190 Precision matrix, 246, 247 Prediction error computational lower bounds, 300, 312 Prediction loss, 289 Pretraining, 212 Prevalidation, 42, 45 Primal-dual witness method, 305 Principal components, 169, 202–204 higher ranks, 207 nonlinear, 210 robust, 192 sparse, 201, 204–210 Prior distribution, 139 Probabilistic graphical model, 241 Probe set, 173 Procrustes problem, 209 Projection, 71 Prototypes, 231 Proximal gradient descent momentum, 108 nuclear norm, 105 Proximal gradient method, 103 <em>`</em> 1 -norm, 104 as MM algorithm, 124 lasso, 107 Proximal map, 104 Pseudo-likelihood, 254</p><p>Quadratic program, 14 Qualitative factors, 58</p><div class="language-"><pre><code>Quantile-quantile plot, 148\n</code></pre></div><div class="language-"><pre><code>Random design matrix\nmutual incoherence, 314\nrestricted eigenvalue, 314\nRandom matrices, 283, 287\nRandom projection, 276\nRank-minimization problem, 170\nRank-r SVD, 169\nRecommender systems, 169\nReconstruction error, 203, 206, 234\nRecovery of matrix entries, 177\nReduced-Rank Regression, 184\nRegression, 7\nmultivariate, 194\nreduced rank, 184\nRegularization, 8\nRelaxed basis pursuit\nanalysis of, 313\nprogram, 276\nRelaxed lasso, 12\nRelevance network, 263\nReparametrization, 78\nReproducing-kernel Hilbert space, 72,\n73\nResampling, 142\nResidual sum of squares, 147\nResponse variable, 7\nRestricted eigenvalues, 294\nrandom designs, 314\nRestricted isometry property, 283,\n287\nRestricted nullspace\nimplied by pairwise incoherence,\n282\nimplied by RIP, 283, 288\nproperty, 281\nRestricted strong convexity, 294, 314\nRidge\npenalty, 57\nregression, 10, 34\nregularized logistic regression, 49\nRight censored, 42\nRIP, see restricted isometry property\n</code></pre></div><h6 id="_350-index"><a class="header-anchor" href="#_350-index" aria-hidden="true">#</a> 350 INDEX</h6><p>RKHS, <em>see</em> reproducing kernel Hilbert space Robust Huber loss, 194, 198 matrix completion, 193 PCA, 192, 193, 199 Rug plot, 144</p><p>Sample splitting, 148 SCoTLASS criterion, 235 procedure, 204 Screening rules, 35, 127 SDP, <em>see</em> semidefinite program, <em>see</em> semidefinite program Second-order cone, 75 Selection event, 150 Self-influence, 18 Semidefinite program, 174, 205, 206 Separability of penalty, 66, 77, 110 Separable data, 49, 53 Sequential control of FDR, 163 Shrinkage, 149 methods, 22 Signal approximation and compressed sensing, 269 Single linkage, 227 Singular value decomposition, 169 singular values, 169 singular vector, 126 singular vectors, 169 sparse, 201 Smoothing spline, 71–74 Soft margin, 31 Soft thresholding, 15, 189 operator, 58, 205 Soft-impute, 173, 176, 181 algorithm, 175 Spacing test, 156–157 Sparse additive model, 69–76 Sparse approximation best <em>k</em> -term, 273, 313 orthogonal bases, 271 overcomplete bases, 274 Sparse backfitting, 70, 73</p><div class="language-"><pre><code>Sparse canonical correlation analysis,\n238\nSparse clustering, 227–232\nhierarchical, 228\nK-means, 230\nSparse LDA, 222\nSparse matrix approximation, 168\nSparse plus low rank, 191, 261\nSparse principal components\nhigher ranks, 207\ntheory, 212\nSparsistency, 20, 301\nSparsity, 12\nSpectral regularization, 175\nSpiked covariance model, 212\nSpikiness ratio, 179\nSpline, 72–74\nSquared hinge loss, 48\nStability selection, 144\nStandardize, 8\nStatistical inference, 139–165\nStrictly convex, 57\nStrong convexity, 106, 292\nFisher information, 293\nHessian-based, 293\nStrong duality, 98\nStrong rules, 35, 130\nSubdifferential, 63, 99\nSubgradient, 15, 99, 305\n` 1 -norm, 100\nequations, 62, 64\nnuclear norm, 100\nSublinear convergence, 106\nSubset selection, 23\nSummation constraint, 68, 88\nSupport recovery, 290\nSupport set, 47\nSupport-vector machine, 31, 46–48\nSurvival\ntime, 42\ncurves, 43\nmodels, 31\nSVD, see Singular value decomposi-\ntion\nSVM, see support-vector machine\n</code></pre></div><h6 id="index-351"><a class="header-anchor" href="#index-351" aria-hidden="true">#</a> INDEX 351</h6><p>Tail bounds <em>χ</em>^2 variables, 286 Gaussian variables, 309 Theory, 289–314 <em>`</em> 2 -error bound for lasso, 294 basic inequality, 298, 312, 313 general <em>M</em> -estimators, 310 group lasso, 310 minimax rates for sparse regres- sion, 296, 299, 312 nuclear norm, 310 prediction error bound for lasso, 299 primal-dual witness method, 305 variable selection guarantee for lasso, 302 Total-variation denoising, 77 Trace norm, 174 Trace regression framework, 185 Training error, 34 Trend filtering, 81–83 Truncated normal distribution, 152 Type I error, 148</p><div class="language-"><pre><code>Upper bounds, 51\n</code></pre></div><div class="language-"><pre><code>Variable selection, 301\nirrepresentability condition, 302\nloss, 290\nmutual incoherence condition,\n302\nVarimax rotation, 236\nVertex set, 241\nVideo\ndenoising, 193\nsequences, 194\nsurveillance data, 193\n</code></pre></div><div class="language-"><pre><code>Warm starts, 36\nWavelets, 17\nWeak sparsity, 290, 299, 312\nWide data, 49\nWithin-class covariance matrix, 218\nWithin-group sparsity, 64\n</code></pre></div>',8058);n.render=function(i,r,n,s,t,d){return e(),a("div",null,[o])};export{r as __pageData,n as default};
